I0503 18:04:07.680566      19 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-963657699
I0503 18:04:07.680654      19 e2e.go:240] Starting e2e run "d7c46e91-6dcd-11e9-8617-baa18b820788" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1556906646 - Will randomize all specs
Will run 204 of 3584 specs

May  3 18:04:07.832: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:04:07.834: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May  3 18:04:07.876: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May  3 18:04:07.925: INFO: 23 / 23 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May  3 18:04:07.925: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
May  3 18:04:07.925: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May  3 18:04:07.940: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
May  3 18:04:07.940: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
May  3 18:04:07.940: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-kube-fluentd' (0 seconds elapsed)
May  3 18:04:07.940: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'ibm-master-proxy' (0 seconds elapsed)
May  3 18:04:07.940: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-driver-installer' (0 seconds elapsed)
May  3 18:04:07.940: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin' (0 seconds elapsed)
May  3 18:04:07.940: INFO: e2e test version: v1.14.1
May  3 18:04:07.942: INFO: kube-apiserver version: v1.14.1+IKS
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:04:07.942: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename statefulset
May  3 18:04:08.021: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
May  3 18:04:08.038: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6810
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6810
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May  3 18:04:08.182: INFO: Found 0 stateful pods, waiting for 3
May  3 18:04:18.191: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:04:18.191: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:04:18.191: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:04:18.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-6810 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:04:18.519: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:04:18.519: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:04:18.519: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May  3 18:04:28.646: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May  3 18:04:38.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-6810 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:04:39.014: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:04:39.014: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:04:39.014: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:04:59.060: INFO: Waiting for StatefulSet statefulset-6810/ss2 to complete update
STEP: Rolling back to a previous revision
May  3 18:05:09.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-6810 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:05:09.404: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:05:09.404: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:05:09.404: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:05:19.518: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May  3 18:05:19.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-6810 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:05:19.807: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:05:19.807: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:05:19.807: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  3 18:05:39.918: INFO: Deleting all statefulset in ns statefulset-6810
May  3 18:05:39.926: INFO: Scaling statefulset ss2 to 0
May  3 18:05:59.962: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:05:59.970: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:06:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6810" for this suite.
May  3 18:06:08.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:06:08.403: INFO: namespace statefulset-6810 deletion completed in 8.39068503s

â€¢ [SLOW TEST:120.461 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:06:08.407: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1131
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-1131
May  3 18:06:10.634: INFO: Started pod liveness-exec in namespace container-probe-1131
STEP: checking the pod's current state and verifying that restartCount is present
May  3 18:06:10.642: INFO: Initial restart count of pod liveness-exec is 0
May  3 18:06:58.978: INFO: Restart count of pod container-probe-1131/liveness-exec is now 1 (48.335579479s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:06:59.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1131" for this suite.
May  3 18:07:05.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:07:05.273: INFO: namespace container-probe-1131 deletion completed in 6.254217436s

â€¢ [SLOW TEST:56.867 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:07:05.275: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2005
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-42572a58-6dce-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:07:05.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788" in namespace "projected-2005" to be "success or failure"
May  3 18:07:05.550: INFO: Pod "pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.91185ms
May  3 18:07:07.560: INFO: Pod "pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01612019s
STEP: Saw pod success
May  3 18:07:07.560: INFO: Pod "pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:07:07.567: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:07:07.703: INFO: Waiting for pod pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:07:07.712: INFO: Pod pod-projected-configmaps-42585049-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:07:07.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2005" for this suite.
May  3 18:07:13.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:07:13.960: INFO: namespace projected-2005 deletion completed in 6.23930068s

â€¢ [SLOW TEST:8.685 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:07:13.961: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:07:14.235: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788" in namespace "projected-2173" to be "success or failure"
May  3 18:07:14.244: INFO: Pod "downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.610692ms
May  3 18:07:16.253: INFO: Pod "downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.01799062s
May  3 18:07:18.262: INFO: Pod "downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026563501s
STEP: Saw pod success
May  3 18:07:18.262: INFO: Pod "downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:07:18.269: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:07:18.312: INFO: Waiting for pod downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:07:18.319: INFO: Pod downwardapi-volume-47864425-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:07:18.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2173" for this suite.
May  3 18:07:24.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:07:24.623: INFO: namespace projected-2173 deletion completed in 6.295371836s

â€¢ [SLOW TEST:10.662 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:07:24.623: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-3491
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-cn7k
STEP: Creating a pod to test atomic-volume-subpath
May  3 18:07:24.844: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-cn7k" in namespace "subpath-3491" to be "success or failure"
May  3 18:07:24.858: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Pending", Reason="", readiness=false. Elapsed: 13.282976ms
May  3 18:07:26.866: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 2.021788745s
May  3 18:07:28.875: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 4.030146315s
May  3 18:07:30.883: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 6.038493336s
May  3 18:07:32.892: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 8.047523161s
May  3 18:07:34.900: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 10.055579979s
May  3 18:07:36.909: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 12.064546406s
May  3 18:07:38.918: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 14.073037244s
May  3 18:07:40.925: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 16.080723418s
May  3 18:07:42.934: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 18.089217414s
May  3 18:07:44.941: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 20.096988144s
May  3 18:07:46.950: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Running", Reason="", readiness=true. Elapsed: 22.105541228s
May  3 18:07:48.958: INFO: Pod "pod-subpath-test-projected-cn7k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.113620922s
STEP: Saw pod success
May  3 18:07:48.958: INFO: Pod "pod-subpath-test-projected-cn7k" satisfied condition "success or failure"
May  3 18:07:48.968: INFO: Trying to get logs from node 10.190.144.133 pod pod-subpath-test-projected-cn7k container test-container-subpath-projected-cn7k: <nil>
STEP: delete the pod
May  3 18:07:49.099: INFO: Waiting for pod pod-subpath-test-projected-cn7k to disappear
May  3 18:07:49.105: INFO: Pod pod-subpath-test-projected-cn7k no longer exists
STEP: Deleting pod pod-subpath-test-projected-cn7k
May  3 18:07:49.105: INFO: Deleting pod "pod-subpath-test-projected-cn7k" in namespace "subpath-3491"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:07:49.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3491" for this suite.
May  3 18:07:55.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:07:55.537: INFO: namespace subpath-3491 deletion completed in 6.41380699s

â€¢ [SLOW TEST:30.914 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:07:55.538: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3914
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May  3 18:07:55.743: INFO: Waiting up to 5m0s for pod "var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788" in namespace "var-expansion-3914" to be "success or failure"
May  3 18:07:55.750: INFO: Pod "var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.928213ms
May  3 18:07:57.758: INFO: Pod "var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01440903s
STEP: Saw pod success
May  3 18:07:57.758: INFO: Pod "var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:07:57.765: INFO: Trying to get logs from node 10.190.144.140 pod var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 18:07:57.808: INFO: Waiting for pod var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:07:57.815: INFO: Pod var-expansion-6043f5dd-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:07:57.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3914" for this suite.
May  3 18:08:03.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:08:04.129: INFO: namespace var-expansion-3914 deletion completed in 6.305105579s

â€¢ [SLOW TEST:8.591 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:08:04.129: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6924
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:08:04.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788" in namespace "projected-6924" to be "success or failure"
May  3 18:08:04.344: INFO: Pod "downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.470979ms
May  3 18:08:06.353: INFO: Pod "downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01854724s
May  3 18:08:08.363: INFO: Pod "downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028627782s
STEP: Saw pod success
May  3 18:08:08.363: INFO: Pod "downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:08:08.370: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:08:08.446: INFO: Waiting for pod downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:08:08.452: INFO: Pod downwardapi-volume-65626fea-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:08:08.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6924" for this suite.
May  3 18:08:14.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:08:14.953: INFO: namespace projected-6924 deletion completed in 6.43472009s

â€¢ [SLOW TEST:10.824 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:08:14.953: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May  3 18:08:15.220: INFO: namespace kubectl-3384
May  3 18:08:15.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3384'
May  3 18:08:15.557: INFO: stderr: ""
May  3 18:08:15.557: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  3 18:08:16.565: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:08:16.565: INFO: Found 0 / 1
May  3 18:08:17.566: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:08:17.566: INFO: Found 1 / 1
May  3 18:08:17.566: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  3 18:08:17.573: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:08:17.573: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  3 18:08:17.573: INFO: wait on redis-master startup in kubectl-3384 
May  3 18:08:17.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 logs redis-master-fh5l8 redis-master --namespace=kubectl-3384'
May  3 18:08:17.781: INFO: stderr: ""
May  3 18:08:17.781: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 May 18:08:16.858 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 May 18:08:16.858 # Server started, Redis version 3.2.12\n1:M 03 May 18:08:16.858 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 May 18:08:16.858 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May  3 18:08:17.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3384'
May  3 18:08:17.916: INFO: stderr: ""
May  3 18:08:17.917: INFO: stdout: "service/rm2 exposed\n"
May  3 18:08:17.924: INFO: Service rm2 in namespace kubectl-3384 found.
STEP: exposing service
May  3 18:08:19.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3384'
May  3 18:08:20.153: INFO: stderr: ""
May  3 18:08:20.153: INFO: stdout: "service/rm3 exposed\n"
May  3 18:08:20.162: INFO: Service rm3 in namespace kubectl-3384 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:08:22.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3384" for this suite.
May  3 18:08:46.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:08:46.471: INFO: namespace kubectl-3384 deletion completed in 24.285454672s

â€¢ [SLOW TEST:31.518 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:08:46.472: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4967
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-7ea953e7-6dce-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:08:46.745: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788" in namespace "projected-4967" to be "success or failure"
May  3 18:08:46.752: INFO: Pod "pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.263271ms
May  3 18:08:48.761: INFO: Pod "pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01620998s
STEP: Saw pod success
May  3 18:08:48.761: INFO: Pod "pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:08:48.769: INFO: Trying to get logs from node 10.190.144.141 pod pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:08:48.864: INFO: Waiting for pod pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:08:48.872: INFO: Pod pod-projected-configmaps-7eaa7f7e-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:08:48.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4967" for this suite.
May  3 18:08:54.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:08:55.131: INFO: namespace projected-4967 deletion completed in 6.250973969s

â€¢ [SLOW TEST:8.660 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:08:55.132: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7318
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May  3 18:08:55.331: INFO: Waiting up to 5m0s for pod "pod-83c87b3d-6dce-11e9-8617-baa18b820788" in namespace "emptydir-7318" to be "success or failure"
May  3 18:08:55.337: INFO: Pod "pod-83c87b3d-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.541309ms
May  3 18:08:57.348: INFO: Pod "pod-83c87b3d-6dce-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.01727819s
May  3 18:08:59.359: INFO: Pod "pod-83c87b3d-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028689211s
STEP: Saw pod success
May  3 18:08:59.359: INFO: Pod "pod-83c87b3d-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:08:59.366: INFO: Trying to get logs from node 10.190.144.140 pod pod-83c87b3d-6dce-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 18:08:59.409: INFO: Waiting for pod pod-83c87b3d-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:08:59.416: INFO: Pod pod-83c87b3d-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:08:59.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7318" for this suite.
May  3 18:09:05.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:09:05.847: INFO: namespace emptydir-7318 deletion completed in 6.423088342s

â€¢ [SLOW TEST:10.716 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:09:05.847: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4712
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 18:09:06.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4712'
May  3 18:09:06.235: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  3 18:09:06.235: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May  3 18:09:06.247: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May  3 18:09:06.300: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May  3 18:09:06.311: INFO: scanned /root for discovery docs: <nil>
May  3 18:09:06.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4712'
May  3 18:09:22.270: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  3 18:09:22.270: INFO: stdout: "Created e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97\nScaling up e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May  3 18:09:22.270: INFO: stdout: "Created e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97\nScaling up e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May  3 18:09:22.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4712'
May  3 18:09:22.389: INFO: stderr: ""
May  3 18:09:22.389: INFO: stdout: "e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97-2fzm2 "
May  3 18:09:22.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97-2fzm2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4712'
May  3 18:09:22.486: INFO: stderr: ""
May  3 18:09:22.486: INFO: stdout: "true"
May  3 18:09:22.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97-2fzm2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4712'
May  3 18:09:22.589: INFO: stderr: ""
May  3 18:09:22.589: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May  3 18:09:22.589: INFO: e2e-test-nginx-rc-79776c5a93065bc62cb99396af451d97-2fzm2 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May  3 18:09:22.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete rc e2e-test-nginx-rc --namespace=kubectl-4712'
May  3 18:09:22.702: INFO: stderr: ""
May  3 18:09:22.702: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:09:22.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4712" for this suite.
May  3 18:09:28.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:09:28.967: INFO: namespace kubectl-4712 deletion completed in 6.25593956s

â€¢ [SLOW TEST:23.120 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:09:28.968: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May  3 18:09:31.829: INFO: Successfully updated pod "annotationupdate97f5e075-6dce-11e9-8617-baa18b820788"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:09:35.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9139" for this suite.
May  3 18:09:57.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:09:58.144: INFO: namespace downward-api-9139 deletion completed in 22.25692152s

â€¢ [SLOW TEST:29.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:09:58.145: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5559
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May  3 18:09:58.974: INFO: created pod pod-service-account-defaultsa
May  3 18:09:58.975: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May  3 18:09:58.985: INFO: created pod pod-service-account-mountsa
May  3 18:09:58.985: INFO: pod pod-service-account-mountsa service account token volume mount: true
May  3 18:09:58.994: INFO: created pod pod-service-account-nomountsa
May  3 18:09:58.994: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May  3 18:09:59.005: INFO: created pod pod-service-account-defaultsa-mountspec
May  3 18:09:59.005: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May  3 18:09:59.018: INFO: created pod pod-service-account-mountsa-mountspec
May  3 18:09:59.018: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May  3 18:09:59.026: INFO: created pod pod-service-account-nomountsa-mountspec
May  3 18:09:59.026: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May  3 18:09:59.037: INFO: created pod pod-service-account-defaultsa-nomountspec
May  3 18:09:59.037: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May  3 18:09:59.050: INFO: created pod pod-service-account-mountsa-nomountspec
May  3 18:09:59.050: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May  3 18:09:59.060: INFO: created pod pod-service-account-nomountsa-nomountspec
May  3 18:09:59.060: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:09:59.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5559" for this suite.
May  3 18:10:05.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:10:05.317: INFO: namespace svcaccounts-5559 deletion completed in 6.246359983s

â€¢ [SLOW TEST:7.173 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:10:05.319: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-934
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ad9f18e0-6dce-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:10:05.530: INFO: Waiting up to 5m0s for pod "pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788" in namespace "secrets-934" to be "success or failure"
May  3 18:10:05.538: INFO: Pod "pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604865ms
May  3 18:10:07.547: INFO: Pod "pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016453373s
May  3 18:10:09.556: INFO: Pod "pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026309425s
STEP: Saw pod success
May  3 18:10:09.556: INFO: Pod "pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:10:09.563: INFO: Trying to get logs from node 10.190.144.133 pod pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 18:10:09.604: INFO: Waiting for pod pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:10:09.618: INFO: Pod pod-secrets-ada031bb-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:10:09.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-934" for this suite.
May  3 18:10:15.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:10:15.893: INFO: namespace secrets-934 deletion completed in 6.266413536s

â€¢ [SLOW TEST:10.574 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:10:15.893: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9855
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:10:16.101: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788" in namespace "projected-9855" to be "success or failure"
May  3 18:10:16.113: INFO: Pod "downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 11.860283ms
May  3 18:10:18.122: INFO: Pod "downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020874601s
STEP: Saw pod success
May  3 18:10:18.122: INFO: Pod "downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:10:18.129: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:10:18.177: INFO: Waiting for pod downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788 to disappear
May  3 18:10:18.184: INFO: Pod downwardapi-volume-b3ecae5d-6dce-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:10:18.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9855" for this suite.
May  3 18:10:24.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:10:24.434: INFO: namespace projected-9855 deletion completed in 6.23999582s

â€¢ [SLOW TEST:8.541 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:10:24.441: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4918
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May  3 18:10:24.633: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:10:32.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4918" for this suite.
May  3 18:10:38.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:10:38.470: INFO: namespace pods-4918 deletion completed in 6.331421688s

â€¢ [SLOW TEST:14.029 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:10:38.470: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7408
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7408
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  3 18:10:38.657: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  3 18:10:58.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.47.134:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:10:58.927: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:10:59.116: INFO: Found all expected endpoints: [netserver-0]
May  3 18:10:59.125: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.112.75:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:10:59.125: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:10:59.289: INFO: Found all expected endpoints: [netserver-1]
May  3 18:10:59.297: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.111.245:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7408 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:10:59.297: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:10:59.500: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:10:59.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7408" for this suite.
May  3 18:11:11.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:11:11.985: INFO: namespace pod-network-test-7408 deletion completed in 12.476784525s

â€¢ [SLOW TEST:33.514 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:11:11.985: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-8367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  3 18:11:16.366: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:16.374: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:18.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:18.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:20.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:20.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:22.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:22.383: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:24.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:24.383: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:26.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:26.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:28.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:28.383: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:30.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:30.383: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:32.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:32.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:34.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:34.384: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:36.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:36.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:38.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:38.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:40.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:40.382: INFO: Pod pod-with-poststart-exec-hook still exists
May  3 18:11:42.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May  3 18:11:42.385: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:11:42.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8367" for this suite.
May  3 18:12:06.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:12:06.842: INFO: namespace container-lifecycle-hook-8367 deletion completed in 24.446909662s

â€¢ [SLOW TEST:54.857 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:12:06.844: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7610
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7610
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7610
May  3 18:12:07.073: INFO: Found 0 stateful pods, waiting for 1
May  3 18:12:17.083: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May  3 18:12:17.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:12:17.417: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:12:17.417: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:12:17.417: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:12:17.425: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  3 18:12:27.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:12:27.434: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:12:27.469: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:12:27.469: INFO: ss-0  10.190.144.140  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  }]
May  3 18:12:27.469: INFO: 
May  3 18:12:27.469: INFO: StatefulSet ss has not reached scale 3, at 1
May  3 18:12:28.478: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991256286s
May  3 18:12:29.486: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982599623s
May  3 18:12:30.495: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974251075s
May  3 18:12:31.503: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965730486s
May  3 18:12:32.511: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.957315918s
May  3 18:12:33.520: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.94924445s
May  3 18:12:34.529: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940879058s
May  3 18:12:35.540: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.931738161s
May  3 18:12:36.548: INFO: Verifying statefulset ss doesn't scale past 3 for another 921.007086ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7610
May  3 18:12:37.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:12:37.908: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:12:37.908: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:12:37.908: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:12:37.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:12:38.210: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  3 18:12:38.210: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:12:38.210: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:12:38.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:12:38.495: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May  3 18:12:38.495: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:12:38.495: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:12:38.504: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May  3 18:12:48.514: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:12:48.514: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:12:48.514: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May  3 18:12:48.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:12:48.793: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:12:48.793: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:12:48.793: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:12:48.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:12:49.094: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:12:49.094: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:12:49.094: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:12:49.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:12:49.447: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:12:49.447: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:12:49.447: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:12:49.447: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:12:49.546: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  3 18:12:59.563: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:12:59.563: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:12:59.563: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:12:59.627: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:12:59.627: INFO: ss-0  10.190.144.140  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  }]
May  3 18:12:59.627: INFO: ss-1  10.190.144.133  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:12:59.627: INFO: ss-2  10.190.144.141  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:12:59.627: INFO: 
May  3 18:12:59.627: INFO: StatefulSet ss has not reached scale 0, at 3
May  3 18:13:00.636: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:00.636: INFO: ss-0  10.190.144.140  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  }]
May  3 18:13:00.636: INFO: ss-1  10.190.144.133  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:00.636: INFO: ss-2  10.190.144.141  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:00.636: INFO: 
May  3 18:13:00.636: INFO: StatefulSet ss has not reached scale 0, at 3
May  3 18:13:01.644: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:01.644: INFO: ss-0  10.190.144.140  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:07 +0000 UTC  }]
May  3 18:13:01.644: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:01.644: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:01.644: INFO: 
May  3 18:13:01.644: INFO: StatefulSet ss has not reached scale 0, at 3
May  3 18:13:02.659: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:02.659: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:02.659: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:02.659: INFO: 
May  3 18:13:02.659: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:03.669: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:03.669: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:03.669: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:03.669: INFO: 
May  3 18:13:03.669: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:04.678: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:04.678: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:04.679: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:04.679: INFO: 
May  3 18:13:04.679: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:05.687: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:05.687: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:05.687: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:05.687: INFO: 
May  3 18:13:05.687: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:06.695: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:06.695: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:06.695: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:06.695: INFO: 
May  3 18:13:06.695: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:07.703: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:07.703: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:07.703: INFO: ss-2  10.190.144.141  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:07.703: INFO: 
May  3 18:13:07.703: INFO: StatefulSet ss has not reached scale 0, at 2
May  3 18:13:08.713: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
May  3 18:13:08.713: INFO: ss-1  10.190.144.133  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:12:27 +0000 UTC  }]
May  3 18:13:08.713: INFO: 
May  3 18:13:08.713: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7610
May  3 18:13:09.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:13:09.909: INFO: rc: 1
May  3 18:13:09.909: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0019477a0 exit status 1 <nil> <nil> true [0xc0000cdd88 0xc0000cde40 0xc0000cdec8] [0xc0000cdd88 0xc0000cde40 0xc0000cdec8] [0xc0000cdde0 0xc0000cdea8] [0x9bf9f0 0x9bf9f0] 0xc002545aa0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May  3 18:13:19.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:13:20.001: INFO: rc: 1
May  3 18:13:20.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0014a6390 exit status 1 <nil> <nil> true [0xc00315a098 0xc00315a0b0 0xc00315a0c8] [0xc00315a098 0xc00315a0b0 0xc00315a0c8] [0xc00315a0a8 0xc00315a0c0] [0x9bf9f0 0x9bf9f0] 0xc002e28000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:13:30.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:13:30.095: INFO: rc: 1
May  3 18:13:30.095: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001947b00 exit status 1 <nil> <nil> true [0xc0000cdee0 0xc0000cdf48 0xc0000cdf90] [0xc0000cdee0 0xc0000cdf48 0xc0000cdf90] [0xc0000cdf18 0xc0000cdf70] [0x9bf9f0 0x9bf9f0] 0xc002545e00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:13:40.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:13:40.252: INFO: rc: 1
May  3 18:13:40.252: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001947e90 exit status 1 <nil> <nil> true [0xc0000cdfb0 0xc002e06008 0xc002e06020] [0xc0000cdfb0 0xc002e06008 0xc002e06020] [0xc002e06000 0xc002e06018] [0x9bf9f0 0x9bf9f0] 0xc0033a6180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:13:50.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:13:50.428: INFO: rc: 1
May  3 18:13:50.429: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001df3f80 exit status 1 <nil> <nil> true [0xc001646450 0xc001646488 0xc0016464c8] [0xc001646450 0xc001646488 0xc0016464c8] [0xc001646460 0xc0016464b8] [0x9bf9f0 0x9bf9f0] 0xc002384300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:00.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:00.523: INFO: rc: 1
May  3 18:14:00.523: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00134c300 exit status 1 <nil> <nil> true [0xc001d16810 0xc001d16888 0xc001d168b0] [0xc001d16810 0xc001d16888 0xc001d168b0] [0xc001d16878 0xc001d168a0] [0x9bf9f0 0x9bf9f0] 0xc002ecdc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:10.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:10.625: INFO: rc: 1
May  3 18:14:10.625: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001260210 exit status 1 <nil> <nil> true [0xc002e06028 0xc002e06040 0xc002e06058] [0xc002e06028 0xc002e06040 0xc002e06058] [0xc002e06038 0xc002e06050] [0x9bf9f0 0x9bf9f0] 0xc0033a64e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:20.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:20.719: INFO: rc: 1
May  3 18:14:20.719: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001564330 exit status 1 <nil> <nil> true [0xc00050a600 0xc00050aa08 0xc00050ab18] [0xc00050a600 0xc00050aa08 0xc00050ab18] [0xc00050a9d8 0xc00050aa78] [0x9bf9f0 0x9bf9f0] 0xc002e46540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:30.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:30.828: INFO: rc: 1
May  3 18:14:30.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001564720 exit status 1 <nil> <nil> true [0xc00050ab28 0xc00050abe0 0xc00050aef8] [0xc00050ab28 0xc00050abe0 0xc00050aef8] [0xc00050abb8 0xc00050ac68] [0x9bf9f0 0x9bf9f0] 0xc002e46ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:40.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:40.929: INFO: rc: 1
May  3 18:14:40.929: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083c330 exit status 1 <nil> <nil> true [0xc0000cc3e8 0xc0000cc8f0 0xc0000ccaa0] [0xc0000cc3e8 0xc0000cc8f0 0xc0000ccaa0] [0xc0000cc880 0xc0000cca20] [0x9bf9f0 0x9bf9f0] 0xc002544360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:14:50.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:14:51.049: INFO: rc: 1
May  3 18:14:51.049: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083c6c0 exit status 1 <nil> <nil> true [0xc0000ccae0 0xc0000ccc18 0xc0000ccd70] [0xc0000ccae0 0xc0000ccc18 0xc0000ccd70] [0xc0000ccba8 0xc0000ccd50] [0x9bf9f0 0x9bf9f0] 0xc002544780 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:01.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:01.228: INFO: rc: 1
May  3 18:15:01.229: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001564ae0 exit status 1 <nil> <nil> true [0xc00050af80 0xc00050b168 0xc00050b298] [0xc00050af80 0xc00050b168 0xc00050b298] [0xc00050b128 0xc00050b250] [0x9bf9f0 0x9bf9f0] 0xc002e47020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:11.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:11.356: INFO: rc: 1
May  3 18:15:11.356: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001564ed0 exit status 1 <nil> <nil> true [0xc00050b400 0xc00050b6e0 0xc00050b730] [0xc00050b400 0xc00050b6e0 0xc00050b730] [0xc00050b650 0xc00050b710] [0x9bf9f0 0x9bf9f0] 0xc002e47380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:21.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:21.447: INFO: rc: 1
May  3 18:15:21.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019464b0 exit status 1 <nil> <nil> true [0xc000725810 0xc000725c08 0xc000725e00] [0xc000725810 0xc000725c08 0xc000725e00] [0xc000725b20 0xc000725dd8] [0x9bf9f0 0x9bf9f0] 0xc00239e2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:31.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:31.540: INFO: rc: 1
May  3 18:15:31.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fae3c0 exit status 1 <nil> <nil> true [0xc001d16008 0xc001d16040 0xc001d16078] [0xc001d16008 0xc001d16040 0xc001d16078] [0xc001d16020 0xc001d16068] [0x9bf9f0 0x9bf9f0] 0xc002ecc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:41.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:41.639: INFO: rc: 1
May  3 18:15:41.639: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001946870 exit status 1 <nil> <nil> true [0xc000725ec8 0xc002e06008 0xc002e06020] [0xc000725ec8 0xc002e06008 0xc002e06020] [0xc002e06000 0xc002e06018] [0x9bf9f0 0x9bf9f0] 0xc00239e600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:15:51.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:15:51.732: INFO: rc: 1
May  3 18:15:51.732: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001565410 exit status 1 <nil> <nil> true [0xc00050b7c0 0xc00050b988 0xc00050bcf0] [0xc00050b7c0 0xc00050b988 0xc00050bcf0] [0xc00050b950 0xc00050bc80] [0x9bf9f0 0x9bf9f0] 0xc002e478c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:01.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:01.825: INFO: rc: 1
May  3 18:16:01.825: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001946c60 exit status 1 <nil> <nil> true [0xc002e06028 0xc002e06040 0xc002e06058] [0xc002e06028 0xc002e06040 0xc002e06058] [0xc002e06038 0xc002e06050] [0x9bf9f0 0x9bf9f0] 0xc00239e960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:11.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:11.927: INFO: rc: 1
May  3 18:16:11.927: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001947050 exit status 1 <nil> <nil> true [0xc002e06060 0xc002e06078 0xc002e06090] [0xc002e06060 0xc002e06078 0xc002e06090] [0xc002e06070 0xc002e06088] [0x9bf9f0 0x9bf9f0] 0xc00239eea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:21.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:22.015: INFO: rc: 1
May  3 18:16:22.015: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001fae330 exit status 1 <nil> <nil> true [0xc000725848 0xc000725ca8 0xc000725ec8] [0xc000725848 0xc000725ca8 0xc000725ec8] [0xc000725c08 0xc000725e00] [0x9bf9f0 0x9bf9f0] 0xc002ecc2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:32.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:32.118: INFO: rc: 1
May  3 18:16:32.118: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001946510 exit status 1 <nil> <nil> true [0xc001d16008 0xc001d16040 0xc001d16078] [0xc001d16008 0xc001d16040 0xc001d16078] [0xc001d16020 0xc001d16068] [0x9bf9f0 0x9bf9f0] 0xc00239e2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:42.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:42.228: INFO: rc: 1
May  3 18:16:42.228: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083c360 exit status 1 <nil> <nil> true [0xc002e06000 0xc002e06018 0xc002e06030] [0xc002e06000 0xc002e06018 0xc002e06030] [0xc002e06010 0xc002e06028] [0x9bf9f0 0x9bf9f0] 0xc002e46540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:16:52.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:16:52.354: INFO: rc: 1
May  3 18:16:52.354: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083c720 exit status 1 <nil> <nil> true [0xc002e06038 0xc002e06050 0xc002e06068] [0xc002e06038 0xc002e06050 0xc002e06068] [0xc002e06048 0xc002e06060] [0x9bf9f0 0x9bf9f0] 0xc002e46ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:02.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:02.529: INFO: rc: 1
May  3 18:17:02.529: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083ca80 exit status 1 <nil> <nil> true [0xc002e06070 0xc002e06088 0xc002e060a0] [0xc002e06070 0xc002e06088 0xc002e060a0] [0xc002e06080 0xc002e06098] [0x9bf9f0 0x9bf9f0] 0xc002e47020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:12.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:12.652: INFO: rc: 1
May  3 18:17:12.652: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083ce10 exit status 1 <nil> <nil> true [0xc002e060a8 0xc002e060c0 0xc002e060d8] [0xc002e060a8 0xc002e060c0 0xc002e060d8] [0xc002e060b8 0xc002e060d0] [0x9bf9f0 0x9bf9f0] 0xc002e47380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:22.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:22.753: INFO: rc: 1
May  3 18:17:22.753: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083d170 exit status 1 <nil> <nil> true [0xc002e060e0 0xc002e060f8 0xc002e06110] [0xc002e060e0 0xc002e060f8 0xc002e06110] [0xc002e060f0 0xc002e06108] [0x9bf9f0 0x9bf9f0] 0xc002e478c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:32.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:32.855: INFO: rc: 1
May  3 18:17:32.855: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc00083d4d0 exit status 1 <nil> <nil> true [0xc002e06118 0xc002e06130 0xc002e06148] [0xc002e06118 0xc002e06130 0xc002e06148] [0xc002e06128 0xc002e06140] [0x9bf9f0 0x9bf9f0] 0xc002e47d40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:42.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:42.960: INFO: rc: 1
May  3 18:17:42.961: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019468a0 exit status 1 <nil> <nil> true [0xc001d160a8 0xc001d16138 0xc001d16188] [0xc001d160a8 0xc001d16138 0xc001d16188] [0xc001d16120 0xc001d16150] [0x9bf9f0 0x9bf9f0] 0xc00239e600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:17:52.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:17:53.119: INFO: rc: 1
May  3 18:17:53.120: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001946cf0 exit status 1 <nil> <nil> true [0xc001d161b0 0xc001d16218 0xc001d162c0] [0xc001d161b0 0xc001d16218 0xc001d162c0] [0xc001d16210 0xc001d16290] [0x9bf9f0 0x9bf9f0] 0xc00239e960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:18:03.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:18:03.227: INFO: rc: 1
May  3 18:18:03.227: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0019470e0 exit status 1 <nil> <nil> true [0xc001d162d8 0xc001d16358 0xc001d163e8] [0xc001d162d8 0xc001d16358 0xc001d163e8] [0xc001d16340 0xc001d163d8] [0x9bf9f0 0x9bf9f0] 0xc00239eea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May  3 18:18:13.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-7610 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:18:13.319: INFO: rc: 1
May  3 18:18:13.319: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
May  3 18:18:13.319: INFO: Scaling statefulset ss to 0
May  3 18:18:13.433: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  3 18:18:13.439: INFO: Deleting all statefulset in ns statefulset-7610
May  3 18:18:13.446: INFO: Scaling statefulset ss to 0
May  3 18:18:13.468: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:18:13.475: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:18:13.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7610" for this suite.
May  3 18:18:19.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:18:19.819: INFO: namespace statefulset-7610 deletion completed in 6.30396723s

â€¢ [SLOW TEST:372.976 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:18:19.820: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1167
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May  3 18:18:20.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-1167'
May  3 18:18:20.306: INFO: stderr: ""
May  3 18:18:20.306: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 18:18:20.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1167'
May  3 18:18:20.417: INFO: stderr: ""
May  3 18:18:20.417: INFO: stdout: "update-demo-nautilus-4hphg update-demo-nautilus-kbvxf "
May  3 18:18:20.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-4hphg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1167'
May  3 18:18:20.530: INFO: stderr: ""
May  3 18:18:20.530: INFO: stdout: ""
May  3 18:18:20.530: INFO: update-demo-nautilus-4hphg is created but not running
May  3 18:18:25.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1167'
May  3 18:18:25.632: INFO: stderr: ""
May  3 18:18:25.632: INFO: stdout: "update-demo-nautilus-4hphg update-demo-nautilus-kbvxf "
May  3 18:18:25.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-4hphg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1167'
May  3 18:18:25.730: INFO: stderr: ""
May  3 18:18:25.730: INFO: stdout: "true"
May  3 18:18:25.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-4hphg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1167'
May  3 18:18:25.834: INFO: stderr: ""
May  3 18:18:25.834: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:18:25.834: INFO: validating pod update-demo-nautilus-4hphg
May  3 18:18:25.918: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:18:25.918: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:18:25.918: INFO: update-demo-nautilus-4hphg is verified up and running
May  3 18:18:25.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-kbvxf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1167'
May  3 18:18:26.025: INFO: stderr: ""
May  3 18:18:26.025: INFO: stdout: "true"
May  3 18:18:26.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-kbvxf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1167'
May  3 18:18:26.124: INFO: stderr: ""
May  3 18:18:26.124: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:18:26.124: INFO: validating pod update-demo-nautilus-kbvxf
May  3 18:18:26.140: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:18:26.140: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:18:26.140: INFO: update-demo-nautilus-kbvxf is verified up and running
STEP: using delete to clean up resources
May  3 18:18:26.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-1167'
May  3 18:18:26.259: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:18:26.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  3 18:18:26.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1167'
May  3 18:18:26.367: INFO: stderr: "No resources found.\n"
May  3 18:18:26.367: INFO: stdout: ""
May  3 18:18:26.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -l name=update-demo --namespace=kubectl-1167 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  3 18:18:26.481: INFO: stderr: ""
May  3 18:18:26.481: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:18:26.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1167" for this suite.
May  3 18:18:50.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:18:50.744: INFO: namespace kubectl-1167 deletion completed in 24.253030216s

â€¢ [SLOW TEST:30.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:18:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  3 18:18:50.997: INFO: Number of nodes with available pods: 0
May  3 18:18:50.997: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:18:52.014: INFO: Number of nodes with available pods: 0
May  3 18:18:52.014: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:18:53.014: INFO: Number of nodes with available pods: 3
May  3 18:18:53.014: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May  3 18:18:53.058: INFO: Number of nodes with available pods: 2
May  3 18:18:53.058: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:54.075: INFO: Number of nodes with available pods: 2
May  3 18:18:54.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:55.075: INFO: Number of nodes with available pods: 2
May  3 18:18:55.076: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:56.075: INFO: Number of nodes with available pods: 2
May  3 18:18:56.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:57.075: INFO: Number of nodes with available pods: 2
May  3 18:18:57.076: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:58.125: INFO: Number of nodes with available pods: 2
May  3 18:18:58.125: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:18:59.075: INFO: Number of nodes with available pods: 2
May  3 18:18:59.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:00.075: INFO: Number of nodes with available pods: 2
May  3 18:19:00.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:01.075: INFO: Number of nodes with available pods: 2
May  3 18:19:01.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:02.076: INFO: Number of nodes with available pods: 2
May  3 18:19:02.076: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:03.075: INFO: Number of nodes with available pods: 2
May  3 18:19:03.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:04.126: INFO: Number of nodes with available pods: 2
May  3 18:19:04.126: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:05.075: INFO: Number of nodes with available pods: 2
May  3 18:19:05.075: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:06.125: INFO: Number of nodes with available pods: 2
May  3 18:19:06.125: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:07.076: INFO: Number of nodes with available pods: 2
May  3 18:19:07.076: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:08.077: INFO: Number of nodes with available pods: 2
May  3 18:19:08.078: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:09.076: INFO: Number of nodes with available pods: 2
May  3 18:19:09.076: INFO: Node 10.190.144.141 is running more than one daemon pod
May  3 18:19:10.076: INFO: Number of nodes with available pods: 3
May  3 18:19:10.076: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4646, will wait for the garbage collector to delete the pods
May  3 18:19:10.158: INFO: Deleting DaemonSet.extensions daemon-set took: 18.801433ms
May  3 18:19:10.359: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.330719ms
May  3 18:19:21.166: INFO: Number of nodes with available pods: 0
May  3 18:19:21.167: INFO: Number of running nodes: 0, number of available pods: 0
May  3 18:19:21.174: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4646/daemonsets","resourceVersion":"32914"},"items":null}

May  3 18:19:21.181: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4646/pods","resourceVersion":"32914"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:19:21.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4646" for this suite.
May  3 18:19:27.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:19:27.526: INFO: namespace daemonsets-4646 deletion completed in 6.313448422s

â€¢ [SLOW TEST:36.782 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:19:27.527: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:19:31.848: INFO: Waiting up to 5m0s for pod "client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788" in namespace "pods-8618" to be "success or failure"
May  3 18:19:31.855: INFO: Pod "client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.311052ms
May  3 18:19:33.872: INFO: Pod "client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.024067996s
May  3 18:19:35.881: INFO: Pod "client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033095276s
STEP: Saw pod success
May  3 18:19:35.881: INFO: Pod "client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:19:35.918: INFO: Trying to get logs from node 10.190.144.141 pod client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788 container env3cont: <nil>
STEP: delete the pod
May  3 18:19:35.962: INFO: Waiting for pod client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788 to disappear
May  3 18:19:35.969: INFO: Pod client-envvars-ff2e3084-6dcf-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:19:35.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8618" for this suite.
May  3 18:20:23.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:20:24.234: INFO: namespace pods-8618 deletion completed in 48.257449765s

â€¢ [SLOW TEST:56.708 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:20:24.235: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-5338
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-5899
STEP: Creating a pod to test atomic-volume-subpath
May  3 18:20:24.446: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5899" in namespace "subpath-5338" to be "success or failure"
May  3 18:20:24.453: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Pending", Reason="", readiness=false. Elapsed: 6.984458ms
May  3 18:20:26.462: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 2.016171786s
May  3 18:20:28.471: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 4.024982494s
May  3 18:20:30.479: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 6.033541477s
May  3 18:20:32.491: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 8.044765135s
May  3 18:20:34.498: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 10.052350288s
May  3 18:20:36.508: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 12.061758212s
May  3 18:20:38.517: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 14.070774533s
May  3 18:20:40.525: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 16.079259416s
May  3 18:20:42.533: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 18.087652089s
May  3 18:20:44.541: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Running", Reason="", readiness=true. Elapsed: 20.095611913s
May  3 18:20:46.550: INFO: Pod "pod-subpath-test-configmap-5899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.103863345s
STEP: Saw pod success
May  3 18:20:46.550: INFO: Pod "pod-subpath-test-configmap-5899" satisfied condition "success or failure"
May  3 18:20:46.557: INFO: Trying to get logs from node 10.190.144.141 pod pod-subpath-test-configmap-5899 container test-container-subpath-configmap-5899: <nil>
STEP: delete the pod
May  3 18:20:46.596: INFO: Waiting for pod pod-subpath-test-configmap-5899 to disappear
May  3 18:20:46.603: INFO: Pod pod-subpath-test-configmap-5899 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5899
May  3 18:20:46.603: INFO: Deleting pod "pod-subpath-test-configmap-5899" in namespace "subpath-5338"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:20:46.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5338" for this suite.
May  3 18:20:52.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:20:52.930: INFO: namespace subpath-5338 deletion completed in 6.311095694s

â€¢ [SLOW TEST:28.695 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:20:52.930: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8628
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8628
May  3 18:20:57.148: INFO: Started pod liveness-http in namespace container-probe-8628
STEP: checking the pod's current state and verifying that restartCount is present
May  3 18:20:57.156: INFO: Initial restart count of pod liveness-http is 0
May  3 18:21:15.240: INFO: Restart count of pod container-probe-8628/liveness-http is now 1 (18.084342041s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:21:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8628" for this suite.
May  3 18:21:21.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:21:21.542: INFO: namespace container-probe-8628 deletion completed in 6.268880314s

â€¢ [SLOW TEST:28.612 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:21:21.542: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May  3 18:21:23.771: INFO: Pod pod-hostip-40add8d3-6dd0-11e9-8617-baa18b820788 has hostIP: 10.190.144.140
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:21:23.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4726" for this suite.
May  3 18:21:45.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:21:46.056: INFO: namespace pods-4726 deletion completed in 22.276540349s

â€¢ [SLOW TEST:24.514 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:21:46.057: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-4f563817-6dd0-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:21:46.344: INFO: Waiting up to 5m0s for pod "pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788" in namespace "configmap-6260" to be "success or failure"
May  3 18:21:46.352: INFO: Pod "pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.438556ms
May  3 18:21:48.362: INFO: Pod "pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01810744s
STEP: Saw pod success
May  3 18:21:48.362: INFO: Pod "pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:21:48.369: INFO: Trying to get logs from node 10.190.144.141 pod pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:21:48.411: INFO: Waiting for pod pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788 to disappear
May  3 18:21:48.421: INFO: Pod pod-configmaps-4f576cf0-6dd0-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:21:48.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6260" for this suite.
May  3 18:21:54.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:21:54.675: INFO: namespace configmap-6260 deletion completed in 6.245825458s

â€¢ [SLOW TEST:8.618 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:21:54.675: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-7470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7470
I0503 18:21:54.930766      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7470, replica count: 1
I0503 18:21:55.981304      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0503 18:21:56.981556      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  3 18:21:57.104: INFO: Created: latency-svc-8hnpp
May  3 18:21:57.112: INFO: Got endpoints: latency-svc-8hnpp [30.723656ms]
May  3 18:21:57.145: INFO: Created: latency-svc-ss5gc
May  3 18:21:57.152: INFO: Got endpoints: latency-svc-ss5gc [39.166088ms]
May  3 18:21:57.157: INFO: Created: latency-svc-58qdr
May  3 18:21:57.165: INFO: Got endpoints: latency-svc-58qdr [52.590888ms]
May  3 18:21:57.173: INFO: Created: latency-svc-plp4n
May  3 18:21:57.181: INFO: Got endpoints: latency-svc-plp4n [67.819143ms]
May  3 18:21:57.187: INFO: Created: latency-svc-r2v5r
May  3 18:21:57.195: INFO: Got endpoints: latency-svc-r2v5r [82.172003ms]
May  3 18:21:57.199: INFO: Created: latency-svc-k5gkw
May  3 18:21:57.209: INFO: Got endpoints: latency-svc-k5gkw [95.736044ms]
May  3 18:21:57.213: INFO: Created: latency-svc-4cspr
May  3 18:21:57.220: INFO: Got endpoints: latency-svc-4cspr [107.3903ms]
May  3 18:21:57.227: INFO: Created: latency-svc-fn2rb
May  3 18:21:57.234: INFO: Got endpoints: latency-svc-fn2rb [122.007166ms]
May  3 18:21:57.240: INFO: Created: latency-svc-r5h98
May  3 18:21:57.247: INFO: Got endpoints: latency-svc-r5h98 [134.045055ms]
May  3 18:21:57.252: INFO: Created: latency-svc-9gfrs
May  3 18:21:57.260: INFO: Got endpoints: latency-svc-9gfrs [147.462548ms]
May  3 18:21:57.264: INFO: Created: latency-svc-rdsgp
May  3 18:21:57.271: INFO: Got endpoints: latency-svc-rdsgp [157.723067ms]
May  3 18:21:57.277: INFO: Created: latency-svc-q9kvl
May  3 18:21:57.286: INFO: Got endpoints: latency-svc-q9kvl [173.490561ms]
May  3 18:21:57.288: INFO: Created: latency-svc-pbsp8
May  3 18:21:57.295: INFO: Got endpoints: latency-svc-pbsp8 [182.657754ms]
May  3 18:21:57.303: INFO: Created: latency-svc-scxgl
May  3 18:21:57.312: INFO: Got endpoints: latency-svc-scxgl [199.517961ms]
May  3 18:21:57.317: INFO: Created: latency-svc-4m6lh
May  3 18:21:57.324: INFO: Got endpoints: latency-svc-4m6lh [210.987193ms]
May  3 18:21:57.327: INFO: Created: latency-svc-rkqkd
May  3 18:21:57.334: INFO: Got endpoints: latency-svc-rkqkd [221.467075ms]
May  3 18:21:57.339: INFO: Created: latency-svc-f45vc
May  3 18:21:57.346: INFO: Got endpoints: latency-svc-f45vc [194.028106ms]
May  3 18:21:57.350: INFO: Created: latency-svc-6l4cs
May  3 18:21:57.358: INFO: Got endpoints: latency-svc-6l4cs [192.905334ms]
May  3 18:21:57.364: INFO: Created: latency-svc-tctq9
May  3 18:21:57.372: INFO: Got endpoints: latency-svc-tctq9 [191.428815ms]
May  3 18:21:57.378: INFO: Created: latency-svc-6zh9v
May  3 18:21:57.386: INFO: Got endpoints: latency-svc-6zh9v [190.753179ms]
May  3 18:21:57.391: INFO: Created: latency-svc-rnqhv
May  3 18:21:57.398: INFO: Got endpoints: latency-svc-rnqhv [189.751278ms]
May  3 18:21:57.403: INFO: Created: latency-svc-kftsw
May  3 18:21:57.410: INFO: Got endpoints: latency-svc-kftsw [189.577465ms]
May  3 18:21:57.414: INFO: Created: latency-svc-8wpsx
May  3 18:21:57.420: INFO: Got endpoints: latency-svc-8wpsx [185.687946ms]
May  3 18:21:57.427: INFO: Created: latency-svc-9kcxf
May  3 18:21:57.434: INFO: Got endpoints: latency-svc-9kcxf [187.378942ms]
May  3 18:21:57.439: INFO: Created: latency-svc-skkfd
May  3 18:21:57.447: INFO: Got endpoints: latency-svc-skkfd [186.237808ms]
May  3 18:21:57.452: INFO: Created: latency-svc-qllld
May  3 18:21:57.459: INFO: Got endpoints: latency-svc-qllld [187.960024ms]
May  3 18:21:57.463: INFO: Created: latency-svc-49tpm
May  3 18:21:57.470: INFO: Got endpoints: latency-svc-49tpm [183.839553ms]
May  3 18:21:57.478: INFO: Created: latency-svc-fswvl
May  3 18:21:57.489: INFO: Got endpoints: latency-svc-fswvl [193.561965ms]
May  3 18:21:57.491: INFO: Created: latency-svc-x2lt9
May  3 18:21:57.497: INFO: Got endpoints: latency-svc-x2lt9 [184.820095ms]
May  3 18:21:57.505: INFO: Created: latency-svc-t8m4v
May  3 18:21:57.512: INFO: Got endpoints: latency-svc-t8m4v [187.916176ms]
May  3 18:21:57.515: INFO: Created: latency-svc-66sl7
May  3 18:21:57.520: INFO: Got endpoints: latency-svc-66sl7 [186.126481ms]
May  3 18:21:57.527: INFO: Created: latency-svc-xwt5j
May  3 18:21:57.538: INFO: Got endpoints: latency-svc-xwt5j [191.635875ms]
May  3 18:21:57.548: INFO: Created: latency-svc-8ldwt
May  3 18:21:57.557: INFO: Created: latency-svc-wln8x
May  3 18:21:57.564: INFO: Got endpoints: latency-svc-8ldwt [205.906846ms]
May  3 18:21:57.569: INFO: Created: latency-svc-qrvkg
May  3 18:21:57.569: INFO: Got endpoints: latency-svc-wln8x [196.518461ms]
May  3 18:21:57.575: INFO: Got endpoints: latency-svc-qrvkg [189.061763ms]
May  3 18:21:57.576: INFO: Created: latency-svc-2p7kh
May  3 18:21:57.582: INFO: Got endpoints: latency-svc-2p7kh [183.62616ms]
May  3 18:21:57.586: INFO: Created: latency-svc-ch8n8
May  3 18:21:57.594: INFO: Got endpoints: latency-svc-ch8n8 [183.856082ms]
May  3 18:21:57.599: INFO: Created: latency-svc-ccmwk
May  3 18:21:57.606: INFO: Got endpoints: latency-svc-ccmwk [185.468682ms]
May  3 18:21:57.614: INFO: Created: latency-svc-qch8g
May  3 18:21:57.621: INFO: Got endpoints: latency-svc-qch8g [186.374928ms]
May  3 18:21:57.625: INFO: Created: latency-svc-hzmhg
May  3 18:21:57.634: INFO: Got endpoints: latency-svc-hzmhg [186.971326ms]
May  3 18:21:57.637: INFO: Created: latency-svc-n5ng9
May  3 18:21:57.644: INFO: Got endpoints: latency-svc-n5ng9 [184.830724ms]
May  3 18:21:57.649: INFO: Created: latency-svc-sw7pp
May  3 18:21:57.655: INFO: Got endpoints: latency-svc-sw7pp [185.399102ms]
May  3 18:21:57.662: INFO: Created: latency-svc-k7h79
May  3 18:21:57.670: INFO: Got endpoints: latency-svc-k7h79 [180.997521ms]
May  3 18:21:57.672: INFO: Created: latency-svc-hdpwq
May  3 18:21:57.681: INFO: Got endpoints: latency-svc-hdpwq [183.33034ms]
May  3 18:21:57.686: INFO: Created: latency-svc-dpjp4
May  3 18:21:57.693: INFO: Got endpoints: latency-svc-dpjp4 [180.878635ms]
May  3 18:21:57.696: INFO: Created: latency-svc-p8pfp
May  3 18:21:57.704: INFO: Got endpoints: latency-svc-p8pfp [183.452633ms]
May  3 18:21:57.708: INFO: Created: latency-svc-r5cq6
May  3 18:21:57.717: INFO: Got endpoints: latency-svc-r5cq6 [178.842943ms]
May  3 18:21:57.721: INFO: Created: latency-svc-ltq6j
May  3 18:21:57.728: INFO: Got endpoints: latency-svc-ltq6j [163.817723ms]
May  3 18:21:57.731: INFO: Created: latency-svc-6dzkd
May  3 18:21:57.737: INFO: Got endpoints: latency-svc-6dzkd [168.303363ms]
May  3 18:21:57.742: INFO: Created: latency-svc-4nzc8
May  3 18:21:57.750: INFO: Got endpoints: latency-svc-4nzc8 [175.212087ms]
May  3 18:21:57.754: INFO: Created: latency-svc-vzx6s
May  3 18:21:57.763: INFO: Got endpoints: latency-svc-vzx6s [180.516083ms]
May  3 18:21:57.767: INFO: Created: latency-svc-h89fc
May  3 18:21:57.775: INFO: Got endpoints: latency-svc-h89fc [180.808072ms]
May  3 18:21:57.780: INFO: Created: latency-svc-59p5k
May  3 18:21:57.788: INFO: Got endpoints: latency-svc-59p5k [181.610062ms]
May  3 18:21:57.793: INFO: Created: latency-svc-mxf5s
May  3 18:21:57.801: INFO: Got endpoints: latency-svc-mxf5s [180.311095ms]
May  3 18:21:57.805: INFO: Created: latency-svc-rs4rc
May  3 18:21:57.812: INFO: Got endpoints: latency-svc-rs4rc [178.624348ms]
May  3 18:21:57.817: INFO: Created: latency-svc-4njbl
May  3 18:21:57.825: INFO: Got endpoints: latency-svc-4njbl [181.072825ms]
May  3 18:21:57.828: INFO: Created: latency-svc-5nwd9
May  3 18:21:57.834: INFO: Got endpoints: latency-svc-5nwd9 [179.020904ms]
May  3 18:21:57.841: INFO: Created: latency-svc-ffk9k
May  3 18:21:57.848: INFO: Got endpoints: latency-svc-ffk9k [178.676032ms]
May  3 18:21:57.854: INFO: Created: latency-svc-r7sl4
May  3 18:21:57.860: INFO: Got endpoints: latency-svc-r7sl4 [179.205612ms]
May  3 18:21:57.867: INFO: Created: latency-svc-w56lq
May  3 18:21:57.873: INFO: Got endpoints: latency-svc-w56lq [180.377755ms]
May  3 18:21:57.877: INFO: Created: latency-svc-xf9hj
May  3 18:21:57.884: INFO: Got endpoints: latency-svc-xf9hj [180.113221ms]
May  3 18:21:57.889: INFO: Created: latency-svc-tgl82
May  3 18:21:57.896: INFO: Got endpoints: latency-svc-tgl82 [178.875755ms]
May  3 18:21:57.902: INFO: Created: latency-svc-277d4
May  3 18:21:57.909: INFO: Got endpoints: latency-svc-277d4 [180.51182ms]
May  3 18:21:57.914: INFO: Created: latency-svc-58hfr
May  3 18:21:57.921: INFO: Got endpoints: latency-svc-58hfr [183.548298ms]
May  3 18:21:57.925: INFO: Created: latency-svc-xgcfx
May  3 18:21:57.933: INFO: Got endpoints: latency-svc-xgcfx [182.348096ms]
May  3 18:21:57.938: INFO: Created: latency-svc-vlplw
May  3 18:21:57.948: INFO: Got endpoints: latency-svc-vlplw [185.680169ms]
May  3 18:21:57.953: INFO: Created: latency-svc-cr9w9
May  3 18:21:57.961: INFO: Created: latency-svc-xflh8
May  3 18:21:57.962: INFO: Got endpoints: latency-svc-cr9w9 [187.081763ms]
May  3 18:21:57.969: INFO: Got endpoints: latency-svc-xflh8 [180.928984ms]
May  3 18:21:57.973: INFO: Created: latency-svc-qxhq5
May  3 18:21:57.980: INFO: Got endpoints: latency-svc-qxhq5 [178.373959ms]
May  3 18:21:57.986: INFO: Created: latency-svc-vb22w
May  3 18:21:57.992: INFO: Got endpoints: latency-svc-vb22w [180.082341ms]
May  3 18:21:57.998: INFO: Created: latency-svc-hmpdw
May  3 18:21:58.003: INFO: Got endpoints: latency-svc-hmpdw [178.615672ms]
May  3 18:21:58.009: INFO: Created: latency-svc-v7c7d
May  3 18:21:58.018: INFO: Got endpoints: latency-svc-v7c7d [183.949661ms]
May  3 18:21:58.023: INFO: Created: latency-svc-65zs7
May  3 18:21:58.031: INFO: Got endpoints: latency-svc-65zs7 [182.508844ms]
May  3 18:21:58.035: INFO: Created: latency-svc-s4llm
May  3 18:21:58.041: INFO: Got endpoints: latency-svc-s4llm [180.99692ms]
May  3 18:21:58.046: INFO: Created: latency-svc-67fj5
May  3 18:21:58.054: INFO: Got endpoints: latency-svc-67fj5 [180.465744ms]
May  3 18:21:58.058: INFO: Created: latency-svc-bxqb6
May  3 18:21:58.066: INFO: Got endpoints: latency-svc-bxqb6 [181.575752ms]
May  3 18:21:58.070: INFO: Created: latency-svc-tjwj8
May  3 18:21:58.077: INFO: Got endpoints: latency-svc-tjwj8 [180.978369ms]
May  3 18:21:58.082: INFO: Created: latency-svc-wdrtp
May  3 18:21:58.088: INFO: Got endpoints: latency-svc-wdrtp [179.772977ms]
May  3 18:21:58.095: INFO: Created: latency-svc-mnngb
May  3 18:21:58.101: INFO: Got endpoints: latency-svc-mnngb [179.62476ms]
May  3 18:21:58.105: INFO: Created: latency-svc-hm9xl
May  3 18:21:58.111: INFO: Got endpoints: latency-svc-hm9xl [178.531078ms]
May  3 18:21:58.115: INFO: Created: latency-svc-qphrc
May  3 18:21:58.123: INFO: Got endpoints: latency-svc-qphrc [174.10972ms]
May  3 18:21:58.127: INFO: Created: latency-svc-wgxrh
May  3 18:21:58.134: INFO: Got endpoints: latency-svc-wgxrh [171.695235ms]
May  3 18:21:58.137: INFO: Created: latency-svc-ld8fr
May  3 18:21:58.144: INFO: Got endpoints: latency-svc-ld8fr [175.396978ms]
May  3 18:21:58.149: INFO: Created: latency-svc-gz58b
May  3 18:21:58.155: INFO: Got endpoints: latency-svc-gz58b [175.123223ms]
May  3 18:21:58.160: INFO: Created: latency-svc-fd4ht
May  3 18:21:58.169: INFO: Got endpoints: latency-svc-fd4ht [176.778615ms]
May  3 18:21:58.173: INFO: Created: latency-svc-k2q58
May  3 18:21:58.182: INFO: Got endpoints: latency-svc-k2q58 [178.089859ms]
May  3 18:21:58.183: INFO: Created: latency-svc-kpts9
May  3 18:21:58.189: INFO: Got endpoints: latency-svc-kpts9 [170.603903ms]
May  3 18:21:58.195: INFO: Created: latency-svc-qpqld
May  3 18:21:58.203: INFO: Got endpoints: latency-svc-qpqld [172.472161ms]
May  3 18:21:58.207: INFO: Created: latency-svc-b7rc9
May  3 18:21:58.217: INFO: Got endpoints: latency-svc-b7rc9 [176.232925ms]
May  3 18:21:58.219: INFO: Created: latency-svc-q2xfj
May  3 18:21:58.227: INFO: Got endpoints: latency-svc-q2xfj [173.680252ms]
May  3 18:21:58.230: INFO: Created: latency-svc-lbdfz
May  3 18:21:58.237: INFO: Got endpoints: latency-svc-lbdfz [171.427654ms]
May  3 18:21:58.243: INFO: Created: latency-svc-tjjgq
May  3 18:21:58.248: INFO: Got endpoints: latency-svc-tjjgq [171.84094ms]
May  3 18:21:58.254: INFO: Created: latency-svc-kspmn
May  3 18:21:58.262: INFO: Got endpoints: latency-svc-kspmn [173.034387ms]
May  3 18:21:58.272: INFO: Created: latency-svc-5dbng
May  3 18:21:58.279: INFO: Created: latency-svc-vlq99
May  3 18:21:58.279: INFO: Got endpoints: latency-svc-5dbng [178.720233ms]
May  3 18:21:58.287: INFO: Got endpoints: latency-svc-vlq99 [175.262549ms]
May  3 18:21:58.290: INFO: Created: latency-svc-2s5p8
May  3 18:21:58.298: INFO: Got endpoints: latency-svc-2s5p8 [175.072022ms]
May  3 18:21:58.302: INFO: Created: latency-svc-rfxqk
May  3 18:21:58.309: INFO: Got endpoints: latency-svc-rfxqk [175.500152ms]
May  3 18:21:58.314: INFO: Created: latency-svc-9xhcg
May  3 18:21:58.321: INFO: Got endpoints: latency-svc-9xhcg [176.850373ms]
May  3 18:21:58.327: INFO: Created: latency-svc-8nb5c
May  3 18:21:58.335: INFO: Got endpoints: latency-svc-8nb5c [180.009516ms]
May  3 18:21:58.340: INFO: Created: latency-svc-2grjb
May  3 18:21:58.347: INFO: Got endpoints: latency-svc-2grjb [178.148253ms]
May  3 18:21:58.353: INFO: Created: latency-svc-w9x9k
May  3 18:21:58.361: INFO: Got endpoints: latency-svc-w9x9k [179.738627ms]
May  3 18:21:58.368: INFO: Created: latency-svc-rm4tk
May  3 18:21:58.376: INFO: Got endpoints: latency-svc-rm4tk [186.661343ms]
May  3 18:21:58.380: INFO: Created: latency-svc-grdxl
May  3 18:21:58.387: INFO: Got endpoints: latency-svc-grdxl [183.913918ms]
May  3 18:21:58.393: INFO: Created: latency-svc-vf7qt
May  3 18:21:58.399: INFO: Got endpoints: latency-svc-vf7qt [182.103361ms]
May  3 18:21:58.405: INFO: Created: latency-svc-ld8dp
May  3 18:21:58.418: INFO: Got endpoints: latency-svc-ld8dp [190.811402ms]
May  3 18:21:58.419: INFO: Created: latency-svc-6mkts
May  3 18:21:58.425: INFO: Got endpoints: latency-svc-6mkts [187.48683ms]
May  3 18:21:58.430: INFO: Created: latency-svc-xb4vz
May  3 18:21:58.437: INFO: Got endpoints: latency-svc-xb4vz [189.031554ms]
May  3 18:21:58.441: INFO: Created: latency-svc-rkdb4
May  3 18:21:58.448: INFO: Got endpoints: latency-svc-rkdb4 [186.302044ms]
May  3 18:21:58.454: INFO: Created: latency-svc-kwcd2
May  3 18:21:58.469: INFO: Got endpoints: latency-svc-kwcd2 [189.484796ms]
May  3 18:21:58.472: INFO: Created: latency-svc-pvlps
May  3 18:21:58.477: INFO: Got endpoints: latency-svc-pvlps [190.225911ms]
May  3 18:21:58.480: INFO: Created: latency-svc-qk88r
May  3 18:21:58.491: INFO: Got endpoints: latency-svc-qk88r [193.479033ms]
May  3 18:21:58.498: INFO: Created: latency-svc-b62dw
May  3 18:21:58.506: INFO: Got endpoints: latency-svc-b62dw [196.959263ms]
May  3 18:21:58.513: INFO: Created: latency-svc-fxfzt
May  3 18:21:58.522: INFO: Got endpoints: latency-svc-fxfzt [201.337401ms]
May  3 18:21:58.530: INFO: Created: latency-svc-7jwvs
May  3 18:21:58.541: INFO: Created: latency-svc-mtzr6
May  3 18:21:58.545: INFO: Got endpoints: latency-svc-7jwvs [210.160061ms]
May  3 18:21:58.546: INFO: Created: latency-svc-fmdv9
May  3 18:21:58.548: INFO: Got endpoints: latency-svc-mtzr6 [200.507788ms]
May  3 18:21:58.554: INFO: Got endpoints: latency-svc-fmdv9 [192.307091ms]
May  3 18:21:58.558: INFO: Created: latency-svc-5jxm5
May  3 18:21:58.567: INFO: Got endpoints: latency-svc-5jxm5 [190.795011ms]
May  3 18:21:58.571: INFO: Created: latency-svc-264xm
May  3 18:21:58.578: INFO: Got endpoints: latency-svc-264xm [190.396058ms]
May  3 18:21:58.586: INFO: Created: latency-svc-l2tq6
May  3 18:21:58.593: INFO: Got endpoints: latency-svc-l2tq6 [193.555911ms]
May  3 18:21:58.597: INFO: Created: latency-svc-k5gvm
May  3 18:21:58.605: INFO: Got endpoints: latency-svc-k5gvm [187.031866ms]
May  3 18:21:58.610: INFO: Created: latency-svc-c8vgf
May  3 18:21:58.616: INFO: Got endpoints: latency-svc-c8vgf [191.669479ms]
May  3 18:21:58.622: INFO: Created: latency-svc-wmbpv
May  3 18:21:58.630: INFO: Got endpoints: latency-svc-wmbpv [192.450953ms]
May  3 18:21:58.635: INFO: Created: latency-svc-g4knz
May  3 18:21:58.643: INFO: Got endpoints: latency-svc-g4knz [195.404953ms]
May  3 18:21:58.651: INFO: Created: latency-svc-6bg4h
May  3 18:21:58.665: INFO: Got endpoints: latency-svc-6bg4h [196.444004ms]
May  3 18:21:58.671: INFO: Created: latency-svc-mz7kd
May  3 18:21:58.680: INFO: Got endpoints: latency-svc-mz7kd [203.186511ms]
May  3 18:21:58.687: INFO: Created: latency-svc-7rj4b
May  3 18:21:58.699: INFO: Got endpoints: latency-svc-7rj4b [207.553669ms]
May  3 18:21:58.700: INFO: Created: latency-svc-f7v7q
May  3 18:21:58.707: INFO: Got endpoints: latency-svc-f7v7q [200.872025ms]
May  3 18:21:58.717: INFO: Created: latency-svc-p67rz
May  3 18:21:58.725: INFO: Got endpoints: latency-svc-p67rz [202.429684ms]
May  3 18:21:58.741: INFO: Created: latency-svc-mwnqp
May  3 18:21:58.748: INFO: Got endpoints: latency-svc-mwnqp [201.757759ms]
May  3 18:21:58.752: INFO: Created: latency-svc-4w8fz
May  3 18:21:58.761: INFO: Got endpoints: latency-svc-4w8fz [213.204975ms]
May  3 18:21:58.765: INFO: Created: latency-svc-x258s
May  3 18:21:58.777: INFO: Got endpoints: latency-svc-x258s [222.841796ms]
May  3 18:21:58.780: INFO: Created: latency-svc-bdx9b
May  3 18:21:58.786: INFO: Got endpoints: latency-svc-bdx9b [219.74119ms]
May  3 18:21:58.791: INFO: Created: latency-svc-sgs4b
May  3 18:21:58.799: INFO: Got endpoints: latency-svc-sgs4b [220.90874ms]
May  3 18:21:58.804: INFO: Created: latency-svc-76x5w
May  3 18:21:58.810: INFO: Got endpoints: latency-svc-76x5w [217.175382ms]
May  3 18:21:58.816: INFO: Created: latency-svc-tnqmp
May  3 18:21:58.823: INFO: Got endpoints: latency-svc-tnqmp [217.586851ms]
May  3 18:21:58.829: INFO: Created: latency-svc-qxt59
May  3 18:21:58.835: INFO: Got endpoints: latency-svc-qxt59 [219.014748ms]
May  3 18:21:58.839: INFO: Created: latency-svc-8cj7r
May  3 18:21:58.849: INFO: Got endpoints: latency-svc-8cj7r [218.812592ms]
May  3 18:21:58.853: INFO: Created: latency-svc-gf86w
May  3 18:21:58.860: INFO: Got endpoints: latency-svc-gf86w [217.002147ms]
May  3 18:21:58.867: INFO: Created: latency-svc-dhfrj
May  3 18:21:58.874: INFO: Got endpoints: latency-svc-dhfrj [208.908851ms]
May  3 18:21:58.879: INFO: Created: latency-svc-9wv77
May  3 18:21:58.892: INFO: Got endpoints: latency-svc-9wv77 [211.891854ms]
May  3 18:21:58.892: INFO: Created: latency-svc-2qkgk
May  3 18:21:58.899: INFO: Got endpoints: latency-svc-2qkgk [199.740678ms]
May  3 18:21:58.904: INFO: Created: latency-svc-vwcrq
May  3 18:21:58.912: INFO: Got endpoints: latency-svc-vwcrq [204.69997ms]
May  3 18:21:58.916: INFO: Created: latency-svc-bd2c9
May  3 18:21:58.923: INFO: Got endpoints: latency-svc-bd2c9 [198.261784ms]
May  3 18:21:58.928: INFO: Created: latency-svc-kfr4r
May  3 18:21:58.935: INFO: Got endpoints: latency-svc-kfr4r [187.666888ms]
May  3 18:21:58.939: INFO: Created: latency-svc-ll776
May  3 18:21:58.946: INFO: Got endpoints: latency-svc-ll776 [184.874977ms]
May  3 18:21:58.952: INFO: Created: latency-svc-cl8vb
May  3 18:21:58.959: INFO: Got endpoints: latency-svc-cl8vb [181.787642ms]
May  3 18:21:58.963: INFO: Created: latency-svc-68sld
May  3 18:21:58.971: INFO: Got endpoints: latency-svc-68sld [184.353525ms]
May  3 18:21:58.977: INFO: Created: latency-svc-k22tn
May  3 18:21:58.983: INFO: Got endpoints: latency-svc-k22tn [184.147847ms]
May  3 18:21:58.990: INFO: Created: latency-svc-kldxp
May  3 18:21:58.996: INFO: Got endpoints: latency-svc-kldxp [185.509881ms]
May  3 18:21:59.001: INFO: Created: latency-svc-dkbqf
May  3 18:21:59.007: INFO: Got endpoints: latency-svc-dkbqf [183.46222ms]
May  3 18:21:59.014: INFO: Created: latency-svc-l62nb
May  3 18:21:59.022: INFO: Got endpoints: latency-svc-l62nb [186.807121ms]
May  3 18:21:59.026: INFO: Created: latency-svc-p8nmc
May  3 18:21:59.033: INFO: Got endpoints: latency-svc-p8nmc [184.085183ms]
May  3 18:21:59.038: INFO: Created: latency-svc-8tngp
May  3 18:21:59.045: INFO: Got endpoints: latency-svc-8tngp [185.103343ms]
May  3 18:21:59.052: INFO: Created: latency-svc-5h2n5
May  3 18:21:59.058: INFO: Got endpoints: latency-svc-5h2n5 [183.799269ms]
May  3 18:21:59.065: INFO: Created: latency-svc-zvsx8
May  3 18:21:59.073: INFO: Got endpoints: latency-svc-zvsx8 [180.79257ms]
May  3 18:21:59.078: INFO: Created: latency-svc-xct5j
May  3 18:21:59.086: INFO: Got endpoints: latency-svc-xct5j [187.264436ms]
May  3 18:21:59.091: INFO: Created: latency-svc-lfr7l
May  3 18:21:59.098: INFO: Got endpoints: latency-svc-lfr7l [186.278112ms]
May  3 18:21:59.103: INFO: Created: latency-svc-k88wf
May  3 18:21:59.110: INFO: Got endpoints: latency-svc-k88wf [186.405794ms]
May  3 18:21:59.114: INFO: Created: latency-svc-8v4lm
May  3 18:21:59.121: INFO: Got endpoints: latency-svc-8v4lm [185.806781ms]
May  3 18:21:59.125: INFO: Created: latency-svc-mvpvv
May  3 18:21:59.132: INFO: Got endpoints: latency-svc-mvpvv [186.222366ms]
May  3 18:21:59.138: INFO: Created: latency-svc-hlkrp
May  3 18:21:59.145: INFO: Got endpoints: latency-svc-hlkrp [186.58221ms]
May  3 18:21:59.150: INFO: Created: latency-svc-mjs4b
May  3 18:21:59.157: INFO: Got endpoints: latency-svc-mjs4b [185.864019ms]
May  3 18:21:59.162: INFO: Created: latency-svc-54hfm
May  3 18:21:59.170: INFO: Got endpoints: latency-svc-54hfm [186.752451ms]
May  3 18:21:59.174: INFO: Created: latency-svc-h82zx
May  3 18:21:59.182: INFO: Got endpoints: latency-svc-h82zx [185.705327ms]
May  3 18:21:59.186: INFO: Created: latency-svc-zpbdf
May  3 18:21:59.192: INFO: Got endpoints: latency-svc-zpbdf [185.703941ms]
May  3 18:21:59.198: INFO: Created: latency-svc-4mzlq
May  3 18:21:59.206: INFO: Got endpoints: latency-svc-4mzlq [183.363656ms]
May  3 18:21:59.211: INFO: Created: latency-svc-2cnhx
May  3 18:21:59.217: INFO: Got endpoints: latency-svc-2cnhx [184.239845ms]
May  3 18:21:59.223: INFO: Created: latency-svc-vg9lc
May  3 18:21:59.238: INFO: Got endpoints: latency-svc-vg9lc [192.309102ms]
May  3 18:21:59.238: INFO: Created: latency-svc-wwt6f
May  3 18:21:59.245: INFO: Got endpoints: latency-svc-wwt6f [187.227386ms]
May  3 18:21:59.247: INFO: Created: latency-svc-55tj6
May  3 18:21:59.254: INFO: Got endpoints: latency-svc-55tj6 [181.124012ms]
May  3 18:21:59.259: INFO: Created: latency-svc-krb4x
May  3 18:21:59.265: INFO: Got endpoints: latency-svc-krb4x [178.728731ms]
May  3 18:21:59.271: INFO: Created: latency-svc-xvwn8
May  3 18:21:59.278: INFO: Got endpoints: latency-svc-xvwn8 [179.838258ms]
May  3 18:21:59.285: INFO: Created: latency-svc-9tgss
May  3 18:21:59.292: INFO: Got endpoints: latency-svc-9tgss [182.797995ms]
May  3 18:21:59.296: INFO: Created: latency-svc-z856g
May  3 18:21:59.307: INFO: Got endpoints: latency-svc-z856g [186.189358ms]
May  3 18:21:59.322: INFO: Created: latency-svc-x8kq9
May  3 18:21:59.325: INFO: Created: latency-svc-m4f2r
May  3 18:21:59.329: INFO: Got endpoints: latency-svc-x8kq9 [196.622104ms]
May  3 18:21:59.332: INFO: Got endpoints: latency-svc-m4f2r [186.937594ms]
May  3 18:21:59.339: INFO: Created: latency-svc-x7f7h
May  3 18:21:59.344: INFO: Got endpoints: latency-svc-x7f7h [186.783122ms]
May  3 18:21:59.350: INFO: Created: latency-svc-7hd6t
May  3 18:21:59.358: INFO: Got endpoints: latency-svc-7hd6t [188.228234ms]
May  3 18:21:59.362: INFO: Created: latency-svc-tw6t7
May  3 18:21:59.372: INFO: Got endpoints: latency-svc-tw6t7 [190.255027ms]
May  3 18:21:59.378: INFO: Created: latency-svc-bq8p8
May  3 18:21:59.383: INFO: Got endpoints: latency-svc-bq8p8 [190.588642ms]
May  3 18:21:59.386: INFO: Created: latency-svc-5fv2r
May  3 18:21:59.393: INFO: Got endpoints: latency-svc-5fv2r [187.576639ms]
May  3 18:21:59.398: INFO: Created: latency-svc-r6tmh
May  3 18:21:59.404: INFO: Got endpoints: latency-svc-r6tmh [186.584707ms]
May  3 18:21:59.410: INFO: Created: latency-svc-8zvwc
May  3 18:21:59.416: INFO: Got endpoints: latency-svc-8zvwc [177.472311ms]
May  3 18:21:59.421: INFO: Created: latency-svc-nd29c
May  3 18:21:59.428: INFO: Got endpoints: latency-svc-nd29c [182.077128ms]
May  3 18:21:59.432: INFO: Created: latency-svc-bl4nj
May  3 18:21:59.440: INFO: Got endpoints: latency-svc-bl4nj [186.131147ms]
May  3 18:21:59.446: INFO: Created: latency-svc-5bwq6
May  3 18:21:59.453: INFO: Got endpoints: latency-svc-5bwq6 [188.162005ms]
May  3 18:21:59.457: INFO: Created: latency-svc-84pfh
May  3 18:21:59.470: INFO: Got endpoints: latency-svc-84pfh [191.789718ms]
May  3 18:21:59.471: INFO: Created: latency-svc-mb25f
May  3 18:21:59.478: INFO: Got endpoints: latency-svc-mb25f [185.168092ms]
May  3 18:21:59.483: INFO: Created: latency-svc-9kjkh
May  3 18:21:59.490: INFO: Got endpoints: latency-svc-9kjkh [182.111353ms]
May  3 18:21:59.496: INFO: Created: latency-svc-7ktqm
May  3 18:21:59.502: INFO: Got endpoints: latency-svc-7ktqm [173.205398ms]
May  3 18:21:59.507: INFO: Created: latency-svc-pdb59
May  3 18:21:59.514: INFO: Got endpoints: latency-svc-pdb59 [181.500824ms]
May  3 18:21:59.518: INFO: Created: latency-svc-cnvt2
May  3 18:21:59.525: INFO: Got endpoints: latency-svc-cnvt2 [181.044082ms]
May  3 18:21:59.532: INFO: Created: latency-svc-625m9
May  3 18:21:59.540: INFO: Got endpoints: latency-svc-625m9 [182.228358ms]
May  3 18:21:59.545: INFO: Created: latency-svc-9w4hq
May  3 18:21:59.568: INFO: Got endpoints: latency-svc-9w4hq [196.367454ms]
May  3 18:21:59.569: INFO: Created: latency-svc-dpf6h
May  3 18:21:59.572: INFO: Created: latency-svc-8rbtp
May  3 18:21:59.576: INFO: Got endpoints: latency-svc-dpf6h [193.083763ms]
May  3 18:21:59.578: INFO: Got endpoints: latency-svc-8rbtp [184.787812ms]
May  3 18:21:59.580: INFO: Created: latency-svc-fjm82
May  3 18:21:59.589: INFO: Got endpoints: latency-svc-fjm82 [185.383817ms]
May  3 18:21:59.593: INFO: Created: latency-svc-j9ndk
May  3 18:21:59.599: INFO: Got endpoints: latency-svc-j9ndk [183.735127ms]
May  3 18:21:59.604: INFO: Created: latency-svc-b8tt5
May  3 18:21:59.610: INFO: Got endpoints: latency-svc-b8tt5 [182.818986ms]
May  3 18:21:59.617: INFO: Created: latency-svc-rlqwg
May  3 18:21:59.623: INFO: Got endpoints: latency-svc-rlqwg [183.15872ms]
May  3 18:21:59.631: INFO: Created: latency-svc-56p8w
May  3 18:21:59.638: INFO: Got endpoints: latency-svc-56p8w [185.388388ms]
May  3 18:21:59.638: INFO: Latencies: [39.166088ms 52.590888ms 67.819143ms 82.172003ms 95.736044ms 107.3903ms 122.007166ms 134.045055ms 147.462548ms 157.723067ms 163.817723ms 168.303363ms 170.603903ms 171.427654ms 171.695235ms 171.84094ms 172.472161ms 173.034387ms 173.205398ms 173.490561ms 173.680252ms 174.10972ms 175.072022ms 175.123223ms 175.212087ms 175.262549ms 175.396978ms 175.500152ms 176.232925ms 176.778615ms 176.850373ms 177.472311ms 178.089859ms 178.148253ms 178.373959ms 178.531078ms 178.615672ms 178.624348ms 178.676032ms 178.720233ms 178.728731ms 178.842943ms 178.875755ms 179.020904ms 179.205612ms 179.62476ms 179.738627ms 179.772977ms 179.838258ms 180.009516ms 180.082341ms 180.113221ms 180.311095ms 180.377755ms 180.465744ms 180.51182ms 180.516083ms 180.79257ms 180.808072ms 180.878635ms 180.928984ms 180.978369ms 180.99692ms 180.997521ms 181.044082ms 181.072825ms 181.124012ms 181.500824ms 181.575752ms 181.610062ms 181.787642ms 182.077128ms 182.103361ms 182.111353ms 182.228358ms 182.348096ms 182.508844ms 182.657754ms 182.797995ms 182.818986ms 183.15872ms 183.33034ms 183.363656ms 183.452633ms 183.46222ms 183.548298ms 183.62616ms 183.735127ms 183.799269ms 183.839553ms 183.856082ms 183.913918ms 183.949661ms 184.085183ms 184.147847ms 184.239845ms 184.353525ms 184.787812ms 184.820095ms 184.830724ms 184.874977ms 185.103343ms 185.168092ms 185.383817ms 185.388388ms 185.399102ms 185.468682ms 185.509881ms 185.680169ms 185.687946ms 185.703941ms 185.705327ms 185.806781ms 185.864019ms 186.126481ms 186.131147ms 186.189358ms 186.222366ms 186.237808ms 186.278112ms 186.302044ms 186.374928ms 186.405794ms 186.58221ms 186.584707ms 186.661343ms 186.752451ms 186.783122ms 186.807121ms 186.937594ms 186.971326ms 187.031866ms 187.081763ms 187.227386ms 187.264436ms 187.378942ms 187.48683ms 187.576639ms 187.666888ms 187.916176ms 187.960024ms 188.162005ms 188.228234ms 189.031554ms 189.061763ms 189.484796ms 189.577465ms 189.751278ms 190.225911ms 190.255027ms 190.396058ms 190.588642ms 190.753179ms 190.795011ms 190.811402ms 191.428815ms 191.635875ms 191.669479ms 191.789718ms 192.307091ms 192.309102ms 192.450953ms 192.905334ms 193.083763ms 193.479033ms 193.555911ms 193.561965ms 194.028106ms 195.404953ms 196.367454ms 196.444004ms 196.518461ms 196.622104ms 196.959263ms 198.261784ms 199.517961ms 199.740678ms 200.507788ms 200.872025ms 201.337401ms 201.757759ms 202.429684ms 203.186511ms 204.69997ms 205.906846ms 207.553669ms 208.908851ms 210.160061ms 210.987193ms 211.891854ms 213.204975ms 217.002147ms 217.175382ms 217.586851ms 218.812592ms 219.014748ms 219.74119ms 220.90874ms 221.467075ms 222.841796ms]
May  3 18:21:59.639: INFO: 50 %ile: 184.874977ms
May  3 18:21:59.639: INFO: 90 %ile: 201.757759ms
May  3 18:21:59.639: INFO: 99 %ile: 221.467075ms
May  3 18:21:59.639: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:21:59.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7470" for this suite.
May  3 18:22:15.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:22:16.064: INFO: namespace svc-latency-7470 deletion completed in 16.416199667s

â€¢ [SLOW TEST:21.389 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:22:16.066: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:22:16.254: INFO: Creating deployment "nginx-deployment"
May  3 18:22:16.263: INFO: Waiting for observed generation 1
May  3 18:22:18.277: INFO: Waiting for all required pods to come up
May  3 18:22:18.287: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May  3 18:22:20.305: INFO: Waiting for deployment "nginx-deployment" to complete
May  3 18:22:20.326: INFO: Updating deployment "nginx-deployment" with a non-existent image
May  3 18:22:20.339: INFO: Updating deployment nginx-deployment
May  3 18:22:20.339: INFO: Waiting for observed generation 2
May  3 18:22:22.351: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May  3 18:22:22.358: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May  3 18:22:22.365: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May  3 18:22:22.387: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May  3 18:22:22.387: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May  3 18:22:22.393: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May  3 18:22:22.417: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May  3 18:22:22.417: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May  3 18:22:22.430: INFO: Updating deployment nginx-deployment
May  3 18:22:22.430: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May  3 18:22:22.442: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May  3 18:22:22.449: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  3 18:22:22.466: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-6610,SelfLink:/apis/apps/v1/namespaces/deployment-6610/deployments/nginx-deployment,UID:612e5705-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35191,Generation:3,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-03 18:22:20 +0000 UTC 2019-05-03 18:22:16 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-03 18:22:22 +0000 UTC 2019-05-03 18:22:22 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May  3 18:22:22.477: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-6610,SelfLink:/apis/apps/v1/namespaces/deployment-6610/replicasets/nginx-deployment-5f9595f595,UID:639d813d-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35182,Generation:3,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 612e5705-6dd0-11e9-a061-0e68f10e50c0 0xc002ea2467 0xc002ea2468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 18:22:22.477: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May  3 18:22:22.477: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-6610,SelfLink:/apis/apps/v1/namespaces/deployment-6610/replicasets/nginx-deployment-6f478d8d8,UID:612f8a30-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35180,Generation:3,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 612e5705-6dd0-11e9-a061-0e68f10e50c0 0xc002ea2537 0xc002ea2538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May  3 18:22:22.495: INFO: Pod "nginx-deployment-5f9595f595-9npz6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9npz6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-9npz6,UID:639eb36d-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35173,Generation:0,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea2e57 0xc002ea2e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea2ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea2ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.140,PodIP:172.30.111.253,StartTime:2019-05-03 18:22:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.495: INFO: Pod "nginx-deployment-5f9595f595-bvbzv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bvbzv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-bvbzv,UID:64e335b7-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35210,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea2ff0 0xc002ea2ff1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.495: INFO: Pod "nginx-deployment-5f9595f595-cxw4f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cxw4f,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-cxw4f,UID:64e2fa86-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35209,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea30f7 0xc002ea30f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea31b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.495: INFO: Pod "nginx-deployment-5f9595f595-jz7tf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jz7tf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-jz7tf,UID:63a8378b-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35127,Generation:0,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3217 0xc002ea3218}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea32b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:,StartTime:2019-05-03 18:22:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.495: INFO: Pod "nginx-deployment-5f9595f595-nm689" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nm689,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-nm689,UID:63a0913e-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35156,Generation:0,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3380 0xc002ea3381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.145,StartTime:2019-05-03 18:22:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-qw5v8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-qw5v8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-qw5v8,UID:64e09117-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35203,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3510 0xc002ea3511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3590} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea35b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-s2pbg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-s2pbg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-s2pbg,UID:64e0c45c-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35204,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3630 0xc002ea3631}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea36b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea36d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-wxl4z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wxl4z,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-wxl4z,UID:64e303fc-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35208,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3750 0xc002ea3751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea37c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea37e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-xqzzc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xqzzc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-xqzzc,UID:64df143a-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35201,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3847 0xc002ea3848}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea38c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea38e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-z7glv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-z7glv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-z7glv,UID:63a05a7e-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35170,Generation:0,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3960 0xc002ea3961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea39e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:172.30.112.88,StartTime:2019-05-03 18:22:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-zb5nl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zb5nl,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-zb5nl,UID:64e2fe0b-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35207,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3af0 0xc002ea3af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-5f9595f595-zmljx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zmljx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-5f9595f595-zmljx,UID:63a69354-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35176,Generation:0,CreationTimestamp:2019-05-03 18:22:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 639d813d-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3be7 0xc002ea3be8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:20 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:172.30.112.89,StartTime:2019-05-03 18:22:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-6f478d8d8-27mgv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-27mgv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-27mgv,UID:6131a959-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35049,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3d70 0xc002ea3d71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3de0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3e00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.140,PodIP:172.30.111.250,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://44cd4cc50447208e2bf0cd0e9b154562e62f14ac38797a78d38301eba45296ec}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-6f478d8d8-579tr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-579tr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-579tr,UID:64e15bae-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35202,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3ed7 0xc002ea3ed8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002ea3f40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002ea3f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-6f478d8d8-5tqxb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5tqxb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-5tqxb,UID:64e0e936-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35213,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc002ea3fc7 0xc002ea3fc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.496: INFO: Pod "nginx-deployment-6f478d8d8-6kxjv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6kxjv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-6kxjv,UID:61333e95-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35033,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e0e0 0xc00211e0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:17 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:17 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:172.30.112.84,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://bb9638c915ba155cd91fbf09325c7bc31c363def53a0f606a58033f75cc2a21d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-btvsg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-btvsg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-btvsg,UID:6134f9d2-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35046,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e247 0xc00211e248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e2c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e2e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:172.30.112.85,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://29c866a677a3f612fbd2385f6463598d286df3615b2684aef1674993dac4959b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-g6drc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g6drc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-g6drc,UID:64df3ab5-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35195,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e3b7 0xc00211e3b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e430} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e450}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-g999r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g999r,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-g999r,UID:64df3e9c-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35200,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e4e0 0xc00211e4e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-h74lj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h74lj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-h74lj,UID:64e0e3bf-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35199,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e5f0 0xc00211e5f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-hg28z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hg28z,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-hg28z,UID:6134f484-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35060,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e6e7 0xc00211e6e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.143,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://01b7bdd5b819ab114f004850c6ab6a33926eb9bd4c62f2a3dc6830ea469ce40b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-jx9gf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jx9gf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-jx9gf,UID:613352fd-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35063,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e867 0xc00211e868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211e8e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211e900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.142,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://95a77b3900b8d8b431b237d6544ec2814c3083c8c7c2d7baed61970470c1fb13}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-l5l95" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l5l95,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-l5l95,UID:6134fa43-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35052,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211e9d7 0xc00211e9d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211ea60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211ea80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.140,PodIP:172.30.111.252,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://f39424cb71385457d68d5abc2b1f16a743d7ee1992ec7f6a7c959fbc7674bee0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.497: INFO: Pod "nginx-deployment-6f478d8d8-rj6zc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rj6zc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-rj6zc,UID:613705d5-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35066,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211eb57 0xc00211eb58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211ebd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211ebf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.144,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://1a02bff258b580bd0f05fa75ee0e06de314c038231a0196c8dcf9c1a6744d5c2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.498: INFO: Pod "nginx-deployment-6f478d8d8-t558q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t558q,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-t558q,UID:64e0d66e-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35211,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211ecc7 0xc00211ecc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211ed40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211ed60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.498: INFO: Pod "nginx-deployment-6f478d8d8-wxxtk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wxxtk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-wxxtk,UID:64dde544-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35205,Generation:0,CreationTimestamp:2019-05-03 18:22:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211ede0 0xc00211ede1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211ee50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211ee70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:22 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:,StartTime:2019-05-03 18:22:22 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May  3 18:22:22.498: INFO: Pod "nginx-deployment-6f478d8d8-xfxch" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xfxch,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6610,SelfLink:/api/v1/namespaces/deployment-6610/pods/nginx-deployment-6f478d8d8-xfxch,UID:6134e748-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:35055,Generation:0,CreationTimestamp:2019-05-03 18:22:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 612f8a30-6dd0-11e9-a061-0e68f10e50c0 0xc00211ef37 0xc00211ef38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r8m9c {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r8m9c,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r8m9c true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.140,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00211efb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00211efd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:22:16 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.140,PodIP:172.30.111.251,StartTime:2019-05-03 18:22:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-03 18:22:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://54fa6002c3ec4cf07a3d3df3974134e1407f9870a9761169354b7b6a8f3f19fc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:22:22.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6610" for this suite.
May  3 18:22:30.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:22:30.851: INFO: namespace deployment-6610 deletion completed in 8.344910302s

â€¢ [SLOW TEST:14.786 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:22:30.852: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7525
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788
May  3 18:22:31.063: INFO: Pod name my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788: Found 0 pods out of 1
May  3 18:22:36.072: INFO: Pod name my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788: Found 1 pods out of 1
May  3 18:22:36.072: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788" are running
May  3 18:22:36.081: INFO: Pod "my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788-qcbm8" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 18:22:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 18:22:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 18:22:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 18:22:31 +0000 UTC Reason: Message:}])
May  3 18:22:36.081: INFO: Trying to dial the pod
May  3 18:22:41.110: INFO: Controller my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788: Got expected result from replica 1 [my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788-qcbm8]: "my-hostname-basic-69ff5529-6dd0-11e9-8617-baa18b820788-qcbm8", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:22:41.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7525" for this suite.
May  3 18:22:47.140: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:22:47.441: INFO: namespace replication-controller-7525 deletion completed in 6.322340227s

â€¢ [SLOW TEST:16.589 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:22:47.442: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2008
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:22:47.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 version'
May  3 18:22:47.731: INFO: stderr: ""
May  3 18:22:47.731: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1+IKS\", GitCommit:\"e168d36ce79fe419a823835dbbdb335a1d5887ad\", GitTreeState:\"clean\", BuildDate:\"2019-04-29T12:06:40Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:22:47.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2008" for this suite.
May  3 18:22:53.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:22:53.991: INFO: namespace kubectl-2008 deletion completed in 6.251211678s

â€¢ [SLOW TEST:6.549 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:22:53.991: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May  3 18:22:54.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 --namespace=kubectl-5436 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May  3 18:22:56.663: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May  3 18:22:56.663: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:22:58.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5436" for this suite.
May  3 18:23:04.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:23:04.976: INFO: namespace kubectl-5436 deletion completed in 6.291142016s

â€¢ [SLOW TEST:10.985 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:23:04.977: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1529
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:23:30.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1529" for this suite.
May  3 18:23:36.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:23:37.058: INFO: namespace container-runtime-1529 deletion completed in 6.315399364s

â€¢ [SLOW TEST:32.081 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:23:37.058: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8471
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:23:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8471" for this suite.
May  3 18:23:59.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:23:59.565: INFO: namespace pods-8471 deletion completed in 22.289664088s

â€¢ [SLOW TEST:22.507 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:23:59.569: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4246
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May  3 18:23:59.780: INFO: Waiting up to 5m0s for pod "pod-9ee019a6-6dd0-11e9-8617-baa18b820788" in namespace "emptydir-4246" to be "success or failure"
May  3 18:23:59.790: INFO: Pod "pod-9ee019a6-6dd0-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.612176ms
May  3 18:24:01.798: INFO: Pod "pod-9ee019a6-6dd0-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.017816554s
May  3 18:24:03.807: INFO: Pod "pod-9ee019a6-6dd0-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026394121s
STEP: Saw pod success
May  3 18:24:03.807: INFO: Pod "pod-9ee019a6-6dd0-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:24:03.814: INFO: Trying to get logs from node 10.190.144.133 pod pod-9ee019a6-6dd0-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 18:24:04.018: INFO: Waiting for pod pod-9ee019a6-6dd0-11e9-8617-baa18b820788 to disappear
May  3 18:24:04.026: INFO: Pod pod-9ee019a6-6dd0-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:24:04.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4246" for this suite.
May  3 18:24:10.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:24:10.341: INFO: namespace emptydir-4246 deletion completed in 6.305901647s

â€¢ [SLOW TEST:10.772 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:24:10.341: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2967
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:24:10.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 version --client'
May  3 18:24:10.580: INFO: stderr: ""
May  3 18:24:10.580: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May  3 18:24:10.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-2967'
May  3 18:24:10.783: INFO: stderr: ""
May  3 18:24:10.783: INFO: stdout: "replicationcontroller/redis-master created\n"
May  3 18:24:10.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-2967'
May  3 18:24:10.992: INFO: stderr: ""
May  3 18:24:10.992: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May  3 18:24:12.002: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:24:12.002: INFO: Found 0 / 1
May  3 18:24:13.000: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:24:13.000: INFO: Found 1 / 1
May  3 18:24:13.000: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  3 18:24:13.007: INFO: Selector matched 1 pods for map[app:redis]
May  3 18:24:13.007: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  3 18:24:13.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 describe pod redis-master-r7bx2 --namespace=kubectl-2967'
May  3 18:24:13.134: INFO: stderr: ""
May  3 18:24:13.134: INFO: stdout: "Name:               redis-master-r7bx2\nNamespace:          kubectl-2967\nPriority:           0\nPriorityClassName:  <none>\nNode:               10.190.144.141/10.190.144.141\nStart Time:         Fri, 03 May 2019 18:24:10 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        kubernetes.io/psp: e2e-test-privileged-psp\nStatus:             Running\nIP:                 172.30.47.155\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://afd469eded530ad6122a4615826f2c6f670dbd568359d0bebbfd15ff55849de8\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 03 May 2019 18:24:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-jclqm (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-jclqm:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-jclqm\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  3s    default-scheduler        Successfully assigned kubectl-2967/redis-master-r7bx2 to 10.190.144.141\n  Normal  Pulled     2s    kubelet, 10.190.144.141  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, 10.190.144.141  Created container redis-master\n  Normal  Started    2s    kubelet, 10.190.144.141  Started container redis-master\n"
May  3 18:24:13.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 describe rc redis-master --namespace=kubectl-2967'
May  3 18:24:13.285: INFO: stderr: ""
May  3 18:24:13.285: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2967\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-r7bx2\n"
May  3 18:24:13.285: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 describe service redis-master --namespace=kubectl-2967'
May  3 18:24:13.416: INFO: stderr: ""
May  3 18:24:13.416: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2967\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.21.109.230\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.30.47.155:6379\nSession Affinity:  None\nEvents:            <none>\n"
May  3 18:24:13.425: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 describe node 10.190.144.133'
May  3 18:24:13.591: INFO: stderr: ""
May  3 18:24:13.591: INFO: stdout: "Name:               10.190.144.133\nRoles:              <none>\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=us-east\n                    failure-domain.beta.kubernetes.io/zone=wdc07\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=169.62.56.73\n                    ibm-cloud.kubernetes.io/ha-worker=true\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.190.144.133\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=UBUNTU_18_64\n                    ibm-cloud.kubernetes.io/region=us-east\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-wdc07-crc2175f9816074c889cbbe7987e82ffd7-w1\n                    ibm-cloud.kubernetes.io/worker-pool-id=c2175f9816074c889cbbe7987e82ffd7-1da2c5f\n                    ibm-cloud.kubernetes.io/worker-version=1.14.1_1516\n                    ibm-cloud.kubernetes.io/zone=wdc07\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.190.144.133\n                    kubernetes.io/os=linux\n                    privateVLAN=2608837\n                    publicVLAN=2608835\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 03 May 2019 14:28:02 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 03 May 2019 18:24:10 +0000   Fri, 03 May 2019 14:28:02 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 03 May 2019 18:24:10 +0000   Fri, 03 May 2019 14:28:02 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 03 May 2019 18:24:10 +0000   Fri, 03 May 2019 14:28:02 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 03 May 2019 18:24:10 +0000   Fri, 03 May 2019 14:28:22 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.190.144.133\n  ExternalIP:  169.62.56.73\n  Hostname:    10.190.144.133\nCapacity:\n cpu:                4\n ephemeral-storage:  102685624Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16419940Ki\n pods:               110\nAllocatable:\n cpu:                3910m\n ephemeral-storage:  99892574949\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             13627492Ki\n pods:               110\nSystem Info:\n Machine ID:                 3f5f62a0b622423da4acc025066fef2a\n System UUID:                D7A3F885-38EF-26FC-3838-869B2C6C450E\n Boot ID:                    5f531609-e090-4efb-90b6-a9d6f6f15ddc\n Kernel Version:             4.15.0-47-generic\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.6\n Kubelet Version:            v1.14.1+IKS\n Kube-Proxy Version:         v1.14.1+IKS\nProviderID:                  ibm://d18c889395112a40d2f4e3065f237a7d///c2175f9816074c889cbbe7987e82ffd7/kube-wdc07-crc2175f9816074c889cbbe7987e82ffd7-w1\nNon-terminated Pods:         (14 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    test-k8s-e2e-pvg-master-verification                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         133m\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-52cca3793892431e-nms5m    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                calico-kube-controllers-5cf89ccdcb-4jsv6                   10m (0%)      0 (0%)      25Mi (0%)        0 (0%)         4h1m\n  kube-system                calico-node-fkw9t                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         3h56m\n  kube-system                coredns-autoscaler-6d8b6f867-v8hwf                         20m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h1m\n  kube-system                coredns-d5dcb59f8-9j6zs                                    100m (2%)     0 (0%)      70Mi (0%)        400Mi (3%)     4h1m\n  kube-system                ibm-file-plugin-547c887746-s8jqz                           50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         4h\n  kube-system                ibm-keepalived-watcher-plptw                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h56m\n  kube-system                ibm-kube-fluentd-hbqql                                     25m (0%)      300m (7%)   50Mi (0%)        1600M (11%)    3h55m\n  kube-system                ibm-master-proxy-static-10.190.144.133                     25m (0%)      300m (7%)   32M (0%)         512M (3%)      3h56m\n  kube-system                ibm-storage-watcher-84958d564f-9xnvg                       50m (1%)      200m (5%)   100Mi (0%)       0 (0%)         4h\n  kube-system                kubernetes-dashboard-6c88b75685-pc4kh                      50m (1%)      0 (0%)      100Mi (0%)       0 (0%)         3h59m\n  kube-system                vpn-6df9c5c5bb-fj8dg                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h59m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests       Limits\n  --------           --------       ------\n  cpu                590m (15%)     1 (25%)\n  memory             594450Ki (4%)  2472100Ki (18%)\n  ephemeral-storage  0 (0%)         0 (0%)\nEvents:              <none>\n"
May  3 18:24:13.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 describe namespace kubectl-2967'
May  3 18:24:13.716: INFO: stderr: ""
May  3 18:24:13.716: INFO: stdout: "Name:         kubectl-2967\nLabels:       e2e-framework=kubectl\n              e2e-run=d7c46e91-6dcd-11e9-8617-baa18b820788\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:24:13.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2967" for this suite.
May  3 18:24:37.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:24:37.977: INFO: namespace kubectl-2967 deletion completed in 24.253496405s

â€¢ [SLOW TEST:27.636 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:24:37.977: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3700
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 18:24:38.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3700'
May  3 18:24:38.339: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  3 18:24:38.339: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May  3 18:24:40.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3700'
May  3 18:24:40.527: INFO: stderr: ""
May  3 18:24:40.527: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:24:40.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3700" for this suite.
May  3 18:25:02.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:25:02.847: INFO: namespace kubectl-3700 deletion completed in 22.311266361s

â€¢ [SLOW TEST:24.870 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:25:02.848: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May  3 18:25:03.058: INFO: Waiting up to 5m0s for pod "pod-c49814b0-6dd0-11e9-8617-baa18b820788" in namespace "emptydir-3470" to be "success or failure"
May  3 18:25:03.069: INFO: Pod "pod-c49814b0-6dd0-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 10.634662ms
May  3 18:25:05.078: INFO: Pod "pod-c49814b0-6dd0-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019396015s
May  3 18:25:07.086: INFO: Pod "pod-c49814b0-6dd0-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027859461s
STEP: Saw pod success
May  3 18:25:07.086: INFO: Pod "pod-c49814b0-6dd0-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:25:07.094: INFO: Trying to get logs from node 10.190.144.133 pod pod-c49814b0-6dd0-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 18:25:07.136: INFO: Waiting for pod pod-c49814b0-6dd0-11e9-8617-baa18b820788 to disappear
May  3 18:25:07.142: INFO: Pod pod-c49814b0-6dd0-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:25:07.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3470" for this suite.
May  3 18:25:13.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:25:13.439: INFO: namespace emptydir-3470 deletion completed in 6.287357044s

â€¢ [SLOW TEST:10.591 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:25:13.439: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9730
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  3 18:25:16.353: INFO: Successfully updated pod "pod-update-cae6c28c-6dd0-11e9-8617-baa18b820788"
STEP: verifying the updated pod is in kubernetes
May  3 18:25:16.368: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:25:16.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9730" for this suite.
May  3 18:25:40.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:25:40.644: INFO: namespace pods-9730 deletion completed in 24.267754716s

â€¢ [SLOW TEST:27.205 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:25:40.645: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May  3 18:25:40.842: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:25:44.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3112" for this suite.
May  3 18:25:50.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:25:51.137: INFO: namespace init-container-3112 deletion completed in 6.286764617s

â€¢ [SLOW TEST:10.492 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:25:51.137: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  3 18:25:51.333: INFO: Waiting up to 5m0s for pod "pod-e15e4825-6dd0-11e9-8617-baa18b820788" in namespace "emptydir-3540" to be "success or failure"
May  3 18:25:51.348: INFO: Pod "pod-e15e4825-6dd0-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 15.740612ms
May  3 18:25:53.358: INFO: Pod "pod-e15e4825-6dd0-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02499118s
STEP: Saw pod success
May  3 18:25:53.358: INFO: Pod "pod-e15e4825-6dd0-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:25:53.364: INFO: Trying to get logs from node 10.190.144.133 pod pod-e15e4825-6dd0-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 18:25:53.408: INFO: Waiting for pod pod-e15e4825-6dd0-11e9-8617-baa18b820788 to disappear
May  3 18:25:53.415: INFO: Pod pod-e15e4825-6dd0-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:25:53.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3540" for this suite.
May  3 18:25:59.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:25:59.746: INFO: namespace emptydir-3540 deletion completed in 6.322369518s

â€¢ [SLOW TEST:8.609 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:25:59.747: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May  3 18:25:59.958: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2999,SelfLink:/api/v1/namespaces/watch-2999/configmaps/e2e-watch-test-watch-closed,UID:e681eeba-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:36557,Generation:0,CreationTimestamp:2019-05-03 18:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  3 18:25:59.958: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2999,SelfLink:/api/v1/namespaces/watch-2999/configmaps/e2e-watch-test-watch-closed,UID:e681eeba-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:36558,Generation:0,CreationTimestamp:2019-05-03 18:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May  3 18:25:59.984: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2999,SelfLink:/api/v1/namespaces/watch-2999/configmaps/e2e-watch-test-watch-closed,UID:e681eeba-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:36559,Generation:0,CreationTimestamp:2019-05-03 18:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  3 18:25:59.984: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2999,SelfLink:/api/v1/namespaces/watch-2999/configmaps/e2e-watch-test-watch-closed,UID:e681eeba-6dd0-11e9-a061-0e68f10e50c0,ResourceVersion:36560,Generation:0,CreationTimestamp:2019-05-03 18:25:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:25:59.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2999" for this suite.
May  3 18:26:06.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:26:06.241: INFO: namespace watch-2999 deletion completed in 6.249652668s

â€¢ [SLOW TEST:6.494 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:26:06.242: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1437
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-ea6115cc-6dd0-11e9-8617-baa18b820788
STEP: Creating configMap with name cm-test-opt-upd-ea61161f-6dd0-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ea6115cc-6dd0-11e9-8617-baa18b820788
STEP: Updating configmap cm-test-opt-upd-ea61161f-6dd0-11e9-8617-baa18b820788
STEP: Creating configMap with name cm-test-opt-create-ea61163e-6dd0-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:27:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1437" for this suite.
May  3 18:27:45.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:27:45.700: INFO: namespace projected-1437 deletion completed in 24.243211905s

â€¢ [SLOW TEST:99.458 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:27:45.700: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1435
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May  3 18:27:45.921: INFO: Pod name pod-release: Found 0 pods out of 1
May  3 18:27:50.930: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:27:50.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1435" for this suite.
May  3 18:27:56.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:27:57.217: INFO: namespace replication-controller-1435 deletion completed in 6.25034099s

â€¢ [SLOW TEST:11.517 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:27:57.217: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8127
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May  3 18:27:57.532: INFO: Waiting up to 5m0s for pod "downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788" in namespace "downward-api-8127" to be "success or failure"
May  3 18:27:57.541: INFO: Pod "downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.826035ms
May  3 18:27:59.550: INFO: Pod "downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017924913s
STEP: Saw pod success
May  3 18:27:59.550: INFO: Pod "downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:27:59.558: INFO: Trying to get logs from node 10.190.144.140 pod downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 18:27:59.618: INFO: Waiting for pod downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788 to disappear
May  3 18:27:59.624: INFO: Pod downward-api-2c87b27b-6dd1-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:27:59.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8127" for this suite.
May  3 18:28:05.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:28:05.931: INFO: namespace downward-api-8127 deletion completed in 6.296974668s

â€¢ [SLOW TEST:8.714 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:28:05.931: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 18:28:06.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-4059'
May  3 18:28:06.233: INFO: stderr: ""
May  3 18:28:06.233: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May  3 18:28:11.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pod e2e-test-nginx-pod --namespace=kubectl-4059 -o json'
May  3 18:28:11.381: INFO: stderr: ""
May  3 18:28:11.381: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-05-03T18:28:06Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-4059\",\n        \"resourceVersion\": \"36949\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-4059/pods/e2e-test-nginx-pod\",\n        \"uid\": \"31c5b3ad-6dd1-11e9-a061-0e68f10e50c0\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-s2nd9\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"10.190.144.140\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-s2nd9\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-s2nd9\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-03T18:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-03T18:28:07Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-03T18:28:07Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-03T18:28:06Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://92713c6e24af4c4e03337ec681e88e5b98ac3a3040c63753da39a95da3d0ca8d\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-03T18:28:07Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.190.144.140\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.111.208\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-03T18:28:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
May  3 18:28:11.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 replace -f - --namespace=kubectl-4059'
May  3 18:28:11.648: INFO: stderr: ""
May  3 18:28:11.648: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May  3 18:28:11.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete pods e2e-test-nginx-pod --namespace=kubectl-4059'
May  3 18:28:14.098: INFO: stderr: ""
May  3 18:28:14.098: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:28:14.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4059" for this suite.
May  3 18:28:20.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:28:20.445: INFO: namespace kubectl-4059 deletion completed in 6.336526569s

â€¢ [SLOW TEST:14.514 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:28:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1156
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-3a5e4c22-6dd1-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:28:20.658: INFO: Waiting up to 5m0s for pod "pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788" in namespace "secrets-1156" to be "success or failure"
May  3 18:28:20.665: INFO: Pod "pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.407736ms
May  3 18:28:22.673: INFO: Pod "pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014661912s
May  3 18:28:24.682: INFO: Pod "pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023562865s
STEP: Saw pod success
May  3 18:28:24.682: INFO: Pod "pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:28:24.689: INFO: Trying to get logs from node 10.190.144.133 pod pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 18:28:24.729: INFO: Waiting for pod pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788 to disappear
May  3 18:28:24.737: INFO: Pod pod-secrets-3a5f6155-6dd1-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:28:24.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1156" for this suite.
May  3 18:28:30.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:28:31.166: INFO: namespace secrets-1156 deletion completed in 6.421006506s

â€¢ [SLOW TEST:10.721 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:28:31.166: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9043
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May  3 18:28:31.421: INFO: PodSpec: initContainers in spec.initContainers
May  3 18:29:13.384: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-40cbf3e7-6dd1-11e9-8617-baa18b820788", GenerateName:"", Namespace:"init-container-9043", SelfLink:"/api/v1/namespaces/init-container-9043/pods/pod-init-40cbf3e7-6dd1-11e9-8617-baa18b820788", UID:"40cd69b5-6dd1-11e9-a061-0e68f10e50c0", ResourceVersion:"37159", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63692504911, loc:(*time.Location)(0x8a060e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"421136193"}, Annotations:map[string]string{"kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6gjc7", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc001dfae40), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6gjc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6gjc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6gjc7", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027bb598), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.190.144.140", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0012dfe60), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027bb620)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027bb640)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027bb648), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027bb64c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692504911, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692504911, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692504911, loc:(*time.Location)(0x8a060e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692504911, loc:(*time.Location)(0x8a060e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.190.144.140", PodIP:"172.30.111.209", StartTime:(*v1.Time)(0xc002389560), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(0xc0023895a0), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001fa2770)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://6b04ae5c5c79046403916c1cf3f2361ecfc1d5ebcc6982cdd37447ddb8176e8f"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0023895c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002389580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:29:13.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9043" for this suite.
May  3 18:29:35.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:29:35.732: INFO: namespace init-container-9043 deletion completed in 22.338057633s

â€¢ [SLOW TEST:64.565 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:29:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May  3 18:29:35.917: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:29:40.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3644" for this suite.
May  3 18:30:04.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:30:04.743: INFO: namespace init-container-3644 deletion completed in 24.276169988s

â€¢ [SLOW TEST:29.012 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:30:04.744: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8517
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0503 18:30:15.081135      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 18:30:15.081: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:30:15.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8517" for this suite.
May  3 18:30:21.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:30:21.344: INFO: namespace gc-8517 deletion completed in 6.255731014s

â€¢ [SLOW TEST:16.600 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:30:21.344: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7686
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  3 18:30:21.620: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  3 18:30:41.998: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.212:8080/dial?request=hostName&protocol=http&host=172.30.47.158&port=8080&tries=1'] Namespace:pod-network-test-7686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:30:41.998: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:30:42.176: INFO: Waiting for endpoints: map[]
May  3 18:30:42.184: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.212:8080/dial?request=hostName&protocol=http&host=172.30.111.211&port=8080&tries=1'] Namespace:pod-network-test-7686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:30:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:30:42.381: INFO: Waiting for endpoints: map[]
May  3 18:30:42.389: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.111.212:8080/dial?request=hostName&protocol=http&host=172.30.112.106&port=8080&tries=1'] Namespace:pod-network-test-7686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:30:42.389: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:30:42.668: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:30:42.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7686" for this suite.
May  3 18:31:06.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:31:06.936: INFO: namespace pod-network-test-7686 deletion completed in 24.258260729s

â€¢ [SLOW TEST:45.593 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:31:06.938: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-76
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  3 18:31:07.128: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  3 18:31:07.233: INFO: Waiting for terminating namespaces to be deleted...
May  3 18:31:07.239: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.133 before test
May  3 18:31:07.266: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-03 18:03:50 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  3 18:31:07.266: INFO: ibm-master-proxy-static-10.190.144.133 from kube-system started at <nil> (0 container statuses recorded)
May  3 18:31:07.266: INFO: ibm-file-plugin-547c887746-s8jqz from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  3 18:31:07.266: INFO: coredns-d5dcb59f8-9j6zs from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container coredns ready: true, restart count 0
May  3 18:31:07.266: INFO: ibm-kube-fluentd-hbqql from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container fluentd ready: true, restart count 0
May  3 18:31:07.266: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-nms5m from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 18:31:07.266: INFO: 	Container systemd-logs ready: true, restart count 0
May  3 18:31:07.266: INFO: calico-node-fkw9t from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.266: INFO: 	Container calico-node ready: true, restart count 0
May  3 18:31:07.266: INFO: kubernetes-dashboard-6c88b75685-pc4kh from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  3 18:31:07.267: INFO: ibm-storage-watcher-84958d564f-9xnvg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  3 18:31:07.267: INFO: vpn-6df9c5c5bb-fj8dg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container vpn ready: true, restart count 0
May  3 18:31:07.267: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-05-03 16:10:24 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  3 18:31:07.267: INFO: coredns-autoscaler-6d8b6f867-v8hwf from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container autoscaler ready: true, restart count 0
May  3 18:31:07.267: INFO: ibm-keepalived-watcher-plptw from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 18:31:07.267: INFO: calico-kube-controllers-5cf89ccdcb-4jsv6 from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.267: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  3 18:31:07.267: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.140 before test
May  3 18:31:07.294: INFO: coredns-d5dcb59f8-gqs2d from kube-system started at 2019-05-03 14:28:49 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container coredns ready: true, restart count 0
May  3 18:31:07.294: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-8hdkw from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
May  3 18:31:07.294: INFO: calico-node-wrx89 from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container calico-node ready: true, restart count 0
May  3 18:31:07.294: INFO: metrics-server-675d59648f-rhfdj from kube-system started at 2019-05-03 14:30:15 +0000 UTC (2 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container metrics-server ready: true, restart count 0
May  3 18:31:07.294: INFO: 	Container metrics-server-nanny ready: true, restart count 0
May  3 18:31:07.294: INFO: ibm-master-proxy-static-10.190.144.140 from kube-system started at <nil> (0 container statuses recorded)
May  3 18:31:07.294: INFO: ibm-keepalived-watcher-7w57c from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 18:31:07.294: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-c8k9z from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 18:31:07.294: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 18:31:07.294: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 18:31:07.294: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 18:31:07.294: INFO: ibm-kube-fluentd-pp6bq from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container fluentd ready: true, restart count 0
May  3 18:31:07.294: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-248dv from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 18:31:07.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 18:31:07.294: INFO: 	Container systemd-logs ready: true, restart count 0
May  3 18:31:07.294: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.141 before test
May  3 18:31:07.349: INFO: sonobuoy-e2e-job-e0d8049d571e43ef from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container e2e ready: true, restart count 0
May  3 18:31:07.349: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 18:31:07.349: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-494vt from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 18:31:07.349: INFO: 	Container systemd-logs ready: true, restart count 0
May  3 18:31:07.349: INFO: calico-node-9wqc8 from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container calico-node ready: true, restart count 0
May  3 18:31:07.349: INFO: ibm-kube-fluentd-tlcsj from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container fluentd ready: true, restart count 0
May  3 18:31:07.349: INFO: ibm-keepalived-watcher-wccsc from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 18:31:07.349: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-59k8h from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 18:31:07.349: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 18:31:07.349: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 18:31:07.349: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 18:31:07.349: INFO: ibm-master-proxy-static-10.190.144.141 from kube-system started at <nil> (0 container statuses recorded)
May  3 18:31:07.349: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-d6x9d from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 18:31:07.349: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9ef82eeb-6dd1-11e9-8617-baa18b820788 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9ef82eeb-6dd1-11e9-8617-baa18b820788 off the node 10.190.144.141
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9ef82eeb-6dd1-11e9-8617-baa18b820788
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:31:11.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-76" for this suite.
May  3 18:31:33.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:31:33.849: INFO: namespace sched-pred-76 deletion completed in 22.348431849s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:26.912 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:31:33.850: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4112
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4112
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4112
May  3 18:31:34.067: INFO: Found 0 stateful pods, waiting for 1
May  3 18:31:44.077: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May  3 18:31:44.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:31:44.398: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:31:44.398: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:31:44.398: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:31:44.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May  3 18:31:54.416: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:31:54.416: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:31:54.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999899s
May  3 18:31:55.455: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.991856408s
May  3 18:31:56.464: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983760332s
May  3 18:31:57.474: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.974674867s
May  3 18:31:58.483: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965164559s
May  3 18:31:59.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.956070084s
May  3 18:32:00.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.947512605s
May  3 18:32:01.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.938883544s
May  3 18:32:02.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.913403278s
May  3 18:32:03.541: INFO: Verifying statefulset ss doesn't scale past 1 for another 905.405555ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4112
May  3 18:32:04.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:32:04.933: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:32:04.933: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:32:04.933: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:32:04.941: INFO: Found 1 stateful pods, waiting for 3
May  3 18:32:14.952: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:32:14.952: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May  3 18:32:14.952: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May  3 18:32:14.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:32:15.296: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:32:15.296: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:32:15.296: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:32:15.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:32:15.582: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:32:15.582: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:32:15.582: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:32:15.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May  3 18:32:15.883: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May  3 18:32:15.883: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May  3 18:32:15.883: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May  3 18:32:15.883: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:32:15.891: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
May  3 18:32:25.908: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:32:25.908: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:32:25.908: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May  3 18:32:25.934: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998222s
May  3 18:32:27.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.9920395s
May  3 18:32:28.033: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.908632723s
May  3 18:32:29.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.89310264s
May  3 18:32:30.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.88437315s
May  3 18:32:31.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.875547326s
May  3 18:32:32.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.867177046s
May  3 18:32:33.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.856912686s
May  3 18:32:34.087: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.847765353s
May  3 18:32:35.096: INFO: Verifying statefulset ss doesn't scale past 3 for another 839.833899ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4112
May  3 18:32:36.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:32:36.395: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:32:36.395: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:32:36.395: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:32:36.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:32:36.698: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:32:36.698: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:32:36.698: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:32:36.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 exec --namespace=statefulset-4112 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May  3 18:32:37.001: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May  3 18:32:37.001: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May  3 18:32:37.001: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May  3 18:32:37.001: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  3 18:32:57.043: INFO: Deleting all statefulset in ns statefulset-4112
May  3 18:32:57.050: INFO: Scaling statefulset ss to 0
May  3 18:32:57.076: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:32:57.084: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:32:57.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4112" for this suite.
May  3 18:33:03.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:33:03.406: INFO: namespace statefulset-4112 deletion completed in 6.239623531s

â€¢ [SLOW TEST:89.556 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:33:03.408: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:33:03.618: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788" in namespace "projected-7906" to be "success or failure"
May  3 18:33:03.625: INFO: Pod "downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.387831ms
May  3 18:33:05.718: INFO: Pod "downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.100203201s
May  3 18:33:07.728: INFO: Pod "downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.109905058s
STEP: Saw pod success
May  3 18:33:07.728: INFO: Pod "downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:33:07.735: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:33:07.818: INFO: Waiting for pod downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788 to disappear
May  3 18:33:07.826: INFO: Pod downwardapi-volume-e3075b2c-6dd1-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:33:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7906" for this suite.
May  3 18:33:13.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:33:14.124: INFO: namespace projected-7906 deletion completed in 6.289651364s

â€¢ [SLOW TEST:10.716 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:33:14.124: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2071
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-e96a5933-6dd1-11e9-8617-baa18b820788
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:33:14.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2071" for this suite.
May  3 18:33:20.353: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:33:20.648: INFO: namespace configmap-2071 deletion completed in 6.321554467s

â€¢ [SLOW TEST:6.524 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:33:20.649: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-6846
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:33:23.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6846" for this suite.
May  3 18:33:46.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:33:46.344: INFO: namespace replication-controller-6846 deletion completed in 22.385303602s

â€¢ [SLOW TEST:25.696 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:33:46.345: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5054
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  3 18:33:50.630: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:33:50.640: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:33:52.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:33:52.650: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:33:54.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:33:54.659: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:33:56.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:33:56.649: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:33:58.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:33:58.650: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:00.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:00.649: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:02.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:02.653: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:04.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:04.650: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:06.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:06.650: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:08.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:08.648: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:10.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:10.657: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:12.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:12.649: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:14.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:14.653: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:16.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:16.650: INFO: Pod pod-with-prestop-exec-hook still exists
May  3 18:34:18.641: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May  3 18:34:18.659: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:34:18.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5054" for this suite.
May  3 18:34:40.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:34:40.946: INFO: namespace container-lifecycle-hook-5054 deletion completed in 22.259028241s

â€¢ [SLOW TEST:54.601 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:34:40.947: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-1d36d9cc-6dd2-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:34:41.242: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788" in namespace "projected-9378" to be "success or failure"
May  3 18:34:41.249: INFO: Pod "pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.923411ms
May  3 18:34:43.258: INFO: Pod "pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.015647452s
May  3 18:34:45.266: INFO: Pod "pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024156908s
STEP: Saw pod success
May  3 18:34:45.266: INFO: Pod "pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:34:45.274: INFO: Trying to get logs from node 10.190.144.133 pod pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  3 18:34:45.313: INFO: Waiting for pod pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:34:45.320: INFO: Pod pod-projected-secrets-1d3801c0-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:34:45.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9378" for this suite.
May  3 18:34:51.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:34:51.626: INFO: namespace projected-9378 deletion completed in 6.297942072s

â€¢ [SLOW TEST:10.680 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:34:51.627: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8999
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-8999/configmap-test-2386a39e-6dd2-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:34:51.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788" in namespace "configmap-8999" to be "success or failure"
May  3 18:34:51.838: INFO: Pod "pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.530856ms
May  3 18:34:53.846: INFO: Pod "pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.014852537s
May  3 18:34:55.951: INFO: Pod "pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.119745662s
STEP: Saw pod success
May  3 18:34:55.951: INFO: Pod "pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:34:55.958: INFO: Trying to get logs from node 10.190.144.140 pod pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788 container env-test: <nil>
STEP: delete the pod
May  3 18:34:55.999: INFO: Waiting for pod pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:34:56.006: INFO: Pod pod-configmaps-2387d949-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:34:56.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8999" for this suite.
May  3 18:35:02.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:35:02.341: INFO: namespace configmap-8999 deletion completed in 6.32754098s

â€¢ [SLOW TEST:10.714 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:35:02.341: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-29eaf890-6dd2-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:35:02.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788" in namespace "projected-5141" to be "success or failure"
May  3 18:35:02.565: INFO: Pod "pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.130531ms
May  3 18:35:04.585: INFO: Pod "pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.029921703s
STEP: Saw pod success
May  3 18:35:04.586: INFO: Pod "pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:35:04.594: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  3 18:35:04.718: INFO: Waiting for pod pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:35:04.726: INFO: Pod pod-projected-secrets-29ec159d-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:35:04.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5141" for this suite.
May  3 18:35:10.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:35:11.048: INFO: namespace projected-5141 deletion completed in 6.314497944s

â€¢ [SLOW TEST:8.707 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:35:11.049: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8041
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-8041
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8041 to expose endpoints map[]
May  3 18:35:11.261: INFO: Get endpoints failed (7.698696ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
May  3 18:35:12.269: INFO: successfully validated that service endpoint-test2 in namespace services-8041 exposes endpoints map[] (1.015921677s elapsed)
STEP: Creating pod pod1 in namespace services-8041
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8041 to expose endpoints map[pod1:[80]]
May  3 18:35:14.335: INFO: successfully validated that service endpoint-test2 in namespace services-8041 exposes endpoints map[pod1:[80]] (2.049251819s elapsed)
STEP: Creating pod pod2 in namespace services-8041
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8041 to expose endpoints map[pod1:[80] pod2:[80]]
May  3 18:35:16.533: INFO: successfully validated that service endpoint-test2 in namespace services-8041 exposes endpoints map[pod1:[80] pod2:[80]] (2.188293425s elapsed)
STEP: Deleting pod pod1 in namespace services-8041
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8041 to expose endpoints map[pod2:[80]]
May  3 18:35:17.737: INFO: successfully validated that service endpoint-test2 in namespace services-8041 exposes endpoints map[pod2:[80]] (1.037062479s elapsed)
STEP: Deleting pod pod2 in namespace services-8041
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8041 to expose endpoints map[]
May  3 18:35:18.767: INFO: successfully validated that service endpoint-test2 in namespace services-8041 exposes endpoints map[] (1.015482954s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:35:18.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8041" for this suite.
May  3 18:35:24.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:35:25.132: INFO: namespace services-8041 deletion completed in 6.314031988s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:14.083 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:35:25.133: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-154
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:35:25.331: INFO: Waiting up to 5m0s for pod "downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788" in namespace "downward-api-154" to be "success or failure"
May  3 18:35:25.338: INFO: Pod "downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.6571ms
May  3 18:35:27.347: INFO: Pod "downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015547127s
STEP: Saw pod success
May  3 18:35:27.347: INFO: Pod "downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:35:27.353: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:35:27.396: INFO: Waiting for pod downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:35:27.405: INFO: Pod downwardapi-volume-377f5f6b-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:35:27.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-154" for this suite.
May  3 18:35:33.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:35:33.735: INFO: namespace downward-api-154 deletion completed in 6.322072305s

â€¢ [SLOW TEST:8.602 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:35:33.735: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7680
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-3ca194c3-6dd2-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:35:34.033: INFO: Waiting up to 5m0s for pod "pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788" in namespace "configmap-7680" to be "success or failure"
May  3 18:35:34.041: INFO: Pod "pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.659185ms
May  3 18:35:36.050: INFO: Pod "pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016814049s
STEP: Saw pod success
May  3 18:35:36.050: INFO: Pod "pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:35:36.059: INFO: Trying to get logs from node 10.190.144.133 pod pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:35:36.099: INFO: Waiting for pod pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:35:36.105: INFO: Pod pod-configmaps-3caf24ef-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:35:36.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7680" for this suite.
May  3 18:35:42.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:35:42.442: INFO: namespace configmap-7680 deletion completed in 6.328294904s

â€¢ [SLOW TEST:8.707 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:35:42.444: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-8735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8735
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-8735
STEP: Creating statefulset with conflicting port in namespace statefulset-8735
STEP: Waiting until pod test-pod will start running in namespace statefulset-8735
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8735
May  3 18:35:44.777: INFO: Observed stateful pod in namespace: statefulset-8735, name: ss-0, uid: 41f51a94-6dd2-11e9-a061-0e68f10e50c0, status phase: Pending. Waiting for statefulset controller to delete.
May  3 18:35:51.089: INFO: Observed stateful pod in namespace: statefulset-8735, name: ss-0, uid: 41f51a94-6dd2-11e9-a061-0e68f10e50c0, status phase: Failed. Waiting for statefulset controller to delete.
May  3 18:35:51.102: INFO: Observed stateful pod in namespace: statefulset-8735, name: ss-0, uid: 41f51a94-6dd2-11e9-a061-0e68f10e50c0, status phase: Failed. Waiting for statefulset controller to delete.
May  3 18:35:51.112: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8735
STEP: Removing pod with conflicting port in namespace statefulset-8735
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8735 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  3 18:35:53.159: INFO: Deleting all statefulset in ns statefulset-8735
May  3 18:35:53.166: INFO: Scaling statefulset ss to 0
May  3 18:36:03.243: INFO: Waiting for statefulset status.replicas updated to 0
May  3 18:36:03.250: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:36:03.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8735" for this suite.
May  3 18:36:09.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:36:09.737: INFO: namespace statefulset-8735 deletion completed in 6.333930332s

â€¢ [SLOW TEST:27.293 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:36:09.737: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 18:36:10.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-4900'
May  3 18:36:10.225: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  3 18:36:10.225: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May  3 18:36:14.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4900'
May  3 18:36:14.360: INFO: stderr: ""
May  3 18:36:14.360: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:36:14.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4900" for this suite.
May  3 18:36:20.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:36:20.625: INFO: namespace kubectl-4900 deletion completed in 6.255436627s

â€¢ [SLOW TEST:10.888 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:36:20.625: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5978
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-58a52afe-6dd2-11e9-8617-baa18b820788
STEP: Creating configMap with name cm-test-opt-upd-58a52b4e-6dd2-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-58a52afe-6dd2-11e9-8617-baa18b820788
STEP: Updating configmap cm-test-opt-upd-58a52b4e-6dd2-11e9-8617-baa18b820788
STEP: Creating configMap with name cm-test-opt-create-58a52b6a-6dd2-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:37:46.040: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5978" for this suite.
May  3 18:38:10.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:38:10.342: INFO: namespace configmap-5978 deletion completed in 24.294651442s

â€¢ [SLOW TEST:109.717 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:38:10.342: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-82
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May  3 18:38:10.542: INFO: Waiting up to 5m0s for pod "client-containers-99f871c3-6dd2-11e9-8617-baa18b820788" in namespace "containers-82" to be "success or failure"
May  3 18:38:10.549: INFO: Pod "client-containers-99f871c3-6dd2-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.889801ms
May  3 18:38:12.557: INFO: Pod "client-containers-99f871c3-6dd2-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015477355s
STEP: Saw pod success
May  3 18:38:12.558: INFO: Pod "client-containers-99f871c3-6dd2-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:38:12.565: INFO: Trying to get logs from node 10.190.144.140 pod client-containers-99f871c3-6dd2-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 18:38:12.611: INFO: Waiting for pod client-containers-99f871c3-6dd2-11e9-8617-baa18b820788 to disappear
May  3 18:38:12.617: INFO: Pod client-containers-99f871c3-6dd2-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:38:12.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-82" for this suite.
May  3 18:38:18.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:38:18.940: INFO: namespace containers-82 deletion completed in 6.313973874s

â€¢ [SLOW TEST:8.598 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:38:18.940: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:38:19.122: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:38:21.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4720" for this suite.
May  3 18:39:01.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:39:01.649: INFO: namespace pods-4720 deletion completed in 40.271643555s

â€¢ [SLOW TEST:42.708 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:39:01.649: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-45
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0503 18:39:42.018394      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 18:39:42.018: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:39:42.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-45" for this suite.
May  3 18:39:50.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:39:50.262: INFO: namespace gc-45 deletion completed in 8.236920706s

â€¢ [SLOW TEST:48.613 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:39:50.264: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8757
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  3 18:39:50.447: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  3 18:40:14.631: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.47.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8757 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:40:14.631: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:40:15.818: INFO: Found all expected endpoints: [netserver-0]
May  3 18:40:15.826: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.111.223 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8757 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:40:15.826: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:40:16.991: INFO: Found all expected endpoints: [netserver-1]
May  3 18:40:17.004: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.112.72 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8757 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:40:17.004: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:40:18.199: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:40:18.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8757" for this suite.
May  3 18:40:42.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:40:42.455: INFO: namespace pod-network-test-8757 deletion completed in 24.247361515s

â€¢ [SLOW TEST:52.191 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:40:42.456: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-417
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:40:42.696: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May  3 18:40:42.717: INFO: Number of nodes with available pods: 0
May  3 18:40:42.717: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:40:43.734: INFO: Number of nodes with available pods: 0
May  3 18:40:43.734: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:40:44.826: INFO: Number of nodes with available pods: 2
May  3 18:40:44.826: INFO: Node 10.190.144.140 is running more than one daemon pod
May  3 18:40:45.736: INFO: Number of nodes with available pods: 3
May  3 18:40:45.736: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May  3 18:40:45.790: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:45.790: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:45.791: INFO: Wrong image for pod: daemon-set-vzwvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:46.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:46.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:46.806: INFO: Wrong image for pod: daemon-set-vzwvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:47.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:47.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:47.806: INFO: Wrong image for pod: daemon-set-vzwvz. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:47.806: INFO: Pod daemon-set-vzwvz is not available
May  3 18:40:48.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:48.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:48.806: INFO: Pod daemon-set-mhxb8 is not available
May  3 18:40:49.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:49.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:49.806: INFO: Pod daemon-set-mhxb8 is not available
May  3 18:40:50.807: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:50.807: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:51.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:51.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:51.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:52.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:52.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:52.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:53.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:53.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:53.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:54.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:54.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:54.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:55.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:55.805: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:55.805: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:56.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:56.805: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:56.805: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:57.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:57.805: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:57.805: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:58.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:58.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:58.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:40:59.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:59.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:40:59.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:41:00.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:00.806: INFO: Wrong image for pod: daemon-set-9bzfq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:00.806: INFO: Pod daemon-set-9bzfq is not available
May  3 18:41:01.808: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:01.808: INFO: Pod daemon-set-kbbwr is not available
May  3 18:41:02.818: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:02.818: INFO: Pod daemon-set-kbbwr is not available
May  3 18:41:03.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:04.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:04.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:05.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:05.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:06.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:06.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:07.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:07.805: INFO: Pod daemon-set-796qt is not available
May  3 18:41:08.805: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:08.805: INFO: Pod daemon-set-796qt is not available
May  3 18:41:09.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:09.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:10.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:10.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:11.806: INFO: Wrong image for pod: daemon-set-796qt. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May  3 18:41:11.806: INFO: Pod daemon-set-796qt is not available
May  3 18:41:12.806: INFO: Pod daemon-set-vszd7 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May  3 18:41:12.840: INFO: Number of nodes with available pods: 2
May  3 18:41:12.840: INFO: Node 10.190.144.140 is running more than one daemon pod
May  3 18:41:13.857: INFO: Number of nodes with available pods: 2
May  3 18:41:13.857: INFO: Node 10.190.144.140 is running more than one daemon pod
May  3 18:41:14.925: INFO: Number of nodes with available pods: 3
May  3 18:41:14.926: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-417, will wait for the garbage collector to delete the pods
May  3 18:41:15.038: INFO: Deleting DaemonSet.extensions daemon-set took: 17.607022ms
May  3 18:41:15.139: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.967678ms
May  3 18:41:27.948: INFO: Number of nodes with available pods: 0
May  3 18:41:27.948: INFO: Number of running nodes: 0, number of available pods: 0
May  3 18:41:27.956: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-417/daemonsets","resourceVersion":"40148"},"items":null}

May  3 18:41:27.963: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-417/pods","resourceVersion":"40148"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:41:27.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-417" for this suite.
May  3 18:41:36.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:41:36.305: INFO: namespace daemonsets-417 deletion completed in 8.306911553s

â€¢ [SLOW TEST:53.850 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:41:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-14d7bed0-6dd3-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:41:36.696: INFO: Waiting up to 5m0s for pod "pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788" in namespace "configmap-8270" to be "success or failure"
May  3 18:41:36.703: INFO: Pod "pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.876362ms
May  3 18:41:38.714: INFO: Pod "pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.018274444s
May  3 18:41:40.722: INFO: Pod "pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026419468s
STEP: Saw pod success
May  3 18:41:40.722: INFO: Pod "pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:41:40.730: INFO: Trying to get logs from node 10.190.144.140 pod pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:41:40.842: INFO: Waiting for pod pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788 to disappear
May  3 18:41:40.848: INFO: Pod pod-configmaps-14d90469-6dd3-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:41:40.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8270" for this suite.
May  3 18:41:46.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:41:47.100: INFO: namespace configmap-8270 deletion completed in 6.244619852s

â€¢ [SLOW TEST:10.795 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:41:47.101: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4305
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:42:47.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4305" for this suite.
May  3 18:43:09.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:43:09.603: INFO: namespace container-probe-4305 deletion completed in 22.261547784s

â€¢ [SLOW TEST:82.502 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:43:09.603: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3580
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:43:09.834: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788" in namespace "downward-api-3580" to be "success or failure"
May  3 18:43:09.842: INFO: Pod "downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.173944ms
May  3 18:43:11.850: INFO: Pod "downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015264872s
STEP: Saw pod success
May  3 18:43:11.850: INFO: Pod "downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:43:11.860: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:43:11.903: INFO: Waiting for pod downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788 to disappear
May  3 18:43:11.910: INFO: Pod downwardapi-volume-4c5d0c7e-6dd3-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:43:11.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3580" for this suite.
May  3 18:43:17.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:43:18.339: INFO: namespace downward-api-3580 deletion completed in 6.420906722s

â€¢ [SLOW TEST:8.736 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:43:18.339: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9734
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May  3 18:43:18.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-9734'
May  3 18:43:18.823: INFO: stderr: ""
May  3 18:43:18.823: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 18:43:18.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:18.963: INFO: stderr: ""
May  3 18:43:18.963: INFO: stdout: "update-demo-nautilus-lsdzc update-demo-nautilus-x7fgk "
May  3 18:43:18.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-lsdzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:19.062: INFO: stderr: ""
May  3 18:43:19.062: INFO: stdout: ""
May  3 18:43:19.062: INFO: update-demo-nautilus-lsdzc is created but not running
May  3 18:43:24.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:24.234: INFO: stderr: ""
May  3 18:43:24.234: INFO: stdout: "update-demo-nautilus-lsdzc update-demo-nautilus-x7fgk "
May  3 18:43:24.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-lsdzc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:24.332: INFO: stderr: ""
May  3 18:43:24.332: INFO: stdout: "true"
May  3 18:43:24.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-lsdzc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:24.441: INFO: stderr: ""
May  3 18:43:24.441: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:43:24.441: INFO: validating pod update-demo-nautilus-lsdzc
May  3 18:43:24.459: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:43:24.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:43:24.459: INFO: update-demo-nautilus-lsdzc is verified up and running
May  3 18:43:24.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:24.654: INFO: stderr: ""
May  3 18:43:24.654: INFO: stdout: "true"
May  3 18:43:24.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:24.754: INFO: stderr: ""
May  3 18:43:24.754: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:43:24.754: INFO: validating pod update-demo-nautilus-x7fgk
May  3 18:43:24.769: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:43:24.769: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:43:24.769: INFO: update-demo-nautilus-x7fgk is verified up and running
STEP: scaling down the replication controller
May  3 18:43:24.771: INFO: scanned /root for discovery docs: <nil>
May  3 18:43:24.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9734'
May  3 18:43:25.924: INFO: stderr: ""
May  3 18:43:25.924: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 18:43:25.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:26.024: INFO: stderr: ""
May  3 18:43:26.024: INFO: stdout: "update-demo-nautilus-lsdzc update-demo-nautilus-x7fgk "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  3 18:43:31.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:31.135: INFO: stderr: ""
May  3 18:43:31.135: INFO: stdout: "update-demo-nautilus-lsdzc update-demo-nautilus-x7fgk "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  3 18:43:36.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:36.257: INFO: stderr: ""
May  3 18:43:36.257: INFO: stdout: "update-demo-nautilus-lsdzc update-demo-nautilus-x7fgk "
STEP: Replicas for name=update-demo: expected=1 actual=2
May  3 18:43:41.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:41.433: INFO: stderr: ""
May  3 18:43:41.433: INFO: stdout: "update-demo-nautilus-x7fgk "
May  3 18:43:41.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:41.533: INFO: stderr: ""
May  3 18:43:41.533: INFO: stdout: "true"
May  3 18:43:41.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:41.639: INFO: stderr: ""
May  3 18:43:41.639: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:43:41.639: INFO: validating pod update-demo-nautilus-x7fgk
May  3 18:43:41.651: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:43:41.651: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:43:41.651: INFO: update-demo-nautilus-x7fgk is verified up and running
STEP: scaling up the replication controller
May  3 18:43:41.653: INFO: scanned /root for discovery docs: <nil>
May  3 18:43:41.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9734'
May  3 18:43:42.820: INFO: stderr: ""
May  3 18:43:42.820: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 18:43:42.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:42.934: INFO: stderr: ""
May  3 18:43:42.934: INFO: stdout: "update-demo-nautilus-jrnrl update-demo-nautilus-x7fgk "
May  3 18:43:42.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-jrnrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:43.042: INFO: stderr: ""
May  3 18:43:43.042: INFO: stdout: ""
May  3 18:43:43.042: INFO: update-demo-nautilus-jrnrl is created but not running
May  3 18:43:48.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9734'
May  3 18:43:48.144: INFO: stderr: ""
May  3 18:43:48.144: INFO: stdout: "update-demo-nautilus-jrnrl update-demo-nautilus-x7fgk "
May  3 18:43:48.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-jrnrl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:48.262: INFO: stderr: ""
May  3 18:43:48.262: INFO: stdout: "true"
May  3 18:43:48.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-jrnrl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:48.356: INFO: stderr: ""
May  3 18:43:48.356: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:43:48.356: INFO: validating pod update-demo-nautilus-jrnrl
May  3 18:43:48.375: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:43:48.375: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:43:48.375: INFO: update-demo-nautilus-jrnrl is verified up and running
May  3 18:43:48.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:48.477: INFO: stderr: ""
May  3 18:43:48.477: INFO: stdout: "true"
May  3 18:43:48.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-x7fgk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9734'
May  3 18:43:48.592: INFO: stderr: ""
May  3 18:43:48.592: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 18:43:48.592: INFO: validating pod update-demo-nautilus-x7fgk
May  3 18:43:48.604: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 18:43:48.604: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 18:43:48.604: INFO: update-demo-nautilus-x7fgk is verified up and running
STEP: using delete to clean up resources
May  3 18:43:48.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-9734'
May  3 18:43:48.715: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:43:48.715: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May  3 18:43:48.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9734'
May  3 18:43:48.823: INFO: stderr: "No resources found.\n"
May  3 18:43:48.823: INFO: stdout: ""
May  3 18:43:48.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -l name=update-demo --namespace=kubectl-9734 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  3 18:43:48.942: INFO: stderr: ""
May  3 18:43:48.942: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:43:48.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9734" for this suite.
May  3 18:44:12.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:44:13.239: INFO: namespace kubectl-9734 deletion completed in 24.287952306s

â€¢ [SLOW TEST:54.900 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:44:13.240: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-2657
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May  3 18:44:13.536: INFO: Waiting up to 5m0s for pod "var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788" in namespace "var-expansion-2657" to be "success or failure"
May  3 18:44:13.543: INFO: Pod "var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.087036ms
May  3 18:44:15.551: INFO: Pod "var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015063374s
STEP: Saw pod success
May  3 18:44:15.551: INFO: Pod "var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:44:15.558: INFO: Trying to get logs from node 10.190.144.141 pod var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 18:44:15.605: INFO: Waiting for pod var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788 to disappear
May  3 18:44:15.611: INFO: Pod var-expansion-7254e2f7-6dd3-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:44:15.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2657" for this suite.
May  3 18:44:21.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:44:21.856: INFO: namespace var-expansion-2657 deletion completed in 6.23592358s

â€¢ [SLOW TEST:8.616 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:44:21.856: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-3091
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May  3 18:44:28.125: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:28.125: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:28.407: INFO: Exec stderr: ""
May  3 18:44:28.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:28.408: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:28.682: INFO: Exec stderr: ""
May  3 18:44:28.682: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:28.682: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:28.907: INFO: Exec stderr: ""
May  3 18:44:28.907: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:28.907: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:29.118: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May  3 18:44:29.118: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:29.118: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:29.304: INFO: Exec stderr: ""
May  3 18:44:29.304: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:29.304: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:29.525: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May  3 18:44:29.526: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:29.526: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:29.718: INFO: Exec stderr: ""
May  3 18:44:29.718: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:29.718: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:29.890: INFO: Exec stderr: ""
May  3 18:44:29.890: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:29.890: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:30.082: INFO: Exec stderr: ""
May  3 18:44:30.082: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3091 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:44:30.082: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:44:30.250: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:44:30.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3091" for this suite.
May  3 18:45:24.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:45:24.536: INFO: namespace e2e-kubelet-etc-hosts-3091 deletion completed in 54.274488008s

â€¢ [SLOW TEST:62.680 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:45:24.536: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-8158
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May  3 18:45:24.739: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8158" to be "success or failure"
May  3 18:45:24.747: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.306996ms
May  3 18:45:26.755: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016147467s
STEP: Saw pod success
May  3 18:45:26.755: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May  3 18:45:26.762: INFO: Trying to get logs from node 10.190.144.141 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May  3 18:45:26.858: INFO: Waiting for pod pod-host-path-test to disappear
May  3 18:45:26.866: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:45:26.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8158" for this suite.
May  3 18:45:32.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:45:33.125: INFO: namespace hostpath-8158 deletion completed in 6.25041331s

â€¢ [SLOW TEST:8.589 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:45:33.127: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-1887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May  3 18:45:35.936: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788"
May  3 18:45:35.936: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788" in namespace "pods-1887" to be "terminated due to deadline exceeded"
May  3 18:45:35.943: INFO: Pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 7.003859ms
May  3 18:45:37.951: INFO: Pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.014956848s
May  3 18:45:39.959: INFO: Pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.023170619s
May  3 18:45:39.959: INFO: Pod "pod-update-activedeadlineseconds-a1e46ba2-6dd3-11e9-8617-baa18b820788" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:45:39.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1887" for this suite.
May  3 18:45:45.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:45:46.242: INFO: namespace pods-1887 deletion completed in 6.275269721s

â€¢ [SLOW TEST:13.116 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:45:46.244: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9547
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May  3 18:45:46.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-9547'
May  3 18:45:46.733: INFO: stderr: ""
May  3 18:45:46.733: INFO: stdout: "pod/pause created\n"
May  3 18:45:46.733: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May  3 18:45:46.733: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9547" to be "running and ready"
May  3 18:45:46.818: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 85.331015ms
May  3 18:45:48.828: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.094702483s
May  3 18:45:48.828: INFO: Pod "pause" satisfied condition "running and ready"
May  3 18:45:48.828: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May  3 18:45:48.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 label pods pause testing-label=testing-label-value --namespace=kubectl-9547'
May  3 18:45:49.039: INFO: stderr: ""
May  3 18:45:49.039: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May  3 18:45:49.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pod pause -L testing-label --namespace=kubectl-9547'
May  3 18:45:49.133: INFO: stderr: ""
May  3 18:45:49.133: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May  3 18:45:49.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 label pods pause testing-label- --namespace=kubectl-9547'
May  3 18:45:49.253: INFO: stderr: ""
May  3 18:45:49.253: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May  3 18:45:49.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pod pause -L testing-label --namespace=kubectl-9547'
May  3 18:45:49.377: INFO: stderr: ""
May  3 18:45:49.377: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May  3 18:45:49.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-9547'
May  3 18:45:49.518: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:45:49.518: INFO: stdout: "pod \"pause\" force deleted\n"
May  3 18:45:49.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get rc,svc -l name=pause --no-headers --namespace=kubectl-9547'
May  3 18:45:49.630: INFO: stderr: "No resources found.\n"
May  3 18:45:49.630: INFO: stdout: ""
May  3 18:45:49.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -l name=pause --namespace=kubectl-9547 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  3 18:45:49.759: INFO: stderr: ""
May  3 18:45:49.759: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:45:49.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9547" for this suite.
May  3 18:45:55.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:45:56.011: INFO: namespace kubectl-9547 deletion completed in 6.242847418s

â€¢ [SLOW TEST:9.767 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:45:56.011: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-82
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:45:58.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-82" for this suite.
May  3 18:46:46.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:46:46.648: INFO: namespace kubelet-test-82 deletion completed in 48.381311835s

â€¢ [SLOW TEST:50.638 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:46:46.649: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May  3 18:46:49.518: INFO: Successfully updated pod "labelsupdatecdc38ec8-6dd3-11e9-8617-baa18b820788"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:46:51.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9217" for this suite.
May  3 18:47:13.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:47:13.956: INFO: namespace projected-9217 deletion completed in 22.383739919s

â€¢ [SLOW TEST:27.307 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:47:13.959: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ddfe2fc2-6dd3-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 18:47:14.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788" in namespace "configmap-7584" to be "success or failure"
May  3 18:47:14.240: INFO: Pod "pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.882743ms
May  3 18:47:16.249: INFO: Pod "pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.016700359s
May  3 18:47:18.257: INFO: Pod "pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025198198s
STEP: Saw pod success
May  3 18:47:18.257: INFO: Pod "pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:47:18.267: INFO: Trying to get logs from node 10.190.144.140 pod pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 18:47:18.305: INFO: Waiting for pod pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788 to disappear
May  3 18:47:18.312: INFO: Pod pod-configmaps-de092af4-6dd3-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:47:18.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7584" for this suite.
May  3 18:47:24.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:47:24.565: INFO: namespace configmap-7584 deletion completed in 6.244186927s

â€¢ [SLOW TEST:10.606 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:47:24.565: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8610
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:47:24.789: INFO: Create a RollingUpdate DaemonSet
May  3 18:47:24.799: INFO: Check that daemon pods launch on every node of the cluster
May  3 18:47:24.812: INFO: Number of nodes with available pods: 0
May  3 18:47:24.812: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:47:25.835: INFO: Number of nodes with available pods: 0
May  3 18:47:25.835: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:47:26.829: INFO: Number of nodes with available pods: 2
May  3 18:47:26.829: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:47:27.832: INFO: Number of nodes with available pods: 3
May  3 18:47:27.832: INFO: Number of running nodes: 3, number of available pods: 3
May  3 18:47:27.832: INFO: Update the DaemonSet to trigger a rollout
May  3 18:47:27.850: INFO: Updating DaemonSet daemon-set
May  3 18:47:38.873: INFO: Roll back the DaemonSet before rollout is complete
May  3 18:47:38.895: INFO: Updating DaemonSet daemon-set
May  3 18:47:38.895: INFO: Make sure DaemonSet rollback is complete
May  3 18:47:38.903: INFO: Wrong image for pod: daemon-set-rwdwt. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May  3 18:47:38.903: INFO: Pod daemon-set-rwdwt is not available
May  3 18:47:39.920: INFO: Wrong image for pod: daemon-set-rwdwt. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May  3 18:47:39.920: INFO: Pod daemon-set-rwdwt is not available
May  3 18:47:40.926: INFO: Pod daemon-set-ckkfq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8610, will wait for the garbage collector to delete the pods
May  3 18:47:41.035: INFO: Deleting DaemonSet.extensions daemon-set took: 18.843012ms
May  3 18:47:41.235: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.317948ms
May  3 18:47:52.143: INFO: Number of nodes with available pods: 0
May  3 18:47:52.143: INFO: Number of running nodes: 0, number of available pods: 0
May  3 18:47:52.150: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8610/daemonsets","resourceVersion":"41528"},"items":null}

May  3 18:47:52.157: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8610/pods","resourceVersion":"41528"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:47:52.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8610" for this suite.
May  3 18:47:58.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:47:58.524: INFO: namespace daemonsets-8610 deletion completed in 6.328890758s

â€¢ [SLOW TEST:33.959 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:47:58.527: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-f88e71bd-6dd3-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:47:58.735: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788" in namespace "projected-7463" to be "success or failure"
May  3 18:47:58.742: INFO: Pod "pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.463026ms
May  3 18:48:00.750: INFO: Pod "pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014522283s
STEP: Saw pod success
May  3 18:48:00.750: INFO: Pod "pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:48:00.818: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  3 18:48:00.858: INFO: Waiting for pod pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788 to disappear
May  3 18:48:00.865: INFO: Pod pod-projected-secrets-f88fad3a-6dd3-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:48:00.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7463" for this suite.
May  3 18:48:06.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:48:07.165: INFO: namespace projected-7463 deletion completed in 6.29173672s

â€¢ [SLOW TEST:8.639 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:48:07.166: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May  3 18:48:07.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 api-versions'
May  3 18:48:07.531: INFO: stderr: ""
May  3 18:48:07.531: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\nbatch/v2alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:48:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7311" for this suite.
May  3 18:48:13.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:48:13.919: INFO: namespace kubectl-7311 deletion completed in 6.379696213s

â€¢ [SLOW TEST:6.753 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:48:13.919: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9189
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9189/secret-test-01bbde79-6dd4-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 18:48:14.133: INFO: Waiting up to 5m0s for pod "pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788" in namespace "secrets-9189" to be "success or failure"
May  3 18:48:14.141: INFO: Pod "pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01525ms
May  3 18:48:16.148: INFO: Pod "pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015582509s
STEP: Saw pod success
May  3 18:48:16.148: INFO: Pod "pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:48:16.155: INFO: Trying to get logs from node 10.190.144.133 pod pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788 container env-test: <nil>
STEP: delete the pod
May  3 18:48:16.204: INFO: Waiting for pod pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788 to disappear
May  3 18:48:16.211: INFO: Pod pod-configmaps-01bd121e-6dd4-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:48:16.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9189" for this suite.
May  3 18:48:22.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:48:22.547: INFO: namespace secrets-9189 deletion completed in 6.325298441s

â€¢ [SLOW TEST:8.628 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:48:22.547: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  3 18:48:30.962: INFO: DNS probes using dns-294/dns-test-06ecbbac-6dd4-11e9-8617-baa18b820788 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:48:30.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-294" for this suite.
May  3 18:48:37.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:48:37.332: INFO: namespace dns-294 deletion completed in 6.335905816s

â€¢ [SLOW TEST:14.785 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:48:37.332: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2219
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:48:37.525: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:48:39.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2219" for this suite.
May  3 18:49:33.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:49:34.037: INFO: namespace pods-2219 deletion completed in 54.310318639s

â€¢ [SLOW TEST:56.705 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:49:34.039: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-927
May  3 18:49:38.255: INFO: Started pod liveness-exec in namespace container-probe-927
STEP: checking the pod's current state and verifying that restartCount is present
May  3 18:49:38.263: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:53:39.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-927" for this suite.
May  3 18:53:46.041: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:53:46.331: INFO: namespace container-probe-927 deletion completed in 6.351786049s

â€¢ [SLOW TEST:252.293 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:53:46.332: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1052
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:53:46.537: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788" in namespace "projected-1052" to be "success or failure"
May  3 18:53:46.544: INFO: Pod "downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.212311ms
May  3 18:53:48.553: INFO: Pod "downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015784683s
STEP: Saw pod success
May  3 18:53:48.553: INFO: Pod "downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:53:48.560: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:53:48.618: INFO: Waiting for pod downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788 to disappear
May  3 18:53:48.625: INFO: Pod downwardapi-volume-c7de0100-6dd4-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:53:48.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1052" for this suite.
May  3 18:53:54.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:53:54.937: INFO: namespace projected-1052 deletion completed in 6.299553953s

â€¢ [SLOW TEST:8.605 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:53:54.938: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3097
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May  3 18:53:55.232: INFO: Waiting up to 5m0s for pod "downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788" in namespace "downward-api-3097" to be "success or failure"
May  3 18:53:55.240: INFO: Pod "downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.787838ms
May  3 18:53:57.248: INFO: Pod "downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016232665s
STEP: Saw pod success
May  3 18:53:57.248: INFO: Pod "downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:53:57.255: INFO: Trying to get logs from node 10.190.144.141 pod downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 18:53:57.304: INFO: Waiting for pod downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788 to disappear
May  3 18:53:57.311: INFO: Pod downward-api-ccfe2c48-6dd4-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:53:57.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3097" for this suite.
May  3 18:54:03.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:54:03.645: INFO: namespace downward-api-3097 deletion completed in 6.326469012s

â€¢ [SLOW TEST:8.707 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:54:03.646: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5818
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May  3 18:54:03.978: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May  3 18:54:26.195: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.112.75:8080/dial?request=hostName&protocol=udp&host=172.30.111.233&port=8081&tries=1'] Namespace:pod-network-test-5818 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:54:26.195: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:54:26.373: INFO: Waiting for endpoints: map[]
May  3 18:54:26.381: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.112.75:8080/dial?request=hostName&protocol=udp&host=172.30.47.184&port=8081&tries=1'] Namespace:pod-network-test-5818 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:54:26.381: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:54:26.562: INFO: Waiting for endpoints: map[]
May  3 18:54:26.570: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.112.75:8080/dial?request=hostName&protocol=udp&host=172.30.112.74&port=8081&tries=1'] Namespace:pod-network-test-5818 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May  3 18:54:26.570: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
May  3 18:54:26.728: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:54:26.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5818" for this suite.
May  3 18:54:50.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:54:51.137: INFO: namespace pod-network-test-5818 deletion completed in 24.311468274s

â€¢ [SLOW TEST:47.492 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:54:51.139: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-498
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:54:51.333: INFO: Creating deployment "test-recreate-deployment"
May  3 18:54:51.343: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May  3 18:54:51.357: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May  3 18:54:53.371: INFO: Waiting deployment "test-recreate-deployment" to complete
May  3 18:54:53.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506491, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506491, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506491, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506491, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 18:54:55.383: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May  3 18:54:55.396: INFO: Updating deployment test-recreate-deployment
May  3 18:54:55.396: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  3 18:54:55.489: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-498,SelfLink:/apis/apps/v1/namespaces/deployment-498/deployments/test-recreate-deployment,UID:ee7fbcdb-6dd4-11e9-a061-0e68f10e50c0,ResourceVersion:42726,Generation:2,CreationTimestamp:2019-05-03 18:54:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-03 18:54:55 +0000 UTC 2019-05-03 18:54:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-03 18:54:55 +0000 UTC 2019-05-03 18:54:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May  3 18:54:55.497: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-498,SelfLink:/apis/apps/v1/namespaces/deployment-498/replicasets/test-recreate-deployment-c9cbd8684,UID:f0f2a968-6dd4-11e9-a061-0e68f10e50c0,ResourceVersion:42724,Generation:1,CreationTimestamp:2019-05-03 18:54:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ee7fbcdb-6dd4-11e9-a061-0e68f10e50c0 0xc0031a98f0 0xc0031a98f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 18:54:55.497: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May  3 18:54:55.497: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-498,SelfLink:/apis/apps/v1/namespaces/deployment-498/replicasets/test-recreate-deployment-7d57d5ff7c,UID:ee810139-6dd4-11e9-a061-0e68f10e50c0,ResourceVersion:42715,Generation:2,CreationTimestamp:2019-05-03 18:54:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment ee7fbcdb-6dd4-11e9-a061-0e68f10e50c0 0xc0031a9827 0xc0031a9828}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 18:54:55.505: INFO: Pod "test-recreate-deployment-c9cbd8684-t2s7n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-t2s7n,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-498,SelfLink:/api/v1/namespaces/deployment-498/pods/test-recreate-deployment-c9cbd8684-t2s7n,UID:f0f40583-6dd4-11e9-a061-0e68f10e50c0,ResourceVersion:42722,Generation:0,CreationTimestamp:2019-05-03 18:54:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 f0f2a968-6dd4-11e9-a061-0e68f10e50c0 0xc0020c48f0 0xc0020c48f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mtwsd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mtwsd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-mtwsd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020c4a70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020c4a90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 18:54:55 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:54:55.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-498" for this suite.
May  3 18:55:01.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:55:01.754: INFO: namespace deployment-498 deletion completed in 6.240350304s

â€¢ [SLOW TEST:10.615 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:55:01.754: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8900
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:55:01.957: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788" in namespace "projected-8900" to be "success or failure"
May  3 18:55:01.965: INFO: Pod "downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.321017ms
May  3 18:55:03.973: INFO: Pod "downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016466355s
STEP: Saw pod success
May  3 18:55:03.973: INFO: Pod "downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:55:03.980: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:55:04.048: INFO: Waiting for pod downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788 to disappear
May  3 18:55:04.055: INFO: Pod downwardapi-volume-f4d1f730-6dd4-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:55:04.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8900" for this suite.
May  3 18:55:10.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:55:10.296: INFO: namespace projected-8900 deletion completed in 6.231871601s

â€¢ [SLOW TEST:8.542 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:55:10.297: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3067
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May  3 18:55:10.484: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May  3 18:55:10.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:10.764: INFO: stderr: ""
May  3 18:55:10.764: INFO: stdout: "service/redis-slave created\n"
May  3 18:55:10.764: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May  3 18:55:10.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:11.142: INFO: stderr: ""
May  3 18:55:11.142: INFO: stdout: "service/redis-master created\n"
May  3 18:55:11.142: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May  3 18:55:11.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:11.401: INFO: stderr: ""
May  3 18:55:11.401: INFO: stdout: "service/frontend created\n"
May  3 18:55:11.401: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May  3 18:55:11.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:11.730: INFO: stderr: ""
May  3 18:55:11.730: INFO: stdout: "deployment.apps/frontend created\n"
May  3 18:55:11.731: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May  3 18:55:11.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:12.016: INFO: stderr: ""
May  3 18:55:12.016: INFO: stdout: "deployment.apps/redis-master created\n"
May  3 18:55:12.017: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May  3 18:55:12.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-3067'
May  3 18:55:12.220: INFO: stderr: ""
May  3 18:55:12.220: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May  3 18:55:12.220: INFO: Waiting for all frontend pods to be Running.
May  3 18:55:27.321: INFO: Waiting for frontend to serve content.
May  3 18:55:27.351: INFO: Trying to add a new entry to the guestbook.
May  3 18:55:27.373: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May  3 18:55:27.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:27.541: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:27.541: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May  3 18:55:27.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:27.684: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:27.684: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  3 18:55:27.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:27.839: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:27.839: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  3 18:55:27.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:27.961: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:27.961: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May  3 18:55:27.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:28.082: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:28.082: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May  3 18:55:28.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-3067'
May  3 18:55:28.220: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 18:55:28.220: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:55:28.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3067" for this suite.
May  3 18:56:12.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:56:12.490: INFO: namespace kubectl-3067 deletion completed in 44.261109844s

â€¢ [SLOW TEST:62.193 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:56:12.490: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 18:56:12.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788" in namespace "downward-api-5101" to be "success or failure"
May  3 18:56:12.700: INFO: Pod "downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.102487ms
May  3 18:56:14.709: INFO: Pod "downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.015754109s
May  3 18:56:16.717: INFO: Pod "downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023594338s
STEP: Saw pod success
May  3 18:56:16.717: INFO: Pod "downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 18:56:16.724: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 18:56:16.843: INFO: Waiting for pod downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788 to disappear
May  3 18:56:16.854: INFO: Pod downwardapi-volume-1efb988b-6dd5-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:56:16.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5101" for this suite.
May  3 18:56:22.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:56:23.221: INFO: namespace downward-api-5101 deletion completed in 6.35784054s

â€¢ [SLOW TEST:10.731 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:56:23.222: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May  3 18:56:23.779: INFO: Pod name wrapped-volume-race-259578fe-6dd5-11e9-8617-baa18b820788: Found 0 pods out of 5
May  3 18:56:28.792: INFO: Pod name wrapped-volume-race-259578fe-6dd5-11e9-8617-baa18b820788: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-259578fe-6dd5-11e9-8617-baa18b820788 in namespace emptydir-wrapper-2943, will wait for the garbage collector to delete the pods
May  3 18:56:38.921: INFO: Deleting ReplicationController wrapped-volume-race-259578fe-6dd5-11e9-8617-baa18b820788 took: 17.437975ms
May  3 18:56:39.121: INFO: Terminating ReplicationController wrapped-volume-race-259578fe-6dd5-11e9-8617-baa18b820788 pods took: 200.408674ms
STEP: Creating RC which spawns configmap-volume pods
May  3 18:57:21.554: INFO: Pod name wrapped-volume-race-48045a6e-6dd5-11e9-8617-baa18b820788: Found 0 pods out of 5
May  3 18:57:26.569: INFO: Pod name wrapped-volume-race-48045a6e-6dd5-11e9-8617-baa18b820788: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-48045a6e-6dd5-11e9-8617-baa18b820788 in namespace emptydir-wrapper-2943, will wait for the garbage collector to delete the pods
May  3 18:57:38.760: INFO: Deleting ReplicationController wrapped-volume-race-48045a6e-6dd5-11e9-8617-baa18b820788 took: 19.179325ms
May  3 18:57:38.960: INFO: Terminating ReplicationController wrapped-volume-race-48045a6e-6dd5-11e9-8617-baa18b820788 pods took: 200.260841ms
STEP: Creating RC which spawns configmap-volume pods
May  3 18:58:21.596: INFO: Pod name wrapped-volume-race-6bcd9cfe-6dd5-11e9-8617-baa18b820788: Found 0 pods out of 5
May  3 18:58:26.619: INFO: Pod name wrapped-volume-race-6bcd9cfe-6dd5-11e9-8617-baa18b820788: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6bcd9cfe-6dd5-11e9-8617-baa18b820788 in namespace emptydir-wrapper-2943, will wait for the garbage collector to delete the pods
May  3 18:58:36.813: INFO: Deleting ReplicationController wrapped-volume-race-6bcd9cfe-6dd5-11e9-8617-baa18b820788 took: 18.964622ms
May  3 18:58:37.013: INFO: Terminating ReplicationController wrapped-volume-race-6bcd9cfe-6dd5-11e9-8617-baa18b820788 pods took: 200.3018ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:59:21.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2943" for this suite.
May  3 18:59:31.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:59:32.243: INFO: namespace emptydir-wrapper-2943 deletion completed in 10.321716931s

â€¢ [SLOW TEST:189.021 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:59:32.244: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:59:32.471: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May  3 18:59:32.490: INFO: Number of nodes with available pods: 0
May  3 18:59:32.490: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May  3 18:59:32.520: INFO: Number of nodes with available pods: 0
May  3 18:59:32.520: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:33.530: INFO: Number of nodes with available pods: 0
May  3 18:59:33.530: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:34.528: INFO: Number of nodes with available pods: 0
May  3 18:59:34.529: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:35.529: INFO: Number of nodes with available pods: 1
May  3 18:59:35.529: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May  3 18:59:35.559: INFO: Number of nodes with available pods: 1
May  3 18:59:35.559: INFO: Number of running nodes: 0, number of available pods: 1
May  3 18:59:36.567: INFO: Number of nodes with available pods: 0
May  3 18:59:36.567: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May  3 18:59:36.586: INFO: Number of nodes with available pods: 0
May  3 18:59:36.586: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:37.596: INFO: Number of nodes with available pods: 0
May  3 18:59:37.596: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:38.595: INFO: Number of nodes with available pods: 0
May  3 18:59:38.595: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:39.595: INFO: Number of nodes with available pods: 0
May  3 18:59:39.595: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:40.595: INFO: Number of nodes with available pods: 0
May  3 18:59:40.595: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:41.594: INFO: Number of nodes with available pods: 0
May  3 18:59:41.594: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:42.595: INFO: Number of nodes with available pods: 0
May  3 18:59:42.595: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 18:59:43.595: INFO: Number of nodes with available pods: 1
May  3 18:59:43.595: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3956, will wait for the garbage collector to delete the pods
May  3 18:59:43.708: INFO: Deleting DaemonSet.extensions daemon-set took: 19.637218ms
May  3 18:59:43.808: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.270836ms
May  3 18:59:51.225: INFO: Number of nodes with available pods: 0
May  3 18:59:51.225: INFO: Number of running nodes: 0, number of available pods: 0
May  3 18:59:51.235: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3956/daemonsets","resourceVersion":"44575"},"items":null}

May  3 18:59:51.242: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3956/pods","resourceVersion":"44575"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 18:59:51.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3956" for this suite.
May  3 18:59:57.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 18:59:57.529: INFO: namespace daemonsets-3956 deletion completed in 6.242424777s

â€¢ [SLOW TEST:25.285 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 18:59:57.530: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1398
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 18:59:57.729: INFO: Pod name rollover-pod: Found 0 pods out of 1
May  3 19:00:02.737: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  3 19:00:02.737: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May  3 19:00:04.745: INFO: Creating deployment "test-rollover-deployment"
May  3 19:00:04.759: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May  3 19:00:06.771: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May  3 19:00:06.793: INFO: Ensure that both replica sets have 1 created replica
May  3 19:00:06.825: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May  3 19:00:06.839: INFO: Updating deployment test-rollover-deployment
May  3 19:00:06.839: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May  3 19:00:08.918: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May  3 19:00:08.931: INFO: Make sure deployment "test-rollover-deployment" is complete
May  3 19:00:08.945: INFO: all replica sets need to contain the pod-template-hash label
May  3 19:00:08.946: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506808, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:00:10.960: INFO: all replica sets need to contain the pod-template-hash label
May  3 19:00:10.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506808, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:00:12.960: INFO: all replica sets need to contain the pod-template-hash label
May  3 19:00:12.960: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506808, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:00:14.963: INFO: all replica sets need to contain the pod-template-hash label
May  3 19:00:14.963: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506808, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:00:16.963: INFO: all replica sets need to contain the pod-template-hash label
May  3 19:00:16.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506808, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692506804, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:00:18.960: INFO: 
May  3 19:00:18.960: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  3 19:00:18.981: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1398,SelfLink:/apis/apps/v1/namespaces/deployment-1398/deployments/test-rollover-deployment,UID:a94e91ac-6dd5-11e9-a061-0e68f10e50c0,ResourceVersion:44739,Generation:2,CreationTimestamp:2019-05-03 19:00:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-03 19:00:04 +0000 UTC 2019-05-03 19:00:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-03 19:00:18 +0000 UTC 2019-05-03 19:00:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  3 19:00:18.988: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-1398,SelfLink:/apis/apps/v1/namespaces/deployment-1398/replicasets/test-rollover-deployment-766b4d6c9d,UID:aa8e2f6f-6dd5-11e9-a061-0e68f10e50c0,ResourceVersion:44728,Generation:2,CreationTimestamp:2019-05-03 19:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a94e91ac-6dd5-11e9-a061-0e68f10e50c0 0xc003188b87 0xc003188b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  3 19:00:18.988: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May  3 19:00:18.988: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1398,SelfLink:/apis/apps/v1/namespaces/deployment-1398/replicasets/test-rollover-controller,UID:a51d8687-6dd5-11e9-a061-0e68f10e50c0,ResourceVersion:44737,Generation:2,CreationTimestamp:2019-05-03 18:59:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a94e91ac-6dd5-11e9-a061-0e68f10e50c0 0xc0031889d7 0xc0031889d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 19:00:18.988: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-1398,SelfLink:/apis/apps/v1/namespaces/deployment-1398/replicasets/test-rollover-deployment-6455657675,UID:a9522f45-6dd5-11e9-a061-0e68f10e50c0,ResourceVersion:44691,Generation:2,CreationTimestamp:2019-05-03 19:00:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment a94e91ac-6dd5-11e9-a061-0e68f10e50c0 0xc003188aa7 0xc003188aa8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 19:00:18.995: INFO: Pod "test-rollover-deployment-766b4d6c9d-vqlwh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-vqlwh,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-1398,SelfLink:/api/v1/namespaces/deployment-1398/pods/test-rollover-deployment-766b4d6c9d-vqlwh,UID:aa9661ec-6dd5-11e9-a061-0e68f10e50c0,ResourceVersion:44709,Generation:0,CreationTimestamp:2019-05-03 19:00:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d aa8e2f6f-6dd5-11e9-a061-0e68f10e50c0 0xc0031896c7 0xc0031896c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fh42z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fh42z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-fh42z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003189740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003189760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:00:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:00:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:00:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:00:06 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.191,StartTime:2019-05-03 19:00:06 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-03 19:00:08 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://2e2b46fa04b1de0be6e0073cf1ce7dae22d63d3f9c88e597257019f8646839cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:00:18.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1398" for this suite.
May  3 19:00:25.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:00:25.265: INFO: namespace deployment-1398 deletion completed in 6.239342081s

â€¢ [SLOW TEST:27.735 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:00:25.265: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:00:43.533: INFO: Container started at 2019-05-03 19:00:26 +0000 UTC, pod became ready at 2019-05-03 19:00:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:00:43.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5112" for this suite.
May  3 19:01:05.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:01:05.835: INFO: namespace container-probe-5112 deletion completed in 22.29486653s

â€¢ [SLOW TEST:40.570 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:01:05.836: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7679
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May  3 19:01:06.022: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-963657699 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:01:06.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7679" for this suite.
May  3 19:01:12.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:01:12.437: INFO: namespace kubectl-7679 deletion completed in 6.32179361s

â€¢ [SLOW TEST:6.601 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:01:12.438: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9662
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-d1c455c0-6dd5-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:01:16.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9662" for this suite.
May  3 19:01:38.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:01:39.031: INFO: namespace configmap-9662 deletion completed in 22.315490286s

â€¢ [SLOW TEST:26.594 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:01:39.032: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3659
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-e19f403a-6dd5-11e9-8617-baa18b820788
STEP: Creating secret with name s-test-opt-upd-e19f408d-6dd5-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-e19f403a-6dd5-11e9-8617-baa18b820788
STEP: Updating secret s-test-opt-upd-e19f408d-6dd5-11e9-8617-baa18b820788
STEP: Creating secret with name s-test-opt-create-e19f40ab-6dd5-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:02:56.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3659" for this suite.
May  3 19:03:20.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:03:20.536: INFO: namespace secrets-3659 deletion completed in 24.309979093s

â€¢ [SLOW TEST:101.504 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:03:20.537: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5036
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May  3 19:03:20.735: INFO: Waiting up to 5m0s for pod "pod-1e1dbac1-6dd6-11e9-8617-baa18b820788" in namespace "emptydir-5036" to be "success or failure"
May  3 19:03:20.742: INFO: Pod "pod-1e1dbac1-6dd6-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.364896ms
May  3 19:03:22.750: INFO: Pod "pod-1e1dbac1-6dd6-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015676898s
STEP: Saw pod success
May  3 19:03:22.750: INFO: Pod "pod-1e1dbac1-6dd6-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:03:22.818: INFO: Trying to get logs from node 10.190.144.133 pod pod-1e1dbac1-6dd6-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:03:22.865: INFO: Waiting for pod pod-1e1dbac1-6dd6-11e9-8617-baa18b820788 to disappear
May  3 19:03:22.872: INFO: Pod pod-1e1dbac1-6dd6-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:03:22.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5036" for this suite.
May  3 19:03:28.903: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:03:29.121: INFO: namespace emptydir-5036 deletion completed in 6.240069323s

â€¢ [SLOW TEST:8.585 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:03:29.122: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-248
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-5304
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:03:35.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9035" for this suite.
May  3 19:03:41.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:03:42.028: INFO: namespace namespaces-9035 deletion completed in 6.295833234s
STEP: Destroying namespace "nsdeletetest-248" for this suite.
May  3 19:03:42.033: INFO: Namespace nsdeletetest-248 was already deleted
STEP: Destroying namespace "nsdeletetest-5304" for this suite.
May  3 19:03:48.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:03:48.334: INFO: namespace nsdeletetest-5304 deletion completed in 6.301047013s

â€¢ [SLOW TEST:19.213 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:03:48.335: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8784
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May  3 19:03:48.539: INFO: Waiting up to 5m0s for pod "pod-2eb06fd1-6dd6-11e9-8617-baa18b820788" in namespace "emptydir-8784" to be "success or failure"
May  3 19:03:48.549: INFO: Pod "pod-2eb06fd1-6dd6-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.430693ms
May  3 19:03:50.557: INFO: Pod "pod-2eb06fd1-6dd6-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017186332s
STEP: Saw pod success
May  3 19:03:50.557: INFO: Pod "pod-2eb06fd1-6dd6-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:03:50.566: INFO: Trying to get logs from node 10.190.144.140 pod pod-2eb06fd1-6dd6-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:03:50.617: INFO: Waiting for pod pod-2eb06fd1-6dd6-11e9-8617-baa18b820788 to disappear
May  3 19:03:50.626: INFO: Pod pod-2eb06fd1-6dd6-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:03:50.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8784" for this suite.
May  3 19:03:56.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:03:56.943: INFO: namespace emptydir-8784 deletion completed in 6.307913768s

â€¢ [SLOW TEST:8.609 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:03:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-371
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:03:59.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-371" for this suite.
May  3 19:04:53.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:04:53.537: INFO: namespace kubelet-test-371 deletion completed in 54.318927009s

â€¢ [SLOW TEST:56.590 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:04:53.537: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:04:57.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1200" for this suite.
May  3 19:05:03.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:05:04.243: INFO: namespace kubelet-test-1200 deletion completed in 6.385728169s

â€¢ [SLOW TEST:10.706 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:05:04.244: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6979
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May  3 19:05:07.178: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6979 pod-service-account-5c3e8c3b-6dd6-11e9-8617-baa18b820788 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May  3 19:05:07.681: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6979 pod-service-account-5c3e8c3b-6dd6-11e9-8617-baa18b820788 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May  3 19:05:07.963: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6979 pod-service-account-5c3e8c3b-6dd6-11e9-8617-baa18b820788 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:05:08.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6979" for this suite.
May  3 19:05:14.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:05:14.526: INFO: namespace svcaccounts-6979 deletion completed in 6.255149218s

â€¢ [SLOW TEST:10.282 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:05:14.526: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-1401
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:05:16.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1401" for this suite.
May  3 19:05:22.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:05:23.242: INFO: namespace emptydir-wrapper-1401 deletion completed in 6.288679456s

â€¢ [SLOW TEST:8.716 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:05:23.243: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-3959
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May  3 19:05:23.443: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45769,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  3 19:05:23.443: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45769,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May  3 19:05:33.458: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45787,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  3 19:05:33.458: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45787,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May  3 19:05:43.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45806,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  3 19:05:43.474: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45806,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May  3 19:05:53.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45823,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  3 19:05:53.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-a,UID:6742c64f-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45823,Generation:0,CreationTimestamp:2019-05-03 19:05:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May  3 19:06:03.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-b,UID:7f22e508-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45841,Generation:0,CreationTimestamp:2019-05-03 19:06:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  3 19:06:03.505: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-b,UID:7f22e508-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45841,Generation:0,CreationTimestamp:2019-05-03 19:06:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May  3 19:06:13.521: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-b,UID:7f22e508-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45858,Generation:0,CreationTimestamp:2019-05-03 19:06:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  3 19:06:13.521: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3959,SelfLink:/api/v1/namespaces/watch-3959/configmaps/e2e-watch-test-configmap-b,UID:7f22e508-6dd6-11e9-a061-0e68f10e50c0,ResourceVersion:45858,Generation:0,CreationTimestamp:2019-05-03 19:06:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:06:23.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3959" for this suite.
May  3 19:06:29.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:06:29.839: INFO: namespace watch-3959 deletion completed in 6.307965648s

â€¢ [SLOW TEST:66.596 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:06:29.840: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-2198
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-2198
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2198
STEP: Deleting pre-stop pod
May  3 19:06:41.272: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:06:41.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2198" for this suite.
May  3 19:07:21.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:07:21.644: INFO: namespace prestop-2198 deletion completed in 40.3179902s

â€¢ [SLOW TEST:51.804 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:07:21.644: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May  3 19:07:21.833: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-963657699 proxy --unix-socket=/tmp/kubectl-proxy-unix333730278/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:07:21.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9986" for this suite.
May  3 19:07:27.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:07:28.127: INFO: namespace kubectl-9986 deletion completed in 6.231367477s

â€¢ [SLOW TEST:6.483 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:07:28.127: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6636
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-b1b22fff-6dd6-11e9-8617-baa18b820788
STEP: Creating secret with name s-test-opt-upd-b1b23052-6dd6-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-b1b22fff-6dd6-11e9-8617-baa18b820788
STEP: Updating secret s-test-opt-upd-b1b23052-6dd6-11e9-8617-baa18b820788
STEP: Creating secret with name s-test-opt-create-b1b23070-6dd6-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:07:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6636" for this suite.
May  3 19:07:56.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:07:56.936: INFO: namespace projected-6636 deletion completed in 24.29465123s

â€¢ [SLOW TEST:28.809 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:07:56.937: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-4932
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-vhl5
STEP: Creating a pod to test atomic-volume-subpath
May  3 19:07:57.171: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vhl5" in namespace "subpath-4932" to be "success or failure"
May  3 19:07:57.179: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.281952ms
May  3 19:07:59.187: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 2.015469026s
May  3 19:08:01.195: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 4.023370577s
May  3 19:08:03.204: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 6.03277759s
May  3 19:08:05.214: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 8.042248703s
May  3 19:08:07.223: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 10.051271532s
May  3 19:08:09.318: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 12.146161365s
May  3 19:08:11.326: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 14.154829725s
May  3 19:08:13.335: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 16.163169093s
May  3 19:08:15.344: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 18.172893812s
May  3 19:08:17.353: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Running", Reason="", readiness=true. Elapsed: 20.181733257s
May  3 19:08:19.366: INFO: Pod "pod-subpath-test-configmap-vhl5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.194151521s
STEP: Saw pod success
May  3 19:08:19.366: INFO: Pod "pod-subpath-test-configmap-vhl5" satisfied condition "success or failure"
May  3 19:08:19.378: INFO: Trying to get logs from node 10.190.144.133 pod pod-subpath-test-configmap-vhl5 container test-container-subpath-configmap-vhl5: <nil>
STEP: delete the pod
May  3 19:08:19.419: INFO: Waiting for pod pod-subpath-test-configmap-vhl5 to disappear
May  3 19:08:19.426: INFO: Pod pod-subpath-test-configmap-vhl5 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vhl5
May  3 19:08:19.426: INFO: Deleting pod "pod-subpath-test-configmap-vhl5" in namespace "subpath-4932"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:08:19.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4932" for this suite.
May  3 19:08:25.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:08:25.756: INFO: namespace subpath-4932 deletion completed in 6.313954476s

â€¢ [SLOW TEST:28.820 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:08:25.757: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8759
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d40ae27e-6dd6-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:08:25.964: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788" in namespace "projected-8759" to be "success or failure"
May  3 19:08:25.974: INFO: Pod "pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 10.147573ms
May  3 19:08:27.983: INFO: Pod "pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.019354392s
May  3 19:08:29.992: INFO: Pod "pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027873314s
STEP: Saw pod success
May  3 19:08:29.992: INFO: Pod "pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:08:29.999: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:08:30.042: INFO: Waiting for pod pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788 to disappear
May  3 19:08:30.050: INFO: Pod pod-projected-configmaps-d40bf87e-6dd6-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:08:30.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8759" for this suite.
May  3 19:08:36.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:08:36.319: INFO: namespace projected-8759 deletion completed in 6.259484166s

â€¢ [SLOW TEST:10.562 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:08:36.319: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0503 19:08:46.722780      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 19:08:46.722: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:08:46.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8723" for this suite.
May  3 19:08:54.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:08:55.043: INFO: namespace gc-8723 deletion completed in 8.312578308s

â€¢ [SLOW TEST:18.724 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:08:55.043: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May  3 19:08:55.243: INFO: Waiting up to 5m0s for pod "downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788" in namespace "downward-api-5288" to be "success or failure"
May  3 19:08:55.250: INFO: Pod "downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.556936ms
May  3 19:08:57.259: INFO: Pod "downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.015906613s
May  3 19:08:59.267: INFO: Pod "downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024156819s
STEP: Saw pod success
May  3 19:08:59.268: INFO: Pod "downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:08:59.275: INFO: Trying to get logs from node 10.190.144.133 pod downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 19:08:59.319: INFO: Waiting for pod downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788 to disappear
May  3 19:08:59.326: INFO: Pod downward-api-e57fc0fe-6dd6-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:08:59.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5288" for this suite.
May  3 19:09:05.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:09:05.833: INFO: namespace downward-api-5288 deletion completed in 6.496047323s

â€¢ [SLOW TEST:10.789 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:09:05.833: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2576
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ebee95bd-6dd6-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:09:06.044: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788" in namespace "configmap-2576" to be "success or failure"
May  3 19:09:06.052: INFO: Pod "pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.123891ms
May  3 19:09:08.062: INFO: Pod "pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.017285942s
May  3 19:09:10.072: INFO: Pod "pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028041635s
STEP: Saw pod success
May  3 19:09:10.072: INFO: Pod "pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:09:10.080: INFO: Trying to get logs from node 10.190.144.140 pod pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:09:10.160: INFO: Waiting for pod pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788 to disappear
May  3 19:09:10.167: INFO: Pod pod-configmaps-ebefe12c-6dd6-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:09:10.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2576" for this suite.
May  3 19:09:16.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:09:16.485: INFO: namespace configmap-2576 deletion completed in 6.308494412s

â€¢ [SLOW TEST:10.652 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:09:16.486: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-43
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May  3 19:09:16.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-43'
May  3 19:09:16.983: INFO: stderr: ""
May  3 19:09:16.984: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May  3 19:09:17.992: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:09:17.992: INFO: Found 0 / 1
May  3 19:09:18.992: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:09:18.992: INFO: Found 1 / 1
May  3 19:09:18.992: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May  3 19:09:19.003: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:09:19.003: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May  3 19:09:19.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 patch pod redis-master-vgs27 --namespace=kubectl-43 -p {"metadata":{"annotations":{"x":"y"}}}'
May  3 19:09:19.119: INFO: stderr: ""
May  3 19:09:19.119: INFO: stdout: "pod/redis-master-vgs27 patched\n"
STEP: checking annotations
May  3 19:09:19.129: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:09:19.129: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:09:19.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-43" for this suite.
May  3 19:09:43.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:09:43.443: INFO: namespace kubectl-43 deletion completed in 24.305843068s

â€¢ [SLOW TEST:26.958 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:09:43.444: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2255
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2255.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2255.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2255.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2255.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2255.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  3 19:09:47.758: INFO: DNS probes using dns-2255/dns-test-025908de-6dd7-11e9-8617-baa18b820788 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:09:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2255" for this suite.
May  3 19:09:53.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:09:54.132: INFO: namespace dns-2255 deletion completed in 6.340253942s

â€¢ [SLOW TEST:10.688 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:09:54.132: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-1440
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May  3 19:09:54.384: INFO: Number of nodes with available pods: 0
May  3 19:09:54.384: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 19:09:55.401: INFO: Number of nodes with available pods: 0
May  3 19:09:55.401: INFO: Node 10.190.144.133 is running more than one daemon pod
May  3 19:09:56.402: INFO: Number of nodes with available pods: 3
May  3 19:09:56.402: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May  3 19:09:56.449: INFO: Number of nodes with available pods: 2
May  3 19:09:56.449: INFO: Node 10.190.144.140 is running more than one daemon pod
May  3 19:09:57.466: INFO: Number of nodes with available pods: 2
May  3 19:09:57.466: INFO: Node 10.190.144.140 is running more than one daemon pod
May  3 19:09:58.466: INFO: Number of nodes with available pods: 3
May  3 19:09:58.466: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1440, will wait for the garbage collector to delete the pods
May  3 19:09:58.557: INFO: Deleting DaemonSet.extensions daemon-set took: 18.4482ms
May  3 19:09:58.757: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.253593ms
May  3 19:10:12.166: INFO: Number of nodes with available pods: 0
May  3 19:10:12.166: INFO: Number of running nodes: 0, number of available pods: 0
May  3 19:10:12.173: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1440/daemonsets","resourceVersion":"47034"},"items":null}

May  3 19:10:12.180: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1440/pods","resourceVersion":"47034"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:10:12.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1440" for this suite.
May  3 19:10:18.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:10:18.548: INFO: namespace daemonsets-1440 deletion completed in 6.333555542s

â€¢ [SLOW TEST:24.416 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:10:18.550: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3655
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0503 19:10:49.354833      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 19:10:49.354: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:10:49.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3655" for this suite.
May  3 19:10:55.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:10:55.657: INFO: namespace gc-3655 deletion completed in 6.238856255s

â€¢ [SLOW TEST:37.107 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:10:55.658: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-2771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-2771
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[]
May  3 19:10:55.877: INFO: Get endpoints failed (7.384911ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May  3 19:10:56.885: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[] (1.015268954s elapsed)
STEP: Creating pod pod1 in namespace services-2771
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100]]
May  3 19:10:58.949: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100]] (2.049455101s elapsed)
STEP: Creating pod pod2 in namespace services-2771
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod1:[100] pod2:[101]]
May  3 19:11:01.086: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod1:[100] pod2:[101]] (2.126431996s elapsed)
STEP: Deleting pod pod1 in namespace services-2771
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[pod2:[101]]
May  3 19:11:02.131: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[pod2:[101]] (1.028763098s elapsed)
STEP: Deleting pod pod2 in namespace services-2771
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2771 to expose endpoints map[]
May  3 19:11:02.156: INFO: successfully validated that service multi-endpoint-test in namespace services-2771 exposes endpoints map[] (8.037349ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:11:02.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2771" for this suite.
May  3 19:11:08.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:11:08.543: INFO: namespace services-2771 deletion completed in 6.315020568s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:12.886 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:11:08.544: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8070
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  3 19:11:08.781: INFO: Waiting up to 5m0s for pod "pod-3517823d-6dd7-11e9-8617-baa18b820788" in namespace "emptydir-8070" to be "success or failure"
May  3 19:11:08.789: INFO: Pod "pod-3517823d-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.946459ms
May  3 19:11:10.798: INFO: Pod "pod-3517823d-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016354732s
STEP: Saw pod success
May  3 19:11:10.798: INFO: Pod "pod-3517823d-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:11:10.804: INFO: Trying to get logs from node 10.190.144.140 pod pod-3517823d-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:11:10.848: INFO: Waiting for pod pod-3517823d-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:11:10.859: INFO: Pod pod-3517823d-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:11:10.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8070" for this suite.
May  3 19:11:16.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:11:17.135: INFO: namespace emptydir-8070 deletion completed in 6.266984705s

â€¢ [SLOW TEST:8.592 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:11:17.136: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6697
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-rv9n
STEP: Creating a pod to test atomic-volume-subpath
May  3 19:11:17.354: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rv9n" in namespace "subpath-6697" to be "success or failure"
May  3 19:11:17.361: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Pending", Reason="", readiness=false. Elapsed: 6.660083ms
May  3 19:11:19.369: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 2.014889224s
May  3 19:11:21.378: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 4.023752834s
May  3 19:11:23.387: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 6.033197714s
May  3 19:11:25.395: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 8.041080411s
May  3 19:11:27.403: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 10.049043505s
May  3 19:11:29.411: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 12.056618554s
May  3 19:11:31.426: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 14.071615694s
May  3 19:11:33.433: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 16.079083006s
May  3 19:11:35.441: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 18.0867844s
May  3 19:11:37.506: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Running", Reason="", readiness=true. Elapsed: 20.152278261s
May  3 19:11:39.515: INFO: Pod "pod-subpath-test-downwardapi-rv9n": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.161305609s
STEP: Saw pod success
May  3 19:11:39.516: INFO: Pod "pod-subpath-test-downwardapi-rv9n" satisfied condition "success or failure"
May  3 19:11:39.522: INFO: Trying to get logs from node 10.190.144.133 pod pod-subpath-test-downwardapi-rv9n container test-container-subpath-downwardapi-rv9n: <nil>
STEP: delete the pod
May  3 19:11:39.565: INFO: Waiting for pod pod-subpath-test-downwardapi-rv9n to disappear
May  3 19:11:39.572: INFO: Pod pod-subpath-test-downwardapi-rv9n no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rv9n
May  3 19:11:39.572: INFO: Deleting pod "pod-subpath-test-downwardapi-rv9n" in namespace "subpath-6697"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:11:39.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6697" for this suite.
May  3 19:11:45.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:11:45.833: INFO: namespace subpath-6697 deletion completed in 6.241600764s

â€¢ [SLOW TEST:28.698 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:11:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:11:46.048: INFO: (0) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 17.536342ms)
May  3 19:11:46.061: INFO: (1) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 13.153177ms)
May  3 19:11:46.129: INFO: (2) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 67.536325ms)
May  3 19:11:46.140: INFO: (3) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.895556ms)
May  3 19:11:46.150: INFO: (4) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.163187ms)
May  3 19:11:46.161: INFO: (5) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.628473ms)
May  3 19:11:46.171: INFO: (6) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.930417ms)
May  3 19:11:46.186: INFO: (7) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 15.336541ms)
May  3 19:11:46.199: INFO: (8) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 12.812173ms)
May  3 19:11:46.210: INFO: (9) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.968533ms)
May  3 19:11:46.221: INFO: (10) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.125832ms)
May  3 19:11:46.232: INFO: (11) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.803984ms)
May  3 19:11:46.244: INFO: (12) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 11.38717ms)
May  3 19:11:46.254: INFO: (13) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.323788ms)
May  3 19:11:46.269: INFO: (14) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 14.701395ms)
May  3 19:11:46.279: INFO: (15) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.339392ms)
May  3 19:11:46.290: INFO: (16) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.78071ms)
May  3 19:11:46.301: INFO: (17) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.697092ms)
May  3 19:11:46.312: INFO: (18) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.757422ms)
May  3 19:11:46.322: INFO: (19) /api/v1/nodes/10.190.144.133/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.219572ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:11:46.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4163" for this suite.
May  3 19:11:52.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:11:52.569: INFO: namespace proxy-4163 deletion completed in 6.240781722s

â€¢ [SLOW TEST:6.736 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:11:52.569: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2076
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:11:52.841: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788" in namespace "projected-2076" to be "success or failure"
May  3 19:11:52.849: INFO: Pod "downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.462163ms
May  3 19:11:54.859: INFO: Pod "downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.01777397s
May  3 19:11:56.868: INFO: Pod "downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02664956s
STEP: Saw pod success
May  3 19:11:56.868: INFO: Pod "downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:11:56.875: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:11:56.922: INFO: Waiting for pod downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:11:56.931: INFO: Pod downwardapi-volume-4f5a22ae-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:11:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2076" for this suite.
May  3 19:12:02.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:03.217: INFO: namespace projected-2076 deletion completed in 6.276407673s

â€¢ [SLOW TEST:10.647 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:03.218: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-55a90441-6dd7-11e9-8617-baa18b820788
STEP: Creating secret with name secret-projected-all-test-volume-55a903f8-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test Check all projections for projected volume plugin
May  3 19:12:03.433: INFO: Waiting up to 5m0s for pod "projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788" in namespace "projected-9432" to be "success or failure"
May  3 19:12:03.442: INFO: Pod "projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.677946ms
May  3 19:12:05.452: INFO: Pod "projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018085801s
STEP: Saw pod success
May  3 19:12:05.452: INFO: Pod "projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:12:05.518: INFO: Trying to get logs from node 10.190.144.141 pod projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788 container projected-all-volume-test: <nil>
STEP: delete the pod
May  3 19:12:05.576: INFO: Waiting for pod projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:12:05.582: INFO: Pod projected-volume-55a903bf-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:05.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9432" for this suite.
May  3 19:12:11.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:11.830: INFO: namespace projected-9432 deletion completed in 6.238855418s

â€¢ [SLOW TEST:8.611 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:11.830: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May  3 19:12:12.029: INFO: Waiting up to 5m0s for pod "client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788" in namespace "containers-9847" to be "success or failure"
May  3 19:12:12.038: INFO: Pod "client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.693901ms
May  3 19:12:14.047: INFO: Pod "client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01746361s
STEP: Saw pod success
May  3 19:12:14.047: INFO: Pod "client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:12:14.054: INFO: Trying to get logs from node 10.190.144.140 pod client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:12:14.095: INFO: Waiting for pod client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:12:14.102: INFO: Pod client-containers-5acaaf46-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:14.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9847" for this suite.
May  3 19:12:20.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:20.416: INFO: namespace containers-9847 deletion completed in 6.306844191s

â€¢ [SLOW TEST:8.586 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:20.417: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 19:12:20.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2926'
May  3 19:12:20.716: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  3 19:12:20.716: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May  3 19:12:20.732: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-kls2w]
May  3 19:12:20.732: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-kls2w" in namespace "kubectl-2926" to be "running and ready"
May  3 19:12:20.741: INFO: Pod "e2e-test-nginx-rc-kls2w": Phase="Pending", Reason="", readiness=false. Elapsed: 8.605192ms
May  3 19:12:22.750: INFO: Pod "e2e-test-nginx-rc-kls2w": Phase="Running", Reason="", readiness=true. Elapsed: 2.017143383s
May  3 19:12:22.750: INFO: Pod "e2e-test-nginx-rc-kls2w" satisfied condition "running and ready"
May  3 19:12:22.750: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-kls2w]
May  3 19:12:22.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 logs rc/e2e-test-nginx-rc --namespace=kubectl-2926'
May  3 19:12:22.889: INFO: stderr: ""
May  3 19:12:22.889: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May  3 19:12:22.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete rc e2e-test-nginx-rc --namespace=kubectl-2926'
May  3 19:12:23.010: INFO: stderr: ""
May  3 19:12:23.010: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:23.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2926" for this suite.
May  3 19:12:29.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:29.334: INFO: namespace kubectl-2926 deletion completed in 6.315682798s

â€¢ [SLOW TEST:8.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:29.335: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7107
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-653a033c-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:12:29.543: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788" in namespace "projected-7107" to be "success or failure"
May  3 19:12:29.551: INFO: Pod "pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948376ms
May  3 19:12:31.558: INFO: Pod "pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014901431s
STEP: Saw pod success
May  3 19:12:31.559: INFO: Pod "pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:12:31.625: INFO: Trying to get logs from node 10.190.144.141 pod pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:12:31.666: INFO: Waiting for pod pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:12:31.674: INFO: Pod pod-projected-configmaps-653b21ea-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:31.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7107" for this suite.
May  3 19:12:37.704: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:37.933: INFO: namespace projected-7107 deletion completed in 6.25107959s

â€¢ [SLOW TEST:8.599 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:37.933: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7615
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May  3 19:12:38.138: INFO: Waiting up to 5m0s for pod "pod-6a5a7f74-6dd7-11e9-8617-baa18b820788" in namespace "emptydir-7615" to be "success or failure"
May  3 19:12:38.146: INFO: Pod "pod-6a5a7f74-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.762246ms
May  3 19:12:40.154: INFO: Pod "pod-6a5a7f74-6dd7-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.015990845s
May  3 19:12:42.162: INFO: Pod "pod-6a5a7f74-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024015018s
STEP: Saw pod success
May  3 19:12:42.162: INFO: Pod "pod-6a5a7f74-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:12:42.170: INFO: Trying to get logs from node 10.190.144.133 pod pod-6a5a7f74-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:12:42.218: INFO: Waiting for pod pod-6a5a7f74-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:12:42.224: INFO: Pod pod-6a5a7f74-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:42.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7615" for this suite.
May  3 19:12:48.253: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:48.524: INFO: namespace emptydir-7615 deletion completed in 6.29142349s

â€¢ [SLOW TEST:10.590 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:48.524: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3454
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May  3 19:12:48.730: INFO: Waiting up to 5m0s for pod "pod-70aa4af2-6dd7-11e9-8617-baa18b820788" in namespace "emptydir-3454" to be "success or failure"
May  3 19:12:48.737: INFO: Pod "pod-70aa4af2-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.552375ms
May  3 19:12:50.747: INFO: Pod "pod-70aa4af2-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01670055s
STEP: Saw pod success
May  3 19:12:50.747: INFO: Pod "pod-70aa4af2-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:12:50.754: INFO: Trying to get logs from node 10.190.144.140 pod pod-70aa4af2-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:12:50.799: INFO: Waiting for pod pod-70aa4af2-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:12:50.824: INFO: Pod pod-70aa4af2-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:50.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3454" for this suite.
May  3 19:12:56.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:12:57.157: INFO: namespace emptydir-3454 deletion completed in 6.323296825s

â€¢ [SLOW TEST:8.633 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:12:57.158: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1284
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:12:57.551: INFO: (0) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.446822ms)
May  3 19:12:57.561: INFO: (1) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.026766ms)
May  3 19:12:57.571: INFO: (2) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.980312ms)
May  3 19:12:57.582: INFO: (3) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.024042ms)
May  3 19:12:57.592: INFO: (4) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.456438ms)
May  3 19:12:57.602: INFO: (5) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.82444ms)
May  3 19:12:57.612: INFO: (6) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.839628ms)
May  3 19:12:57.622: INFO: (7) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.840977ms)
May  3 19:12:57.632: INFO: (8) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.734779ms)
May  3 19:12:57.642: INFO: (9) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.968918ms)
May  3 19:12:57.652: INFO: (10) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.948949ms)
May  3 19:12:57.661: INFO: (11) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.703298ms)
May  3 19:12:57.671: INFO: (12) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.683712ms)
May  3 19:12:57.681: INFO: (13) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.436609ms)
May  3 19:12:57.690: INFO: (14) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.787959ms)
May  3 19:12:57.700: INFO: (15) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.909533ms)
May  3 19:12:57.710: INFO: (16) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.545558ms)
May  3 19:12:57.720: INFO: (17) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.081501ms)
May  3 19:12:57.730: INFO: (18) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 9.85232ms)
May  3 19:12:57.740: INFO: (19) /api/v1/nodes/10.190.144.133:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="at/">at/</... (200; 10.220923ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:12:57.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1284" for this suite.
May  3 19:13:03.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:13:04.020: INFO: namespace proxy-1284 deletion completed in 6.272132576s

â€¢ [SLOW TEST:6.862 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:13:04.020: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May  3 19:13:04.224: INFO: Waiting up to 5m0s for pod "downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788" in namespace "downward-api-7002" to be "success or failure"
May  3 19:13:04.232: INFO: Pod "downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.143681ms
May  3 19:13:06.239: INFO: Pod "downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015059019s
STEP: Saw pod success
May  3 19:13:06.239: INFO: Pod "downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:13:06.247: INFO: Trying to get logs from node 10.190.144.140 pod downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 19:13:06.290: INFO: Waiting for pod downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:13:06.297: INFO: Pod downward-api-79e72bfc-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:13:06.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7002" for this suite.
May  3 19:13:12.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:13:12.649: INFO: namespace downward-api-7002 deletion completed in 6.343914877s

â€¢ [SLOW TEST:8.629 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:13:12.649: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-7f0dcbef-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:13:12.875: INFO: Waiting up to 5m0s for pod "pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788" in namespace "secrets-8731" to be "success or failure"
May  3 19:13:12.883: INFO: Pod "pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.958592ms
May  3 19:13:14.902: INFO: Pod "pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.027213823s
STEP: Saw pod success
May  3 19:13:14.903: INFO: Pod "pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:13:14.913: INFO: Trying to get logs from node 10.190.144.133 pod pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:13:14.953: INFO: Waiting for pod pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:13:15.018: INFO: Pod pod-secrets-7f0f16cc-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:13:15.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8731" for this suite.
May  3 19:13:21.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:13:21.341: INFO: namespace secrets-8731 deletion completed in 6.313446238s

â€¢ [SLOW TEST:8.692 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:13:21.341: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6370
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-843a9116-6dd7-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-843a9116-6dd7-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:13:25.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6370" for this suite.
May  3 19:13:49.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:13:50.069: INFO: namespace configmap-6370 deletion completed in 24.251334332s

â€¢ [SLOW TEST:28.728 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:13:50.070: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2443
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  3 19:13:50.281: INFO: Waiting up to 5m0s for pod "pod-955abb22-6dd7-11e9-8617-baa18b820788" in namespace "emptydir-2443" to be "success or failure"
May  3 19:13:50.289: INFO: Pod "pod-955abb22-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.856023ms
May  3 19:13:52.298: INFO: Pod "pod-955abb22-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01626877s
STEP: Saw pod success
May  3 19:13:52.298: INFO: Pod "pod-955abb22-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:13:52.305: INFO: Trying to get logs from node 10.190.144.140 pod pod-955abb22-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:13:52.350: INFO: Waiting for pod pod-955abb22-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:13:52.357: INFO: Pod pod-955abb22-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:13:52.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2443" for this suite.
May  3 19:13:58.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:13:58.635: INFO: namespace emptydir-2443 deletion completed in 6.269565362s

â€¢ [SLOW TEST:8.565 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:13:58.635: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6115
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qllvq in namespace proxy-6115
I0503 19:13:58.930290      19 runners.go:184] Created replication controller with name: proxy-service-qllvq, namespace: proxy-6115, replica count: 1
I0503 19:13:59.980759      19 runners.go:184] proxy-service-qllvq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0503 19:14:00.981058      19 runners.go:184] proxy-service-qllvq Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0503 19:14:01.981337      19 runners.go:184] proxy-service-qllvq Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0503 19:14:02.981618      19 runners.go:184] proxy-service-qllvq Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May  3 19:14:02.990: INFO: setup took 4.16453486s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May  3 19:14:03.006: INFO: (0) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 15.542216ms)
May  3 19:14:03.011: INFO: (0) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 20.347163ms)
May  3 19:14:03.011: INFO: (0) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 20.550818ms)
May  3 19:14:03.019: INFO: (0) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 29.036623ms)
May  3 19:14:03.019: INFO: (0) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 29.134221ms)
May  3 19:14:03.020: INFO: (0) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 29.592255ms)
May  3 19:14:03.025: INFO: (0) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 34.553045ms)
May  3 19:14:03.025: INFO: (0) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 34.645725ms)
May  3 19:14:03.026: INFO: (0) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 35.921136ms)
May  3 19:14:03.029: INFO: (0) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 38.272684ms)
May  3 19:14:03.031: INFO: (0) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 40.145462ms)
May  3 19:14:03.032: INFO: (0) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 41.073722ms)
May  3 19:14:03.033: INFO: (0) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 42.731572ms)
May  3 19:14:03.041: INFO: (0) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 50.2751ms)
May  3 19:14:03.046: INFO: (0) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 56.186383ms)
May  3 19:14:03.057: INFO: (0) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 66.537535ms)
May  3 19:14:03.073: INFO: (1) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.882676ms)
May  3 19:14:03.073: INFO: (1) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 15.73084ms)
May  3 19:14:03.076: INFO: (1) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 19.25813ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 19.638361ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 19.557032ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 19.601345ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 19.649145ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 19.815301ms)
May  3 19:14:03.077: INFO: (1) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 19.93189ms)
May  3 19:14:03.078: INFO: (1) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 20.810558ms)
May  3 19:14:03.081: INFO: (1) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 23.047196ms)
May  3 19:14:03.081: INFO: (1) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 23.601405ms)
May  3 19:14:03.084: INFO: (1) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 26.058793ms)
May  3 19:14:03.084: INFO: (1) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 26.266835ms)
May  3 19:14:03.084: INFO: (1) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 26.398293ms)
May  3 19:14:03.085: INFO: (1) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 27.484392ms)
May  3 19:14:03.098: INFO: (2) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 12.67405ms)
May  3 19:14:03.101: INFO: (2) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.417575ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 16.596777ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 16.770421ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 16.700789ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 16.58622ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 16.687863ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 16.716791ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 16.740899ms)
May  3 19:14:03.102: INFO: (2) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 16.754026ms)
May  3 19:14:03.103: INFO: (2) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 17.506268ms)
May  3 19:14:03.106: INFO: (2) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 20.741355ms)
May  3 19:14:03.106: INFO: (2) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.679623ms)
May  3 19:14:03.106: INFO: (2) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 21.114002ms)
May  3 19:14:03.106: INFO: (2) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 21.006502ms)
May  3 19:14:03.109: INFO: (2) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 23.863421ms)
May  3 19:14:03.120: INFO: (3) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 11.282569ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.252939ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 14.430326ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 13.57179ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 14.320748ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.256796ms)
May  3 19:14:03.124: INFO: (3) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.446952ms)
May  3 19:14:03.125: INFO: (3) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 14.628622ms)
May  3 19:14:03.125: INFO: (3) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.956454ms)
May  3 19:14:03.125: INFO: (3) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.617933ms)
May  3 19:14:03.127: INFO: (3) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 17.117914ms)
May  3 19:14:03.127: INFO: (3) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 17.425779ms)
May  3 19:14:03.128: INFO: (3) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 17.908032ms)
May  3 19:14:03.130: INFO: (3) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 19.480161ms)
May  3 19:14:03.130: INFO: (3) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.321765ms)
May  3 19:14:03.130: INFO: (3) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 20.388611ms)
May  3 19:14:03.141: INFO: (4) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 10.454122ms)
May  3 19:14:03.143: INFO: (4) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 12.745705ms)
May  3 19:14:03.144: INFO: (4) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.081842ms)
May  3 19:14:03.144: INFO: (4) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 13.059103ms)
May  3 19:14:03.144: INFO: (4) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.22254ms)
May  3 19:14:03.144: INFO: (4) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 13.636091ms)
May  3 19:14:03.144: INFO: (4) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.15189ms)
May  3 19:14:03.145: INFO: (4) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 14.115688ms)
May  3 19:14:03.145: INFO: (4) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 13.92737ms)
May  3 19:14:03.145: INFO: (4) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.532415ms)
May  3 19:14:03.148: INFO: (4) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 17.307677ms)
May  3 19:14:03.150: INFO: (4) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 19.017287ms)
May  3 19:14:03.150: INFO: (4) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 19.416661ms)
May  3 19:14:03.150: INFO: (4) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.490777ms)
May  3 19:14:03.150: INFO: (4) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 19.334866ms)
May  3 19:14:03.150: INFO: (4) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 19.61227ms)
May  3 19:14:03.160: INFO: (5) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 10.504058ms)
May  3 19:14:03.163: INFO: (5) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 12.303617ms)
May  3 19:14:03.163: INFO: (5) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 12.922231ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.628759ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 13.7684ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.114465ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.196442ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.86024ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.64336ms)
May  3 19:14:03.165: INFO: (5) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 14.489828ms)
May  3 19:14:03.167: INFO: (5) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 17.201857ms)
May  3 19:14:03.169: INFO: (5) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 18.330585ms)
May  3 19:14:03.169: INFO: (5) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 18.751291ms)
May  3 19:14:03.170: INFO: (5) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 19.260751ms)
May  3 19:14:03.170: INFO: (5) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 18.94431ms)
May  3 19:14:03.170: INFO: (5) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.537195ms)
May  3 19:14:03.181: INFO: (6) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 11.123886ms)
May  3 19:14:03.183: INFO: (6) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 12.841101ms)
May  3 19:14:03.183: INFO: (6) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.037992ms)
May  3 19:14:03.185: INFO: (6) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.976172ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 15.182075ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 15.408771ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 15.728813ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 16.323396ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 15.69692ms)
May  3 19:14:03.186: INFO: (6) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 16.367349ms)
May  3 19:14:03.187: INFO: (6) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 16.820453ms)
May  3 19:14:03.191: INFO: (6) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 21.293095ms)
May  3 19:14:03.191: INFO: (6) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 21.270491ms)
May  3 19:14:03.191: INFO: (6) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.68416ms)
May  3 19:14:03.191: INFO: (6) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 20.812658ms)
May  3 19:14:03.191: INFO: (6) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 20.783561ms)
May  3 19:14:03.202: INFO: (7) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 10.638895ms)
May  3 19:14:03.202: INFO: (7) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 10.817926ms)
May  3 19:14:03.205: INFO: (7) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.341687ms)
May  3 19:14:03.205: INFO: (7) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 13.558542ms)
May  3 19:14:03.205: INFO: (7) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.578745ms)
May  3 19:14:03.206: INFO: (7) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.006452ms)
May  3 19:14:03.206: INFO: (7) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 14.165234ms)
May  3 19:14:03.206: INFO: (7) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.383748ms)
May  3 19:14:03.206: INFO: (7) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 14.296273ms)
May  3 19:14:03.206: INFO: (7) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.406585ms)
May  3 19:14:03.208: INFO: (7) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 16.551173ms)
May  3 19:14:03.212: INFO: (7) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 20.126951ms)
May  3 19:14:03.213: INFO: (7) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 20.730264ms)
May  3 19:14:03.213: INFO: (7) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 21.052918ms)
May  3 19:14:03.213: INFO: (7) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 21.414777ms)
May  3 19:14:03.213: INFO: (7) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 21.527493ms)
May  3 19:14:03.223: INFO: (8) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 9.870686ms)
May  3 19:14:03.226: INFO: (8) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.726311ms)
May  3 19:14:03.226: INFO: (8) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 12.656242ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 12.91939ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 13.063321ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 13.272783ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 13.210816ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 13.151303ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.25088ms)
May  3 19:14:03.227: INFO: (8) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.789193ms)
May  3 19:14:03.229: INFO: (8) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 15.421937ms)
May  3 19:14:03.229: INFO: (8) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 15.582371ms)
May  3 19:14:03.232: INFO: (8) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 18.534466ms)
May  3 19:14:03.232: INFO: (8) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 18.892107ms)
May  3 19:14:03.232: INFO: (8) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 18.918859ms)
May  3 19:14:03.233: INFO: (8) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 19.075334ms)
May  3 19:14:03.243: INFO: (9) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 10.379636ms)
May  3 19:14:03.246: INFO: (9) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.734785ms)
May  3 19:14:03.246: INFO: (9) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 12.964609ms)
May  3 19:14:03.247: INFO: (9) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.618981ms)
May  3 19:14:03.247: INFO: (9) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 13.985472ms)
May  3 19:14:03.247: INFO: (9) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 14.05141ms)
May  3 19:14:03.247: INFO: (9) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.330317ms)
May  3 19:14:03.247: INFO: (9) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 14.303ms)
May  3 19:14:03.248: INFO: (9) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.516138ms)
May  3 19:14:03.248: INFO: (9) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.449658ms)
May  3 19:14:03.249: INFO: (9) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 16.120958ms)
May  3 19:14:03.253: INFO: (9) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 19.791489ms)
May  3 19:14:03.253: INFO: (9) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.153606ms)
May  3 19:14:03.254: INFO: (9) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 20.622856ms)
May  3 19:14:03.254: INFO: (9) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 20.758775ms)
May  3 19:14:03.254: INFO: (9) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 20.702163ms)
May  3 19:14:03.264: INFO: (10) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 10.281257ms)
May  3 19:14:03.268: INFO: (10) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.591147ms)
May  3 19:14:03.268: INFO: (10) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.02153ms)
May  3 19:14:03.268: INFO: (10) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 14.51264ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 14.22793ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.616905ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.494528ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 14.733468ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 14.746755ms)
May  3 19:14:03.269: INFO: (10) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.031599ms)
May  3 19:14:03.271: INFO: (10) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 16.825018ms)
May  3 19:14:03.274: INFO: (10) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 20.447425ms)
May  3 19:14:03.275: INFO: (10) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 20.436874ms)
May  3 19:14:03.275: INFO: (10) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.532569ms)
May  3 19:14:03.275: INFO: (10) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 20.967814ms)
May  3 19:14:03.275: INFO: (10) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 21.040036ms)
May  3 19:14:03.285: INFO: (11) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 9.5914ms)
May  3 19:14:03.288: INFO: (11) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.734332ms)
May  3 19:14:03.288: INFO: (11) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.114184ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.324893ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 13.681822ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.533926ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.633232ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 13.607306ms)
May  3 19:14:03.289: INFO: (11) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 13.80134ms)
May  3 19:14:03.290: INFO: (11) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 14.056256ms)
May  3 19:14:03.292: INFO: (11) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 15.952591ms)
May  3 19:14:03.294: INFO: (11) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 18.055328ms)
May  3 19:14:03.294: INFO: (11) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 18.724078ms)
May  3 19:14:03.294: INFO: (11) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 18.382296ms)
May  3 19:14:03.294: INFO: (11) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 18.27637ms)
May  3 19:14:03.294: INFO: (11) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 18.503163ms)
May  3 19:14:03.306: INFO: (12) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 11.923074ms)
May  3 19:14:03.310: INFO: (12) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 15.850302ms)
May  3 19:14:03.310: INFO: (12) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 15.97854ms)
May  3 19:14:03.311: INFO: (12) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 16.745331ms)
May  3 19:14:03.311: INFO: (12) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 16.832085ms)
May  3 19:14:03.311: INFO: (12) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 16.884784ms)
May  3 19:14:03.311: INFO: (12) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 16.914796ms)
May  3 19:14:03.311: INFO: (12) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 16.838786ms)
May  3 19:14:03.312: INFO: (12) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 17.055258ms)
May  3 19:14:03.312: INFO: (12) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 17.364275ms)
May  3 19:14:03.313: INFO: (12) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 18.958231ms)
May  3 19:14:03.316: INFO: (12) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 22.024163ms)
May  3 19:14:03.316: INFO: (12) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 21.729655ms)
May  3 19:14:03.316: INFO: (12) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 21.717367ms)
May  3 19:14:03.316: INFO: (12) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 21.942702ms)
May  3 19:14:03.317: INFO: (12) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 22.221819ms)
May  3 19:14:03.327: INFO: (13) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 10.425586ms)
May  3 19:14:03.331: INFO: (13) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 12.971894ms)
May  3 19:14:03.331: INFO: (13) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 13.046999ms)
May  3 19:14:03.331: INFO: (13) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.365139ms)
May  3 19:14:03.332: INFO: (13) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.773899ms)
May  3 19:14:03.332: INFO: (13) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.543167ms)
May  3 19:14:03.332: INFO: (13) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.885034ms)
May  3 19:14:03.332: INFO: (13) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 14.505981ms)
May  3 19:14:03.332: INFO: (13) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 15.248808ms)
May  3 19:14:03.334: INFO: (13) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 16.660658ms)
May  3 19:14:03.334: INFO: (13) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 16.986165ms)
May  3 19:14:03.338: INFO: (13) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 20.024884ms)
May  3 19:14:03.338: INFO: (13) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 20.066546ms)
May  3 19:14:03.338: INFO: (13) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 19.786273ms)
May  3 19:14:03.338: INFO: (13) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 20.394819ms)
May  3 19:14:03.338: INFO: (13) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 20.754748ms)
May  3 19:14:03.348: INFO: (14) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 9.712678ms)
May  3 19:14:03.351: INFO: (14) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.803193ms)
May  3 19:14:03.352: INFO: (14) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 13.64217ms)
May  3 19:14:03.352: INFO: (14) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.574339ms)
May  3 19:14:03.352: INFO: (14) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.740645ms)
May  3 19:14:03.352: INFO: (14) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 13.763576ms)
May  3 19:14:03.352: INFO: (14) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.254688ms)
May  3 19:14:03.353: INFO: (14) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 14.545454ms)
May  3 19:14:03.353: INFO: (14) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.440556ms)
May  3 19:14:03.353: INFO: (14) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 14.449697ms)
May  3 19:14:03.353: INFO: (14) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 15.233483ms)
May  3 19:14:03.357: INFO: (14) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 18.450896ms)
May  3 19:14:03.357: INFO: (14) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 18.879876ms)
May  3 19:14:03.357: INFO: (14) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 18.925201ms)
May  3 19:14:03.357: INFO: (14) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 19.228748ms)
May  3 19:14:03.358: INFO: (14) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.884495ms)
May  3 19:14:03.368: INFO: (15) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 9.73243ms)
May  3 19:14:03.371: INFO: (15) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 11.679182ms)
May  3 19:14:03.371: INFO: (15) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 12.375551ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 12.974995ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 13.338886ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.572594ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 13.331323ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 13.68599ms)
May  3 19:14:03.372: INFO: (15) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.745688ms)
May  3 19:14:03.373: INFO: (15) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 13.345046ms)
May  3 19:14:03.376: INFO: (15) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 17.478775ms)
May  3 19:14:03.377: INFO: (15) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 18.687168ms)
May  3 19:14:03.377: INFO: (15) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 18.658138ms)
May  3 19:14:03.377: INFO: (15) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 18.252875ms)
May  3 19:14:03.377: INFO: (15) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 17.859175ms)
May  3 19:14:03.379: INFO: (15) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.900964ms)
May  3 19:14:03.391: INFO: (16) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 10.981845ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 15.120107ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 15.145645ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 15.420254ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 15.605077ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 15.832912ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 15.752019ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 15.855154ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.876317ms)
May  3 19:14:03.395: INFO: (16) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.938992ms)
May  3 19:14:03.399: INFO: (16) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 19.22494ms)
May  3 19:14:03.401: INFO: (16) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 21.365473ms)
May  3 19:14:03.401: INFO: (16) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 21.484365ms)
May  3 19:14:03.401: INFO: (16) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 21.626561ms)
May  3 19:14:03.401: INFO: (16) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 21.714791ms)
May  3 19:14:03.401: INFO: (16) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 21.732731ms)
May  3 19:14:03.412: INFO: (17) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 10.222868ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 12.88217ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 12.980948ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 13.134217ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 13.42683ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 13.476021ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 13.570221ms)
May  3 19:14:03.415: INFO: (17) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 14.071091ms)
May  3 19:14:03.416: INFO: (17) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 14.026084ms)
May  3 19:14:03.416: INFO: (17) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.151122ms)
May  3 19:14:03.417: INFO: (17) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 15.735376ms)
May  3 19:14:03.421: INFO: (17) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 19.381799ms)
May  3 19:14:03.421: INFO: (17) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 19.339723ms)
May  3 19:14:03.421: INFO: (17) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 19.450269ms)
May  3 19:14:03.421: INFO: (17) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.51403ms)
May  3 19:14:03.421: INFO: (17) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 19.640874ms)
May  3 19:14:03.432: INFO: (18) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 10.324868ms)
May  3 19:14:03.432: INFO: (18) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 10.42304ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 14.733966ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 14.783213ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 15.040984ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 15.164406ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 15.030878ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 15.003688ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 15.07288ms)
May  3 19:14:03.437: INFO: (18) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 15.023521ms)
May  3 19:14:03.438: INFO: (18) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 16.21699ms)
May  3 19:14:03.441: INFO: (18) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 19.24397ms)
May  3 19:14:03.441: INFO: (18) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 19.298121ms)
May  3 19:14:03.442: INFO: (18) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 19.827236ms)
May  3 19:14:03.444: INFO: (18) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 22.629026ms)
May  3 19:14:03.444: INFO: (18) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 22.552334ms)
May  3 19:14:03.458: INFO: (19) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:443/proxy/tlsrewritem... (200; 13.078688ms)
May  3 19:14:03.464: INFO: (19) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname2/proxy/: bar (200; 19.779678ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">... (200; 19.897135ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 20.281156ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 20.33607ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8:1080/proxy/rewriteme">test<... (200; 20.310716ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:462/proxy/: tls qux (200; 20.530904ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:160/proxy/: foo (200; 20.410815ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/https:proxy-service-qllvq-zjrl8:460/proxy/: tls baz (200; 20.624089ms)
May  3 19:14:03.465: INFO: (19) /api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/: <a href="/api/v1/namespaces/proxy-6115/pods/proxy-service-qllvq-zjrl8/proxy/rewriteme">test</a> (200; 20.533062ms)
May  3 19:14:03.468: INFO: (19) /api/v1/namespaces/proxy-6115/pods/http:proxy-service-qllvq-zjrl8:162/proxy/: bar (200; 23.879376ms)
May  3 19:14:03.469: INFO: (19) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname2/proxy/: bar (200; 23.881802ms)
May  3 19:14:03.470: INFO: (19) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname1/proxy/: tls baz (200; 25.612822ms)
May  3 19:14:03.471: INFO: (19) /api/v1/namespaces/proxy-6115/services/proxy-service-qllvq:portname1/proxy/: foo (200; 25.964765ms)
May  3 19:14:03.471: INFO: (19) /api/v1/namespaces/proxy-6115/services/http:proxy-service-qllvq:portname1/proxy/: foo (200; 26.668755ms)
May  3 19:14:03.472: INFO: (19) /api/v1/namespaces/proxy-6115/services/https:proxy-service-qllvq:tlsportname2/proxy/: tls qux (200; 27.082133ms)
STEP: deleting ReplicationController proxy-service-qllvq in namespace proxy-6115, will wait for the garbage collector to delete the pods
May  3 19:14:03.547: INFO: Deleting ReplicationController proxy-service-qllvq took: 17.61008ms
May  3 19:14:03.647: INFO: Terminating ReplicationController proxy-service-qllvq pods took: 100.232823ms
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:14:05.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6115" for this suite.
May  3 19:14:11.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:14:11.910: INFO: namespace proxy-6115 deletion completed in 6.254226489s

â€¢ [SLOW TEST:13.275 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:14:11.911: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2448
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-2448/configmap-test-a25e66f7-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:14:12.123: INFO: Waiting up to 5m0s for pod "pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788" in namespace "configmap-2448" to be "success or failure"
May  3 19:14:12.132: INFO: Pod "pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.27272ms
May  3 19:14:14.140: INFO: Pod "pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017077046s
May  3 19:14:16.149: INFO: Pod "pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025668403s
STEP: Saw pod success
May  3 19:14:16.149: INFO: Pod "pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:14:16.158: INFO: Trying to get logs from node 10.190.144.141 pod pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788 container env-test: <nil>
STEP: delete the pod
May  3 19:14:16.199: INFO: Waiting for pod pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:14:16.206: INFO: Pod pod-configmaps-a25fa334-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:14:16.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2448" for this suite.
May  3 19:14:22.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:14:22.553: INFO: namespace configmap-2448 deletion completed in 6.332970895s

â€¢ [SLOW TEST:10.642 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:14:22.553: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:14:22.760: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788" in namespace "projected-4798" to be "success or failure"
May  3 19:14:22.771: INFO: Pod "downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 10.853232ms
May  3 19:14:24.780: INFO: Pod "downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019893345s
STEP: Saw pod success
May  3 19:14:24.780: INFO: Pod "downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:14:24.787: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:14:24.826: INFO: Waiting for pod downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:14:24.832: INFO: Pod downwardapi-volume-a8b63fee-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:14:24.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4798" for this suite.
May  3 19:14:30.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:14:31.119: INFO: namespace projected-4798 deletion completed in 6.278691336s

â€¢ [SLOW TEST:8.566 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:14:31.121: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1518
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:14:31.313: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May  3 19:14:31.329: INFO: Pod name sample-pod: Found 0 pods out of 1
May  3 19:14:36.339: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  3 19:14:36.339: INFO: Creating deployment "test-rolling-update-deployment"
May  3 19:14:36.347: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May  3 19:14:36.361: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May  3 19:14:38.375: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May  3 19:14:38.382: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  3 19:14:38.418: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1518,SelfLink:/apis/apps/v1/namespaces/deployment-1518/deployments/test-rolling-update-deployment,UID:b0d13463-6dd7-11e9-a061-0e68f10e50c0,ResourceVersion:48367,Generation:1,CreationTimestamp:2019-05-03 19:14:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-03 19:14:36 +0000 UTC 2019-05-03 19:14:36 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-03 19:14:37 +0000 UTC 2019-05-03 19:14:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  3 19:14:38.425: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-1518,SelfLink:/apis/apps/v1/namespaces/deployment-1518/replicasets/test-rolling-update-deployment-67599b4d9,UID:b0d4d0b9-6dd7-11e9-a061-0e68f10e50c0,ResourceVersion:48356,Generation:1,CreationTimestamp:2019-05-03 19:14:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b0d13463-6dd7-11e9-a061-0e68f10e50c0 0xc0025cd870 0xc0025cd871}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  3 19:14:38.425: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May  3 19:14:38.425: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1518,SelfLink:/apis/apps/v1/namespaces/deployment-1518/replicasets/test-rolling-update-controller,UID:add25a9e-6dd7-11e9-a061-0e68f10e50c0,ResourceVersion:48365,Generation:2,CreationTimestamp:2019-05-03 19:14:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b0d13463-6dd7-11e9-a061-0e68f10e50c0 0xc0025cd7a7 0xc0025cd7a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May  3 19:14:38.433: INFO: Pod "test-rolling-update-deployment-67599b4d9-768kb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-768kb,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-1518,SelfLink:/api/v1/namespaces/deployment-1518/pods/test-rolling-update-deployment-67599b4d9-768kb,UID:b0d5f364-6dd7-11e9-a061-0e68f10e50c0,ResourceVersion:48355,Generation:0,CreationTimestamp:2019-05-03 19:14:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 b0d4d0b9-6dd7-11e9-a061-0e68f10e50c0 0xc0032b6500 0xc0032b6501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4hkf5 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4hkf5,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-4hkf5 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032b6580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032b65a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:14:36 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:14:36 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.142,StartTime:2019-05-03 19:14:36 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-03 19:14:37 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://f471188848901640235400702e9d30be7bca3ec5671091b3f6f4ee8ae455a6c8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:14:38.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1518" for this suite.
May  3 19:14:44.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:14:44.749: INFO: namespace deployment-1518 deletion completed in 6.308264297s

â€¢ [SLOW TEST:13.628 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:14:44.749: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-b5f172c8-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:14:44.965: INFO: Waiting up to 5m0s for pod "pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788" in namespace "secrets-4640" to be "success or failure"
May  3 19:14:44.974: INFO: Pod "pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.084821ms
May  3 19:14:46.982: INFO: Pod "pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017688168s
STEP: Saw pod success
May  3 19:14:46.982: INFO: Pod "pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:14:47.018: INFO: Trying to get logs from node 10.190.144.141 pod pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788 container secret-env-test: <nil>
STEP: delete the pod
May  3 19:14:47.061: INFO: Waiting for pod pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:14:47.070: INFO: Pod pod-secrets-b5f2b259-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:14:47.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4640" for this suite.
May  3 19:14:53.110: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:14:53.326: INFO: namespace secrets-4640 deletion completed in 6.244317168s

â€¢ [SLOW TEST:8.577 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:14:53.327: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 19:14:53.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7139'
May  3 19:14:53.632: INFO: stderr: ""
May  3 19:14:53.632: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May  3 19:14:53.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete pods e2e-test-nginx-pod --namespace=kubectl-7139'
May  3 19:15:01.111: INFO: stderr: ""
May  3 19:15:01.111: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:01.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7139" for this suite.
May  3 19:15:07.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:15:07.443: INFO: namespace kubectl-7139 deletion completed in 6.316432027s

â€¢ [SLOW TEST:14.117 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:15:07.444: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:15:07.652: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788" in namespace "downward-api-8385" to be "success or failure"
May  3 19:15:07.854: INFO: Pod "downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 201.759971ms
May  3 19:15:09.863: INFO: Pod "downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.210709837s
STEP: Saw pod success
May  3 19:15:09.863: INFO: Pod "downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:15:09.871: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:15:09.914: INFO: Waiting for pod downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:15:09.920: INFO: Pod downwardapi-volume-c3786cb2-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:09.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8385" for this suite.
May  3 19:15:15.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:15:16.383: INFO: namespace downward-api-8385 deletion completed in 6.45422115s

â€¢ [SLOW TEST:8.939 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:15:16.383: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May  3 19:15:17.171: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
May  3 19:15:19.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:15:21.254: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63692507717, loc:(*time.Location)(0x8a060e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May  3 19:15:28.426: INFO: Waited 5.162313534s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:28.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8436" for this suite.
May  3 19:15:35.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:15:35.330: INFO: namespace aggregator-8436 deletion completed in 6.403768233s

â€¢ [SLOW TEST:18.947 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:15:35.336: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5259
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May  3 19:15:35.639: INFO: Waiting up to 5m0s for pod "pod-d426ff61-6dd7-11e9-8617-baa18b820788" in namespace "emptydir-5259" to be "success or failure"
May  3 19:15:35.650: INFO: Pod "pod-d426ff61-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 10.272889ms
May  3 19:15:37.658: INFO: Pod "pod-d426ff61-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018925491s
May  3 19:15:39.667: INFO: Pod "pod-d426ff61-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027559697s
STEP: Saw pod success
May  3 19:15:39.667: INFO: Pod "pod-d426ff61-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:15:39.674: INFO: Trying to get logs from node 10.190.144.133 pod pod-d426ff61-6dd7-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:15:39.720: INFO: Waiting for pod pod-d426ff61-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:15:39.726: INFO: Pod pod-d426ff61-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:39.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5259" for this suite.
May  3 19:15:45.760: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:15:46.031: INFO: namespace emptydir-5259 deletion completed in 6.296543099s

â€¢ [SLOW TEST:10.696 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:15:46.032: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-da775b4a-6dd7-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:15:46.239: INFO: Waiting up to 5m0s for pod "pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788" in namespace "secrets-3917" to be "success or failure"
May  3 19:15:46.250: INFO: Pod "pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 10.923625ms
May  3 19:15:48.258: INFO: Pod "pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018812885s
STEP: Saw pod success
May  3 19:15:48.258: INFO: Pod "pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:15:48.325: INFO: Trying to get logs from node 10.190.144.141 pod pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:15:48.372: INFO: Waiting for pod pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788 to disappear
May  3 19:15:48.379: INFO: Pod pod-secrets-da788b1d-6dd7-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:48.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3917" for this suite.
May  3 19:15:54.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:15:54.660: INFO: namespace secrets-3917 deletion completed in 6.273304287s

â€¢ [SLOW TEST:8.629 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:15:54.661: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3885
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:15:56.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3885" for this suite.
May  3 19:16:37.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:16:37.255: INFO: namespace kubelet-test-3885 deletion completed in 40.266911867s

â€¢ [SLOW TEST:42.594 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:16:37.257: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2847
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-f90291be-6dd7-11e9-8617-baa18b820788
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-f90291be-6dd7-11e9-8617-baa18b820788
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:16:41.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2847" for this suite.
May  3 19:17:03.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:17:04.041: INFO: namespace projected-2847 deletion completed in 22.350680558s

â€¢ [SLOW TEST:26.784 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:17:04.041: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7304
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May  3 19:17:04.332: INFO: Waiting up to 5m0s for pod "pod-08f7b31c-6dd8-11e9-8617-baa18b820788" in namespace "emptydir-7304" to be "success or failure"
May  3 19:17:04.339: INFO: Pod "pod-08f7b31c-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.844914ms
May  3 19:17:06.347: INFO: Pod "pod-08f7b31c-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014848007s
STEP: Saw pod success
May  3 19:17:06.347: INFO: Pod "pod-08f7b31c-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:17:06.355: INFO: Trying to get logs from node 10.190.144.141 pod pod-08f7b31c-6dd8-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:17:06.406: INFO: Waiting for pod pod-08f7b31c-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:17:06.413: INFO: Pod pod-08f7b31c-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:17:06.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7304" for this suite.
May  3 19:17:12.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:17:12.733: INFO: namespace emptydir-7304 deletion completed in 6.309775645s

â€¢ [SLOW TEST:8.692 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:17:12.733: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:17:12.934: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788" in namespace "downward-api-6374" to be "success or failure"
May  3 19:17:12.941: INFO: Pod "downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.293661ms
May  3 19:17:14.952: INFO: Pod "downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017543566s
May  3 19:17:16.960: INFO: Pod "downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026287528s
STEP: Saw pod success
May  3 19:17:16.960: INFO: Pod "downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:17:16.968: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:17:17.012: INFO: Waiting for pod downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:17:17.019: INFO: Pod downwardapi-volume-0e2536c1-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:17:17.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6374" for this suite.
May  3 19:17:23.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:17:23.436: INFO: namespace downward-api-6374 deletion completed in 6.407939992s

â€¢ [SLOW TEST:10.703 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:17:23.437: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5504
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May  3 19:17:26.202: INFO: Successfully updated pod "annotationupdate14862064-6dd8-11e9-8617-baa18b820788"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:17:30.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5504" for this suite.
May  3 19:17:54.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:17:54.564: INFO: namespace projected-5504 deletion completed in 24.245827975s

â€¢ [SLOW TEST:31.127 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:17:54.564: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-1929
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  3 19:17:54.748: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  3 19:17:54.763: INFO: Waiting for terminating namespaces to be deleted...
May  3 19:17:54.769: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.133 before test
May  3 19:17:54.790: INFO: calico-node-fkw9t from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.790: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:17:54.790: INFO: kubernetes-dashboard-6c88b75685-pc4kh from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.790: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  3 19:17:54.790: INFO: ibm-storage-watcher-84958d564f-9xnvg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.790: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  3 19:17:54.790: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-nms5m from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:17:54.790: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:17:54.790: INFO: 	Container systemd-logs ready: true, restart count 1
May  3 19:17:54.790: INFO: coredns-autoscaler-6d8b6f867-v8hwf from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.790: INFO: 	Container autoscaler ready: true, restart count 0
May  3 19:17:54.790: INFO: ibm-keepalived-watcher-plptw from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:17:54.791: INFO: calico-kube-controllers-5cf89ccdcb-4jsv6 from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  3 19:17:54.791: INFO: vpn-6df9c5c5bb-fj8dg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container vpn ready: true, restart count 0
May  3 19:17:54.791: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-05-03 16:10:24 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  3 19:17:54.791: INFO: ibm-master-proxy-static-10.190.144.133 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:17:54.791: INFO: ibm-file-plugin-547c887746-s8jqz from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  3 19:17:54.791: INFO: coredns-d5dcb59f8-9j6zs from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container coredns ready: true, restart count 0
May  3 19:17:54.791: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-03 18:03:50 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  3 19:17:54.791: INFO: ibm-kube-fluentd-hbqql from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.791: INFO: 	Container fluentd ready: true, restart count 0
May  3 19:17:54.791: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.140 before test
May  3 19:17:54.815: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-248dv from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:17:54.815: INFO: 	Container systemd-logs ready: true, restart count 1
May  3 19:17:54.815: INFO: ibm-kube-fluentd-pp6bq from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container fluentd ready: true, restart count 0
May  3 19:17:54.815: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-8hdkw from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
May  3 19:17:54.815: INFO: coredns-d5dcb59f8-gqs2d from kube-system started at 2019-05-03 14:28:49 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container coredns ready: true, restart count 0
May  3 19:17:54.815: INFO: metrics-server-675d59648f-rhfdj from kube-system started at 2019-05-03 14:30:15 +0000 UTC (2 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container metrics-server ready: true, restart count 0
May  3 19:17:54.815: INFO: 	Container metrics-server-nanny ready: true, restart count 0
May  3 19:17:54.815: INFO: calico-node-wrx89 from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:17:54.815: INFO: ibm-keepalived-watcher-7w57c from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:17:54.815: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-c8k9z from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 19:17:54.815: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 19:17:54.815: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 19:17:54.815: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 19:17:54.815: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 19:17:54.815: INFO: ibm-master-proxy-static-10.190.144.140 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:17:54.815: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.141 before test
May  3 19:17:54.835: INFO: calico-node-9wqc8 from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:17:54.835: INFO: ibm-kube-fluentd-tlcsj from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container fluentd ready: true, restart count 0
May  3 19:17:54.835: INFO: ibm-keepalived-watcher-wccsc from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:17:54.835: INFO: ibm-master-proxy-static-10.190.144.141 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:17:54.835: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-d6x9d from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
May  3 19:17:54.835: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-59k8h from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 19:17:54.835: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 19:17:54.835: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 19:17:54.835: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 19:17:54.835: INFO: sonobuoy-e2e-job-e0d8049d571e43ef from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container e2e ready: true, restart count 0
May  3 19:17:54.835: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 19:17:54.835: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-494vt from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:17:54.835: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:17:54.835: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node 10.190.144.133
STEP: verifying the node has the label node 10.190.144.140
STEP: verifying the node has the label node 10.190.144.141
May  3 19:17:54.920: INFO: Pod test-k8s-e2e-pvg-master-verification requesting resource cpu=0m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod sonobuoy-e2e-job-e0d8049d571e43ef requesting resource cpu=0m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod sonobuoy-systemd-logs-daemon-set-52cca3793892431e-248dv requesting resource cpu=0m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod sonobuoy-systemd-logs-daemon-set-52cca3793892431e-494vt requesting resource cpu=0m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod sonobuoy-systemd-logs-daemon-set-52cca3793892431e-nms5m requesting resource cpu=0m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-8hdkw requesting resource cpu=5m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-d6x9d requesting resource cpu=5m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod calico-kube-controllers-5cf89ccdcb-4jsv6 requesting resource cpu=10m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod calico-node-9wqc8 requesting resource cpu=250m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod calico-node-fkw9t requesting resource cpu=250m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod calico-node-wrx89 requesting resource cpu=250m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod coredns-autoscaler-6d8b6f867-v8hwf requesting resource cpu=20m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod coredns-d5dcb59f8-9j6zs requesting resource cpu=100m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod coredns-d5dcb59f8-gqs2d requesting resource cpu=100m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod ibm-file-plugin-547c887746-s8jqz requesting resource cpu=50m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod ibm-keepalived-watcher-7w57c requesting resource cpu=5m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod ibm-keepalived-watcher-plptw requesting resource cpu=5m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod ibm-keepalived-watcher-wccsc requesting resource cpu=5m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod ibm-kube-fluentd-hbqql requesting resource cpu=25m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod ibm-kube-fluentd-pp6bq requesting resource cpu=25m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod ibm-kube-fluentd-tlcsj requesting resource cpu=25m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod ibm-master-proxy-static-10.190.144.133 requesting resource cpu=25m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod ibm-master-proxy-static-10.190.144.140 requesting resource cpu=25m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod ibm-master-proxy-static-10.190.144.141 requesting resource cpu=25m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod ibm-storage-watcher-84958d564f-9xnvg requesting resource cpu=50m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod kubernetes-dashboard-6c88b75685-pc4kh requesting resource cpu=50m on Node 10.190.144.133
May  3 19:17:54.920: INFO: Pod metrics-server-675d59648f-rhfdj requesting resource cpu=53m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-59k8h requesting resource cpu=0m on Node 10.190.144.141
May  3 19:17:54.920: INFO: Pod public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-c8k9z requesting resource cpu=0m on Node 10.190.144.140
May  3 19:17:54.920: INFO: Pod vpn-6df9c5c5bb-fj8dg requesting resource cpu=5m on Node 10.190.144.133
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-272e2180-6dd8-11e9-8617-baa18b820788.159b425fb13d0f14], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1929/filler-pod-272e2180-6dd8-11e9-8617-baa18b820788 to 10.190.144.133]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-272e2180-6dd8-11e9-8617-baa18b820788.159b425feb39a6c2], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-272e2180-6dd8-11e9-8617-baa18b820788.159b425fef14dc16], Reason = [Created], Message = [Created container filler-pod-272e2180-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-272e2180-6dd8-11e9-8617-baa18b820788.159b425ffc5a416a], Reason = [Started], Message = [Started container filler-pod-272e2180-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27303059-6dd8-11e9-8617-baa18b820788.159b425fb1bf1648], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1929/filler-pod-27303059-6dd8-11e9-8617-baa18b820788 to 10.190.144.140]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27303059-6dd8-11e9-8617-baa18b820788.159b425fec244f31], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27303059-6dd8-11e9-8617-baa18b820788.159b425fefa2f398], Reason = [Created], Message = [Created container filler-pod-27303059-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-27303059-6dd8-11e9-8617-baa18b820788.159b425ff93508d5], Reason = [Started], Message = [Started container filler-pod-27303059-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-273193ed-6dd8-11e9-8617-baa18b820788.159b425fb24e908e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1929/filler-pod-273193ed-6dd8-11e9-8617-baa18b820788 to 10.190.144.141]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-273193ed-6dd8-11e9-8617-baa18b820788.159b425fec1f84a0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-273193ed-6dd8-11e9-8617-baa18b820788.159b425fef7b2d48], Reason = [Created], Message = [Created container filler-pod-273193ed-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-273193ed-6dd8-11e9-8617-baa18b820788.159b425ff8771d42], Reason = [Started], Message = [Started container filler-pod-273193ed-6dd8-11e9-8617-baa18b820788]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159b4260a3522f79], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node 10.190.144.140
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.190.144.141
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node 10.190.144.133
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:18:00.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1929" for this suite.
May  3 19:18:06.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:18:06.447: INFO: namespace sched-pred-1929 deletion completed in 6.356237705s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:11.883 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:18:06.448: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-332
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-9kgk
STEP: Creating a pod to test atomic-volume-subpath
May  3 19:18:06.753: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-9kgk" in namespace "subpath-332" to be "success or failure"
May  3 19:18:06.760: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079842ms
May  3 19:18:08.770: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 2.016570615s
May  3 19:18:10.778: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 4.025076803s
May  3 19:18:12.787: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 6.034148375s
May  3 19:18:14.796: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 8.043216585s
May  3 19:18:16.804: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 10.051003573s
May  3 19:18:18.813: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 12.060254175s
May  3 19:18:20.821: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 14.067980666s
May  3 19:18:22.831: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 16.077636434s
May  3 19:18:24.839: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 18.085777942s
May  3 19:18:26.850: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Running", Reason="", readiness=true. Elapsed: 20.096631919s
May  3 19:18:28.860: INFO: Pod "pod-subpath-test-secret-9kgk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.107302291s
STEP: Saw pod success
May  3 19:18:28.861: INFO: Pod "pod-subpath-test-secret-9kgk" satisfied condition "success or failure"
May  3 19:18:28.918: INFO: Trying to get logs from node 10.190.144.141 pod pod-subpath-test-secret-9kgk container test-container-subpath-secret-9kgk: <nil>
STEP: delete the pod
May  3 19:18:28.961: INFO: Waiting for pod pod-subpath-test-secret-9kgk to disappear
May  3 19:18:28.967: INFO: Pod pod-subpath-test-secret-9kgk no longer exists
STEP: Deleting pod pod-subpath-test-secret-9kgk
May  3 19:18:28.967: INFO: Deleting pod "pod-subpath-test-secret-9kgk" in namespace "subpath-332"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:18:28.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-332" for this suite.
May  3 19:18:35.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:18:35.235: INFO: namespace subpath-332 deletion completed in 6.252853639s

â€¢ [SLOW TEST:28.787 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:18:35.235: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1051
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-3f60cee3-6dd8-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:18:35.541: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788" in namespace "projected-1051" to be "success or failure"
May  3 19:18:35.551: INFO: Pod "pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 9.198435ms
May  3 19:18:37.559: INFO: Pod "pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.017994352s
May  3 19:18:39.570: INFO: Pod "pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028230395s
STEP: Saw pod success
May  3 19:18:39.570: INFO: Pod "pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:18:39.577: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:18:39.619: INFO: Waiting for pod pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:18:39.626: INFO: Pod pod-projected-configmaps-3f621a36-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:18:39.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1051" for this suite.
May  3 19:18:45.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:18:45.924: INFO: namespace projected-1051 deletion completed in 6.287125163s

â€¢ [SLOW TEST:10.689 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:18:45.925: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-722
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-45b072aa-6dd8-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:18:46.128: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788" in namespace "projected-722" to be "success or failure"
May  3 19:18:46.136: INFO: Pod "pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.815636ms
May  3 19:18:48.143: INFO: Pod "pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015377805s
May  3 19:18:50.152: INFO: Pod "pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024461311s
STEP: Saw pod success
May  3 19:18:50.152: INFO: Pod "pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:18:50.218: INFO: Trying to get logs from node 10.190.144.133 pod pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  3 19:18:50.262: INFO: Waiting for pod pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:18:50.269: INFO: Pod pod-projected-secrets-45b196e2-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:18:50.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-722" for this suite.
May  3 19:18:56.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:18:56.526: INFO: namespace projected-722 deletion completed in 6.248589862s

â€¢ [SLOW TEST:10.601 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:18:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4821
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:18:56.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4821" for this suite.
May  3 19:19:02.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:19:02.975: INFO: namespace services-4821 deletion completed in 6.242629125s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:6.449 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:19:02.976: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6060
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-4fdb3c58-6dd8-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:19:03.185: INFO: Waiting up to 5m0s for pod "pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788" in namespace "configmap-6060" to be "success or failure"
May  3 19:19:03.192: INFO: Pod "pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750343ms
May  3 19:19:05.200: INFO: Pod "pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01508405s
STEP: Saw pod success
May  3 19:19:05.200: INFO: Pod "pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:19:05.218: INFO: Trying to get logs from node 10.190.144.140 pod pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:19:05.257: INFO: Waiting for pod pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:19:05.264: INFO: Pod pod-configmaps-4fdc4a0e-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:19:05.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6060" for this suite.
May  3 19:19:11.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:19:11.527: INFO: namespace configmap-6060 deletion completed in 6.254828717s

â€¢ [SLOW TEST:8.551 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:19:11.527: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May  3 19:19:11.714: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:19:15.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6327" for this suite.
May  3 19:19:21.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:19:21.650: INFO: namespace init-container-6327 deletion completed in 6.312569918s

â€¢ [SLOW TEST:10.123 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:19:21.651: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May  3 19:19:21.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 cluster-info'
May  3 19:19:22.126: INFO: stderr: ""
May  3 19:19:22.126: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:19:22.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9090" for this suite.
May  3 19:19:28.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:19:28.452: INFO: namespace kubectl-9090 deletion completed in 6.316207762s

â€¢ [SLOW TEST:6.801 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:19:28.452: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May  3 19:19:28.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-2126'
May  3 19:19:28.915: INFO: stderr: ""
May  3 19:19:28.915: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 19:19:28.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2126'
May  3 19:19:29.029: INFO: stderr: ""
May  3 19:19:29.029: INFO: stdout: "update-demo-nautilus-27t8n update-demo-nautilus-kf28b "
May  3 19:19:29.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-27t8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:29.135: INFO: stderr: ""
May  3 19:19:29.135: INFO: stdout: ""
May  3 19:19:29.135: INFO: update-demo-nautilus-27t8n is created but not running
May  3 19:19:34.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2126'
May  3 19:19:34.233: INFO: stderr: ""
May  3 19:19:34.233: INFO: stdout: "update-demo-nautilus-27t8n update-demo-nautilus-kf28b "
May  3 19:19:34.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-27t8n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:34.333: INFO: stderr: ""
May  3 19:19:34.333: INFO: stdout: "true"
May  3 19:19:34.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-27t8n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:34.437: INFO: stderr: ""
May  3 19:19:34.437: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 19:19:34.437: INFO: validating pod update-demo-nautilus-27t8n
May  3 19:19:34.453: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 19:19:34.453: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 19:19:34.453: INFO: update-demo-nautilus-27t8n is verified up and running
May  3 19:19:34.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-kf28b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:34.550: INFO: stderr: ""
May  3 19:19:34.550: INFO: stdout: "true"
May  3 19:19:34.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-nautilus-kf28b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:34.644: INFO: stderr: ""
May  3 19:19:34.644: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May  3 19:19:34.644: INFO: validating pod update-demo-nautilus-kf28b
May  3 19:19:34.667: INFO: got data: {
  "image": "nautilus.jpg"
}

May  3 19:19:34.667: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May  3 19:19:34.667: INFO: update-demo-nautilus-kf28b is verified up and running
STEP: rolling-update to new replication controller
May  3 19:19:34.669: INFO: scanned /root for discovery docs: <nil>
May  3 19:19:34.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2126'
May  3 19:19:57.549: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May  3 19:19:57.550: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May  3 19:19:57.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2126'
May  3 19:19:57.653: INFO: stderr: ""
May  3 19:19:57.653: INFO: stdout: "update-demo-kitten-4ngpv update-demo-kitten-fs9qr "
May  3 19:19:57.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-kitten-4ngpv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:57.898: INFO: stderr: ""
May  3 19:19:57.898: INFO: stdout: "true"
May  3 19:19:57.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-kitten-4ngpv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:57.993: INFO: stderr: ""
May  3 19:19:57.994: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  3 19:19:57.994: INFO: validating pod update-demo-kitten-4ngpv
May  3 19:19:58.010: INFO: got data: {
  "image": "kitten.jpg"
}

May  3 19:19:58.010: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  3 19:19:58.010: INFO: update-demo-kitten-4ngpv is verified up and running
May  3 19:19:58.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-kitten-fs9qr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:58.111: INFO: stderr: ""
May  3 19:19:58.111: INFO: stdout: "true"
May  3 19:19:58.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods update-demo-kitten-fs9qr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2126'
May  3 19:19:58.205: INFO: stderr: ""
May  3 19:19:58.205: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May  3 19:19:58.205: INFO: validating pod update-demo-kitten-fs9qr
May  3 19:19:58.221: INFO: got data: {
  "image": "kitten.jpg"
}

May  3 19:19:58.222: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May  3 19:19:58.222: INFO: update-demo-kitten-fs9qr is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:19:58.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2126" for this suite.
May  3 19:20:22.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:20:22.526: INFO: namespace kubectl-2126 deletion completed in 24.29572062s

â€¢ [SLOW TEST:54.074 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:20:22.527: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5912
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:20:22.756: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7f497498-6dd8-11e9-a061-0e68f10e50c0", Controller:(*bool)(0xc002adf7a6), BlockOwnerDeletion:(*bool)(0xc002adf7a7)}}
May  3 19:20:22.771: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"7f460a3e-6dd8-11e9-a061-0e68f10e50c0", Controller:(*bool)(0xc00211e676), BlockOwnerDeletion:(*bool)(0xc00211e677)}}
May  3 19:20:22.780: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7f478b61-6dd8-11e9-a061-0e68f10e50c0", Controller:(*bool)(0xc0019b5356), BlockOwnerDeletion:(*bool)(0xc0019b5357)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:20:27.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5912" for this suite.
May  3 19:20:33.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:20:34.132: INFO: namespace gc-5912 deletion completed in 6.319115767s

â€¢ [SLOW TEST:11.606 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:20:34.133: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:20:34.358: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May  3 19:20:39.366: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May  3 19:20:39.367: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May  3 19:20:41.531: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9416,SelfLink:/apis/apps/v1/namespaces/deployment-9416/deployments/test-cleanup-deployment,UID:893c7258-6dd8-11e9-a061-0e68f10e50c0,ResourceVersion:50035,Generation:1,CreationTimestamp:2019-05-03 19:20:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-03 19:20:39 +0000 UTC 2019-05-03 19:20:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-03 19:20:40 +0000 UTC 2019-05-03 19:20:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May  3 19:20:41.538: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-9416,SelfLink:/apis/apps/v1/namespaces/deployment-9416/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:893f79d9-6dd8-11e9-a061-0e68f10e50c0,ResourceVersion:50024,Generation:1,CreationTimestamp:2019-05-03 19:20:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 893c7258-6dd8-11e9-a061-0e68f10e50c0 0xc000b05487 0xc000b05488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May  3 19:20:41.545: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-q8lwp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-q8lwp,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-9416,SelfLink:/api/v1/namespaces/deployment-9416/pods/test-cleanup-deployment-55cbfbc8f5-q8lwp,UID:89408e06-6dd8-11e9-a061-0e68f10e50c0,ResourceVersion:50023,Generation:0,CreationTimestamp:2019-05-03 19:20:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 893f79d9-6dd8-11e9-a061-0e68f10e50c0 0xc000b05cf7 0xc000b05cf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4kr2s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4kr2s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-4kr2s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.141,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000b05d80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000b05df0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:20:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:20:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:20:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:20:39 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.141,PodIP:172.30.47.157,StartTime:2019-05-03 19:20:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-03 19:20:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://bf5d89c0aea5bda9b6571061cf479bca9035996af9b7e7eba8f4d959a3a62c65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:20:41.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9416" for this suite.
May  3 19:20:47.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:20:47.803: INFO: namespace deployment-9416 deletion completed in 6.248971218s

â€¢ [SLOW TEST:13.670 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:20:47.807: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May  3 19:20:52.094: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  3 19:20:52.101: INFO: Pod pod-with-poststart-http-hook still exists
May  3 19:20:54.101: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  3 19:20:54.110: INFO: Pod pod-with-poststart-http-hook still exists
May  3 19:20:56.101: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May  3 19:20:56.126: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:20:56.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3390" for this suite.
May  3 19:21:12.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:21:12.447: INFO: namespace container-lifecycle-hook-3390 deletion completed in 16.313873137s

â€¢ [SLOW TEST:24.641 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:21:12.448: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-3007
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May  3 19:21:12.652: INFO: Waiting up to 5m0s for pod "var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788" in namespace "var-expansion-3007" to be "success or failure"
May  3 19:21:12.659: INFO: Pod "var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.583201ms
May  3 19:21:14.671: INFO: Pod "var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018948682s
STEP: Saw pod success
May  3 19:21:14.671: INFO: Pod "var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:21:14.678: INFO: Trying to get logs from node 10.190.144.140 pod var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 19:21:14.717: INFO: Waiting for pod var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:21:14.724: INFO: Pod var-expansion-9d072cf5-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:21:14.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3007" for this suite.
May  3 19:21:20.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:21:21.142: INFO: namespace var-expansion-3007 deletion completed in 6.316631773s

â€¢ [SLOW TEST:8.694 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:21:21.143: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:21:21.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788" in namespace "projected-114" to be "success or failure"
May  3 19:21:21.442: INFO: Pod "downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.90246ms
May  3 19:21:23.450: INFO: Pod "downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016321049s
STEP: Saw pod success
May  3 19:21:23.450: INFO: Pod "downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:21:23.459: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:21:23.518: INFO: Waiting for pod downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788 to disappear
May  3 19:21:23.525: INFO: Pod downwardapi-volume-a23558cd-6dd8-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:21:23.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-114" for this suite.
May  3 19:21:29.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:21:29.861: INFO: namespace projected-114 deletion completed in 6.328143301s

â€¢ [SLOW TEST:8.718 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:21:29.861: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3062
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May  3 19:21:33.120: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:21:34.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3062" for this suite.
May  3 19:21:56.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:21:56.451: INFO: namespace replicaset-3062 deletion completed in 22.286762605s

â€¢ [SLOW TEST:26.590 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:21:56.451: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2014
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-2014
May  3 19:21:58.753: INFO: Started pod liveness-http in namespace container-probe-2014
STEP: checking the pod's current state and verifying that restartCount is present
May  3 19:21:58.763: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:00.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2014" for this suite.
May  3 19:26:06.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:06.536: INFO: namespace container-probe-2014 deletion completed in 6.316605159s

â€¢ [SLOW TEST:250.084 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:06.536: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3553
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May  3 19:26:06.742: INFO: Waiting up to 5m0s for pod "downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788" in namespace "downward-api-3553" to be "success or failure"
May  3 19:26:06.749: INFO: Pod "downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.780698ms
May  3 19:26:08.758: INFO: Pod "downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015807039s
STEP: Saw pod success
May  3 19:26:08.758: INFO: Pod "downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:26:08.766: INFO: Trying to get logs from node 10.190.144.141 pod downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788 container dapi-container: <nil>
STEP: delete the pod
May  3 19:26:08.814: INFO: Waiting for pod downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:26:08.820: INFO: Pod downward-api-4c51cc2a-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:08.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3553" for this suite.
May  3 19:26:14.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:15.142: INFO: namespace downward-api-3553 deletion completed in 6.314429949s

â€¢ [SLOW TEST:8.606 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:15.145: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6773
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May  3 19:26:15.480: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6773,SelfLink:/api/v1/namespaces/watch-6773/configmaps/e2e-watch-test-resource-version,UID:51729252-6dd9-11e9-a061-0e68f10e50c0,ResourceVersion:50899,Generation:0,CreationTimestamp:2019-05-03 19:26:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  3 19:26:15.480: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6773,SelfLink:/api/v1/namespaces/watch-6773/configmaps/e2e-watch-test-resource-version,UID:51729252-6dd9-11e9-a061-0e68f10e50c0,ResourceVersion:50900,Generation:0,CreationTimestamp:2019-05-03 19:26:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:15.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6773" for this suite.
May  3 19:26:21.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:21.730: INFO: namespace watch-6773 deletion completed in 6.241612148s

â€¢ [SLOW TEST:6.585 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:21.730: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4309
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May  3 19:26:21.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 create -f - --namespace=kubectl-4309'
May  3 19:26:22.197: INFO: stderr: ""
May  3 19:26:22.197: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May  3 19:26:23.204: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:26:23.205: INFO: Found 0 / 1
May  3 19:26:24.206: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:26:24.206: INFO: Found 1 / 1
May  3 19:26:24.206: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May  3 19:26:24.218: INFO: Selector matched 1 pods for map[app:redis]
May  3 19:26:24.218: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May  3 19:26:24.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 logs redis-master-d2l4j redis-master --namespace=kubectl-4309'
May  3 19:26:24.371: INFO: stderr: ""
May  3 19:26:24.371: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 May 19:26:23.386 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 May 19:26:23.386 # Server started, Redis version 3.2.12\n1:M 03 May 19:26:23.386 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 May 19:26:23.386 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May  3 19:26:24.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 log redis-master-d2l4j redis-master --namespace=kubectl-4309 --tail=1'
May  3 19:26:24.504: INFO: stderr: ""
May  3 19:26:24.504: INFO: stdout: "1:M 03 May 19:26:23.386 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May  3 19:26:24.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 log redis-master-d2l4j redis-master --namespace=kubectl-4309 --limit-bytes=1'
May  3 19:26:24.619: INFO: stderr: ""
May  3 19:26:24.619: INFO: stdout: " "
STEP: exposing timestamps
May  3 19:26:24.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 log redis-master-d2l4j redis-master --namespace=kubectl-4309 --tail=1 --timestamps'
May  3 19:26:24.753: INFO: stderr: ""
May  3 19:26:24.753: INFO: stdout: "2019-05-03T19:26:23.387006909Z 1:M 03 May 19:26:23.386 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May  3 19:26:27.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 log redis-master-d2l4j redis-master --namespace=kubectl-4309 --since=1s'
May  3 19:26:27.441: INFO: stderr: ""
May  3 19:26:27.441: INFO: stdout: ""
May  3 19:26:27.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 log redis-master-d2l4j redis-master --namespace=kubectl-4309 --since=24h'
May  3 19:26:27.567: INFO: stderr: ""
May  3 19:26:27.567: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 May 19:26:23.386 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 May 19:26:23.386 # Server started, Redis version 3.2.12\n1:M 03 May 19:26:23.386 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 May 19:26:23.386 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May  3 19:26:27.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete --grace-period=0 --force -f - --namespace=kubectl-4309'
May  3 19:26:27.693: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May  3 19:26:27.693: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May  3 19:26:27.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4309'
May  3 19:26:27.802: INFO: stderr: "No resources found.\n"
May  3 19:26:27.802: INFO: stdout: ""
May  3 19:26:27.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 get pods -l name=nginx --namespace=kubectl-4309 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May  3 19:26:27.916: INFO: stderr: ""
May  3 19:26:27.916: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:27.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4309" for this suite.
May  3 19:26:33.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:34.212: INFO: namespace kubectl-4309 deletion completed in 6.287112488s

â€¢ [SLOW TEST:12.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:34.213: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6069
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May  3 19:26:34.422: INFO: Waiting up to 5m0s for pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788" in namespace "containers-6069" to be "success or failure"
May  3 19:26:34.429: INFO: Pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.361305ms
May  3 19:26:36.438: INFO: Pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016273263s
May  3 19:26:38.446: INFO: Pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024236784s
May  3 19:26:40.455: INFO: Pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033183757s
STEP: Saw pod success
May  3 19:26:40.455: INFO: Pod "client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:26:40.462: INFO: Trying to get logs from node 10.190.144.133 pod client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:26:40.507: INFO: Waiting for pod client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:26:40.515: INFO: Pod client-containers-5cd195ba-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:40.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6069" for this suite.
May  3 19:26:46.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:46.778: INFO: namespace containers-6069 deletion completed in 6.252076066s

â€¢ [SLOW TEST:12.565 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:46.779: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:26:47.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4035" for this suite.
May  3 19:26:53.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:26:53.336: INFO: namespace kubelet-test-4035 deletion completed in 6.310755324s

â€¢ [SLOW TEST:6.557 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:26:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5752
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May  3 19:26:56.102: INFO: Successfully updated pod "labelsupdate683692ef-6dd9-11e9-8617-baa18b820788"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:27:00.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5752" for this suite.
May  3 19:27:22.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:27:22.424: INFO: namespace downward-api-5752 deletion completed in 22.26375745s

â€¢ [SLOW TEST:29.088 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:27:22.427: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6798
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:27:22.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788" in namespace "downward-api-6798" to be "success or failure"
May  3 19:27:22.632: INFO: Pod "downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.391196ms
May  3 19:27:24.641: INFO: Pod "downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016236495s
May  3 19:27:26.649: INFO: Pod "downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024789408s
STEP: Saw pod success
May  3 19:27:26.649: INFO: Pod "downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:27:26.662: INFO: Trying to get logs from node 10.190.144.133 pod downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:27:26.706: INFO: Waiting for pod downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:27:26.718: INFO: Pod downwardapi-volume-798cb592-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:27:26.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6798" for this suite.
May  3 19:27:32.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:27:33.033: INFO: namespace downward-api-6798 deletion completed in 6.30667674s

â€¢ [SLOW TEST:10.606 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:27:33.034: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: Gathering metrics
W0503 19:27:34.307773      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 19:27:34.307: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:27:34.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8943" for this suite.
May  3 19:27:40.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:27:40.634: INFO: namespace gc-8943 deletion completed in 6.316530304s

â€¢ [SLOW TEST:7.600 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:27:40.635: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3932
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May  3 19:27:40.935: INFO: Waiting up to 5m0s for pod "client-containers-84768bc3-6dd9-11e9-8617-baa18b820788" in namespace "containers-3932" to be "success or failure"
May  3 19:27:40.942: INFO: Pod "client-containers-84768bc3-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.28537ms
May  3 19:27:42.951: INFO: Pod "client-containers-84768bc3-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015650574s
STEP: Saw pod success
May  3 19:27:42.951: INFO: Pod "client-containers-84768bc3-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:27:42.958: INFO: Trying to get logs from node 10.190.144.133 pod client-containers-84768bc3-6dd9-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:27:42.999: INFO: Waiting for pod client-containers-84768bc3-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:27:43.007: INFO: Pod client-containers-84768bc3-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:27:43.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3932" for this suite.
May  3 19:27:49.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:27:49.337: INFO: namespace containers-3932 deletion completed in 6.320476704s

â€¢ [SLOW TEST:8.702 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:27:49.337: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7527
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:27:49.537: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788" in namespace "downward-api-7527" to be "success or failure"
May  3 19:27:49.545: INFO: Pod "downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.480553ms
May  3 19:27:51.552: INFO: Pod "downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015228253s
STEP: Saw pod success
May  3 19:27:51.552: INFO: Pod "downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:27:51.559: INFO: Trying to get logs from node 10.190.144.141 pod downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:27:51.598: INFO: Waiting for pod downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:27:51.605: INFO: Pod downwardapi-volume-8996f928-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:27:51.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7527" for this suite.
May  3 19:27:57.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:27:57.932: INFO: namespace downward-api-7527 deletion completed in 6.318141392s

â€¢ [SLOW TEST:8.595 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:27:57.933: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May  3 19:27:58.134: INFO: Waiting up to 5m0s for pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788" in namespace "emptydir-8978" to be "success or failure"
May  3 19:27:58.141: INFO: Pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.670609ms
May  3 19:28:00.149: INFO: Pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014930024s
May  3 19:28:02.158: INFO: Pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024256831s
May  3 19:28:04.167: INFO: Pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033099728s
STEP: Saw pod success
May  3 19:28:04.167: INFO: Pod "pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:28:04.175: INFO: Trying to get logs from node 10.190.144.141 pod pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:28:04.226: INFO: Waiting for pod pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:28:04.234: INFO: Pod pod-8eb6d5e6-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:28:04.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8978" for this suite.
May  3 19:28:10.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:28:10.543: INFO: namespace emptydir-8978 deletion completed in 6.297757772s

â€¢ [SLOW TEST:12.610 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:28:10.545: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1501
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4506
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:28:35.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4052" for this suite.
May  3 19:28:41.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:28:41.609: INFO: namespace namespaces-4052 deletion completed in 6.380660032s
STEP: Destroying namespace "nsdeletetest-1501" for this suite.
May  3 19:28:41.615: INFO: Namespace nsdeletetest-1501 was already deleted
STEP: Destroying namespace "nsdeletetest-4506" for this suite.
May  3 19:28:47.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:28:47.934: INFO: namespace nsdeletetest-4506 deletion completed in 6.318987524s

â€¢ [SLOW TEST:37.389 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:28:47.937: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7605
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ac93da18-6dd9-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:28:48.243: INFO: Waiting up to 5m0s for pod "pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788" in namespace "secrets-7605" to be "success or failure"
May  3 19:28:48.250: INFO: Pod "pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.974099ms
May  3 19:28:50.258: INFO: Pod "pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015413264s
STEP: Saw pod success
May  3 19:28:50.258: INFO: Pod "pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:28:50.266: INFO: Trying to get logs from node 10.190.144.140 pod pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:28:50.318: INFO: Waiting for pod pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:28:50.325: INFO: Pod pod-secrets-ac9512f9-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:28:50.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7605" for this suite.
May  3 19:28:56.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:28:56.573: INFO: namespace secrets-7605 deletion completed in 6.238867194s

â€¢ [SLOW TEST:8.636 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:28:56.573: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7351
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-b1b40567-6dd9-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:28:56.844: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788" in namespace "projected-7351" to be "success or failure"
May  3 19:28:56.852: INFO: Pod "pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650291ms
May  3 19:28:58.860: INFO: Pod "pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.016222134s
May  3 19:29:00.870: INFO: Pod "pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025541396s
STEP: Saw pod success
May  3 19:29:00.870: INFO: Pod "pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:29:00.878: INFO: Trying to get logs from node 10.190.144.140 pod pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:29:00.925: INFO: Waiting for pod pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788 to disappear
May  3 19:29:00.932: INFO: Pod pod-projected-configmaps-b1b56ec2-6dd9-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:29:00.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7351" for this suite.
May  3 19:29:06.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:29:07.244: INFO: namespace projected-7351 deletion completed in 6.303517868s

â€¢ [SLOW TEST:10.671 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:29:07.247: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7583
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7583
May  3 19:29:09.487: INFO: Started pod liveness-http in namespace container-probe-7583
STEP: checking the pod's current state and verifying that restartCount is present
May  3 19:29:09.494: INFO: Initial restart count of pod liveness-http is 0
May  3 19:29:21.703: INFO: Restart count of pod container-probe-7583/liveness-http is now 1 (12.208651791s elapsed)
May  3 19:29:41.786: INFO: Restart count of pod container-probe-7583/liveness-http is now 2 (32.292066509s elapsed)
May  3 19:30:01.868: INFO: Restart count of pod container-probe-7583/liveness-http is now 3 (52.374089266s elapsed)
May  3 19:30:21.954: INFO: Restart count of pod container-probe-7583/liveness-http is now 4 (1m12.460316623s elapsed)
May  3 19:31:32.384: INFO: Restart count of pod container-probe-7583/liveness-http is now 5 (2m22.889880983s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:31:32.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7583" for this suite.
May  3 19:31:38.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:31:38.735: INFO: namespace container-probe-7583 deletion completed in 6.308369774s

â€¢ [SLOW TEST:151.489 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:31:38.737: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2266
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-1253831f-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:31:38.951: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788" in namespace "projected-2266" to be "success or failure"
May  3 19:31:38.959: INFO: Pod "pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.675934ms
May  3 19:31:40.970: INFO: Pod "pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018685383s
STEP: Saw pod success
May  3 19:31:40.970: INFO: Pod "pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:31:40.978: INFO: Trying to get logs from node 10.190.144.141 pod pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:31:41.030: INFO: Waiting for pod pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:31:41.037: INFO: Pod pod-projected-secrets-1254bcd4-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:31:41.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2266" for this suite.
May  3 19:31:47.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:31:47.349: INFO: namespace projected-2266 deletion completed in 6.303756147s

â€¢ [SLOW TEST:8.613 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:31:47.350: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3191
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:31:47.548: INFO: Creating ReplicaSet my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788
May  3 19:31:47.564: INFO: Pod name my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788: Found 0 pods out of 1
May  3 19:31:52.575: INFO: Pod name my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788: Found 1 pods out of 1
May  3 19:31:52.575: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788" is running
May  3 19:31:52.625: INFO: Pod "my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788-gtg9r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 19:31:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 19:31:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 19:31:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-03 19:31:47 +0000 UTC Reason: Message:}])
May  3 19:31:52.625: INFO: Trying to dial the pod
May  3 19:31:57.657: INFO: Controller my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788: Got expected result from replica 1 [my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788-gtg9r]: "my-hostname-basic-1776f4cf-6dda-11e9-8617-baa18b820788-gtg9r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:31:57.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3191" for this suite.
May  3 19:32:03.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:32:03.955: INFO: namespace replicaset-3191 deletion completed in 6.285842522s

â€¢ [SLOW TEST:16.605 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:32:03.956: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-389
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May  3 19:32:04.257: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52114,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May  3 19:32:04.257: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52115,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May  3 19:32:04.258: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52116,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May  3 19:32:14.332: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52138,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May  3 19:32:14.332: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52139,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May  3 19:32:14.332: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-389,SelfLink:/api/v1/namespaces/watch-389/configmaps/e2e-watch-test-label-changed,UID:21675268-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:52140,Generation:0,CreationTimestamp:2019-05-03 19:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:32:14.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-389" for this suite.
May  3 19:32:20.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:32:20.587: INFO: namespace watch-389 deletion completed in 6.245412219s

â€¢ [SLOW TEST:16.632 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:32:20.587: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1145
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-1145.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1145.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 197.134.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.134.197_udp@PTR;check="$$(dig +tcp +noall +answer +search 197.134.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.134.197_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-1145.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-1145.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-1145.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-1145.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-1145.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1145.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 197.134.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.134.197_udp@PTR;check="$$(dig +tcp +noall +answer +search 197.134.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.134.197_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May  3 19:32:24.866: INFO: Unable to read wheezy_udp@dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:24.878: INFO: Unable to read wheezy_tcp@dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:24.889: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:24.901: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:24.978: INFO: Unable to read jessie_udp@dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:24.989: INFO: Unable to read jessie_tcp@dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:25.001: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:25.012: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local from pod dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788: the server could not find the requested resource (get pods dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788)
May  3 19:32:25.087: INFO: Lookups using dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788 failed for: [wheezy_udp@dns-test-service.dns-1145.svc.cluster.local wheezy_tcp@dns-test-service.dns-1145.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local jessie_udp@dns-test-service.dns-1145.svc.cluster.local jessie_tcp@dns-test-service.dns-1145.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-1145.svc.cluster.local]

May  3 19:32:30.412: INFO: DNS probes using dns-1145/dns-test-2b4a09b7-6dda-11e9-8617-baa18b820788 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:32:30.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1145" for this suite.
May  3 19:32:36.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:32:36.869: INFO: namespace dns-1145 deletion completed in 6.340979704s

â€¢ [SLOW TEST:16.282 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:32:36.869: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1739
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-34fb6926-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:32:37.091: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788" in namespace "projected-1739" to be "success or failure"
May  3 19:32:37.098: INFO: Pod "pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.730528ms
May  3 19:32:39.108: INFO: Pod "pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016763407s
STEP: Saw pod success
May  3 19:32:39.108: INFO: Pod "pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:32:39.114: INFO: Trying to get logs from node 10.190.144.141 pod pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788 container projected-secret-volume-test: <nil>
STEP: delete the pod
May  3 19:32:39.158: INFO: Waiting for pod pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:32:39.164: INFO: Pod pod-projected-secrets-34fc7deb-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:32:39.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1739" for this suite.
May  3 19:32:45.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:32:45.430: INFO: namespace projected-1739 deletion completed in 6.248046994s

â€¢ [SLOW TEST:8.561 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:32:45.431: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2659
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May  3 19:32:45.625: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May  3 19:32:45.725: INFO: Waiting for terminating namespaces to be deleted...
May  3 19:32:45.730: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.133 before test
May  3 19:32:45.757: INFO: ibm-keepalived-watcher-plptw from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:32:45.758: INFO: calico-kube-controllers-5cf89ccdcb-4jsv6 from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container calico-kube-controllers ready: true, restart count 0
May  3 19:32:45.758: INFO: vpn-6df9c5c5bb-fj8dg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container vpn ready: true, restart count 0
May  3 19:32:45.758: INFO: test-k8s-e2e-pvg-master-verification from default started at 2019-05-03 16:10:24 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
May  3 19:32:45.758: INFO: coredns-autoscaler-6d8b6f867-v8hwf from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container autoscaler ready: true, restart count 0
May  3 19:32:45.758: INFO: ibm-file-plugin-547c887746-s8jqz from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
May  3 19:32:45.758: INFO: coredns-d5dcb59f8-9j6zs from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container coredns ready: true, restart count 0
May  3 19:32:45.758: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-03 18:03:50 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May  3 19:32:45.758: INFO: ibm-master-proxy-static-10.190.144.133 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:32:45.758: INFO: ibm-kube-fluentd-hbqql from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container fluentd ready: true, restart count 0
May  3 19:32:45.758: INFO: kubernetes-dashboard-6c88b75685-pc4kh from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
May  3 19:32:45.758: INFO: ibm-storage-watcher-84958d564f-9xnvg from kube-system started at 2019-05-03 14:28:27 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
May  3 19:32:45.758: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-nms5m from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:32:45.758: INFO: 	Container systemd-logs ready: true, restart count 1
May  3 19:32:45.758: INFO: calico-node-fkw9t from kube-system started at 2019-05-03 14:28:03 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.758: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:32:45.758: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.140 before test
May  3 19:32:45.788: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-248dv from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:32:45.788: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:32:45.788: INFO: 	Container systemd-logs ready: true, restart count 1
May  3 19:32:45.788: INFO: ibm-kube-fluentd-pp6bq from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.788: INFO: 	Container fluentd ready: true, restart count 0
May  3 19:32:45.788: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-8hdkw from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.788: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
May  3 19:32:45.789: INFO: coredns-d5dcb59f8-gqs2d from kube-system started at 2019-05-03 14:28:49 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.789: INFO: 	Container coredns ready: true, restart count 0
May  3 19:32:45.789: INFO: metrics-server-675d59648f-rhfdj from kube-system started at 2019-05-03 14:30:15 +0000 UTC (2 container statuses recorded)
May  3 19:32:45.789: INFO: 	Container metrics-server ready: true, restart count 0
May  3 19:32:45.789: INFO: 	Container metrics-server-nanny ready: true, restart count 0
May  3 19:32:45.789: INFO: calico-node-wrx89 from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.789: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:32:45.789: INFO: ibm-keepalived-watcher-7w57c from kube-system started at 2019-05-03 14:28:34 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.789: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:32:45.789: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-c8k9z from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 19:32:45.789: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 19:32:45.789: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 19:32:45.789: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 19:32:45.789: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 19:32:45.789: INFO: ibm-master-proxy-static-10.190.144.140 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:32:45.789: INFO: 
Logging pods the kubelet thinks is on node 10.190.144.141 before test
May  3 19:32:45.816: INFO: ibm-keepalived-watcher-wccsc from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.816: INFO: 	Container keepalived-watcher ready: true, restart count 0
May  3 19:32:45.816: INFO: ibm-master-proxy-static-10.190.144.141 from kube-system started at <nil> (0 container statuses recorded)
May  3 19:32:45.817: INFO: ibm-cloud-provider-ip-169-61-107-254-74b5f87d5d-d6x9d from ibm-system started at 2019-05-03 14:30:24 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container ibm-cloud-provider-ip-169-61-107-254 ready: true, restart count 0
May  3 19:32:45.817: INFO: public-crc2175f9816074c889cbbe7987e82ffd7-alb1-798874949c-59k8h from kube-system started at 2019-05-03 14:30:41 +0000 UTC (4 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container ingress-auth-1 ready: true, restart count 0
May  3 19:32:45.817: INFO: 	Container ingress-auth-2 ready: true, restart count 0
May  3 19:32:45.817: INFO: 	Container ingress-auth-3 ready: true, restart count 0
May  3 19:32:45.817: INFO: 	Container nginx-ingress ready: true, restart count 0
May  3 19:32:45.817: INFO: sonobuoy-e2e-job-e0d8049d571e43ef from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container e2e ready: true, restart count 0
May  3 19:32:45.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May  3 19:32:45.817: INFO: sonobuoy-systemd-logs-daemon-set-52cca3793892431e-494vt from heptio-sonobuoy started at 2019-05-03 18:03:52 +0000 UTC (2 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May  3 19:32:45.817: INFO: 	Container systemd-logs ready: true, restart count 1
May  3 19:32:45.817: INFO: calico-node-9wqc8 from kube-system started at 2019-05-03 14:27:59 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container calico-node ready: true, restart count 0
May  3 19:32:45.817: INFO: ibm-kube-fluentd-tlcsj from kube-system started at 2019-05-03 14:28:41 +0000 UTC (1 container statuses recorded)
May  3 19:32:45.817: INFO: 	Container fluentd ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159b432f204b7fe7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:32:46.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2659" for this suite.
May  3 19:32:52.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:32:53.132: INFO: namespace sched-pred-2659 deletion completed in 6.246639217s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.701 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:32:53.132: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-3eab1051-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:32:53.342: INFO: Waiting up to 5m0s for pod "pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788" in namespace "configmap-3723" to be "success or failure"
May  3 19:32:53.349: INFO: Pod "pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.150078ms
May  3 19:32:55.357: INFO: Pod "pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014985151s
STEP: Saw pod success
May  3 19:32:55.357: INFO: Pod "pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:32:55.365: INFO: Trying to get logs from node 10.190.144.141 pod pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788 container configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:32:55.404: INFO: Waiting for pod pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:32:55.411: INFO: Pod pod-configmaps-3eac2d6f-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:32:55.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3723" for this suite.
May  3 19:33:01.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:33:01.750: INFO: namespace configmap-3723 deletion completed in 6.330808652s

â€¢ [SLOW TEST:8.618 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:33:01.750: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-43cfa84d-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:33:01.974: INFO: Waiting up to 5m0s for pod "pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788" in namespace "secrets-8560" to be "success or failure"
May  3 19:33:01.983: INFO: Pod "pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.3711ms
May  3 19:33:03.991: INFO: Pod "pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016572773s
STEP: Saw pod success
May  3 19:33:03.991: INFO: Pod "pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:33:04.000: INFO: Trying to get logs from node 10.190.144.133 pod pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:33:04.048: INFO: Waiting for pod pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:33:04.055: INFO: Pod pod-secrets-43d0fdcf-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:33:04.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8560" for this suite.
May  3 19:33:10.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:33:10.322: INFO: namespace secrets-8560 deletion completed in 6.258980709s

â€¢ [SLOW TEST:8.572 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:33:10.323: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9463
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0503 19:33:16.580116      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May  3 19:33:16.580: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:33:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9463" for this suite.
May  3 19:33:24.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:33:24.836: INFO: namespace gc-9463 deletion completed in 8.249019079s

â€¢ [SLOW TEST:14.514 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:33:24.838: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-22
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May  3 19:33:29.340: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:29.354: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:31.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:31.363: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:33.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:33.362: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:35.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:35.362: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:37.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:37.362: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:39.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:39.362: INFO: Pod pod-with-prestop-http-hook still exists
May  3 19:33:41.354: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May  3 19:33:41.362: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:33:41.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-22" for this suite.
May  3 19:34:05.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:34:05.632: INFO: namespace container-lifecycle-hook-22 deletion completed in 24.245841521s

â€¢ [SLOW TEST:40.794 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:34:05.634: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1557
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4615
STEP: Creating secret with name secret-test-69e2d2f5-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume secrets
May  3 19:34:06.062: INFO: Waiting up to 5m0s for pod "pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788" in namespace "secrets-1557" to be "success or failure"
May  3 19:34:06.070: INFO: Pod "pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.118503ms
May  3 19:34:08.078: INFO: Pod "pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016374297s
STEP: Saw pod success
May  3 19:34:08.078: INFO: Pod "pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:34:08.085: INFO: Trying to get logs from node 10.190.144.140 pod pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788 container secret-volume-test: <nil>
STEP: delete the pod
May  3 19:34:08.127: INFO: Waiting for pod pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:34:08.218: INFO: Pod pod-secrets-6a043e67-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:34:08.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1557" for this suite.
May  3 19:34:14.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:34:14.571: INFO: namespace secrets-1557 deletion completed in 6.343329941s
STEP: Destroying namespace "secret-namespace-4615" for this suite.
May  3 19:34:20.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:34:20.851: INFO: namespace secret-namespace-4615 deletion completed in 6.280340896s

â€¢ [SLOW TEST:15.217 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:34:20.854: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May  3 19:34:21.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6601'
May  3 19:34:21.312: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May  3 19:34:21.312: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May  3 19:34:21.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-963657699 delete jobs e2e-test-nginx-job --namespace=kubectl-6601'
May  3 19:34:21.443: INFO: stderr: ""
May  3 19:34:21.443: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:34:21.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6601" for this suite.
May  3 19:34:43.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:34:43.747: INFO: namespace kubectl-6601 deletion completed in 22.293637731s

â€¢ [SLOW TEST:22.893 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:34:43.747: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-3251
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May  3 19:34:46.066: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-80a62cd8-6dda-11e9-8617-baa18b820788,GenerateName:,Namespace:events-3251,SelfLink:/api/v1/namespaces/events-3251/pods/send-events-80a62cd8-6dda-11e9-8617-baa18b820788,UID:80a75d3b-6dda-11e9-a061-0e68f10e50c0,ResourceVersion:53055,Generation:0,CreationTimestamp:2019-05-03 19:34:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 18305275,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9l9zn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9l9zn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-9l9zn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.190.144.133,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002eefe10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002eefe30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:34:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:34:45 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:34:45 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-03 19:34:44 +0000 UTC  }],Message:,Reason:,HostIP:10.190.144.133,PodIP:172.30.112.82,StartTime:2019-05-03 19:34:44 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-03 19:34:45 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://efb735b7ec51b84df749031407aabc7d37e14c9206d4bf749fd378409b447587}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May  3 19:34:48.074: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May  3 19:34:50.083: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:34:50.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3251" for this suite.
May  3 19:35:30.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:35:30.440: INFO: namespace events-3251 deletion completed in 40.332457443s

â€¢ [SLOW TEST:46.693 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:35:30.441: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7395
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May  3 19:35:30.740: INFO: Found 0 stateful pods, waiting for 3
May  3 19:35:40.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  3 19:35:40.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  3 19:35:40.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May  3 19:35:40.845: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May  3 19:35:50.903: INFO: Updating stateful set ss2
May  3 19:35:50.918: INFO: Waiting for Pod statefulset-7395/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May  3 19:36:01.056: INFO: Found 2 stateful pods, waiting for 3
May  3 19:36:11.066: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May  3 19:36:11.066: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May  3 19:36:11.066: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May  3 19:36:11.109: INFO: Updating stateful set ss2
May  3 19:36:11.123: INFO: Waiting for Pod statefulset-7395/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May  3 19:36:21.168: INFO: Updating stateful set ss2
May  3 19:36:21.182: INFO: Waiting for StatefulSet statefulset-7395/ss2 to complete update
May  3 19:36:21.182: INFO: Waiting for Pod statefulset-7395/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May  3 19:36:31.199: INFO: Waiting for StatefulSet statefulset-7395/ss2 to complete update
May  3 19:36:31.199: INFO: Waiting for Pod statefulset-7395/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May  3 19:36:41.199: INFO: Deleting all statefulset in ns statefulset-7395
May  3 19:36:41.206: INFO: Scaling statefulset ss2 to 0
May  3 19:37:21.240: INFO: Waiting for statefulset status.replicas updated to 0
May  3 19:37:21.248: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:37:21.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7395" for this suite.
May  3 19:37:29.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:37:29.631: INFO: namespace statefulset-7395 deletion completed in 8.312677045s

â€¢ [SLOW TEST:119.190 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:37:29.632: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9267
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:37:29.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788" in namespace "downward-api-9267" to be "success or failure"
May  3 19:37:29.848: INFO: Pod "downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.694171ms
May  3 19:37:31.856: INFO: Pod "downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788": Phase="Running", Reason="", readiness=true. Elapsed: 2.017031734s
May  3 19:37:33.865: INFO: Pod "downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026443158s
STEP: Saw pod success
May  3 19:37:33.865: INFO: Pod "downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:37:33.918: INFO: Trying to get logs from node 10.190.144.133 pod downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:37:33.978: INFO: Waiting for pod downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:37:33.988: INFO: Pod downwardapi-volume-e379ebfd-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:37:33.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9267" for this suite.
May  3 19:37:40.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:37:40.343: INFO: namespace downward-api-9267 deletion completed in 6.346789282s

â€¢ [SLOW TEST:10.711 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:37:40.345: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May  3 19:37:40.546: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788" in namespace "downward-api-7805" to be "success or failure"
May  3 19:37:40.555: INFO: Pod "downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 8.302493ms
May  3 19:37:42.564: INFO: Pod "downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017253924s
STEP: Saw pod success
May  3 19:37:42.564: INFO: Pod "downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:37:42.570: INFO: Trying to get logs from node 10.190.144.140 pod downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788 container client-container: <nil>
STEP: delete the pod
May  3 19:37:42.640: INFO: Waiting for pod downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:37:42.648: INFO: Pod downwardapi-volume-e9dbf3ce-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:37:42.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7805" for this suite.
May  3 19:37:48.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:37:48.922: INFO: namespace downward-api-7805 deletion completed in 6.259056301s

â€¢ [SLOW TEST:8.577 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:37:48.922: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7558
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May  3 19:37:49.119: INFO: Waiting up to 5m0s for pod "pod-eef84e3b-6dda-11e9-8617-baa18b820788" in namespace "emptydir-7558" to be "success or failure"
May  3 19:37:49.126: INFO: Pod "pod-eef84e3b-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 6.764386ms
May  3 19:37:51.133: INFO: Pod "pod-eef84e3b-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014086936s
STEP: Saw pod success
May  3 19:37:51.133: INFO: Pod "pod-eef84e3b-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:37:51.140: INFO: Trying to get logs from node 10.190.144.141 pod pod-eef84e3b-6dda-11e9-8617-baa18b820788 container test-container: <nil>
STEP: delete the pod
May  3 19:37:51.182: INFO: Waiting for pod pod-eef84e3b-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:37:51.189: INFO: Pod pod-eef84e3b-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:37:51.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7558" for this suite.
May  3 19:37:57.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:37:57.555: INFO: namespace emptydir-7558 deletion completed in 6.357383109s

â€¢ [SLOW TEST:8.633 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:37:57.557: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-5651
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May  3 19:37:57.746: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:37:58.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5651" for this suite.
May  3 19:38:04.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:38:05.130: INFO: namespace custom-resource-definition-5651 deletion completed in 6.269262224s

â€¢ [SLOW TEST:7.574 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May  3 19:38:05.131: INFO: >>> kubeConfig: /tmp/kubeconfig-963657699
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9771
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f8a22845-6dda-11e9-8617-baa18b820788
STEP: Creating a pod to test consume configMaps
May  3 19:38:05.342: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788" in namespace "projected-9771" to be "success or failure"
May  3 19:38:05.349: INFO: Pod "pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788": Phase="Pending", Reason="", readiness=false. Elapsed: 7.049019ms
May  3 19:38:07.357: INFO: Pod "pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015129056s
STEP: Saw pod success
May  3 19:38:07.357: INFO: Pod "pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788" satisfied condition "success or failure"
May  3 19:38:07.418: INFO: Trying to get logs from node 10.190.144.133 pod pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May  3 19:38:07.471: INFO: Waiting for pod pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788 to disappear
May  3 19:38:07.478: INFO: Pod pod-projected-configmaps-f8a35104-6dda-11e9-8617-baa18b820788 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May  3 19:38:07.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9771" for this suite.
May  3 19:38:13.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May  3 19:38:13.737: INFO: namespace projected-9771 deletion completed in 6.250148689s

â€¢ [SLOW TEST:8.607 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSMay  3 19:38:13.738: INFO: Running AfterSuite actions on all nodes
May  3 19:38:13.738: INFO: Running AfterSuite actions on node 1
May  3 19:38:13.738: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5645.907 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h34m7.104883778s
Test Suite Passed
