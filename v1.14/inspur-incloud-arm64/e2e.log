I0722 01:03:52.623105      18 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-346668565
I0722 01:03:52.623350      18 e2e.go:240] Starting e2e run "908bc1e9-ac1c-11e9-906c-f6b46dea7a80" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1563757429 - Will randomize all specs
Will run 204 of 3584 specs

Jul 22 01:03:53.073: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:03:53.078: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 22 01:03:53.115: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 22 01:03:53.187: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 22 01:03:53.187: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jul 22 01:03:53.187: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 22 01:03:53.214: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jul 22 01:03:53.214: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 22 01:03:53.214: INFO: e2e test version: v1.14.1
Jul 22 01:03:53.216: INFO: kube-apiserver version: v1.14.1
SSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:03:53.216: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename var-expansion
Jul 22 01:03:53.299: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jul 22 01:03:53.315: INFO: Waiting up to 5m0s for pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80" in namespace "var-expansion-8079" to be "success or failure"
Jul 22 01:03:53.320: INFO: Pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.74693ms
Jul 22 01:03:55.327: INFO: Pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011373487s
Jul 22 01:03:57.334: INFO: Pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018556118s
Jul 22 01:03:59.341: INFO: Pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025537943s
STEP: Saw pod success
Jul 22 01:03:59.341: INFO: Pod "var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:03:59.346: INFO: Trying to get logs from node k8s3 pod var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 01:03:59.391: INFO: Waiting for pod var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:03:59.396: INFO: Pod var-expansion-92c5ee82-ac1c-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:03:59.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8079" for this suite.
Jul 22 01:04:05.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:04:05.564: INFO: namespace var-expansion-8079 deletion completed in 6.160809785s

â€¢ [SLOW TEST:12.348 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:04:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:04:05.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 version --client'
Jul 22 01:04:05.782: INFO: stderr: ""
Jul 22 01:04:05.782: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
Jul 22 01:04:05.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-4567'
Jul 22 01:04:10.062: INFO: stderr: ""
Jul 22 01:04:10.062: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 22 01:04:10.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-4567'
Jul 22 01:04:10.355: INFO: stderr: ""
Jul 22 01:04:10.355: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 01:04:11.362: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 01:04:11.362: INFO: Found 0 / 1
Jul 22 01:04:12.370: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 01:04:12.370: INFO: Found 0 / 1
Jul 22 01:04:13.362: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 01:04:13.362: INFO: Found 1 / 1
Jul 22 01:04:13.362: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 01:04:13.367: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 01:04:13.367: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 01:04:13.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 describe pod redis-master-wf5qd --namespace=kubectl-4567'
Jul 22 01:04:13.582: INFO: stderr: ""
Jul 22 01:04:13.582: INFO: stdout: "Name:               redis-master-wf5qd\nNamespace:          kubectl-4567\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s1/100.2.97.90\nStart Time:         Mon, 22 Jul 2019 03:21:34 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.233.91.230\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://b8320663926e7d4e0aed5fc2e15bbbc5aeb811667b05e7d36337dcfcb22ac1a8\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 22 Jul 2019 03:21:36 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-l8w9j (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-l8w9j:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-l8w9j\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  <invalid>  default-scheduler  Successfully assigned kubectl-4567/redis-master-wf5qd to k8s1\n  Normal  Pulled     <invalid>  kubelet, k8s1      Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    <invalid>  kubelet, k8s1      Created container redis-master\n  Normal  Started    <invalid>  kubelet, k8s1      Started container redis-master\n"
Jul 22 01:04:13.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 describe rc redis-master --namespace=kubectl-4567'
Jul 22 01:04:13.820: INFO: stderr: ""
Jul 22 01:04:13.821: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4567\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age        From                    Message\n  ----    ------            ----       ----                    -------\n  Normal  SuccessfulCreate  <invalid>  replication-controller  Created pod: redis-master-wf5qd\n"
Jul 22 01:04:13.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 describe service redis-master --namespace=kubectl-4567'
Jul 22 01:04:14.037: INFO: stderr: ""
Jul 22 01:04:14.037: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4567\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.233.50.208\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.233.91.230:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 22 01:04:14.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 describe node k8s1'
Jul 22 01:04:14.301: INFO: stderr: ""
Jul 22 01:04:14.301: INFO: stdout: "Name:               k8s1\nRoles:              master,node\nLabels:             beta.kubernetes.io/arch=arm64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=arm64\n                    kubernetes.io/hostname=k8s1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/node=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 11 Jul 2019 19:28:24 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Mon, 22 Jul 2019 03:20:49 +0000   Thu, 11 Jul 2019 19:27:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Mon, 22 Jul 2019 03:20:49 +0000   Thu, 11 Jul 2019 19:27:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Mon, 22 Jul 2019 03:20:49 +0000   Thu, 11 Jul 2019 19:27:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Mon, 22 Jul 2019 03:20:49 +0000   Fri, 12 Jul 2019 06:22:05 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.2.97.90\n  Hostname:    k8s1\nCapacity:\n cpu:                16\n ephemeral-storage:  953231244Ki\n hugepages-2Mi:      0\n memory:             65958472Ki\n pods:               110\nAllocatable:\n cpu:                15800m\n ephemeral-storage:  878497913016\n hugepages-2Mi:      0\n memory:             65356072Ki\n pods:               110\nSystem Info:\n Machine ID:                 47e3aaac55314718ad4e196d007df659\n System UUID:                47e3aaac55314718ad4e196d007df659\n Boot ID:                    b867d22c-8935-4773-b83f-092fd93fd043\n Kernel Version:             4.4.58-20180615.kylin.server.YUN+-generic\n OS Image:                   Kylin 4.0.2\n Operating System:           linux\n Architecture:               arm64\n Container Runtime Version:  docker://18.9.5\n Kubelet Version:            v1.14.1\n Kube-Proxy Version:         v1.14.1\nPodCIDR:                     10.233.64.0/24\nNon-terminated Pods:         (21 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    etcdtest2-57bf45d5c8-9xj4s                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         <invalid>\n  default                    listening-dragon-rabbitmq-ha-0                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d17h\n  deployment-9981            nginx-deployment-6f478d8d8-8kpvr                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  deployment-9981            nginx-deployment-6f478d8d8-cb865                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  deployment-9981            nginx-deployment-6f478d8d8-t2dws                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  deployment-9981            nginx-deployment-6f478d8d8-v5r52                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d18h\n  deployment-9981            nginx-deployment-6f478d8d8-znlw8                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3d21h\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-75w7m    0 (0%)        0 (0%)      0 (0%)           0 (0%)         <invalid>\n  ingress-nginx              ingress-nginx-controller-bhvc9                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d\n  kube-system                calico-node-qfw58                                          150m (0%)     300m (1%)   64M (0%)         500M (0%)      10d\n  kube-system                coredns-848c785ddb-rghg5                                   100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     2d18h\n  kube-system                kube-apiserver-k8s1                                        250m (1%)     0 (0%)      0 (0%)           0 (0%)         10d\n  kube-system                kube-controller-manager-k8s1                               200m (1%)     0 (0%)      0 (0%)           0 (0%)         10d\n  kube-system                kube-proxy-5vpfs                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         10d\n  kube-system                kube-scheduler-k8s1                                        100m (0%)     0 (0%)      0 (0%)           0 (0%)         10d\n  kube-system                tiller-deploy-5f4c64779f-jrkt8                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d18h\n  kubectl-4567               redis-master-wf5qd                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         <invalid>\n  local-path-storage         local-path-provisioner-546d678b86-qlhkz                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9d\n  monitoring                 kube-state-metrics-7d47cbc8c5-w92wp                        136m (0%)     196m (1%)   310Mi (0%)       350Mi (0%)     2d18h\n  monitoring                 node-exporter-dtt5q                                        112m (0%)     270m (1%)   200Mi (0%)       220Mi (0%)     10d\n  monitoring                 prometheus-adapter-d7ff7d6cd-b9689                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d18h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests        Limits\n  --------           --------        ------\n  cpu                1048m (6%)      766m (4%)\n  memory             672174080 (1%)  1275946240 (1%)\n  ephemeral-storage  0 (0%)          0 (0%)\nEvents:              <none>\n"
Jul 22 01:04:14.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 describe namespace kubectl-4567'
Jul 22 01:04:14.515: INFO: stderr: ""
Jul 22 01:04:14.515: INFO: stdout: "Name:         kubectl-4567\nLabels:       e2e-framework=kubectl\n              e2e-run=908bc1e9-ac1c-11e9-906c-f6b46dea7a80\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:04:14.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4567" for this suite.
Jul 22 01:04:38.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:04:38.697: INFO: namespace kubectl-4567 deletion completed in 24.173973339s

â€¢ [SLOW TEST:33.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:04:38.697: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 22 01:04:38.780: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 01:04:38.794: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 01:04:38.799: INFO: 
Logging pods the kubelet thinks is on node k8s1 before test
Jul 22 01:04:38.817: INFO: calico-node-qfw58 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 01:04:38.817: INFO: etcdtest2-57bf45d5c8-9xj4s from default started at 2019-07-22 01:41:31 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container etcdtest2 ready: true, restart count 24
Jul 22 01:04:38.817: INFO: tiller-deploy-5f4c64779f-jrkt8 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container tiller ready: true, restart count 0
Jul 22 01:04:38.817: INFO: nginx-deployment-6f478d8d8-znlw8 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.817: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-75w7m from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:04:38.817: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:04:38.817: INFO: kube-proxy-5vpfs from kube-system started at 2019-07-11 19:32:18 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 01:04:38.817: INFO: local-path-provisioner-546d678b86-qlhkz from local-path-storage started at 2019-07-12 06:09:22 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jul 22 01:04:38.817: INFO: prometheus-adapter-d7ff7d6cd-b9689 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 22 01:04:38.817: INFO: kube-state-metrics-7d47cbc8c5-w92wp from monitoring started at 2019-07-19 06:26:32 +0000 UTC (4 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 22 01:04:38.817: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 22 01:04:38.817: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 22 01:04:38.817: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 22 01:04:38.817: INFO: kube-scheduler-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.817: INFO: listening-dragon-rabbitmq-ha-0 from default started at 2019-07-17 07:12:45 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container rabbitmq-ha ready: true, restart count 1
Jul 22 01:04:38.817: INFO: nginx-deployment-6f478d8d8-t2dws from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.817: INFO: nginx-deployment-6f478d8d8-cb865 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.817: INFO: ingress-nginx-controller-bhvc9 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container ingress-nginx-controller ready: true, restart count 6
Jul 22 01:04:38.817: INFO: nginx-deployment-6f478d8d8-8kpvr from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.817: INFO: kube-controller-manager-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.817: INFO: coredns-848c785ddb-rghg5 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container coredns ready: true, restart count 0
Jul 22 01:04:38.817: INFO: nginx-deployment-6f478d8d8-v5r52 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.817: INFO: node-exporter-dtt5q from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.817: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 01:04:38.817: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 01:04:38.818: INFO: kube-apiserver-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.818: INFO: 
Logging pods the kubelet thinks is on node k8s2 before test
Jul 22 01:04:38.837: INFO: nginx-deployment-6f478d8d8-p4pgc from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.837: INFO: alertmanager-main-0 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container alertmanager ready: true, restart count 0
Jul 22 01:04:38.837: INFO: 	Container config-reloader ready: true, restart count 0
Jul 22 01:04:38.837: INFO: node-exporter-97jbc from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 22 01:04:38.837: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 01:04:38.837: INFO: coredns-848c785ddb-r66tb from kube-system started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container coredns ready: true, restart count 0
Jul 22 01:04:38.837: INFO: nginx-deployment-6f478d8d8-smzdp from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.837: INFO: kube-apiserver-k8s2 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.837: INFO: ingress-nginx-controller-6drg8 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container ingress-nginx-controller ready: true, restart count 10
Jul 22 01:04:38.837: INFO: grafana-5ccccd76c8-nkcbl from monitoring started at 2019-07-11 20:05:14 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container grafana ready: true, restart count 0
Jul 22 01:04:38.837: INFO: prometheus-operator-9695d44c6-kjvfp from monitoring started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 22 01:04:38.837: INFO: calico-node-zjpdk from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 01:04:38.837: INFO: calico-kube-controllers-5bc4f6d669-l2mgl from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 01:04:38.837: INFO: dns-autoscaler-554d755c54-qnnsl from kube-system started at 2019-07-11 19:34:10 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 01:04:38.837: INFO: prometheus-k8s-0 from monitoring started at 2019-07-11 20:05:28 +0000 UTC (3 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 01:04:38.837: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 01:04:38.837: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 22 01:04:38.837: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-9htdd from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:04:38.837: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:04:38.837: INFO: kube-proxy-dlvss from kube-system started at 2019-07-11 19:32:23 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 01:04:38.837: INFO: nginx-deployment-6f478d8d8-vb77p from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.837: INFO: nginx-deployment-6f478d8d8-xrrg6 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.837: INFO: listening-dragon-rabbitmq-ha-1 from default started at 2019-07-17 07:16:09 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 01:04:38.837: INFO: nginx-deployment-6f478d8d8-pvn55 from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.837: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:04:38.837: INFO: 
Logging pods the kubelet thinks is on node k8s3 before test
Jul 22 01:04:38.853: INFO: testetcdxyys-769d48b897-fdl8m from default started at 2019-07-18 23:02:11 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container testetcdxyys ready: true, restart count 0
Jul 22 01:04:38.853: INFO: sonobuoy-e2e-job-3b2fad7799ec403c from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container e2e ready: true, restart count 0
Jul 22 01:04:38.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:04:38.853: INFO: calico-node-wqns7 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 01:04:38.853: INFO: listening-dragon-rabbitmq-ha-2 from default started at 2019-07-18 22:52:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 01:04:38.853: INFO: etcdtest-cbfd679c7-9cksx from default started at 2019-07-21 23:08:38 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container etcdtest ready: false, restart count 0
Jul 22 01:04:38.853: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 01:03:40 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 22 01:04:38.853: INFO: kube-apiserver-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.853: INFO: kube-scheduler-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:04:38.853: INFO: kube-proxy-lmr2d from kube-system started at 2019-07-11 19:32:28 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 01:04:38.853: INFO: node-exporter-hg26v from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 01:04:38.853: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 01:04:38.853: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-dk2k4 from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:04:38.853: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:04:38.853: INFO: ingress-nginx-controller-vhvx7 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container ingress-nginx-controller ready: true, restart count 3
Jul 22 01:04:38.853: INFO: prometheus-k8s-1 from monitoring started at 2019-07-18 22:51:45 +0000 UTC (3 container statuses recorded)
Jul 22 01:04:38.853: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 01:04:38.853: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 01:04:38.853: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node k8s1
STEP: verifying the node has the label node k8s2
STEP: verifying the node has the label node k8s3
Jul 22 01:04:38.989: INFO: Pod etcdtest-cbfd679c7-9cksx requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod etcdtest2-57bf45d5c8-9xj4s requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod listening-dragon-rabbitmq-ha-0 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod listening-dragon-rabbitmq-ha-1 requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod listening-dragon-rabbitmq-ha-2 requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod testetcdxyys-769d48b897-fdl8m requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-8kpvr requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-cb865 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-p4pgc requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-pvn55 requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-smzdp requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-t2dws requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-v5r52 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-vb77p requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-xrrg6 requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod nginx-deployment-6f478d8d8-znlw8 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod sonobuoy-e2e-job-3b2fad7799ec403c requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-75w7m requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-9htdd requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-dk2k4 requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod ingress-nginx-controller-6drg8 requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod ingress-nginx-controller-bhvc9 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod ingress-nginx-controller-vhvx7 requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod calico-kube-controllers-5bc4f6d669-l2mgl requesting resource cpu=30m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod calico-node-qfw58 requesting resource cpu=150m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod calico-node-wqns7 requesting resource cpu=150m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod calico-node-zjpdk requesting resource cpu=150m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod coredns-848c785ddb-r66tb requesting resource cpu=100m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod coredns-848c785ddb-rghg5 requesting resource cpu=100m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod dns-autoscaler-554d755c54-qnnsl requesting resource cpu=20m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod kube-apiserver-k8s1 requesting resource cpu=250m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod kube-apiserver-k8s2 requesting resource cpu=250m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod kube-apiserver-k8s3 requesting resource cpu=250m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod kube-controller-manager-k8s1 requesting resource cpu=200m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod kube-proxy-5vpfs requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod kube-proxy-dlvss requesting resource cpu=0m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod kube-proxy-lmr2d requesting resource cpu=0m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod kube-scheduler-k8s1 requesting resource cpu=100m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod kube-scheduler-k8s3 requesting resource cpu=100m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod tiller-deploy-5f4c64779f-jrkt8 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod local-path-provisioner-546d678b86-qlhkz requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod alertmanager-main-0 requesting resource cpu=100m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod grafana-5ccccd76c8-nkcbl requesting resource cpu=100m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod kube-state-metrics-7d47cbc8c5-w92wp requesting resource cpu=136m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod node-exporter-97jbc requesting resource cpu=112m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod node-exporter-dtt5q requesting resource cpu=112m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod node-exporter-hg26v requesting resource cpu=112m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod prometheus-adapter-d7ff7d6cd-b9689 requesting resource cpu=0m on Node k8s1
Jul 22 01:04:38.989: INFO: Pod prometheus-k8s-0 requesting resource cpu=150m on Node k8s2
Jul 22 01:04:38.989: INFO: Pod prometheus-k8s-1 requesting resource cpu=150m on Node k8s3
Jul 22 01:04:38.989: INFO: Pod prometheus-operator-9695d44c6-kjvfp requesting resource cpu=100m on Node k8s2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80.15b39ca36d52526c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3601/filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80 to k8s1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80.15b39ca3cc685fd0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80.15b39ca3d4787750], Reason = [Created], Message = [Created container filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80.15b39ca3e81cf31c], Reason = [Started], Message = [Started container filler-pod-ae0164ce-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80.15b39ca36ec9b090], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3601/filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80 to k8s2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80.15b39ca3d435704c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80.15b39ca3dc05c1e1], Reason = [Created], Message = [Created container filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80.15b39ca3f062b0b6], Reason = [Started], Message = [Started container filler-pod-ae03d999-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80.15b39524ee910494], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80.15b3952500d2f9b9], Reason = [Created], Message = [Created container filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80.15b395252046725f], Reason = [Started], Message = [Started container filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80.15b39ca36fa27c90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3601/filler-pod-ae06002a-ac1c-11e9-906c-f6b46dea7a80 to k8s3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b39ca4bc1bcaa8], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node k8s1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:04:46.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3601" for this suite.
Jul 22 01:04:52.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:04:52.400: INFO: namespace sched-pred-3601 deletion completed in 6.184860382s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:13.703 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:04:52.400: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:04:52.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80" in namespace "downward-api-9767" to be "success or failure"
Jul 22 01:04:52.496: INFO: Pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 6.341121ms
Jul 22 01:04:54.503: INFO: Pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012926706s
Jul 22 01:04:56.510: INFO: Pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020012206s
Jul 22 01:04:58.516: INFO: Pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026694272s
STEP: Saw pod success
Jul 22 01:04:58.516: INFO: Pod "downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:04:58.521: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:04:58.559: INFO: Waiting for pod downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:04:58.565: INFO: Pod downwardapi-volume-b60ae36d-ac1c-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:04:58.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9767" for this suite.
Jul 22 01:05:04.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:05:04.758: INFO: namespace downward-api-9767 deletion completed in 6.185709498s

â€¢ [SLOW TEST:12.358 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:05:04.758: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-bd696ae1-ac1c-11e9-906c-f6b46dea7a80
STEP: Creating secret with name s-test-opt-upd-bd696b6e-ac1c-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-bd696ae1-ac1c-11e9-906c-f6b46dea7a80
STEP: Updating secret s-test-opt-upd-bd696b6e-ac1c-11e9-906c-f6b46dea7a80
STEP: Creating secret with name s-test-opt-create-bd696ba5-ac1c-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:05:13.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8937" for this suite.
Jul 22 01:05:37.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:05:37.206: INFO: namespace secrets-8937 deletion completed in 24.190318506s

â€¢ [SLOW TEST:32.449 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:05:37.207: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 22 01:05:37.301: INFO: Waiting up to 5m0s for pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80" in namespace "downward-api-4246" to be "success or failure"
Jul 22 01:05:37.307: INFO: Pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.723144ms
Jul 22 01:05:39.317: INFO: Pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016038431s
Jul 22 01:05:41.325: INFO: Pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023848053s
Jul 22 01:05:43.331: INFO: Pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030065586s
STEP: Saw pod success
Jul 22 01:05:43.331: INFO: Pod "downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:05:43.336: INFO: Trying to get logs from node k8s3 pod downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 01:05:43.374: INFO: Waiting for pod downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:05:43.379: INFO: Pod downward-api-d0c02d91-ac1c-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:05:43.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4246" for this suite.
Jul 22 01:05:49.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:05:49.554: INFO: namespace downward-api-4246 deletion completed in 6.166214712s

â€¢ [SLOW TEST:12.347 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:05:49.554: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8754
Jul 22 01:05:53.642: INFO: Started pod liveness-exec in namespace container-probe-8754
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 01:05:53.647: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:09:54.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8754" for this suite.
Jul 22 01:10:00.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:00.691: INFO: namespace container-probe-8754 deletion completed in 6.207609151s

â€¢ [SLOW TEST:251.137 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:00.691: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-6dcbf6e3-ac1d-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:10:00.851: INFO: Waiting up to 5m0s for pod "pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80" in namespace "secrets-1206" to be "success or failure"
Jul 22 01:10:00.856: INFO: Pod "pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.940132ms
Jul 22 01:10:02.863: INFO: Pod "pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011098405s
Jul 22 01:10:04.868: INFO: Pod "pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016854067s
STEP: Saw pod success
Jul 22 01:10:04.868: INFO: Pod "pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:10:04.873: INFO: Trying to get logs from node k8s2 pod pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:10:04.919: INFO: Waiting for pod pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:10:04.934: INFO: Pod pod-secrets-6dd7be4b-ac1d-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:10:04.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1206" for this suite.
Jul 22 01:10:10.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:11.111: INFO: namespace secrets-1206 deletion completed in 6.170514392s
STEP: Destroying namespace "secret-namespace-4502" for this suite.
Jul 22 01:10:17.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:17.296: INFO: namespace secret-namespace-4502 deletion completed in 6.18533637s

â€¢ [SLOW TEST:16.605 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:17.297: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 22 01:10:17.375: INFO: Waiting up to 5m0s for pod "pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80" in namespace "emptydir-1535" to be "success or failure"
Jul 22 01:10:17.389: INFO: Pod "pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.929463ms
Jul 22 01:10:19.396: INFO: Pod "pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020786334s
Jul 22 01:10:21.402: INFO: Pod "pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026753789s
STEP: Saw pod success
Jul 22 01:10:21.402: INFO: Pod "pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:10:21.406: INFO: Trying to get logs from node k8s1 pod pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:10:21.442: INFO: Waiting for pod pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:10:21.446: INFO: Pod pod-77b068dd-ac1d-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:10:21.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1535" for this suite.
Jul 22 01:10:27.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:27.631: INFO: namespace emptydir-1535 deletion completed in 6.17723429s

â€¢ [SLOW TEST:10.334 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:27.632: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 01:10:27.725: INFO: Waiting up to 5m0s for pod "pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80" in namespace "emptydir-8064" to be "success or failure"
Jul 22 01:10:27.731: INFO: Pod "pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.442127ms
Jul 22 01:10:29.738: INFO: Pod "pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012403172s
Jul 22 01:10:31.746: INFO: Pod "pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020948051s
STEP: Saw pod success
Jul 22 01:10:31.747: INFO: Pod "pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:10:31.751: INFO: Trying to get logs from node k8s2 pod pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:10:31.788: INFO: Waiting for pod pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:10:31.792: INFO: Pod pod-7dd98834-ac1d-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:10:31.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8064" for this suite.
Jul 22 01:10:37.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:37.971: INFO: namespace emptydir-8064 deletion completed in 6.17211179s

â€¢ [SLOW TEST:10.340 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:37.972: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-8403f1b6-ac1d-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:10:38.058: INFO: Waiting up to 5m0s for pod "pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80" in namespace "configmap-4592" to be "success or failure"
Jul 22 01:10:38.063: INFO: Pod "pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.778911ms
Jul 22 01:10:40.069: INFO: Pod "pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010893785s
Jul 22 01:10:42.075: INFO: Pod "pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017078859s
STEP: Saw pod success
Jul 22 01:10:42.075: INFO: Pod "pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:10:42.080: INFO: Trying to get logs from node k8s3 pod pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:10:42.114: INFO: Waiting for pod pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:10:42.118: INFO: Pod pod-configmaps-84055ca2-ac1d-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:10:42.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4592" for this suite.
Jul 22 01:10:48.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:48.320: INFO: namespace configmap-4592 deletion completed in 6.194582582s

â€¢ [SLOW TEST:10.348 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:48.320: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-2131/secret-test-8a2ed19c-ac1d-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:10:48.407: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80" in namespace "secrets-2131" to be "success or failure"
Jul 22 01:10:48.412: INFO: Pod "pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060426ms
Jul 22 01:10:50.433: INFO: Pod "pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026521135s
Jul 22 01:10:52.440: INFO: Pod "pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032875621s
STEP: Saw pod success
Jul 22 01:10:52.440: INFO: Pod "pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:10:52.444: INFO: Trying to get logs from node k8s1 pod pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80 container env-test: <nil>
STEP: delete the pod
Jul 22 01:10:52.485: INFO: Waiting for pod pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:10:52.492: INFO: Pod pod-configmaps-8a30719a-ac1d-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:10:52.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2131" for this suite.
Jul 22 01:10:58.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:10:58.679: INFO: namespace secrets-2131 deletion completed in 6.17928892s

â€¢ [SLOW TEST:10.359 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:10:58.679: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 01:11:08.809: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 01:11:08.814: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 01:11:10.814: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 01:11:10.820: INFO: Pod pod-with-prestop-http-hook still exists
Jul 22 01:11:12.814: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 22 01:11:12.819: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:11:12.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6224" for this suite.
Jul 22 01:11:36.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:11:37.004: INFO: namespace container-lifecycle-hook-6224 deletion completed in 24.165169084s

â€¢ [SLOW TEST:38.325 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:11:37.004: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 22 01:11:37.072: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:11:42.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3441" for this suite.
Jul 22 01:11:48.992: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:11:49.161: INFO: namespace init-container-3441 deletion completed in 6.189746623s

â€¢ [SLOW TEST:12.157 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:11:49.162: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0722 01:11:55.276953      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 01:11:55.277: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:11:55.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3366" for this suite.
Jul 22 01:12:03.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:12:03.466: INFO: namespace gc-3366 deletion completed in 8.180324691s

â€¢ [SLOW TEST:14.304 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:12:03.466: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1004
I0722 01:12:03.540233      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1004, replica count: 1
I0722 01:12:04.592086      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 01:12:05.592434      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 01:12:06.592914      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 01:12:07.596619      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 01:12:07.722: INFO: Created: latency-svc-vgxmq
Jul 22 01:12:07.729: INFO: Got endpoints: latency-svc-vgxmq [32.439782ms]
Jul 22 01:12:07.762: INFO: Created: latency-svc-s62ll
Jul 22 01:12:07.768: INFO: Got endpoints: latency-svc-s62ll [38.865533ms]
Jul 22 01:12:07.775: INFO: Created: latency-svc-pd458
Jul 22 01:12:07.790: INFO: Got endpoints: latency-svc-pd458 [60.730653ms]
Jul 22 01:12:07.792: INFO: Created: latency-svc-qwtqc
Jul 22 01:12:07.812: INFO: Got endpoints: latency-svc-qwtqc [82.278098ms]
Jul 22 01:12:07.813: INFO: Created: latency-svc-4v8wq
Jul 22 01:12:07.835: INFO: Got endpoints: latency-svc-4v8wq [105.341409ms]
Jul 22 01:12:07.837: INFO: Created: latency-svc-gwcwc
Jul 22 01:12:07.856: INFO: Got endpoints: latency-svc-gwcwc [126.715434ms]
Jul 22 01:12:07.857: INFO: Created: latency-svc-5hh7c
Jul 22 01:12:07.864: INFO: Got endpoints: latency-svc-5hh7c [134.467661ms]
Jul 22 01:12:07.875: INFO: Created: latency-svc-t6xgn
Jul 22 01:12:07.887: INFO: Got endpoints: latency-svc-t6xgn [156.915746ms]
Jul 22 01:12:07.890: INFO: Created: latency-svc-rgcls
Jul 22 01:12:07.912: INFO: Got endpoints: latency-svc-rgcls [181.959911ms]
Jul 22 01:12:07.914: INFO: Created: latency-svc-ls2gk
Jul 22 01:12:07.925: INFO: Got endpoints: latency-svc-ls2gk [195.328424ms]
Jul 22 01:12:07.935: INFO: Created: latency-svc-m8hlg
Jul 22 01:12:07.957: INFO: Created: latency-svc-jwhmg
Jul 22 01:12:07.957: INFO: Got endpoints: latency-svc-m8hlg [226.998886ms]
Jul 22 01:12:07.965: INFO: Got endpoints: latency-svc-jwhmg [235.412815ms]
Jul 22 01:12:07.975: INFO: Created: latency-svc-zp6dj
Jul 22 01:12:07.993: INFO: Got endpoints: latency-svc-zp6dj [263.492069ms]
Jul 22 01:12:07.995: INFO: Created: latency-svc-jwvd2
Jul 22 01:12:08.015: INFO: Got endpoints: latency-svc-jwvd2 [285.186024ms]
Jul 22 01:12:08.017: INFO: Created: latency-svc-t4hbr
Jul 22 01:12:08.037: INFO: Got endpoints: latency-svc-t4hbr [306.924763ms]
Jul 22 01:12:08.039: INFO: Created: latency-svc-bxd59
Jul 22 01:12:08.056: INFO: Got endpoints: latency-svc-bxd59 [325.935993ms]
Jul 22 01:12:08.063: INFO: Created: latency-svc-kpljc
Jul 22 01:12:08.090: INFO: Got endpoints: latency-svc-kpljc [321.164493ms]
Jul 22 01:12:08.092: INFO: Created: latency-svc-6xdvc
Jul 22 01:12:08.100: INFO: Got endpoints: latency-svc-6xdvc [309.762166ms]
Jul 22 01:12:08.100: INFO: Created: latency-svc-c75wz
Jul 22 01:12:08.130: INFO: Got endpoints: latency-svc-c75wz [317.971974ms]
Jul 22 01:12:08.136: INFO: Created: latency-svc-txjnp
Jul 22 01:12:08.156: INFO: Got endpoints: latency-svc-txjnp [320.692268ms]
Jul 22 01:12:08.161: INFO: Created: latency-svc-2bghk
Jul 22 01:12:08.182: INFO: Got endpoints: latency-svc-2bghk [325.795031ms]
Jul 22 01:12:08.186: INFO: Created: latency-svc-p97vc
Jul 22 01:12:08.208: INFO: Got endpoints: latency-svc-p97vc [343.444163ms]
Jul 22 01:12:08.213: INFO: Created: latency-svc-m4hrl
Jul 22 01:12:08.234: INFO: Got endpoints: latency-svc-m4hrl [347.44719ms]
Jul 22 01:12:08.239: INFO: Created: latency-svc-bxtzq
Jul 22 01:12:08.251: INFO: Got endpoints: latency-svc-bxtzq [339.409914ms]
Jul 22 01:12:08.264: INFO: Created: latency-svc-z5bms
Jul 22 01:12:08.272: INFO: Got endpoints: latency-svc-z5bms [346.944869ms]
Jul 22 01:12:08.283: INFO: Created: latency-svc-rwjtw
Jul 22 01:12:08.301: INFO: Got endpoints: latency-svc-rwjtw [344.608617ms]
Jul 22 01:12:08.304: INFO: Created: latency-svc-f88n9
Jul 22 01:12:08.325: INFO: Got endpoints: latency-svc-f88n9 [359.240036ms]
Jul 22 01:12:08.329: INFO: Created: latency-svc-sp644
Jul 22 01:12:08.351: INFO: Got endpoints: latency-svc-sp644 [357.297272ms]
Jul 22 01:12:08.355: INFO: Created: latency-svc-rr9cn
Jul 22 01:12:08.385: INFO: Got endpoints: latency-svc-rr9cn [370.098053ms]
Jul 22 01:12:08.386: INFO: Created: latency-svc-rkhsb
Jul 22 01:12:08.410: INFO: Got endpoints: latency-svc-rkhsb [373.520175ms]
Jul 22 01:12:08.412: INFO: Created: latency-svc-v57lh
Jul 22 01:12:08.424: INFO: Created: latency-svc-6sk56
Jul 22 01:12:08.436: INFO: Got endpoints: latency-svc-v57lh [380.169916ms]
Jul 22 01:12:08.442: INFO: Created: latency-svc-jqpvj
Jul 22 01:12:08.465: INFO: Created: latency-svc-7pfsd
Jul 22 01:12:08.494: INFO: Created: latency-svc-xvvw8
Jul 22 01:12:08.494: INFO: Got endpoints: latency-svc-jqpvj [393.751109ms]
Jul 22 01:12:08.502: INFO: Got endpoints: latency-svc-7pfsd [371.711592ms]
Jul 22 01:12:08.502: INFO: Got endpoints: latency-svc-6sk56 [411.514707ms]
Jul 22 01:12:08.520: INFO: Got endpoints: latency-svc-xvvw8 [364.668926ms]
Jul 22 01:12:08.528: INFO: Created: latency-svc-qvmwk
Jul 22 01:12:08.539: INFO: Created: latency-svc-v5jxs
Jul 22 01:12:08.557: INFO: Got endpoints: latency-svc-v5jxs [349.422649ms]
Jul 22 01:12:08.557: INFO: Got endpoints: latency-svc-qvmwk [374.938259ms]
Jul 22 01:12:08.561: INFO: Created: latency-svc-drp89
Jul 22 01:12:08.580: INFO: Got endpoints: latency-svc-drp89 [345.821756ms]
Jul 22 01:12:08.585: INFO: Created: latency-svc-9d5v5
Jul 22 01:12:08.607: INFO: Got endpoints: latency-svc-9d5v5 [355.39821ms]
Jul 22 01:12:08.610: INFO: Created: latency-svc-sv9hq
Jul 22 01:12:08.635: INFO: Got endpoints: latency-svc-sv9hq [363.002932ms]
Jul 22 01:12:08.636: INFO: Created: latency-svc-4982r
Jul 22 01:12:08.658: INFO: Got endpoints: latency-svc-4982r [356.417194ms]
Jul 22 01:12:08.659: INFO: Created: latency-svc-nfc5g
Jul 22 01:12:08.667: INFO: Got endpoints: latency-svc-nfc5g [342.052101ms]
Jul 22 01:12:08.677: INFO: Created: latency-svc-b2xfr
Jul 22 01:12:08.704: INFO: Got endpoints: latency-svc-b2xfr [353.679848ms]
Jul 22 01:12:08.714: INFO: Created: latency-svc-p56s7
Jul 22 01:12:08.732: INFO: Got endpoints: latency-svc-p56s7 [346.554328ms]
Jul 22 01:12:08.737: INFO: Created: latency-svc-wjl75
Jul 22 01:12:08.754: INFO: Got endpoints: latency-svc-wjl75 [343.262076ms]
Jul 22 01:12:08.761: INFO: Created: latency-svc-t8p7v
Jul 22 01:12:08.775: INFO: Got endpoints: latency-svc-t8p7v [339.224448ms]
Jul 22 01:12:08.785: INFO: Created: latency-svc-87m6b
Jul 22 01:12:08.791: INFO: Got endpoints: latency-svc-87m6b [297.190185ms]
Jul 22 01:12:08.802: INFO: Created: latency-svc-blk8g
Jul 22 01:12:08.821: INFO: Got endpoints: latency-svc-blk8g [319.222791ms]
Jul 22 01:12:08.825: INFO: Created: latency-svc-xvj8z
Jul 22 01:12:08.840: INFO: Got endpoints: latency-svc-xvj8z [338.871952ms]
Jul 22 01:12:08.846: INFO: Created: latency-svc-8bqbh
Jul 22 01:12:08.867: INFO: Got endpoints: latency-svc-8bqbh [346.523539ms]
Jul 22 01:12:08.872: INFO: Created: latency-svc-bbp6l
Jul 22 01:12:08.886: INFO: Got endpoints: latency-svc-bbp6l [329.302237ms]
Jul 22 01:12:08.896: INFO: Created: latency-svc-8v4xs
Jul 22 01:12:08.910: INFO: Created: latency-svc-m995h
Jul 22 01:12:08.929: INFO: Got endpoints: latency-svc-8v4xs [372.036593ms]
Jul 22 01:12:08.932: INFO: Created: latency-svc-qpk2r
Jul 22 01:12:08.953: INFO: Created: latency-svc-6rglf
Jul 22 01:12:08.976: INFO: Got endpoints: latency-svc-m995h [395.847555ms]
Jul 22 01:12:08.976: INFO: Created: latency-svc-8krkk
Jul 22 01:12:08.997: INFO: Created: latency-svc-szljc
Jul 22 01:12:09.007: INFO: Created: latency-svc-s5fjp
Jul 22 01:12:09.028: INFO: Created: latency-svc-d6ksh
Jul 22 01:12:09.038: INFO: Got endpoints: latency-svc-qpk2r [431.319717ms]
Jul 22 01:12:09.048: INFO: Created: latency-svc-g5bmb
Jul 22 01:12:09.062: INFO: Created: latency-svc-sscfx
Jul 22 01:12:09.082: INFO: Created: latency-svc-skzpj
Jul 22 01:12:09.083: INFO: Got endpoints: latency-svc-6rglf [448.006525ms]
Jul 22 01:12:09.106: INFO: Created: latency-svc-psggk
Jul 22 01:12:09.125: INFO: Created: latency-svc-279t8
Jul 22 01:12:09.144: INFO: Created: latency-svc-z5s2b
Jul 22 01:12:09.144: INFO: Got endpoints: latency-svc-8krkk [485.972154ms]
Jul 22 01:12:09.160: INFO: Created: latency-svc-zpcm7
Jul 22 01:12:09.179: INFO: Created: latency-svc-b5rxj
Jul 22 01:12:09.200: INFO: Created: latency-svc-wh7w7
Jul 22 01:12:09.207: INFO: Got endpoints: latency-svc-szljc [540.056034ms]
Jul 22 01:12:09.212: INFO: Created: latency-svc-trr86
Jul 22 01:12:09.231: INFO: Created: latency-svc-bgjrf
Jul 22 01:12:09.252: INFO: Got endpoints: latency-svc-s5fjp [547.132241ms]
Jul 22 01:12:09.259: INFO: Created: latency-svc-74gp5
Jul 22 01:12:09.274: INFO: Created: latency-svc-67fvq
Jul 22 01:12:09.294: INFO: Created: latency-svc-2s54m
Jul 22 01:12:09.300: INFO: Got endpoints: latency-svc-d6ksh [568.142958ms]
Jul 22 01:12:09.326: INFO: Created: latency-svc-f92g8
Jul 22 01:12:09.353: INFO: Got endpoints: latency-svc-g5bmb [598.991278ms]
Jul 22 01:12:09.378: INFO: Created: latency-svc-kmbwd
Jul 22 01:12:09.409: INFO: Got endpoints: latency-svc-sscfx [633.545749ms]
Jul 22 01:12:09.435: INFO: Created: latency-svc-rvhf8
Jul 22 01:12:09.462: INFO: Got endpoints: latency-svc-skzpj [670.328377ms]
Jul 22 01:12:09.482: INFO: Created: latency-svc-rlgcl
Jul 22 01:12:09.523: INFO: Got endpoints: latency-svc-psggk [702.296193ms]
Jul 22 01:12:09.536: INFO: Created: latency-svc-mx8pn
Jul 22 01:12:09.577: INFO: Got endpoints: latency-svc-279t8 [736.192235ms]
Jul 22 01:12:09.602: INFO: Created: latency-svc-rphfg
Jul 22 01:12:09.624: INFO: Got endpoints: latency-svc-z5s2b [757.127574ms]
Jul 22 01:12:09.651: INFO: Created: latency-svc-nxjvc
Jul 22 01:12:09.678: INFO: Got endpoints: latency-svc-zpcm7 [791.208064ms]
Jul 22 01:12:09.703: INFO: Created: latency-svc-4zpwb
Jul 22 01:12:09.733: INFO: Got endpoints: latency-svc-b5rxj [803.285265ms]
Jul 22 01:12:09.762: INFO: Created: latency-svc-55q2f
Jul 22 01:12:09.787: INFO: Got endpoints: latency-svc-wh7w7 [810.727595ms]
Jul 22 01:12:09.807: INFO: Created: latency-svc-7l2wz
Jul 22 01:12:09.848: INFO: Got endpoints: latency-svc-trr86 [810.22157ms]
Jul 22 01:12:09.879: INFO: Created: latency-svc-64czl
Jul 22 01:12:09.895: INFO: Got endpoints: latency-svc-bgjrf [811.207469ms]
Jul 22 01:12:09.915: INFO: Created: latency-svc-z2zgm
Jul 22 01:12:09.956: INFO: Got endpoints: latency-svc-74gp5 [812.421322ms]
Jul 22 01:12:09.970: INFO: Created: latency-svc-6klk2
Jul 22 01:12:10.011: INFO: Got endpoints: latency-svc-67fvq [804.115229ms]
Jul 22 01:12:10.035: INFO: Created: latency-svc-4hcnv
Jul 22 01:12:10.058: INFO: Got endpoints: latency-svc-2s54m [806.161126ms]
Jul 22 01:12:10.084: INFO: Created: latency-svc-qst56
Jul 22 01:12:10.112: INFO: Got endpoints: latency-svc-f92g8 [812.096993ms]
Jul 22 01:12:10.139: INFO: Created: latency-svc-hmzxl
Jul 22 01:12:10.166: INFO: Got endpoints: latency-svc-kmbwd [812.924986ms]
Jul 22 01:12:10.195: INFO: Created: latency-svc-nhpbk
Jul 22 01:12:10.221: INFO: Got endpoints: latency-svc-rvhf8 [811.491736ms]
Jul 22 01:12:10.242: INFO: Created: latency-svc-hwtxf
Jul 22 01:12:10.275: INFO: Got endpoints: latency-svc-rlgcl [812.82937ms]
Jul 22 01:12:10.306: INFO: Created: latency-svc-5wq9v
Jul 22 01:12:10.330: INFO: Got endpoints: latency-svc-mx8pn [806.242679ms]
Jul 22 01:12:10.362: INFO: Created: latency-svc-59sg7
Jul 22 01:12:10.383: INFO: Got endpoints: latency-svc-rphfg [806.51867ms]
Jul 22 01:12:10.409: INFO: Created: latency-svc-g649j
Jul 22 01:12:10.444: INFO: Got endpoints: latency-svc-nxjvc [819.768211ms]
Jul 22 01:12:10.470: INFO: Created: latency-svc-nnndf
Jul 22 01:12:10.495: INFO: Got endpoints: latency-svc-4zpwb [816.970718ms]
Jul 22 01:12:10.522: INFO: Created: latency-svc-b4m5s
Jul 22 01:12:10.549: INFO: Got endpoints: latency-svc-55q2f [816.322212ms]
Jul 22 01:12:10.574: INFO: Created: latency-svc-wjsfb
Jul 22 01:12:10.602: INFO: Got endpoints: latency-svc-7l2wz [814.760391ms]
Jul 22 01:12:10.630: INFO: Created: latency-svc-cwr2w
Jul 22 01:12:10.654: INFO: Got endpoints: latency-svc-64czl [805.813116ms]
Jul 22 01:12:10.679: INFO: Created: latency-svc-6fqfw
Jul 22 01:12:10.715: INFO: Got endpoints: latency-svc-z2zgm [820.227479ms]
Jul 22 01:12:10.730: INFO: Created: latency-svc-bh98b
Jul 22 01:12:10.769: INFO: Got endpoints: latency-svc-6klk2 [812.154714ms]
Jul 22 01:12:10.793: INFO: Created: latency-svc-nxkxn
Jul 22 01:12:10.816: INFO: Got endpoints: latency-svc-4hcnv [805.470002ms]
Jul 22 01:12:10.844: INFO: Created: latency-svc-fckdw
Jul 22 01:12:10.871: INFO: Got endpoints: latency-svc-qst56 [812.637317ms]
Jul 22 01:12:10.897: INFO: Created: latency-svc-hchtf
Jul 22 01:12:10.924: INFO: Got endpoints: latency-svc-hmzxl [812.028809ms]
Jul 22 01:12:10.954: INFO: Created: latency-svc-96ggf
Jul 22 01:12:10.979: INFO: Got endpoints: latency-svc-nhpbk [813.363909ms]
Jul 22 01:12:10.999: INFO: Created: latency-svc-6vcnh
Jul 22 01:12:11.040: INFO: Got endpoints: latency-svc-hwtxf [819.416473ms]
Jul 22 01:12:11.054: INFO: Created: latency-svc-w6gvp
Jul 22 01:12:11.099: INFO: Got endpoints: latency-svc-5wq9v [823.99813ms]
Jul 22 01:12:11.123: INFO: Created: latency-svc-fggm5
Jul 22 01:12:11.141: INFO: Got endpoints: latency-svc-59sg7 [811.096534ms]
Jul 22 01:12:11.171: INFO: Created: latency-svc-6xcgf
Jul 22 01:12:11.195: INFO: Got endpoints: latency-svc-g649j [811.872353ms]
Jul 22 01:12:11.234: INFO: Created: latency-svc-kcdfc
Jul 22 01:12:11.250: INFO: Got endpoints: latency-svc-nnndf [805.969679ms]
Jul 22 01:12:11.277: INFO: Created: latency-svc-v2k9t
Jul 22 01:12:11.304: INFO: Got endpoints: latency-svc-b4m5s [808.948306ms]
Jul 22 01:12:11.329: INFO: Created: latency-svc-jvx7k
Jul 22 01:12:11.365: INFO: Got endpoints: latency-svc-wjsfb [815.648422ms]
Jul 22 01:12:11.380: INFO: Created: latency-svc-tmrz7
Jul 22 01:12:11.417: INFO: Got endpoints: latency-svc-cwr2w [815.887384ms]
Jul 22 01:12:11.443: INFO: Created: latency-svc-j6slm
Jul 22 01:12:11.466: INFO: Got endpoints: latency-svc-6fqfw [812.253406ms]
Jul 22 01:12:11.493: INFO: Created: latency-svc-q6528
Jul 22 01:12:11.522: INFO: Got endpoints: latency-svc-bh98b [806.679523ms]
Jul 22 01:12:11.550: INFO: Created: latency-svc-vhvx5
Jul 22 01:12:11.577: INFO: Got endpoints: latency-svc-nxkxn [808.309031ms]
Jul 22 01:12:11.604: INFO: Created: latency-svc-2qjz5
Jul 22 01:12:11.631: INFO: Got endpoints: latency-svc-fckdw [814.451902ms]
Jul 22 01:12:11.660: INFO: Created: latency-svc-7cbx6
Jul 22 01:12:11.691: INFO: Got endpoints: latency-svc-hchtf [820.135656ms]
Jul 22 01:12:11.708: INFO: Created: latency-svc-xq2sk
Jul 22 01:12:11.752: INFO: Got endpoints: latency-svc-96ggf [827.544684ms]
Jul 22 01:12:11.777: INFO: Created: latency-svc-vl6pm
Jul 22 01:12:11.791: INFO: Got endpoints: latency-svc-6vcnh [811.526467ms]
Jul 22 01:12:11.818: INFO: Created: latency-svc-7ff7n
Jul 22 01:12:11.846: INFO: Got endpoints: latency-svc-w6gvp [805.626414ms]
Jul 22 01:12:11.880: INFO: Created: latency-svc-vgwp4
Jul 22 01:12:11.901: INFO: Got endpoints: latency-svc-fggm5 [802.535511ms]
Jul 22 01:12:11.939: INFO: Created: latency-svc-sq7np
Jul 22 01:12:11.954: INFO: Got endpoints: latency-svc-6xcgf [813.610346ms]
Jul 22 01:12:11.974: INFO: Created: latency-svc-5wwzg
Jul 22 01:12:12.010: INFO: Got endpoints: latency-svc-kcdfc [815.05365ms]
Jul 22 01:12:12.048: INFO: Created: latency-svc-2qwcs
Jul 22 01:12:12.073: INFO: Got endpoints: latency-svc-v2k9t [822.950244ms]
Jul 22 01:12:12.100: INFO: Created: latency-svc-gcq2z
Jul 22 01:12:12.117: INFO: Got endpoints: latency-svc-jvx7k [812.585837ms]
Jul 22 01:12:12.144: INFO: Created: latency-svc-tplm7
Jul 22 01:12:12.171: INFO: Got endpoints: latency-svc-tmrz7 [806.233492ms]
Jul 22 01:12:12.205: INFO: Created: latency-svc-4fmhv
Jul 22 01:12:12.234: INFO: Got endpoints: latency-svc-j6slm [815.977626ms]
Jul 22 01:12:12.260: INFO: Created: latency-svc-7ptcn
Jul 22 01:12:12.281: INFO: Got endpoints: latency-svc-q6528 [814.799521ms]
Jul 22 01:12:12.295: INFO: Created: latency-svc-24wzk
Jul 22 01:12:12.342: INFO: Got endpoints: latency-svc-vhvx5 [820.004334ms]
Jul 22 01:12:12.366: INFO: Created: latency-svc-8ltr6
Jul 22 01:12:12.387: INFO: Got endpoints: latency-svc-2qjz5 [809.679318ms]
Jul 22 01:12:12.414: INFO: Created: latency-svc-pxgnk
Jul 22 01:12:12.441: INFO: Got endpoints: latency-svc-7cbx6 [810.305809ms]
Jul 22 01:12:12.489: INFO: Created: latency-svc-d8g4g
Jul 22 01:12:12.498: INFO: Got endpoints: latency-svc-xq2sk [806.787597ms]
Jul 22 01:12:12.529: INFO: Created: latency-svc-4dhhq
Jul 22 01:12:12.549: INFO: Got endpoints: latency-svc-vl6pm [797.432077ms]
Jul 22 01:12:12.580: INFO: Created: latency-svc-smvh7
Jul 22 01:12:12.604: INFO: Got endpoints: latency-svc-7ff7n [812.607829ms]
Jul 22 01:12:12.631: INFO: Created: latency-svc-gj6l7
Jul 22 01:12:12.667: INFO: Got endpoints: latency-svc-vgwp4 [820.688004ms]
Jul 22 01:12:12.707: INFO: Created: latency-svc-pxpzl
Jul 22 01:12:12.717: INFO: Got endpoints: latency-svc-sq7np [815.182264ms]
Jul 22 01:12:12.731: INFO: Created: latency-svc-7vtqs
Jul 22 01:12:12.775: INFO: Got endpoints: latency-svc-5wwzg [820.555491ms]
Jul 22 01:12:12.801: INFO: Created: latency-svc-hd88s
Jul 22 01:12:12.820: INFO: Got endpoints: latency-svc-2qwcs [809.744946ms]
Jul 22 01:12:12.849: INFO: Created: latency-svc-728c2
Jul 22 01:12:12.875: INFO: Got endpoints: latency-svc-gcq2z [801.827271ms]
Jul 22 01:12:12.902: INFO: Created: latency-svc-8pq5s
Jul 22 01:12:12.928: INFO: Got endpoints: latency-svc-tplm7 [811.741596ms]
Jul 22 01:12:12.953: INFO: Created: latency-svc-gw4mf
Jul 22 01:12:12.990: INFO: Got endpoints: latency-svc-4fmhv [818.596866ms]
Jul 22 01:12:13.003: INFO: Created: latency-svc-wlcnk
Jul 22 01:12:13.043: INFO: Got endpoints: latency-svc-7ptcn [809.830486ms]
Jul 22 01:12:13.057: INFO: Created: latency-svc-fcw7w
Jul 22 01:12:13.100: INFO: Got endpoints: latency-svc-24wzk [818.547401ms]
Jul 22 01:12:13.124: INFO: Created: latency-svc-vpm4k
Jul 22 01:12:13.145: INFO: Got endpoints: latency-svc-8ltr6 [802.813147ms]
Jul 22 01:12:13.170: INFO: Created: latency-svc-2m5cj
Jul 22 01:12:13.200: INFO: Got endpoints: latency-svc-pxgnk [812.960584ms]
Jul 22 01:12:13.226: INFO: Created: latency-svc-v77b9
Jul 22 01:12:13.253: INFO: Got endpoints: latency-svc-d8g4g [811.542868ms]
Jul 22 01:12:13.278: INFO: Created: latency-svc-mvqqr
Jul 22 01:12:13.307: INFO: Got endpoints: latency-svc-4dhhq [809.586303ms]
Jul 22 01:12:13.337: INFO: Created: latency-svc-slnjw
Jul 22 01:12:13.362: INFO: Got endpoints: latency-svc-smvh7 [812.822155ms]
Jul 22 01:12:13.393: INFO: Created: latency-svc-msd9z
Jul 22 01:12:13.425: INFO: Got endpoints: latency-svc-gj6l7 [821.264533ms]
Jul 22 01:12:13.450: INFO: Created: latency-svc-bsc5k
Jul 22 01:12:13.470: INFO: Got endpoints: latency-svc-pxpzl [803.770122ms]
Jul 22 01:12:13.495: INFO: Created: latency-svc-sj8dc
Jul 22 01:12:13.525: INFO: Got endpoints: latency-svc-7vtqs [808.639837ms]
Jul 22 01:12:13.552: INFO: Created: latency-svc-lrrxd
Jul 22 01:12:13.579: INFO: Got endpoints: latency-svc-hd88s [803.825372ms]
Jul 22 01:12:13.606: INFO: Created: latency-svc-flvcw
Jul 22 01:12:13.633: INFO: Got endpoints: latency-svc-728c2 [812.277802ms]
Jul 22 01:12:13.651: INFO: Created: latency-svc-skgrs
Jul 22 01:12:13.692: INFO: Got endpoints: latency-svc-8pq5s [816.844011ms]
Jul 22 01:12:13.706: INFO: Created: latency-svc-lxv4x
Jul 22 01:12:13.750: INFO: Got endpoints: latency-svc-gw4mf [821.321971ms]
Jul 22 01:12:13.779: INFO: Created: latency-svc-t9n6g
Jul 22 01:12:13.795: INFO: Got endpoints: latency-svc-wlcnk [805.488181ms]
Jul 22 01:12:13.820: INFO: Created: latency-svc-gg4kg
Jul 22 01:12:13.849: INFO: Got endpoints: latency-svc-fcw7w [805.814958ms]
Jul 22 01:12:13.879: INFO: Created: latency-svc-hqzjq
Jul 22 01:12:13.904: INFO: Got endpoints: latency-svc-vpm4k [803.666186ms]
Jul 22 01:12:13.933: INFO: Created: latency-svc-7r5mp
Jul 22 01:12:13.958: INFO: Got endpoints: latency-svc-2m5cj [813.167111ms]
Jul 22 01:12:13.981: INFO: Created: latency-svc-t5g79
Jul 22 01:12:14.017: INFO: Got endpoints: latency-svc-v77b9 [817.084944ms]
Jul 22 01:12:14.031: INFO: Created: latency-svc-6dqdx
Jul 22 01:12:14.075: INFO: Got endpoints: latency-svc-mvqqr [821.949785ms]
Jul 22 01:12:14.100: INFO: Created: latency-svc-q5hcx
Jul 22 01:12:14.120: INFO: Got endpoints: latency-svc-slnjw [813.109781ms]
Jul 22 01:12:14.145: INFO: Created: latency-svc-z6swz
Jul 22 01:12:14.174: INFO: Got endpoints: latency-svc-msd9z [811.975661ms]
Jul 22 01:12:14.204: INFO: Created: latency-svc-xckwj
Jul 22 01:12:14.229: INFO: Got endpoints: latency-svc-bsc5k [803.918755ms]
Jul 22 01:12:14.258: INFO: Created: latency-svc-mzsjv
Jul 22 01:12:14.283: INFO: Got endpoints: latency-svc-sj8dc [812.450442ms]
Jul 22 01:12:14.306: INFO: Created: latency-svc-jgdjr
Jul 22 01:12:14.342: INFO: Got endpoints: latency-svc-lrrxd [816.243844ms]
Jul 22 01:12:14.354: INFO: Created: latency-svc-fr648
Jul 22 01:12:14.399: INFO: Got endpoints: latency-svc-flvcw [820.323896ms]
Jul 22 01:12:14.425: INFO: Created: latency-svc-6fp9r
Jul 22 01:12:14.446: INFO: Got endpoints: latency-svc-skgrs [812.967582ms]
Jul 22 01:12:14.472: INFO: Created: latency-svc-jtgxf
Jul 22 01:12:14.500: INFO: Got endpoints: latency-svc-lxv4x [808.266152ms]
Jul 22 01:12:14.526: INFO: Created: latency-svc-rjjdl
Jul 22 01:12:14.553: INFO: Got endpoints: latency-svc-t9n6g [803.325586ms]
Jul 22 01:12:14.578: INFO: Created: latency-svc-7v2kb
Jul 22 01:12:14.608: INFO: Got endpoints: latency-svc-gg4kg [812.548549ms]
Jul 22 01:12:14.623: INFO: Created: latency-svc-prc9n
Jul 22 01:12:14.667: INFO: Got endpoints: latency-svc-hqzjq [817.880024ms]
Jul 22 01:12:14.695: INFO: Created: latency-svc-r5hst
Jul 22 01:12:14.718: INFO: Got endpoints: latency-svc-7r5mp [814.559455ms]
Jul 22 01:12:14.739: INFO: Created: latency-svc-mj6st
Jul 22 01:12:14.775: INFO: Got endpoints: latency-svc-t5g79 [817.164548ms]
Jul 22 01:12:14.788: INFO: Created: latency-svc-hcqzp
Jul 22 01:12:14.833: INFO: Got endpoints: latency-svc-6dqdx [815.96586ms]
Jul 22 01:12:14.863: INFO: Created: latency-svc-dpdhx
Jul 22 01:12:14.881: INFO: Got endpoints: latency-svc-q5hcx [806.001963ms]
Jul 22 01:12:14.912: INFO: Created: latency-svc-jxvh6
Jul 22 01:12:14.940: INFO: Got endpoints: latency-svc-z6swz [819.501602ms]
Jul 22 01:12:14.966: INFO: Created: latency-svc-mc5r6
Jul 22 01:12:14.987: INFO: Got endpoints: latency-svc-xckwj [812.190897ms]
Jul 22 01:12:15.014: INFO: Created: latency-svc-589nc
Jul 22 01:12:15.041: INFO: Got endpoints: latency-svc-mzsjv [812.272213ms]
Jul 22 01:12:15.062: INFO: Created: latency-svc-cmzhn
Jul 22 01:12:15.100: INFO: Got endpoints: latency-svc-jgdjr [817.013336ms]
Jul 22 01:12:15.113: INFO: Created: latency-svc-kprrq
Jul 22 01:12:15.158: INFO: Got endpoints: latency-svc-fr648 [816.225774ms]
Jul 22 01:12:15.183: INFO: Created: latency-svc-rcc29
Jul 22 01:12:15.204: INFO: Got endpoints: latency-svc-6fp9r [804.654317ms]
Jul 22 01:12:15.232: INFO: Created: latency-svc-lfk76
Jul 22 01:12:15.258: INFO: Got endpoints: latency-svc-jtgxf [812.061157ms]
Jul 22 01:12:15.284: INFO: Created: latency-svc-db286
Jul 22 01:12:15.313: INFO: Got endpoints: latency-svc-rjjdl [812.385334ms]
Jul 22 01:12:15.339: INFO: Created: latency-svc-lr2qc
Jul 22 01:12:15.367: INFO: Got endpoints: latency-svc-7v2kb [813.472611ms]
Jul 22 01:12:15.401: INFO: Created: latency-svc-ngbvc
Jul 22 01:12:15.425: INFO: Got endpoints: latency-svc-prc9n [817.260401ms]
Jul 22 01:12:15.439: INFO: Created: latency-svc-jf72z
Jul 22 01:12:15.483: INFO: Got endpoints: latency-svc-r5hst [815.832675ms]
Jul 22 01:12:15.509: INFO: Created: latency-svc-9f27z
Jul 22 01:12:15.535: INFO: Got endpoints: latency-svc-mj6st [816.623487ms]
Jul 22 01:12:15.560: INFO: Created: latency-svc-lzwnt
Jul 22 01:12:15.592: INFO: Got endpoints: latency-svc-hcqzp [817.183159ms]
Jul 22 01:12:15.619: INFO: Created: latency-svc-tw9mh
Jul 22 01:12:15.638: INFO: Got endpoints: latency-svc-dpdhx [804.929224ms]
Jul 22 01:12:15.667: INFO: Created: latency-svc-nzhlx
Jul 22 01:12:15.691: INFO: Got endpoints: latency-svc-jxvh6 [810.223585ms]
Jul 22 01:12:15.722: INFO: Created: latency-svc-nzs6n
Jul 22 01:12:15.746: INFO: Got endpoints: latency-svc-mc5r6 [806.042783ms]
Jul 22 01:12:15.766: INFO: Created: latency-svc-v4tlc
Jul 22 01:12:15.807: INFO: Got endpoints: latency-svc-589nc [820.268169ms]
Jul 22 01:12:15.845: INFO: Created: latency-svc-rlb5h
Jul 22 01:12:15.860: INFO: Got endpoints: latency-svc-cmzhn [818.914522ms]
Jul 22 01:12:15.885: INFO: Created: latency-svc-4cbrz
Jul 22 01:12:15.908: INFO: Got endpoints: latency-svc-kprrq [808.254929ms]
Jul 22 01:12:15.937: INFO: Created: latency-svc-lqqsr
Jul 22 01:12:15.963: INFO: Got endpoints: latency-svc-rcc29 [804.411889ms]
Jul 22 01:12:15.991: INFO: Created: latency-svc-nkw6q
Jul 22 01:12:16.016: INFO: Got endpoints: latency-svc-lfk76 [812.331904ms]
Jul 22 01:12:16.043: INFO: Created: latency-svc-jffjv
Jul 22 01:12:16.075: INFO: Got endpoints: latency-svc-db286 [817.009545ms]
Jul 22 01:12:16.089: INFO: Created: latency-svc-gtbgp
Jul 22 01:12:16.129: INFO: Got endpoints: latency-svc-lr2qc [816.666994ms]
Jul 22 01:12:16.154: INFO: Created: latency-svc-dplp9
Jul 22 01:12:16.179: INFO: Got endpoints: latency-svc-ngbvc [811.799359ms]
Jul 22 01:12:16.207: INFO: Created: latency-svc-2c2hm
Jul 22 01:12:16.235: INFO: Got endpoints: latency-svc-jf72z [809.366776ms]
Jul 22 01:12:16.287: INFO: Got endpoints: latency-svc-9f27z [803.495973ms]
Jul 22 01:12:16.350: INFO: Got endpoints: latency-svc-lzwnt [814.498246ms]
Jul 22 01:12:16.401: INFO: Got endpoints: latency-svc-tw9mh [807.930361ms]
Jul 22 01:12:16.449: INFO: Got endpoints: latency-svc-nzhlx [811.056192ms]
Jul 22 01:12:16.504: INFO: Got endpoints: latency-svc-nzs6n [812.624881ms]
Jul 22 01:12:16.558: INFO: Got endpoints: latency-svc-v4tlc [811.941406ms]
Jul 22 01:12:16.617: INFO: Got endpoints: latency-svc-rlb5h [809.922764ms]
Jul 22 01:12:16.674: INFO: Got endpoints: latency-svc-4cbrz [813.791696ms]
Jul 22 01:12:16.736: INFO: Got endpoints: latency-svc-lqqsr [827.60093ms]
Jul 22 01:12:16.798: INFO: Got endpoints: latency-svc-nkw6q [835.653675ms]
Jul 22 01:12:16.829: INFO: Got endpoints: latency-svc-jffjv [812.731849ms]
Jul 22 01:12:16.883: INFO: Got endpoints: latency-svc-gtbgp [807.755967ms]
Jul 22 01:12:16.936: INFO: Got endpoints: latency-svc-dplp9 [806.929144ms]
Jul 22 01:12:17.000: INFO: Got endpoints: latency-svc-2c2hm [821.193769ms]
Jul 22 01:12:17.000: INFO: Latencies: [38.865533ms 60.730653ms 82.278098ms 105.341409ms 126.715434ms 134.467661ms 156.915746ms 181.959911ms 195.328424ms 226.998886ms 235.412815ms 263.492069ms 285.186024ms 297.190185ms 306.924763ms 309.762166ms 317.971974ms 319.222791ms 320.692268ms 321.164493ms 325.795031ms 325.935993ms 329.302237ms 338.871952ms 339.224448ms 339.409914ms 342.052101ms 343.262076ms 343.444163ms 344.608617ms 345.821756ms 346.523539ms 346.554328ms 346.944869ms 347.44719ms 349.422649ms 353.679848ms 355.39821ms 356.417194ms 357.297272ms 359.240036ms 363.002932ms 364.668926ms 370.098053ms 371.711592ms 372.036593ms 373.520175ms 374.938259ms 380.169916ms 393.751109ms 395.847555ms 411.514707ms 431.319717ms 448.006525ms 485.972154ms 540.056034ms 547.132241ms 568.142958ms 598.991278ms 633.545749ms 670.328377ms 702.296193ms 736.192235ms 757.127574ms 791.208064ms 797.432077ms 801.827271ms 802.535511ms 802.813147ms 803.285265ms 803.325586ms 803.495973ms 803.666186ms 803.770122ms 803.825372ms 803.918755ms 804.115229ms 804.411889ms 804.654317ms 804.929224ms 805.470002ms 805.488181ms 805.626414ms 805.813116ms 805.814958ms 805.969679ms 806.001963ms 806.042783ms 806.161126ms 806.233492ms 806.242679ms 806.51867ms 806.679523ms 806.787597ms 806.929144ms 807.755967ms 807.930361ms 808.254929ms 808.266152ms 808.309031ms 808.639837ms 808.948306ms 809.366776ms 809.586303ms 809.679318ms 809.744946ms 809.830486ms 809.922764ms 810.22157ms 810.223585ms 810.305809ms 810.727595ms 811.056192ms 811.096534ms 811.207469ms 811.491736ms 811.526467ms 811.542868ms 811.741596ms 811.799359ms 811.872353ms 811.941406ms 811.975661ms 812.028809ms 812.061157ms 812.096993ms 812.154714ms 812.190897ms 812.253406ms 812.272213ms 812.277802ms 812.331904ms 812.385334ms 812.421322ms 812.450442ms 812.548549ms 812.585837ms 812.607829ms 812.624881ms 812.637317ms 812.731849ms 812.822155ms 812.82937ms 812.924986ms 812.960584ms 812.967582ms 813.109781ms 813.167111ms 813.363909ms 813.472611ms 813.610346ms 813.791696ms 814.451902ms 814.498246ms 814.559455ms 814.760391ms 814.799521ms 815.05365ms 815.182264ms 815.648422ms 815.832675ms 815.887384ms 815.96586ms 815.977626ms 816.225774ms 816.243844ms 816.322212ms 816.623487ms 816.666994ms 816.844011ms 816.970718ms 817.009545ms 817.013336ms 817.084944ms 817.164548ms 817.183159ms 817.260401ms 817.880024ms 818.547401ms 818.596866ms 818.914522ms 819.416473ms 819.501602ms 819.768211ms 820.004334ms 820.135656ms 820.227479ms 820.268169ms 820.323896ms 820.555491ms 820.688004ms 821.193769ms 821.264533ms 821.321971ms 821.949785ms 822.950244ms 823.99813ms 827.544684ms 827.60093ms 835.653675ms]
Jul 22 01:12:17.000: INFO: 50 %ile: 808.639837ms
Jul 22 01:12:17.000: INFO: 90 %ile: 818.914522ms
Jul 22 01:12:17.000: INFO: 99 %ile: 827.60093ms
Jul 22 01:12:17.000: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:12:17.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1004" for this suite.
Jul 22 01:12:35.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:12:35.188: INFO: namespace svc-latency-1004 deletion completed in 18.178539015s

â€¢ [SLOW TEST:31.722 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:12:35.189: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:12:35.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5027" for this suite.
Jul 22 01:12:41.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:12:41.515: INFO: namespace kubelet-test-5027 deletion completed in 6.173445503s

â€¢ [SLOW TEST:6.326 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:12:41.515: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:12:45.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1300" for this suite.
Jul 22 01:13:27.674: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:13:27.818: INFO: namespace kubelet-test-1300 deletion completed in 42.168415782s

â€¢ [SLOW TEST:46.303 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:13:27.818: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0722 01:13:37.959036      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 01:13:37.959: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:13:37.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4496" for this suite.
Jul 22 01:13:43.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:13:44.133: INFO: namespace gc-4496 deletion completed in 6.167417184s

â€¢ [SLOW TEST:16.315 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:13:44.134: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-3950
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3950 to expose endpoints map[]
Jul 22 01:13:44.231: INFO: Get endpoints failed (5.761386ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 22 01:13:45.236: INFO: successfully validated that service multi-endpoint-test in namespace services-3950 exposes endpoints map[] (1.011227551s elapsed)
STEP: Creating pod pod1 in namespace services-3950
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3950 to expose endpoints map[pod1:[100]]
Jul 22 01:13:49.320: INFO: successfully validated that service multi-endpoint-test in namespace services-3950 exposes endpoints map[pod1:[100]] (4.057880915s elapsed)
STEP: Creating pod pod2 in namespace services-3950
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3950 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 22 01:13:52.403: INFO: successfully validated that service multi-endpoint-test in namespace services-3950 exposes endpoints map[pod1:[100] pod2:[101]] (3.065026685s elapsed)
STEP: Deleting pod pod1 in namespace services-3950
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3950 to expose endpoints map[pod2:[101]]
Jul 22 01:13:53.433: INFO: successfully validated that service multi-endpoint-test in namespace services-3950 exposes endpoints map[pod2:[101]] (1.022443024s elapsed)
STEP: Deleting pod pod2 in namespace services-3950
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3950 to expose endpoints map[]
Jul 22 01:13:54.453: INFO: successfully validated that service multi-endpoint-test in namespace services-3950 exposes endpoints map[] (1.010583076s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:13:54.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3950" for this suite.
Jul 22 01:14:00.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:14:00.682: INFO: namespace services-3950 deletion completed in 6.162605839s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:16.548 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:14:00.683: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-cgdq
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 01:14:00.775: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cgdq" in namespace "subpath-710" to be "success or failure"
Jul 22 01:14:00.797: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Pending", Reason="", readiness=false. Elapsed: 21.626831ms
Jul 22 01:14:02.803: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027694992s
Jul 22 01:14:04.809: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 4.033434708s
Jul 22 01:14:06.817: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 6.041648173s
Jul 22 01:14:08.824: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 8.048627808s
Jul 22 01:14:10.830: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 10.05512317s
Jul 22 01:14:12.837: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 12.061577757s
Jul 22 01:14:14.843: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 14.068046144s
Jul 22 01:14:16.850: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 16.074740321s
Jul 22 01:14:18.856: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 18.080366026s
Jul 22 01:14:20.864: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 20.088421801s
Jul 22 01:14:22.871: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 22.095522618s
Jul 22 01:14:24.877: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Running", Reason="", readiness=true. Elapsed: 24.101790654s
Jul 22 01:14:26.885: INFO: Pod "pod-subpath-test-configmap-cgdq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.110075597s
STEP: Saw pod success
Jul 22 01:14:26.885: INFO: Pod "pod-subpath-test-configmap-cgdq" satisfied condition "success or failure"
Jul 22 01:14:26.890: INFO: Trying to get logs from node k8s1 pod pod-subpath-test-configmap-cgdq container test-container-subpath-configmap-cgdq: <nil>
STEP: delete the pod
Jul 22 01:14:26.945: INFO: Waiting for pod pod-subpath-test-configmap-cgdq to disappear
Jul 22 01:14:26.949: INFO: Pod pod-subpath-test-configmap-cgdq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cgdq
Jul 22 01:14:26.949: INFO: Deleting pod "pod-subpath-test-configmap-cgdq" in namespace "subpath-710"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:14:26.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-710" for this suite.
Jul 22 01:14:32.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:14:33.134: INFO: namespace subpath-710 deletion completed in 6.174453568s

â€¢ [SLOW TEST:32.452 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:14:33.135: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6185
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6185
STEP: Creating statefulset with conflicting port in namespace statefulset-6185
STEP: Waiting until pod test-pod will start running in namespace statefulset-6185
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6185
Jul 22 01:14:39.260: INFO: Observed stateful pod in namespace: statefulset-6185, name: ss-0, uid: 28e51805-ac31-11e9-9d2b-6c92bf900680, status phase: Pending. Waiting for statefulset controller to delete.
Jul 22 01:14:39.957: INFO: Observed stateful pod in namespace: statefulset-6185, name: ss-0, uid: 28e51805-ac31-11e9-9d2b-6c92bf900680, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 01:14:39.975: INFO: Observed stateful pod in namespace: statefulset-6185, name: ss-0, uid: 28e51805-ac31-11e9-9d2b-6c92bf900680, status phase: Failed. Waiting for statefulset controller to delete.
Jul 22 01:14:39.982: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6185
STEP: Removing pod with conflicting port in namespace statefulset-6185
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6185 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 22 01:14:44.020: INFO: Deleting all statefulset in ns statefulset-6185
Jul 22 01:14:44.025: INFO: Scaling statefulset ss to 0
Jul 22 01:14:54.062: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 01:14:54.067: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:14:54.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6185" for this suite.
Jul 22 01:15:00.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:15:00.244: INFO: namespace statefulset-6185 deletion completed in 6.15422805s

â€¢ [SLOW TEST:27.109 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:15:00.244: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-20584aa1-ac1e-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:15:00.341: INFO: Waiting up to 5m0s for pod "pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80" in namespace "secrets-5009" to be "success or failure"
Jul 22 01:15:00.346: INFO: Pod "pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.018024ms
Jul 22 01:15:02.352: INFO: Pod "pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010817345s
Jul 22 01:15:04.360: INFO: Pod "pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018329005s
STEP: Saw pod success
Jul 22 01:15:04.360: INFO: Pod "pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:15:04.367: INFO: Trying to get logs from node k8s2 pod pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:15:04.402: INFO: Waiting for pod pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:15:04.407: INFO: Pod pod-secrets-205a886a-ac1e-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:15:04.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5009" for this suite.
Jul 22 01:15:10.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:15:10.579: INFO: namespace secrets-5009 deletion completed in 6.164187593s

â€¢ [SLOW TEST:10.335 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:15:10.579: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 22 01:15:16.720: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-26815e7d-ac1e-11e9-906c-f6b46dea7a80,GenerateName:,Namespace:events-4987,SelfLink:/api/v1/namespaces/events-4987/pods/send-events-26815e7d-ac1e-11e9-906c-f6b46dea7a80,UID:3a5db061-ac31-11e9-b4b6-6c92bf130c27,ResourceVersion:2287621,Generation:0,CreationTimestamp:2019-07-22 03:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 651976721,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8nvm9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8nvm9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-8nvm9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40013a5aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40013a5ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:15:10 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:15:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:15:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 03:31:44 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.168,StartTime:2019-07-22 01:15:10 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-22 01:15:13 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker://sha256:87cf084a4bafea8d0139f1c19c4598fadcd1710c88e46fe7f80222492c626091 docker://8e5ca954b3c92d5fd6c5115edcfcf085667cf8567e23f8a95ec5289ab23b3fe6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 22 01:15:18.727: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 22 01:15:20.734: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:15:20.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-4987" for this suite.
Jul 22 01:16:02.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:16:02.940: INFO: namespace events-4987 deletion completed in 42.179811978s

â€¢ [SLOW TEST:52.361 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:16:02.941: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-45b80ecd-ac1e-11e9-906c-f6b46dea7a80
STEP: Creating secret with name s-test-opt-upd-45b80f4b-ac1e-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-45b80ecd-ac1e-11e9-906c-f6b46dea7a80
STEP: Updating secret s-test-opt-upd-45b80f4b-ac1e-11e9-906c-f6b46dea7a80
STEP: Creating secret with name s-test-opt-create-45b80f89-ac1e-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:16:13.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-880" for this suite.
Jul 22 01:16:37.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:16:37.385: INFO: namespace projected-880 deletion completed in 24.166700896s

â€¢ [SLOW TEST:34.444 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:16:37.385: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:16:37.484: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80" in namespace "projected-6004" to be "success or failure"
Jul 22 01:16:37.489: INFO: Pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.553581ms
Jul 22 01:16:39.495: INFO: Pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011524589s
Jul 22 01:16:41.502: INFO: Pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018305496s
Jul 22 01:16:43.509: INFO: Pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024801401s
STEP: Saw pod success
Jul 22 01:16:43.509: INFO: Pod "downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:16:43.513: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:16:43.549: INFO: Waiting for pod downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:16:43.553: INFO: Pod downwardapi-volume-5a3ee341-ac1e-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:16:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6004" for this suite.
Jul 22 01:16:49.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:16:49.767: INFO: namespace projected-6004 deletion completed in 6.206974274s

â€¢ [SLOW TEST:12.381 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:16:49.767: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:16:49.850: INFO: Waiting up to 5m0s for pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80" in namespace "projected-4543" to be "success or failure"
Jul 22 01:16:49.855: INFO: Pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.283419ms
Jul 22 01:16:51.863: INFO: Pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013280087s
Jul 22 01:16:53.869: INFO: Pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019186571s
Jul 22 01:16:55.876: INFO: Pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025882719s
STEP: Saw pod success
Jul 22 01:16:55.876: INFO: Pod "downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:16:55.880: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:16:55.922: INFO: Waiting for pod downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:16:55.927: INFO: Pod downwardapi-volume-619f802d-ac1e-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:16:55.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4543" for this suite.
Jul 22 01:17:01.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:17:02.092: INFO: namespace projected-4543 deletion completed in 6.158199205s

â€¢ [SLOW TEST:12.325 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:17:02.092: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-5wp4
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 01:17:02.178: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-5wp4" in namespace "subpath-4567" to be "success or failure"
Jul 22 01:17:02.183: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377344ms
Jul 22 01:17:04.191: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013232486s
Jul 22 01:17:06.198: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 4.019679207s
Jul 22 01:17:08.204: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 6.026088163s
Jul 22 01:17:10.211: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 8.033488435s
Jul 22 01:17:12.224: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 10.045726225s
Jul 22 01:17:14.231: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 12.052603029s
Jul 22 01:17:16.237: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 14.059458273s
Jul 22 01:17:18.253: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 16.07486921s
Jul 22 01:17:20.259: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 18.081225471s
Jul 22 01:17:22.267: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 20.088638111s
Jul 22 01:17:24.273: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 22.095029125s
Jul 22 01:17:26.283: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Running", Reason="", readiness=true. Elapsed: 24.105316261s
Jul 22 01:17:28.290: INFO: Pod "pod-subpath-test-configmap-5wp4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.112080353s
STEP: Saw pod success
Jul 22 01:17:28.290: INFO: Pod "pod-subpath-test-configmap-5wp4" satisfied condition "success or failure"
Jul 22 01:17:28.295: INFO: Trying to get logs from node k8s1 pod pod-subpath-test-configmap-5wp4 container test-container-subpath-configmap-5wp4: <nil>
STEP: delete the pod
Jul 22 01:17:28.331: INFO: Waiting for pod pod-subpath-test-configmap-5wp4 to disappear
Jul 22 01:17:28.336: INFO: Pod pod-subpath-test-configmap-5wp4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-5wp4
Jul 22 01:17:28.336: INFO: Deleting pod "pod-subpath-test-configmap-5wp4" in namespace "subpath-4567"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:17:28.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4567" for this suite.
Jul 22 01:17:34.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:17:34.525: INFO: namespace subpath-4567 deletion completed in 6.17667409s

â€¢ [SLOW TEST:32.433 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:17:34.526: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:17:34.602: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80" in namespace "downward-api-3799" to be "success or failure"
Jul 22 01:17:34.607: INFO: Pod "downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.280559ms
Jul 22 01:17:36.613: INFO: Pod "downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011360764s
Jul 22 01:17:38.621: INFO: Pod "downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018481012s
STEP: Saw pod success
Jul 22 01:17:38.621: INFO: Pod "downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:17:38.625: INFO: Trying to get logs from node k8s1 pod downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:17:38.662: INFO: Waiting for pod downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:17:38.667: INFO: Pod downwardapi-volume-7c4c3523-ac1e-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:17:38.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3799" for this suite.
Jul 22 01:17:44.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:17:44.864: INFO: namespace downward-api-3799 deletion completed in 6.190276754s

â€¢ [SLOW TEST:10.339 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:17:44.865: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:17:44.939: INFO: Creating ReplicaSet my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80
Jul 22 01:17:44.961: INFO: Pod name my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80: Found 0 pods out of 1
Jul 22 01:17:49.969: INFO: Pod name my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80: Found 1 pods out of 1
Jul 22 01:17:49.969: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80" is running
Jul 22 01:17:49.973: INFO: Pod "my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80-gh75b" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 01:17:45 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 01:17:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 01:17:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 03:34:06 +0000 UTC Reason: Message:}])
Jul 22 01:17:49.973: INFO: Trying to dial the pod
Jul 22 01:17:54.997: INFO: Controller my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80: Got expected result from replica 1 [my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80-gh75b]: "my-hostname-basic-8277c786-ac1e-11e9-906c-f6b46dea7a80-gh75b", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:17:54.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3521" for this suite.
Jul 22 01:18:01.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:18:01.168: INFO: namespace replicaset-3521 deletion completed in 6.163330494s

â€¢ [SLOW TEST:16.304 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:18:01.169: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 22 01:18:01.258: INFO: Waiting up to 5m0s for pod "pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80" in namespace "emptydir-6094" to be "success or failure"
Jul 22 01:18:01.270: INFO: Pod "pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.203279ms
Jul 22 01:18:03.276: INFO: Pod "pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018398339s
Jul 22 01:18:05.282: INFO: Pod "pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024438915s
STEP: Saw pod success
Jul 22 01:18:05.282: INFO: Pod "pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:18:05.287: INFO: Trying to get logs from node k8s1 pod pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:18:05.337: INFO: Waiting for pod pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:18:05.342: INFO: Pod pod-8c2e6946-ac1e-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:18:05.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6094" for this suite.
Jul 22 01:18:11.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:18:11.530: INFO: namespace emptydir-6094 deletion completed in 6.180852076s

â€¢ [SLOW TEST:10.361 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:18:11.530: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:18:17.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6552" for this suite.
Jul 22 01:18:23.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:18:23.964: INFO: namespace namespaces-6552 deletion completed in 6.170550264s
STEP: Destroying namespace "nsdeletetest-3280" for this suite.
Jul 22 01:18:23.968: INFO: Namespace nsdeletetest-3280 was already deleted
STEP: Destroying namespace "nsdeletetest-2819" for this suite.
Jul 22 01:18:29.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:18:30.129: INFO: namespace nsdeletetest-2819 deletion completed in 6.160508605s

â€¢ [SLOW TEST:18.599 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:18:30.129: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0722 01:18:40.339633      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 01:18:40.339: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:18:40.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2795" for this suite.
Jul 22 01:18:48.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:18:48.529: INFO: namespace gc-2795 deletion completed in 8.183446741s

â€¢ [SLOW TEST:18.400 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:18:48.529: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jul 22 01:18:48.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-1551'
Jul 22 01:18:48.938: INFO: stderr: ""
Jul 22 01:18:48.938: INFO: stdout: "pod/pause created\n"
Jul 22 01:18:48.938: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 22 01:18:48.938: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1551" to be "running and ready"
Jul 22 01:18:48.944: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 5.603869ms
Jul 22 01:18:50.950: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011787792s
Jul 22 01:18:52.956: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017845897s
Jul 22 01:18:54.964: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.025582671s
Jul 22 01:18:54.964: INFO: Pod "pause" satisfied condition "running and ready"
Jul 22 01:18:54.964: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 22 01:18:54.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 label pods pause testing-label=testing-label-value --namespace=kubectl-1551'
Jul 22 01:18:55.161: INFO: stderr: ""
Jul 22 01:18:55.162: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 22 01:18:55.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pod pause -L testing-label --namespace=kubectl-1551'
Jul 22 01:18:55.339: INFO: stderr: ""
Jul 22 01:18:55.340: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 22 01:18:55.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 label pods pause testing-label- --namespace=kubectl-1551'
Jul 22 01:18:55.674: INFO: stderr: ""
Jul 22 01:18:55.674: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 22 01:18:55.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pod pause -L testing-label --namespace=kubectl-1551'
Jul 22 01:18:55.856: INFO: stderr: ""
Jul 22 01:18:55.856: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          7s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jul 22 01:18:55.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-1551'
Jul 22 01:18:56.051: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:18:56.051: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 22 01:18:56.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get rc,svc -l name=pause --no-headers --namespace=kubectl-1551'
Jul 22 01:18:56.250: INFO: stderr: "No resources found.\n"
Jul 22 01:18:56.250: INFO: stdout: ""
Jul 22 01:18:56.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -l name=pause --namespace=kubectl-1551 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 01:18:56.434: INFO: stderr: ""
Jul 22 01:18:56.434: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:18:56.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1551" for this suite.
Jul 22 01:19:02.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:19:02.603: INFO: namespace kubectl-1551 deletion completed in 6.160158666s

â€¢ [SLOW TEST:14.073 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:19:02.603: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1982
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1982
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1982
Jul 22 01:19:02.714: INFO: Found 0 stateful pods, waiting for 1
Jul 22 01:19:12.721: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 22 01:19:12.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:19:13.225: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:19:13.225: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:19:13.225: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 01:19:13.231: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 01:19:23.239: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 01:19:23.239: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 01:19:23.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999241s
Jul 22 01:19:24.265: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994768031s
Jul 22 01:19:25.271: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.987454112s
Jul 22 01:19:26.277: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981162339s
Jul 22 01:19:27.285: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.973419182s
Jul 22 01:19:28.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.963867073s
Jul 22 01:19:29.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.956366928s
Jul 22 01:19:30.309: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.949581719s
Jul 22 01:19:31.315: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.94314218s
Jul 22 01:19:32.321: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.056523ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1982
Jul 22 01:19:33.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:19:33.806: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 01:19:33.806: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 01:19:33.806: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 01:19:33.812: INFO: Found 1 stateful pods, waiting for 3
Jul 22 01:19:43.820: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:19:43.820: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:19:43.821: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 22 01:19:43.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:19:44.335: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:19:44.335: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:19:44.335: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 01:19:44.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:19:44.870: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:19:44.870: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:19:44.870: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 01:19:44.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:19:45.388: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:19:45.388: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:19:45.388: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 01:19:45.388: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 01:19:45.393: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 01:19:55.406: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 01:19:55.406: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 01:19:55.406: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 01:19:55.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999307s
Jul 22 01:19:56.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993336753s
Jul 22 01:19:57.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985908802s
Jul 22 01:19:58.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978847971s
Jul 22 01:19:59.451: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97169419s
Jul 22 01:20:00.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.963547165s
Jul 22 01:20:01.470: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.952156769s
Jul 22 01:20:02.477: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.945548771s
Jul 22 01:20:03.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.9383647s
Jul 22 01:20:04.491: INFO: Verifying statefulset ss doesn't scale past 3 for another 932.090261ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1982
Jul 22 01:20:05.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:05.962: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 01:20:05.963: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 01:20:05.963: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 01:20:05.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:06.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 01:20:06.415: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 01:20:06.416: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 01:20:06.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:06.973: INFO: rc: 1
Jul 22 01:20:06.973: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server: 
 [] <nil> 0x40027779e0 exit status 1 <nil> <nil> true [0x4000011de8 0x4000011e88 0x4000011f20] [0x4000011de8 0x4000011e88 0x4000011f20] [0x4000011e68 0x4000011ec8] [0x9214f0 0x9214f0] 0x4002162c00 <nil>}:
Command stdout:

stderr:
Error from server: 

error:
exit status 1

Jul 22 01:20:16.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:17.143: INFO: rc: 1
Jul 22 01:20:17.144: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002777d10 exit status 1 <nil> <nil> true [0x4000011f40 0x4000011f98 0x4003048000] [0x4000011f40 0x4000011f98 0x4003048000] [0x4000011f80 0x4000011fe0] [0x9214f0 0x9214f0] 0x4002162f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:20:27.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:27.311: INFO: rc: 1
Jul 22 01:20:27.312: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400286c330 exit status 1 <nil> <nil> true [0x4002402010 0x4002402060 0x40024020a8] [0x4002402010 0x4002402060 0x40024020a8] [0x4002402040 0x40024020a0] [0x9214f0 0x9214f0] 0x400258c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:20:37.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:37.481: INFO: rc: 1
Jul 22 01:20:37.481: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400286c690 exit status 1 <nil> <nil> true [0x40024020b0 0x40024020f8 0x4002402148] [0x40024020b0 0x40024020f8 0x4002402148] [0x40024020d8 0x4002402128] [0x9214f0 0x9214f0] 0x400258cb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:20:47.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:47.653: INFO: rc: 1
Jul 22 01:20:47.654: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002d8e1e0 exit status 1 <nil> <nil> true [0x4000575d28 0x4000575d58 0x4000575d88] [0x4000575d28 0x4000575d58 0x4000575d88] [0x4000575d48 0x4000575d78] [0x9214f0 0x9214f0] 0x4002e75800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:20:57.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:20:57.827: INFO: rc: 1
Jul 22 01:20:57.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40024300c0 exit status 1 <nil> <nil> true [0x4003048008 0x4003048020 0x4003048038] [0x4003048008 0x4003048020 0x4003048038] [0x4003048018 0x4003048030] [0x9214f0 0x9214f0] 0x40021632c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:07.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:07.996: INFO: rc: 1
Jul 22 01:21:07.996: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x400286c9c0 exit status 1 <nil> <nil> true [0x4002402158 0x4002402188 0x40024021c8] [0x4002402158 0x4002402188 0x40024021c8] [0x4002402178 0x40024021c0] [0x9214f0 0x9214f0] 0x400258cea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:17.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:18.166: INFO: rc: 1
Jul 22 01:21:18.166: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40020db320 exit status 1 <nil> <nil> true [0x40000eb028 0x40000eb0b0 0x40000eb178] [0x40000eb028 0x40000eb0b0 0x40000eb178] [0x40000eb048 0x40000eb168] [0x9214f0 0x9214f0] 0x4002f7b5c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:28.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:28.335: INFO: rc: 1
Jul 22 01:21:28.335: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002d8e540 exit status 1 <nil> <nil> true [0x4000575d90 0x4000575db0 0x4000575dd8] [0x4000575d90 0x4000575db0 0x4000575dd8] [0x4000575da0 0x4000575dd0] [0x9214f0 0x9214f0] 0x4002e75b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:38.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:38.516: INFO: rc: 1
Jul 22 01:21:38.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002d8e8a0 exit status 1 <nil> <nil> true [0x4000575df8 0x4000575e20 0x4000575e50] [0x4000575df8 0x4000575e20 0x4000575e50] [0x4000575e18 0x4000575e40] [0x9214f0 0x9214f0] 0x4002e75ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:48.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:48.689: INFO: rc: 1
Jul 22 01:21:48.689: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002776330 exit status 1 <nil> <nil> true [0x40001d8568 0x4000010190 0x4000010370] [0x40001d8568 0x4000010190 0x4000010370] [0x40000100f0 0x4000010228] [0x9214f0 0x9214f0] 0x400248e660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:21:58.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:21:58.858: INFO: rc: 1
Jul 22 01:21:58.858: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022300 exit status 1 <nil> <nil> true [0x4002402000 0x4002402040 0x40024020a0] [0x4002402000 0x4002402040 0x40024020a0] [0x4002402030 0x4002402098] [0x9214f0 0x9214f0] 0x400258c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:08.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:09.033: INFO: rc: 1
Jul 22 01:22:09.034: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022660 exit status 1 <nil> <nil> true [0x40024020a8 0x40024020d8 0x4002402128] [0x40024020a8 0x40024020d8 0x4002402128] [0x40024020c0 0x4002402110] [0x9214f0 0x9214f0] 0x400258cb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:19.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:19.209: INFO: rc: 1
Jul 22 01:22:19.209: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002776690 exit status 1 <nil> <nil> true [0x4000010528 0x40000106a0 0x4000010718] [0x4000010528 0x40000106a0 0x4000010718] [0x4000010608 0x40000106f8] [0x9214f0 0x9214f0] 0x400248ecc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:29.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:29.390: INFO: rc: 1
Jul 22 01:22:29.390: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002776ea0 exit status 1 <nil> <nil> true [0x4000010788 0x4000010930 0x4000010a08] [0x4000010788 0x4000010930 0x4000010a08] [0x40000108d0 0x40000109a0] [0x9214f0 0x9214f0] 0x400248f500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:39.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:39.580: INFO: rc: 1
Jul 22 01:22:39.580: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x40020229c0 exit status 1 <nil> <nil> true [0x4002402148 0x4002402178 0x40024021c0] [0x4002402148 0x4002402178 0x40024021c0] [0x4002402168 0x40024021a0] [0x9214f0 0x9214f0] 0x400258cea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:49.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:49.792: INFO: rc: 1
Jul 22 01:22:49.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022d20 exit status 1 <nil> <nil> true [0x40024021c8 0x4002402230 0x4002402288] [0x40024021c8 0x4002402230 0x4002402288] [0x4002402218 0x4002402260] [0x9214f0 0x9214f0] 0x400258d200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:22:59.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:22:59.959: INFO: rc: 1
Jul 22 01:22:59.959: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002023080 exit status 1 <nil> <nil> true [0x40024022a8 0x40024022f0 0x4002402338] [0x40024022a8 0x40024022f0 0x4002402338] [0x40024022e8 0x4002402328] [0x9214f0 0x9214f0] 0x400258d560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:23:09.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:23:10.126: INFO: rc: 1
Jul 22 01:23:10.126: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c02330 exit status 1 <nil> <nil> true [0x40000ea098 0x40000ea5d0 0x40000ea8a0] [0x40000ea098 0x40000ea5d0 0x40000ea8a0] [0x40000ea410 0x40000ea6f8] [0x9214f0 0x9214f0] 0x4002f7a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:23:20.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:23:20.302: INFO: rc: 1
Jul 22 01:23:20.302: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c02690 exit status 1 <nil> <nil> true [0x40000eaa68 0x40000ead20 0x40000eae78] [0x40000eaa68 0x40000ead20 0x40000eae78] [0x40000eacf8 0x40000eae30] [0x9214f0 0x9214f0] 0x4002f7ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:23:30.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:23:30.468: INFO: rc: 1
Jul 22 01:23:30.468: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002777290 exit status 1 <nil> <nil> true [0x4000010a20 0x4000010ee0 0x4000010f28] [0x4000010a20 0x4000010ee0 0x4000010f28] [0x4000010e88 0x4000010f10] [0x9214f0 0x9214f0] 0x400248f9e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:23:40.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:23:40.637: INFO: rc: 1
Jul 22 01:23:40.638: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002776300 exit status 1 <nil> <nil> true [0x4000010010 0x4000010200 0x4000010528] [0x4000010010 0x4000010200 0x4000010528] [0x4000010190 0x4000010370] [0x9214f0 0x9214f0] 0x400248e660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:23:50.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:23:50.804: INFO: rc: 1
Jul 22 01:23:50.804: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c02300 exit status 1 <nil> <nil> true [0x40000ea098 0x40000ea5d0 0x40000ea8a0] [0x40000ea098 0x40000ea5d0 0x40000ea8a0] [0x40000ea410 0x40000ea6f8] [0x9214f0 0x9214f0] 0x4002f7a8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:00.804: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:00.971: INFO: rc: 1
Jul 22 01:24:00.971: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c02660 exit status 1 <nil> <nil> true [0x40000eaa68 0x40000ead20 0x40000eae78] [0x40000eaa68 0x40000ead20 0x40000eae78] [0x40000eacf8 0x40000eae30] [0x9214f0 0x9214f0] 0x4002f7ad20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:10.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:11.148: INFO: rc: 1
Jul 22 01:24:11.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022330 exit status 1 <nil> <nil> true [0x4002402000 0x4002402040 0x40024020a0] [0x4002402000 0x4002402040 0x40024020a0] [0x4002402030 0x4002402098] [0x9214f0 0x9214f0] 0x400258c5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:21.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:21.323: INFO: rc: 1
Jul 22 01:24:21.323: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c029f0 exit status 1 <nil> <nil> true [0x40000eae88 0x40000eaf28 0x40000eb028] [0x40000eae88 0x40000eaf28 0x40000eb028] [0x40000eaef8 0x40000eaff0] [0x9214f0 0x9214f0] 0x4002f7b080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:31.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:31.516: INFO: rc: 1
Jul 22 01:24:31.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022690 exit status 1 <nil> <nil> true [0x40024020a8 0x40024020d8 0x4002402128] [0x40024020a8 0x40024020d8 0x4002402128] [0x40024020c0 0x4002402110] [0x9214f0 0x9214f0] 0x400258cb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:41.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:41.682: INFO: rc: 1
Jul 22 01:24:41.682: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4002022a20 exit status 1 <nil> <nil> true [0x4002402148 0x4002402178 0x40024021c0] [0x4002402148 0x4002402178 0x40024021c0] [0x4002402168 0x40024021a0] [0x9214f0 0x9214f0] 0x400258cea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:24:51.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:24:51.852: INFO: rc: 1
Jul 22 01:24:51.853: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c03050 exit status 1 <nil> <nil> true [0x40000eb038 0x40000eb110 0x40000eb1a0] [0x40000eb038 0x40000eb110 0x40000eb1a0] [0x40000eb0b0 0x40000eb178] [0x9214f0 0x9214f0] 0x4002f7b3e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:25:01.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:25:02.026: INFO: rc: 1
Jul 22 01:25:02.026: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0x4001c03380 exit status 1 <nil> <nil> true [0x40000eb2a8 0x40000eb400 0x40000eb4f0] [0x40000eb2a8 0x40000eb400 0x40000eb4f0] [0x40000eb3b8 0x40000eb4c0] [0x9214f0 0x9214f0] 0x4002f7b740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jul 22 01:25:12.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-1982 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:25:12.189: INFO: rc: 1
Jul 22 01:25:12.189: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jul 22 01:25:12.190: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 22 01:25:12.207: INFO: Deleting all statefulset in ns statefulset-1982
Jul 22 01:25:12.210: INFO: Scaling statefulset ss to 0
Jul 22 01:25:12.223: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 01:25:12.226: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:25:12.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1982" for this suite.
Jul 22 01:25:20.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:25:20.441: INFO: namespace statefulset-1982 deletion completed in 8.172327167s

â€¢ [SLOW TEST:377.839 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:25:20.442: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:25:20.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7050" for this suite.
Jul 22 01:25:44.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:25:44.742: INFO: namespace pods-7050 deletion completed in 24.192794985s

â€¢ [SLOW TEST:24.300 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:25:44.742: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a07ffee1-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:25:44.858: INFO: Waiting up to 5m0s for pod "pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80" in namespace "configmap-5277" to be "success or failure"
Jul 22 01:25:44.863: INFO: Pod "pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.999737ms
Jul 22 01:25:46.871: INFO: Pod "pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012853697s
Jul 22 01:25:48.879: INFO: Pod "pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020266818s
STEP: Saw pod success
Jul 22 01:25:48.879: INFO: Pod "pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:25:48.892: INFO: Trying to get logs from node k8s2 pod pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:25:48.949: INFO: Waiting for pod pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:25:48.954: INFO: Pod pod-configmaps-a081e801-ac1f-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:25:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5277" for this suite.
Jul 22 01:25:54.979: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:25:55.128: INFO: namespace configmap-5277 deletion completed in 6.166920775s

â€¢ [SLOW TEST:10.385 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:25:55.128: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-a6af0664-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:25:55.224: INFO: Waiting up to 5m0s for pod "pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80" in namespace "secrets-5634" to be "success or failure"
Jul 22 01:25:55.229: INFO: Pod "pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.957552ms
Jul 22 01:25:57.236: INFO: Pod "pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012387563s
Jul 22 01:25:59.245: INFO: Pod "pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021369666s
STEP: Saw pod success
Jul 22 01:25:59.245: INFO: Pod "pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:25:59.251: INFO: Trying to get logs from node k8s3 pod pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:25:59.283: INFO: Waiting for pod pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:25:59.288: INFO: Pod pod-secrets-a6b095e6-ac1f-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:25:59.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5634" for this suite.
Jul 22 01:26:05.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:26:05.463: INFO: namespace secrets-5634 deletion completed in 6.167404874s

â€¢ [SLOW TEST:10.335 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:26:05.464: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-acdb1137-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating configMap with name cm-test-opt-upd-acdb11af-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-acdb1137-ac1f-11e9-906c-f6b46dea7a80
STEP: Updating configmap cm-test-opt-upd-acdb11af-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating configMap with name cm-test-opt-create-acdb11e9-ac1f-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:26:15.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2343" for this suite.
Jul 22 01:26:39.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:26:39.925: INFO: namespace configmap-2343 deletion completed in 24.174463934s

â€¢ [SLOW TEST:34.461 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:26:39.926: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 01:26:39.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6341'
Jul 22 01:26:40.204: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 01:26:40.204: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 22 01:26:40.225: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-jmx8m]
Jul 22 01:26:40.225: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-jmx8m" in namespace "kubectl-6341" to be "running and ready"
Jul 22 01:26:40.230: INFO: Pod "e2e-test-nginx-rc-jmx8m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955472ms
Jul 22 01:26:42.237: INFO: Pod "e2e-test-nginx-rc-jmx8m": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012060635s
Jul 22 01:26:44.243: INFO: Pod "e2e-test-nginx-rc-jmx8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.018368073s
Jul 22 01:26:44.244: INFO: Pod "e2e-test-nginx-rc-jmx8m" satisfied condition "running and ready"
Jul 22 01:26:44.244: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-jmx8m]
Jul 22 01:26:44.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 logs rc/e2e-test-nginx-rc --namespace=kubectl-6341'
Jul 22 01:26:44.476: INFO: stderr: ""
Jul 22 01:26:44.476: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jul 22 01:26:44.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete rc e2e-test-nginx-rc --namespace=kubectl-6341'
Jul 22 01:26:44.676: INFO: stderr: ""
Jul 22 01:26:44.677: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:26:44.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6341" for this suite.
Jul 22 01:26:50.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:26:50.847: INFO: namespace kubectl-6341 deletion completed in 6.162790348s

â€¢ [SLOW TEST:10.921 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:26:50.848: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jul 22 01:26:54.984: INFO: Pod pod-hostip-c7e5aad0-ac1f-11e9-906c-f6b46dea7a80 has hostIP: 100.2.97.92
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:26:54.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2063" for this suite.
Jul 22 01:27:19.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:27:19.151: INFO: namespace pods-2063 deletion completed in 24.160062809s

â€¢ [SLOW TEST:28.304 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:27:19.151: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 01:27:19.292: INFO: Number of nodes with available pods: 0
Jul 22 01:27:19.292: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:27:20.306: INFO: Number of nodes with available pods: 0
Jul 22 01:27:20.306: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:27:21.307: INFO: Number of nodes with available pods: 0
Jul 22 01:27:21.307: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:27:22.306: INFO: Number of nodes with available pods: 1
Jul 22 01:27:22.306: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:27:23.308: INFO: Number of nodes with available pods: 2
Jul 22 01:27:23.308: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:27:24.305: INFO: Number of nodes with available pods: 3
Jul 22 01:27:24.305: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 22 01:27:24.342: INFO: Number of nodes with available pods: 2
Jul 22 01:27:24.342: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:27:25.357: INFO: Number of nodes with available pods: 2
Jul 22 01:27:25.357: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:27:26.355: INFO: Number of nodes with available pods: 2
Jul 22 01:27:26.355: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:27:27.356: INFO: Number of nodes with available pods: 2
Jul 22 01:27:27.356: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:27:28.355: INFO: Number of nodes with available pods: 3
Jul 22 01:27:28.355: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4348, will wait for the garbage collector to delete the pods
Jul 22 01:27:28.429: INFO: Deleting DaemonSet.extensions daemon-set took: 10.17686ms
Jul 22 01:27:28.830: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.367729ms
Jul 22 01:27:33.835: INFO: Number of nodes with available pods: 0
Jul 22 01:27:33.835: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 01:27:33.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4348/daemonsets","resourceVersion":"2290584"},"items":null}

Jul 22 01:27:33.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4348/pods","resourceVersion":"2290584"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:27:33.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4348" for this suite.
Jul 22 01:27:39.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:27:40.048: INFO: namespace daemonsets-4348 deletion completed in 6.171227006s

â€¢ [SLOW TEST:20.897 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:27:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 01:27:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5116'
Jul 22 01:27:40.329: INFO: stderr: ""
Jul 22 01:27:40.329: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 22 01:27:45.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pod e2e-test-nginx-pod --namespace=kubectl-5116 -o json'
Jul 22 01:27:45.569: INFO: stderr: ""
Jul 22 01:27:45.569: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-22T01:27:40Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5116\",\n        \"resourceVersion\": \"2290656\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5116/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e553d51e-ac1f-11e9-812c-eeeeeeeeeeee\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8tbzw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8tbzw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8tbzw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T03:43:16Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T03:43:19Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T03:43:19Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-22T03:43:16Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://5829acb7a9db7baa53b8c6b4aab9f98defeaa38c17484c102b964d8f90fc28b9\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-22T03:43:18Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.2.97.90\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.91.1\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-22T03:43:16Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 22 01:27:45.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 replace -f - --namespace=kubectl-5116'
Jul 22 01:27:45.866: INFO: stderr: ""
Jul 22 01:27:45.866: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jul 22 01:27:45.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete pods e2e-test-nginx-pod --namespace=kubectl-5116'
Jul 22 01:27:48.294: INFO: stderr: ""
Jul 22 01:27:48.294: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:27:48.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5116" for this suite.
Jul 22 01:27:54.329: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:27:54.474: INFO: namespace kubectl-5116 deletion completed in 6.17010387s

â€¢ [SLOW TEST:14.425 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:27:54.474: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:27:54.576: INFO: Waiting up to 5m0s for pod "downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80" in namespace "projected-996" to be "success or failure"
Jul 22 01:27:54.582: INFO: Pod "downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.826386ms
Jul 22 01:27:56.592: INFO: Pod "downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015566757s
Jul 22 01:27:58.599: INFO: Pod "downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022596758s
STEP: Saw pod success
Jul 22 01:27:58.599: INFO: Pod "downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:27:58.604: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:27:58.633: INFO: Waiting for pod downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:27:58.640: INFO: Pod downwardapi-volume-edd383e6-ac1f-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:27:58.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-996" for this suite.
Jul 22 01:28:04.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:28:04.822: INFO: namespace projected-996 deletion completed in 6.174965092s

â€¢ [SLOW TEST:10.348 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:28:04.823: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jul 22 01:28:04.909: INFO: Waiting up to 5m0s for pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80" in namespace "var-expansion-2229" to be "success or failure"
Jul 22 01:28:04.929: INFO: Pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 20.269934ms
Jul 22 01:28:06.936: INFO: Pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026969063s
Jul 22 01:28:08.942: INFO: Pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033166911s
Jul 22 01:28:10.948: INFO: Pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039119471s
STEP: Saw pod success
Jul 22 01:28:10.948: INFO: Pod "var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:28:10.953: INFO: Trying to get logs from node k8s3 pod var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 01:28:10.990: INFO: Waiting for pod var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:28:10.995: INFO: Pod var-expansion-f3fc2a11-ac1f-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:28:10.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2229" for this suite.
Jul 22 01:28:17.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:28:17.175: INFO: namespace var-expansion-2229 deletion completed in 6.170948155s

â€¢ [SLOW TEST:12.352 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:28:17.175: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-fb58f059-ac1f-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:28:17.268: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80" in namespace "projected-9481" to be "success or failure"
Jul 22 01:28:17.288: INFO: Pod "pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 19.851789ms
Jul 22 01:28:19.294: INFO: Pod "pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026039129s
Jul 22 01:28:21.300: INFO: Pod "pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031989652s
STEP: Saw pod success
Jul 22 01:28:21.300: INFO: Pod "pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:28:21.305: INFO: Trying to get logs from node k8s1 pod pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:28:21.346: INFO: Waiting for pod pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:28:21.351: INFO: Pod pod-projected-configmaps-fb5a8aad-ac1f-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:28:21.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9481" for this suite.
Jul 22 01:28:27.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:28:27.548: INFO: namespace projected-9481 deletion completed in 6.189509456s

â€¢ [SLOW TEST:10.373 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:28:27.548: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 22 01:28:27.616: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 01:28:27.630: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 01:28:27.635: INFO: 
Logging pods the kubelet thinks is on node k8s1 before test
Jul 22 01:28:27.652: INFO: kube-state-metrics-7d47cbc8c5-w92wp from monitoring started at 2019-07-19 06:26:32 +0000 UTC (4 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 22 01:28:27.652: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 22 01:28:27.652: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 22 01:28:27.652: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 22 01:28:27.652: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-75w7m from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:28:27.652: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:28:27.652: INFO: kube-proxy-5vpfs from kube-system started at 2019-07-11 19:32:18 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 01:28:27.652: INFO: local-path-provisioner-546d678b86-qlhkz from local-path-storage started at 2019-07-12 06:09:22 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jul 22 01:28:27.652: INFO: prometheus-adapter-d7ff7d6cd-b9689 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 22 01:28:27.652: INFO: nginx-deployment-6f478d8d8-cb865 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.652: INFO: kube-scheduler-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.652: INFO: listening-dragon-rabbitmq-ha-0 from default started at 2019-07-17 07:12:45 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container rabbitmq-ha ready: true, restart count 1
Jul 22 01:28:27.652: INFO: nginx-deployment-6f478d8d8-t2dws from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.652: INFO: ingress-nginx-controller-bhvc9 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container ingress-nginx-controller ready: true, restart count 6
Jul 22 01:28:27.652: INFO: nginx-deployment-6f478d8d8-8kpvr from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.652: INFO: kube-controller-manager-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.652: INFO: coredns-848c785ddb-rghg5 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container coredns ready: true, restart count 0
Jul 22 01:28:27.652: INFO: nginx-deployment-6f478d8d8-v5r52 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.652: INFO: node-exporter-dtt5q from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 01:28:27.652: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 01:28:27.652: INFO: kube-apiserver-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.652: INFO: calico-node-qfw58 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 01:28:27.652: INFO: etcdtest2-57bf45d5c8-9xj4s from default started at 2019-07-22 01:41:31 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.652: INFO: 	Container etcdtest2 ready: true, restart count 24
Jul 22 01:28:27.652: INFO: tiller-deploy-5f4c64779f-jrkt8 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.653: INFO: 	Container tiller ready: true, restart count 0
Jul 22 01:28:27.653: INFO: nginx-deployment-6f478d8d8-znlw8 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.653: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.653: INFO: 
Logging pods the kubelet thinks is on node k8s2 before test
Jul 22 01:28:27.672: INFO: kube-apiserver-k8s2 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.672: INFO: ingress-nginx-controller-6drg8 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container ingress-nginx-controller ready: true, restart count 10
Jul 22 01:28:27.672: INFO: grafana-5ccccd76c8-nkcbl from monitoring started at 2019-07-11 20:05:14 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container grafana ready: true, restart count 0
Jul 22 01:28:27.672: INFO: prometheus-operator-9695d44c6-kjvfp from monitoring started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 22 01:28:27.672: INFO: calico-node-zjpdk from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 01:28:27.672: INFO: calico-kube-controllers-5bc4f6d669-l2mgl from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 01:28:27.672: INFO: dns-autoscaler-554d755c54-qnnsl from kube-system started at 2019-07-11 19:34:10 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 01:28:27.672: INFO: prometheus-k8s-0 from monitoring started at 2019-07-11 20:05:28 +0000 UTC (3 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 01:28:27.672: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 01:28:27.672: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 22 01:28:27.672: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-9htdd from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:28:27.672: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:28:27.672: INFO: kube-proxy-dlvss from kube-system started at 2019-07-11 19:32:23 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 01:28:27.672: INFO: nginx-deployment-6f478d8d8-vb77p from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.672: INFO: nginx-deployment-6f478d8d8-xrrg6 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.672: INFO: listening-dragon-rabbitmq-ha-1 from default started at 2019-07-17 07:16:09 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 01:28:27.672: INFO: nginx-deployment-6f478d8d8-pvn55 from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.672: INFO: nginx-deployment-6f478d8d8-p4pgc from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.672: INFO: alertmanager-main-0 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container alertmanager ready: true, restart count 0
Jul 22 01:28:27.672: INFO: 	Container config-reloader ready: true, restart count 0
Jul 22 01:28:27.672: INFO: node-exporter-97jbc from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 22 01:28:27.672: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 01:28:27.672: INFO: coredns-848c785ddb-r66tb from kube-system started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container coredns ready: true, restart count 0
Jul 22 01:28:27.672: INFO: nginx-deployment-6f478d8d8-smzdp from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.672: INFO: 	Container nginx ready: true, restart count 0
Jul 22 01:28:27.672: INFO: 
Logging pods the kubelet thinks is on node k8s3 before test
Jul 22 01:28:27.687: INFO: kube-apiserver-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.687: INFO: kube-scheduler-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 01:28:27.687: INFO: kube-proxy-lmr2d from kube-system started at 2019-07-11 19:32:28 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 01:28:27.687: INFO: node-exporter-hg26v from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 01:28:27.687: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 01:28:27.687: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-dk2k4 from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:28:27.687: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 22 01:28:27.687: INFO: ingress-nginx-controller-vhvx7 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container ingress-nginx-controller ready: true, restart count 3
Jul 22 01:28:27.687: INFO: prometheus-k8s-1 from monitoring started at 2019-07-18 22:51:45 +0000 UTC (3 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 01:28:27.687: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 01:28:27.687: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 22 01:28:27.687: INFO: testetcdxyys-769d48b897-fdl8m from default started at 2019-07-18 23:02:11 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container testetcdxyys ready: true, restart count 0
Jul 22 01:28:27.687: INFO: sonobuoy-e2e-job-3b2fad7799ec403c from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container e2e ready: true, restart count 0
Jul 22 01:28:27.687: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 01:28:27.687: INFO: calico-node-wqns7 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 01:28:27.687: INFO: listening-dragon-rabbitmq-ha-2 from default started at 2019-07-18 22:52:25 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 01:28:27.687: INFO: etcdtest-cbfd679c7-9cksx from default started at 2019-07-21 23:08:38 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container etcdtest ready: false, restart count 0
Jul 22 01:28:27.687: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 01:03:40 +0000 UTC (1 container statuses recorded)
Jul 22 01:28:27.687: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-03fdefb6-ac20-11e9-906c-f6b46dea7a80 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-03fdefb6-ac20-11e9-906c-f6b46dea7a80 off the node k8s2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-03fdefb6-ac20-11e9-906c-f6b46dea7a80
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:28:35.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6542" for this suite.
Jul 22 01:28:51.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:28:52.060: INFO: namespace sched-pred-6542 deletion completed in 16.199655224s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:24.512 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:28:52.060: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-10265955-ac20-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:28:52.168: INFO: Waiting up to 5m0s for pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80" in namespace "configmap-7849" to be "success or failure"
Jul 22 01:28:52.174: INFO: Pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.981021ms
Jul 22 01:28:54.180: INFO: Pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011850987s
Jul 22 01:28:56.186: INFO: Pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018464879s
Jul 22 01:28:58.195: INFO: Pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026991462s
STEP: Saw pod success
Jul 22 01:28:58.195: INFO: Pod "pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:28:58.199: INFO: Trying to get logs from node k8s3 pod pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:28:58.237: INFO: Waiting for pod pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:28:58.241: INFO: Pod pod-configmaps-10293138-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:28:58.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7849" for this suite.
Jul 22 01:29:04.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:29:04.411: INFO: namespace configmap-7849 deletion completed in 6.162816023s

â€¢ [SLOW TEST:12.351 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:29:04.412: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 01:29:04.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-613'
Jul 22 01:29:04.688: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 01:29:04.688: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jul 22 01:29:04.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete jobs e2e-test-nginx-job --namespace=kubectl-613'
Jul 22 01:29:04.890: INFO: stderr: ""
Jul 22 01:29:04.890: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:29:04.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-613" for this suite.
Jul 22 01:29:10.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:29:11.062: INFO: namespace kubectl-613 deletion completed in 6.163595958s

â€¢ [SLOW TEST:6.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:29:11.063: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:29:11.129: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:29:15.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6355" for this suite.
Jul 22 01:29:57.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:29:57.369: INFO: namespace pods-6355 deletion completed in 42.171952888s

â€¢ [SLOW TEST:46.306 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:29:57.370: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 01:29:57.478: INFO: Waiting up to 5m0s for pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80" in namespace "emptydir-5370" to be "success or failure"
Jul 22 01:29:57.483: INFO: Pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.309311ms
Jul 22 01:29:59.490: INFO: Pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012262503s
Jul 22 01:30:01.497: INFO: Pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019134943s
Jul 22 01:30:03.503: INFO: Pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02536462s
STEP: Saw pod success
Jul 22 01:30:03.503: INFO: Pod "pod-3715f38f-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:30:03.508: INFO: Trying to get logs from node k8s3 pod pod-3715f38f-ac20-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:30:03.537: INFO: Waiting for pod pod-3715f38f-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:30:03.542: INFO: Pod pod-3715f38f-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:30:03.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5370" for this suite.
Jul 22 01:30:09.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:30:09.730: INFO: namespace emptydir-5370 deletion completed in 6.179899712s

â€¢ [SLOW TEST:12.360 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:30:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6543
Jul 22 01:30:13.849: INFO: Started pod liveness-http in namespace container-probe-6543
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 01:30:13.853: INFO: Initial restart count of pod liveness-http is 0
Jul 22 01:30:35.931: INFO: Restart count of pod container-probe-6543/liveness-http is now 1 (22.077300636s elapsed)
Jul 22 01:30:58.004: INFO: Restart count of pod container-probe-6543/liveness-http is now 2 (44.150222003s elapsed)
Jul 22 01:31:20.077: INFO: Restart count of pod container-probe-6543/liveness-http is now 3 (1m6.223489561s elapsed)
Jul 22 01:31:42.155: INFO: Restart count of pod container-probe-6543/liveness-http is now 4 (1m28.302058694s elapsed)
Jul 22 01:32:46.364: INFO: Restart count of pod container-probe-6543/liveness-http is now 5 (2m32.510861754s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:32:46.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6543" for this suite.
Jul 22 01:32:52.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:32:52.583: INFO: namespace container-probe-6543 deletion completed in 6.167635352s

â€¢ [SLOW TEST:162.853 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:32:52.584: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:32:52.672: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80" in namespace "projected-2364" to be "success or failure"
Jul 22 01:32:52.677: INFO: Pod "downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.128935ms
Jul 22 01:32:54.683: INFO: Pod "downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011263729s
Jul 22 01:32:56.690: INFO: Pod "downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017802151s
STEP: Saw pod success
Jul 22 01:32:56.690: INFO: Pod "downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:32:56.695: INFO: Trying to get logs from node k8s1 pod downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:32:56.728: INFO: Waiting for pod downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:32:56.733: INFO: Pod downwardapi-volume-9f81148d-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:32:56.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2364" for this suite.
Jul 22 01:33:02.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:33:02.942: INFO: namespace projected-2364 deletion completed in 6.202065522s

â€¢ [SLOW TEST:10.358 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:33:02.942: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 22 01:33:13.064: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:13.064: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:13.331: INFO: Exec stderr: ""
Jul 22 01:33:13.331: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:13.331: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:13.594: INFO: Exec stderr: ""
Jul 22 01:33:13.594: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:13.594: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:13.909: INFO: Exec stderr: ""
Jul 22 01:33:13.909: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:13.909: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:14.233: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 22 01:33:14.233: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:14.233: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:14.523: INFO: Exec stderr: ""
Jul 22 01:33:14.524: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:14.524: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:14.797: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 22 01:33:14.797: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:14.797: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:15.084: INFO: Exec stderr: ""
Jul 22 01:33:15.084: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:15.084: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:15.380: INFO: Exec stderr: ""
Jul 22 01:33:15.380: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:15.380: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:15.732: INFO: Exec stderr: ""
Jul 22 01:33:15.732: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9731 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:33:15.732: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:33:16.033: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:33:16.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9731" for this suite.
Jul 22 01:33:58.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:33:58.209: INFO: namespace e2e-kubelet-etc-hosts-9731 deletion completed in 42.16796183s

â€¢ [SLOW TEST:55.267 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:33:58.210: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:33:58.276: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 22 01:33:58.291: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 22 01:34:03.297: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 01:34:03.297: INFO: Creating deployment "test-rolling-update-deployment"
Jul 22 01:34:03.304: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 22 01:34:03.313: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 22 01:34:05.325: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 22 01:34:05.329: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 01:34:07.368: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364153, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699364149, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 01:34:09.335: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 22 01:34:09.350: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2403,SelfLink:/apis/apps/v1/namespaces/deployment-2403/deployments/test-rolling-update-deployment,UID:a98a137c-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292092,Generation:1,CreationTimestamp:2019-07-22 03:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-22 03:49:09 +0000 UTC 2019-07-22 03:49:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-22 03:49:13 +0000 UTC 2019-07-22 03:49:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 01:34:09.356: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2403,SelfLink:/apis/apps/v1/namespaces/deployment-2403/replicasets/test-rolling-update-deployment-67599b4d9,UID:a98cc9d8-ac33-11e9-9d2b-6c92bf900680,ResourceVersion:2292082,Generation:1,CreationTimestamp:2019-07-22 03:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a98a137c-ac33-11e9-b4b6-6c92bf130c27 0x4002f6ae20 0x4002f6ae21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 01:34:09.356: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 22 01:34:09.356: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2403,SelfLink:/apis/apps/v1/namespaces/deployment-2403/replicasets/test-rolling-update-controller,UID:a6c6b6e6-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292091,Generation:2,CreationTimestamp:2019-07-22 03:49:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment a98a137c-ac33-11e9-b4b6-6c92bf130c27 0x4002f6ad57 0x4002f6ad58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 01:34:09.362: INFO: Pod "test-rolling-update-deployment-67599b4d9-tkwt5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-tkwt5,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2403,SelfLink:/api/v1/namespaces/deployment-2403/pods/test-rolling-update-deployment-67599b4d9-tkwt5,UID:a98ef1ed-ac33-11e9-9d2b-6c92bf900680,ResourceVersion:2292081,Generation:0,CreationTimestamp:2019-07-22 03:49:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 a98cc9d8-ac33-11e9-9d2b-6c92bf900680 0x40027276e0 0x40027276e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-577xb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-577xb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-577xb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002727750} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002727770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:34:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:34:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 01:34:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 03:49:09 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.187,StartTime:2019-07-22 01:34:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-22 01:34:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50 docker://a520728ec297ed170a5846c09da44403209983bb53c792aa4622114368e73667}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:34:09.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2403" for this suite.
Jul 22 01:34:17.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:34:17.554: INFO: namespace deployment-2403 deletion completed in 8.184277423s

â€¢ [SLOW TEST:19.344 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:34:17.554: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d227e0bf-ac20-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:34:17.664: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80" in namespace "projected-1808" to be "success or failure"
Jul 22 01:34:17.670: INFO: Pod "pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.701109ms
Jul 22 01:34:19.679: INFO: Pod "pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014490548s
Jul 22 01:34:21.687: INFO: Pod "pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022503235s
STEP: Saw pod success
Jul 22 01:34:21.687: INFO: Pod "pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:34:21.691: INFO: Trying to get logs from node k8s1 pod pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:34:21.730: INFO: Waiting for pod pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:34:21.735: INFO: Pod pod-projected-configmaps-d22ad89a-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:34:21.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1808" for this suite.
Jul 22 01:34:27.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:34:27.911: INFO: namespace projected-1808 deletion completed in 6.167332863s

â€¢ [SLOW TEST:10.357 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:34:27.911: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-2s2f
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 01:34:28.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-2s2f" in namespace "subpath-9980" to be "success or failure"
Jul 22 01:34:28.031: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101788ms
Jul 22 01:34:30.039: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012706994s
Jul 22 01:34:32.047: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 4.020606629s
Jul 22 01:34:34.053: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 6.02680257s
Jul 22 01:34:36.059: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 8.032874469s
Jul 22 01:34:38.065: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 10.038537345s
Jul 22 01:34:40.077: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 12.051322837s
Jul 22 01:34:42.089: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 14.063444289s
Jul 22 01:34:44.096: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 16.069556142s
Jul 22 01:34:46.102: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 18.076436446s
Jul 22 01:34:48.108: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 20.082415677s
Jul 22 01:34:50.115: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 22.08936213s
Jul 22 01:34:52.123: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Running", Reason="", readiness=true. Elapsed: 24.096481093s
Jul 22 01:34:54.129: INFO: Pod "pod-subpath-test-projected-2s2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.103467434s
STEP: Saw pod success
Jul 22 01:34:54.130: INFO: Pod "pod-subpath-test-projected-2s2f" satisfied condition "success or failure"
Jul 22 01:34:54.135: INFO: Trying to get logs from node k8s1 pod pod-subpath-test-projected-2s2f container test-container-subpath-projected-2s2f: <nil>
STEP: delete the pod
Jul 22 01:34:54.175: INFO: Waiting for pod pod-subpath-test-projected-2s2f to disappear
Jul 22 01:34:54.180: INFO: Pod pod-subpath-test-projected-2s2f no longer exists
STEP: Deleting pod pod-subpath-test-projected-2s2f
Jul 22 01:34:54.180: INFO: Deleting pod "pod-subpath-test-projected-2s2f" in namespace "subpath-9980"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:34:54.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9980" for this suite.
Jul 22 01:35:00.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:35:00.375: INFO: namespace subpath-9980 deletion completed in 6.171402246s

â€¢ [SLOW TEST:32.464 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:35:00.376: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 01:35:00.455: INFO: Waiting up to 5m0s for pod "pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80" in namespace "emptydir-5774" to be "success or failure"
Jul 22 01:35:00.460: INFO: Pod "pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.952049ms
Jul 22 01:35:02.468: INFO: Pod "pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01299048s
Jul 22 01:35:04.475: INFO: Pod "pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019427116s
STEP: Saw pod success
Jul 22 01:35:04.475: INFO: Pod "pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:35:04.479: INFO: Trying to get logs from node k8s3 pod pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:35:04.507: INFO: Waiting for pod pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:35:04.511: INFO: Pod pod-ebacd1f9-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:35:04.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5774" for this suite.
Jul 22 01:35:10.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:35:10.678: INFO: namespace emptydir-5774 deletion completed in 6.159793479s

â€¢ [SLOW TEST:10.302 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:35:10.678: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-f1cff38e-ac20-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:35:10.763: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80" in namespace "projected-3429" to be "success or failure"
Jul 22 01:35:10.768: INFO: Pod "pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.09102ms
Jul 22 01:35:12.774: INFO: Pod "pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011064227s
Jul 22 01:35:14.780: INFO: Pod "pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017563761s
STEP: Saw pod success
Jul 22 01:35:14.780: INFO: Pod "pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:35:14.791: INFO: Trying to get logs from node k8s1 pod pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:35:14.836: INFO: Waiting for pod pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:35:14.841: INFO: Pod pod-projected-configmaps-f1d162ab-ac20-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:35:14.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3429" for this suite.
Jul 22 01:35:20.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:35:21.027: INFO: namespace projected-3429 deletion completed in 6.17873451s

â€¢ [SLOW TEST:10.349 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:35:21.027: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 22 01:35:21.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292435,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 01:35:21.126: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292435,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 22 01:35:31.137: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292456,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 22 01:35:31.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292456,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 22 01:35:41.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292479,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 01:35:41.149: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292479,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 22 01:35:51.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292501,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 01:35:51.171: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-a,UID:d45a3cea-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292501,Generation:0,CreationTimestamp:2019-07-22 03:50:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 22 01:36:01.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-b,UID:ea64abcf-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292526,Generation:0,CreationTimestamp:2019-07-22 03:50:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 01:36:01.183: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-b,UID:ea64abcf-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292526,Generation:0,CreationTimestamp:2019-07-22 03:50:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 22 01:36:11.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-b,UID:ea64abcf-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292549,Generation:0,CreationTimestamp:2019-07-22 03:50:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 01:36:11.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5890,SelfLink:/api/v1/namespaces/watch-5890/configmaps/e2e-watch-test-configmap-b,UID:ea64abcf-ac33-11e9-b4b6-6c92bf130c27,ResourceVersion:2292549,Generation:0,CreationTimestamp:2019-07-22 03:50:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:36:21.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5890" for this suite.
Jul 22 01:36:27.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:36:27.396: INFO: namespace watch-5890 deletion completed in 6.189993685s

â€¢ [SLOW TEST:66.369 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:36:27.397: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:36:59.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3370" for this suite.
Jul 22 01:37:06.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:37:06.157: INFO: namespace container-runtime-3370 deletion completed in 6.183919498s

â€¢ [SLOW TEST:38.759 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:37:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3637
Jul 22 01:37:10.264: INFO: Started pod liveness-exec in namespace container-probe-3637
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 01:37:10.268: INFO: Initial restart count of pod liveness-exec is 0
Jul 22 01:38:04.448: INFO: Restart count of pod container-probe-3637/liveness-exec is now 1 (54.17920792s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:38:04.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3637" for this suite.
Jul 22 01:38:10.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:38:10.694: INFO: namespace container-probe-3637 deletion completed in 6.222098934s

â€¢ [SLOW TEST:64.537 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:38:10.694: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:38:10.791: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80" in namespace "downward-api-1383" to be "success or failure"
Jul 22 01:38:10.796: INFO: Pod "downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.951681ms
Jul 22 01:38:12.805: INFO: Pod "downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013560293s
Jul 22 01:38:14.811: INFO: Pod "downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020097657s
STEP: Saw pod success
Jul 22 01:38:14.811: INFO: Pod "downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:38:14.816: INFO: Trying to get logs from node k8s1 pod downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:38:14.855: INFO: Waiting for pod downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:38:14.860: INFO: Pod downwardapi-volume-5d1e87c8-ac21-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:38:14.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1383" for this suite.
Jul 22 01:38:20.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:38:21.026: INFO: namespace downward-api-1383 deletion completed in 6.15865457s

â€¢ [SLOW TEST:10.333 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:38:21.027: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jul 22 01:38:21.101: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-346668565 proxy --unix-socket=/tmp/kubectl-proxy-unix060908656/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:38:21.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4424" for this suite.
Jul 22 01:38:27.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:38:27.452: INFO: namespace kubectl-4424 deletion completed in 6.190248658s

â€¢ [SLOW TEST:6.425 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:38:27.452: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-6719ca4e-ac21-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:38:27.552: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80" in namespace "projected-4729" to be "success or failure"
Jul 22 01:38:27.569: INFO: Pod "pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 16.615676ms
Jul 22 01:38:29.575: INFO: Pod "pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022898327s
Jul 22 01:38:31.582: INFO: Pod "pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029252998s
STEP: Saw pod success
Jul 22 01:38:31.582: INFO: Pod "pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:38:31.587: INFO: Trying to get logs from node k8s1 pod pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:38:31.626: INFO: Waiting for pod pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:38:31.631: INFO: Pod pod-projected-secrets-671b6304-ac21-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:38:31.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4729" for this suite.
Jul 22 01:38:37.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:38:37.812: INFO: namespace projected-4729 deletion completed in 6.174454496s

â€¢ [SLOW TEST:10.360 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:38:37.813: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:38:41.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1384" for this suite.
Jul 22 01:39:23.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:39:24.099: INFO: namespace kubelet-test-1384 deletion completed in 42.162680143s

â€¢ [SLOW TEST:46.286 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:39:24.100: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-88de2cb4-ac21-11e9-906c-f6b46dea7a80
STEP: Creating secret with name secret-projected-all-test-volume-88de2c7d-ac21-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 22 01:39:24.200: INFO: Waiting up to 5m0s for pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80" in namespace "projected-1737" to be "success or failure"
Jul 22 01:39:24.205: INFO: Pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.035098ms
Jul 22 01:39:26.213: INFO: Pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012832707s
Jul 22 01:39:28.223: INFO: Pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023238529s
Jul 22 01:39:30.232: INFO: Pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031836461s
STEP: Saw pod success
Jul 22 01:39:30.232: INFO: Pod "projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:39:30.237: INFO: Trying to get logs from node k8s3 pod projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 22 01:39:30.266: INFO: Waiting for pod projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:39:30.270: INFO: Pod projected-volume-88de2c17-ac21-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:39:30.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1737" for this suite.
Jul 22 01:39:36.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:39:36.448: INFO: namespace projected-1737 deletion completed in 6.171283499s

â€¢ [SLOW TEST:12.348 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:39:36.449: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9039b8ed-ac21-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:39:36.542: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80" in namespace "projected-6261" to be "success or failure"
Jul 22 01:39:36.555: INFO: Pod "pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.5324ms
Jul 22 01:39:38.561: INFO: Pod "pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019608591s
Jul 22 01:39:40.569: INFO: Pod "pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027157445s
STEP: Saw pod success
Jul 22 01:39:40.569: INFO: Pod "pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:39:40.574: INFO: Trying to get logs from node k8s2 pod pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:39:40.612: INFO: Waiting for pod pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:39:40.617: INFO: Pod pod-projected-configmaps-903cc3fb-ac21-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:39:40.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6261" for this suite.
Jul 22 01:39:46.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:39:46.795: INFO: namespace projected-6261 deletion completed in 6.172111144s

â€¢ [SLOW TEST:10.347 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:39:46.796: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0722 01:40:26.984974      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 01:40:26.985: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:40:26.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8141" for this suite.
Jul 22 01:40:35.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:40:35.259: INFO: namespace gc-8141 deletion completed in 8.268197039s

â€¢ [SLOW TEST:48.464 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:40:35.260: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:40:35.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80" in namespace "projected-5972" to be "success or failure"
Jul 22 01:40:35.428: INFO: Pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 19.93759ms
Jul 22 01:40:37.435: INFO: Pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026598435s
Jul 22 01:40:39.442: INFO: Pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033637428s
Jul 22 01:40:41.448: INFO: Pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.040289195s
STEP: Saw pod success
Jul 22 01:40:41.448: INFO: Pod "downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:40:41.454: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:40:41.491: INFO: Waiting for pod downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:40:41.496: INFO: Pod downwardapi-volume-b350d18e-ac21-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:40:41.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5972" for this suite.
Jul 22 01:40:47.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:40:47.690: INFO: namespace projected-5972 deletion completed in 6.187895708s

â€¢ [SLOW TEST:12.431 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:40:47.691: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80
Jul 22 01:40:47.777: INFO: Pod name my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80: Found 0 pods out of 1
Jul 22 01:40:52.796: INFO: Pod name my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80: Found 1 pods out of 1
Jul 22 01:40:52.796: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80" are running
Jul 22 01:40:52.801: INFO: Pod "my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80-rnpj6" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 03:55:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 03:55:26 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 03:55:26 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-22 03:55:23 +0000 UTC Reason: Message:}])
Jul 22 01:40:52.801: INFO: Trying to dial the pod
Jul 22 01:40:57.817: INFO: Controller my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80: Got expected result from replica 1 [my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80-rnpj6]: "my-hostname-basic-bab1a409-ac21-11e9-906c-f6b46dea7a80-rnpj6", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:40:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9794" for this suite.
Jul 22 01:41:03.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:41:03.999: INFO: namespace replication-controller-9794 deletion completed in 6.174925862s

â€¢ [SLOW TEST:16.308 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:41:04.000: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 01:41:04.135: INFO: Number of nodes with available pods: 0
Jul 22 01:41:04.135: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:41:05.148: INFO: Number of nodes with available pods: 0
Jul 22 01:41:05.148: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:41:06.167: INFO: Number of nodes with available pods: 0
Jul 22 01:41:06.167: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:41:07.149: INFO: Number of nodes with available pods: 0
Jul 22 01:41:07.149: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:41:08.149: INFO: Number of nodes with available pods: 2
Jul 22 01:41:08.149: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:41:09.149: INFO: Number of nodes with available pods: 3
Jul 22 01:41:09.149: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 22 01:41:09.181: INFO: Number of nodes with available pods: 2
Jul 22 01:41:09.181: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:10.197: INFO: Number of nodes with available pods: 2
Jul 22 01:41:10.197: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:11.195: INFO: Number of nodes with available pods: 2
Jul 22 01:41:11.195: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:12.195: INFO: Number of nodes with available pods: 2
Jul 22 01:41:12.195: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:13.194: INFO: Number of nodes with available pods: 2
Jul 22 01:41:13.194: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:14.196: INFO: Number of nodes with available pods: 2
Jul 22 01:41:14.196: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:15.195: INFO: Number of nodes with available pods: 2
Jul 22 01:41:15.195: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:16.195: INFO: Number of nodes with available pods: 2
Jul 22 01:41:16.195: INFO: Node k8s2 is running more than one daemon pod
Jul 22 01:41:17.195: INFO: Number of nodes with available pods: 3
Jul 22 01:41:17.195: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3664, will wait for the garbage collector to delete the pods
Jul 22 01:41:17.278: INFO: Deleting DaemonSet.extensions daemon-set took: 19.926475ms
Jul 22 01:41:17.679: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.915211ms
Jul 22 01:41:22.785: INFO: Number of nodes with available pods: 0
Jul 22 01:41:22.785: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 01:41:22.789: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3664/daemonsets","resourceVersion":"2294045"},"items":null}

Jul 22 01:41:22.794: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3664/pods","resourceVersion":"2294045"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:41:22.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3664" for this suite.
Jul 22 01:41:28.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:41:29.008: INFO: namespace daemonsets-3664 deletion completed in 6.182863579s

â€¢ [SLOW TEST:25.008 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:41:29.008: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:41:29.115: INFO: (0) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.823595ms)
Jul 22 01:41:29.121: INFO: (1) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.218228ms)
Jul 22 01:41:29.127: INFO: (2) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.195478ms)
Jul 22 01:41:29.134: INFO: (3) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.231813ms)
Jul 22 01:41:29.140: INFO: (4) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.225096ms)
Jul 22 01:41:29.146: INFO: (5) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.218531ms)
Jul 22 01:41:29.152: INFO: (6) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.013435ms)
Jul 22 01:41:29.158: INFO: (7) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 5.932878ms)
Jul 22 01:41:29.164: INFO: (8) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.168741ms)
Jul 22 01:41:29.171: INFO: (9) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.487155ms)
Jul 22 01:41:29.178: INFO: (10) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.677714ms)
Jul 22 01:41:29.184: INFO: (11) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.445338ms)
Jul 22 01:41:29.190: INFO: (12) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.23203ms)
Jul 22 01:41:29.197: INFO: (13) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.252072ms)
Jul 22 01:41:29.203: INFO: (14) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.283336ms)
Jul 22 01:41:29.209: INFO: (15) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.216321ms)
Jul 22 01:41:29.217: INFO: (16) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.317162ms)
Jul 22 01:41:29.223: INFO: (17) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.28273ms)
Jul 22 01:41:29.229: INFO: (18) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.186551ms)
Jul 22 01:41:29.235: INFO: (19) /api/v1/nodes/k8s1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.112429ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:41:29.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-961" for this suite.
Jul 22 01:41:35.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:41:35.408: INFO: namespace proxy-961 deletion completed in 6.16595263s

â€¢ [SLOW TEST:6.400 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:41:35.408: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-d724e334-ac21-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-d724e334-ac21-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:41:43.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2800" for this suite.
Jul 22 01:42:07.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:42:07.760: INFO: namespace configmap-2800 deletion completed in 24.15763276s

â€¢ [SLOW TEST:32.352 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:42:07.761: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jul 22 01:42:07.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 --namespace=kubectl-2189 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 22 01:42:12.373: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 22 01:42:12.373: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:42:14.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2189" for this suite.
Jul 22 01:42:20.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:42:20.568: INFO: namespace kubectl-2189 deletion completed in 6.179328098s

â€¢ [SLOW TEST:12.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:42:20.569: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:42:20.698: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 22 01:42:20.721: INFO: Number of nodes with available pods: 0
Jul 22 01:42:20.721: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:42:21.735: INFO: Number of nodes with available pods: 0
Jul 22 01:42:21.735: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:42:22.736: INFO: Number of nodes with available pods: 0
Jul 22 01:42:22.736: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:42:23.734: INFO: Number of nodes with available pods: 0
Jul 22 01:42:23.734: INFO: Node k8s1 is running more than one daemon pod
Jul 22 01:42:24.734: INFO: Number of nodes with available pods: 2
Jul 22 01:42:24.734: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:42:25.735: INFO: Number of nodes with available pods: 3
Jul 22 01:42:25.735: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 22 01:42:25.786: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:25.786: INFO: Wrong image for pod: daemon-set-tlctl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:25.786: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:26.800: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:26.800: INFO: Wrong image for pod: daemon-set-tlctl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:26.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:27.802: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:27.802: INFO: Wrong image for pod: daemon-set-tlctl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:27.802: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:28.802: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:28.802: INFO: Wrong image for pod: daemon-set-tlctl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:28.802: INFO: Pod daemon-set-tlctl is not available
Jul 22 01:42:28.802: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:29.801: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:29.801: INFO: Wrong image for pod: daemon-set-tlctl. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:29.801: INFO: Pod daemon-set-tlctl is not available
Jul 22 01:42:29.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:30.801: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:30.801: INFO: Pod daemon-set-n9vgc is not available
Jul 22 01:42:30.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:31.806: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:31.806: INFO: Pod daemon-set-n9vgc is not available
Jul 22 01:42:31.806: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:32.801: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:32.801: INFO: Pod daemon-set-n9vgc is not available
Jul 22 01:42:32.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:33.800: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:33.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:34.800: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:34.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:35.801: INFO: Wrong image for pod: daemon-set-5h8f2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:35.801: INFO: Pod daemon-set-5h8f2 is not available
Jul 22 01:42:35.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:36.803: INFO: Pod daemon-set-8zcs4 is not available
Jul 22 01:42:36.803: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:37.801: INFO: Pod daemon-set-8zcs4 is not available
Jul 22 01:42:37.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:38.802: INFO: Pod daemon-set-8zcs4 is not available
Jul 22 01:42:38.802: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:39.800: INFO: Pod daemon-set-8zcs4 is not available
Jul 22 01:42:39.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:40.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:41.801: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:42.800: INFO: Wrong image for pod: daemon-set-tns8v. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 22 01:42:42.800: INFO: Pod daemon-set-tns8v is not available
Jul 22 01:42:43.801: INFO: Pod daemon-set-trzd8 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 22 01:42:43.819: INFO: Number of nodes with available pods: 2
Jul 22 01:42:43.819: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:42:44.833: INFO: Number of nodes with available pods: 2
Jul 22 01:42:44.833: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:42:45.839: INFO: Number of nodes with available pods: 2
Jul 22 01:42:45.840: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:42:46.834: INFO: Number of nodes with available pods: 2
Jul 22 01:42:46.834: INFO: Node k8s3 is running more than one daemon pod
Jul 22 01:42:47.834: INFO: Number of nodes with available pods: 3
Jul 22 01:42:47.834: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1621, will wait for the garbage collector to delete the pods
Jul 22 01:42:47.936: INFO: Deleting DaemonSet.extensions daemon-set took: 20.384941ms
Jul 22 01:42:48.336: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.342951ms
Jul 22 01:42:52.843: INFO: Number of nodes with available pods: 0
Jul 22 01:42:52.843: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 01:42:52.848: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1621/daemonsets","resourceVersion":"2294518"},"items":null}

Jul 22 01:42:52.853: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1621/pods","resourceVersion":"2294518"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:42:52.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1621" for this suite.
Jul 22 01:43:00.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:43:01.058: INFO: namespace daemonsets-1621 deletion completed in 8.171851392s

â€¢ [SLOW TEST:40.490 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:43:01.059: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:43:01.159: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80" in namespace "downward-api-984" to be "success or failure"
Jul 22 01:43:01.165: INFO: Pod "downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.422823ms
Jul 22 01:43:03.172: INFO: Pod "downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012647436s
Jul 22 01:43:05.179: INFO: Pod "downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019631291s
STEP: Saw pod success
Jul 22 01:43:05.179: INFO: Pod "downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:43:05.184: INFO: Trying to get logs from node k8s1 pod downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:43:05.234: INFO: Waiting for pod downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:43:05.239: INFO: Pod downwardapi-volume-0a2fe109-ac22-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:43:05.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-984" for this suite.
Jul 22 01:43:11.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:43:11.466: INFO: namespace downward-api-984 deletion completed in 6.220026947s

â€¢ [SLOW TEST:10.407 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:43:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 22 01:43:11.945: INFO: Pod name wrapped-volume-race-109dfc07-ac22-11e9-906c-f6b46dea7a80: Found 0 pods out of 5
Jul 22 01:43:16.966: INFO: Pod name wrapped-volume-race-109dfc07-ac22-11e9-906c-f6b46dea7a80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-109dfc07-ac22-11e9-906c-f6b46dea7a80 in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Jul 22 01:43:31.087: INFO: Deleting ReplicationController wrapped-volume-race-109dfc07-ac22-11e9-906c-f6b46dea7a80 took: 13.546895ms
Jul 22 01:43:31.588: INFO: Terminating ReplicationController wrapped-volume-race-109dfc07-ac22-11e9-906c-f6b46dea7a80 pods took: 500.31976ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 01:44:15.624: INFO: Pod name wrapped-volume-race-369191d9-ac22-11e9-906c-f6b46dea7a80: Found 0 pods out of 5
Jul 22 01:44:20.636: INFO: Pod name wrapped-volume-race-369191d9-ac22-11e9-906c-f6b46dea7a80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-369191d9-ac22-11e9-906c-f6b46dea7a80 in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Jul 22 01:44:34.754: INFO: Deleting ReplicationController wrapped-volume-race-369191d9-ac22-11e9-906c-f6b46dea7a80 took: 11.671644ms
Jul 22 01:44:35.255: INFO: Terminating ReplicationController wrapped-volume-race-369191d9-ac22-11e9-906c-f6b46dea7a80 pods took: 500.421962ms
STEP: Creating RC which spawns configmap-volume pods
Jul 22 01:45:20.603: INFO: Pod name wrapped-volume-race-5d4ad1bd-ac22-11e9-906c-f6b46dea7a80: Found 0 pods out of 5
Jul 22 01:45:25.617: INFO: Pod name wrapped-volume-race-5d4ad1bd-ac22-11e9-906c-f6b46dea7a80: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5d4ad1bd-ac22-11e9-906c-f6b46dea7a80 in namespace emptydir-wrapper-4462, will wait for the garbage collector to delete the pods
Jul 22 01:45:39.737: INFO: Deleting ReplicationController wrapped-volume-race-5d4ad1bd-ac22-11e9-906c-f6b46dea7a80 took: 13.866132ms
Jul 22 01:45:40.242: INFO: Terminating ReplicationController wrapped-volume-race-5d4ad1bd-ac22-11e9-906c-f6b46dea7a80 pods took: 504.635459ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:46:26.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4462" for this suite.
Jul 22 01:46:34.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:46:34.257: INFO: namespace emptydir-wrapper-4462 deletion completed in 8.168105109s

â€¢ [SLOW TEST:202.790 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:46:34.258: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 01:46:34.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8788'
Jul 22 01:46:34.553: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 01:46:34.553: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jul 22 01:46:36.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8788'
Jul 22 01:46:36.808: INFO: stderr: ""
Jul 22 01:46:36.808: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:46:36.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8788" for this suite.
Jul 22 01:48:52.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:48:52.994: INFO: namespace kubectl-8788 deletion completed in 2m16.170834587s

â€¢ [SLOW TEST:138.737 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:48:52.995: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 22 01:48:53.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296440,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 01:48:53.105: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296441,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 22 01:48:53.105: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296442,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 22 01:49:03.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296512,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 01:49:03.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296513,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 22 01:49:03.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-613,SelfLink:/api/v1/namespaces/watch-613/configmaps/e2e-watch-test-label-changed,UID:93166c2c-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2296514,Generation:0,CreationTimestamp:2019-07-22 04:02:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:49:03.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-613" for this suite.
Jul 22 01:49:09.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:49:09.344: INFO: namespace watch-613 deletion completed in 6.179017321s

â€¢ [SLOW TEST:16.350 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:49:09.345: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 22 01:49:14.468: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:49:15.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8526" for this suite.
Jul 22 01:49:39.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:49:39.712: INFO: namespace replicaset-8526 deletion completed in 24.197159256s

â€¢ [SLOW TEST:30.368 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:49:39.713: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 01:49:39.800: INFO: Waiting up to 5m0s for pod "pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80" in namespace "emptydir-9879" to be "success or failure"
Jul 22 01:49:39.825: INFO: Pod "pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 24.849707ms
Jul 22 01:49:41.831: INFO: Pod "pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030864419s
Jul 22 01:49:43.838: INFO: Pod "pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037439518s
STEP: Saw pod success
Jul 22 01:49:43.838: INFO: Pod "pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:49:43.842: INFO: Trying to get logs from node k8s1 pod pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:49:43.880: INFO: Waiting for pod pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:49:43.885: INFO: Pod pod-f7cc98b6-ac22-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:49:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9879" for this suite.
Jul 22 01:49:49.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:49:50.088: INFO: namespace emptydir-9879 deletion completed in 6.195331723s

â€¢ [SLOW TEST:10.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:49:50.088: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 22 01:49:50.170: INFO: Waiting up to 5m0s for pod "pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80" in namespace "emptydir-8792" to be "success or failure"
Jul 22 01:49:50.175: INFO: Pod "pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.306473ms
Jul 22 01:49:52.182: INFO: Pod "pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011824459s
Jul 22 01:49:54.188: INFO: Pod "pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017962194s
STEP: Saw pod success
Jul 22 01:49:54.188: INFO: Pod "pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:49:54.195: INFO: Trying to get logs from node k8s3 pod pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:49:54.240: INFO: Waiting for pod pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:49:54.245: INFO: Pod pod-fdfb2dd6-ac22-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:49:54.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8792" for this suite.
Jul 22 01:50:00.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:50:00.453: INFO: namespace emptydir-8792 deletion completed in 6.182668482s

â€¢ [SLOW TEST:10.365 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:50:00.454: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2132.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2132.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2132.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2132.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2132.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2132.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 01:50:06.607: INFO: DNS probes using dns-2132/dns-test-042baf08-ac23-11e9-906c-f6b46dea7a80 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:50:06.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2132" for this suite.
Jul 22 01:50:12.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:50:12.823: INFO: namespace dns-2132 deletion completed in 6.184206231s

â€¢ [SLOW TEST:12.369 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:50:12.823: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jul 22 01:50:12.896: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 22 01:50:12.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:13.203: INFO: stderr: ""
Jul 22 01:50:13.203: INFO: stdout: "service/redis-slave created\n"
Jul 22 01:50:13.204: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 22 01:50:13.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:13.530: INFO: stderr: ""
Jul 22 01:50:13.530: INFO: stdout: "service/redis-master created\n"
Jul 22 01:50:13.531: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 22 01:50:13.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:13.872: INFO: stderr: ""
Jul 22 01:50:13.872: INFO: stdout: "service/frontend created\n"
Jul 22 01:50:13.873: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 22 01:50:13.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:14.185: INFO: stderr: ""
Jul 22 01:50:14.185: INFO: stdout: "deployment.apps/frontend created\n"
Jul 22 01:50:14.186: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 22 01:50:14.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:14.506: INFO: stderr: ""
Jul 22 01:50:14.506: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 22 01:50:14.507: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 22 01:50:14.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7536'
Jul 22 01:50:14.837: INFO: stderr: ""
Jul 22 01:50:14.837: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 22 01:50:14.837: INFO: Waiting for all frontend pods to be Running.
Jul 22 01:50:24.889: INFO: Waiting for frontend to serve content.
Jul 22 01:50:24.912: INFO: Trying to add a new entry to the guestbook.
Jul 22 01:50:24.937: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul 22 01:50:24.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:25.192: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:25.192: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 01:50:25.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:25.408: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:25.408: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 01:50:25.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:25.634: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:25.634: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 01:50:25.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:25.830: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:25.830: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 01:50:25.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:26.023: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:26.024: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 22 01:50:26.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-7536'
Jul 22 01:50:26.251: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:50:26.252: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:50:26.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7536" for this suite.
Jul 22 01:51:08.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:51:08.473: INFO: namespace kubectl-7536 deletion completed in 42.210281173s

â€¢ [SLOW TEST:55.650 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:51:08.474: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2cb60579-ac23-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:51:08.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80" in namespace "projected-8607" to be "success or failure"
Jul 22 01:51:08.582: INFO: Pod "pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.252696ms
Jul 22 01:51:10.588: INFO: Pod "pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011132706s
Jul 22 01:51:12.596: INFO: Pod "pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019227224s
STEP: Saw pod success
Jul 22 01:51:12.596: INFO: Pod "pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:51:12.600: INFO: Trying to get logs from node k8s2 pod pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:51:12.635: INFO: Waiting for pod pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:51:12.639: INFO: Pod pod-projected-configmaps-2cb7e499-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:51:12.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8607" for this suite.
Jul 22 01:51:18.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:51:18.818: INFO: namespace projected-8607 deletion completed in 6.171386794s

â€¢ [SLOW TEST:10.343 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:51:18.818: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:51:18.901: INFO: Creating deployment "test-recreate-deployment"
Jul 22 01:51:18.920: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 22 01:51:18.943: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 22 01:51:20.953: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 22 01:51:20.958: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 01:51:22.964: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365105, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 01:51:24.964: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 22 01:51:24.981: INFO: Updating deployment test-recreate-deployment
Jul 22 01:51:24.981: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 22 01:51:25.139: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3350,SelfLink:/apis/apps/v1/namespaces/deployment-3350/deployments/test-recreate-deployment,UID:e3534284-ac35-11e9-b4b6-6c92bf130c27,ResourceVersion:2297333,Generation:2,CreationTimestamp:2019-07-22 04:05:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-22 04:05:11 +0000 UTC 2019-07-22 04:05:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-22 04:05:11 +0000 UTC 2019-07-22 04:05:05 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 22 01:51:25.145: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3350,SelfLink:/apis/apps/v1/namespaces/deployment-3350/replicasets/test-recreate-deployment-c9cbd8684,UID:e6b482bc-ac35-11e9-9d2b-6c92bf900680,ResourceVersion:2297331,Generation:1,CreationTimestamp:2019-07-22 04:05:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e3534284-ac35-11e9-b4b6-6c92bf130c27 0x4003501970 0x4003501971}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 01:51:25.145: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 22 01:51:25.145: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3350,SelfLink:/apis/apps/v1/namespaces/deployment-3350/replicasets/test-recreate-deployment-7d57d5ff7c,UID:e35637a5-ac35-11e9-9d2b-6c92bf900680,ResourceVersion:2297321,Generation:2,CreationTimestamp:2019-07-22 04:05:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment e3534284-ac35-11e9-b4b6-6c92bf130c27 0x40035018a7 0x40035018a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 01:51:25.151: INFO: Pod "test-recreate-deployment-c9cbd8684-pjbxk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-pjbxk,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3350,SelfLink:/api/v1/namespaces/deployment-3350/pods/test-recreate-deployment-c9cbd8684-pjbxk,UID:e6b54ed0-ac35-11e9-9d2b-6c92bf900680,ResourceVersion:2297334,Generation:0,CreationTimestamp:2019-07-22 04:05:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 e6b482bc-ac35-11e9-9d2b-6c92bf900680 0x4002ab41a0 0x4002ab41a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nrfnn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nrfnn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nrfnn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002ab4210} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002ab4230}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:05:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:05:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:05:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:05:11 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:,StartTime:2019-07-22 04:05:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:51:25.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3350" for this suite.
Jul 22 01:51:31.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:51:31.376: INFO: namespace deployment-3350 deletion completed in 6.21870307s

â€¢ [SLOW TEST:12.558 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:51:31.377: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 01:51:31.482: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80" in namespace "downward-api-2005" to be "success or failure"
Jul 22 01:51:31.487: INFO: Pod "downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.460501ms
Jul 22 01:51:33.493: INFO: Pod "downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01147907s
Jul 22 01:51:35.499: INFO: Pod "downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017777225s
STEP: Saw pod success
Jul 22 01:51:35.499: INFO: Pod "downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:51:35.504: INFO: Trying to get logs from node k8s2 pod downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 01:51:35.541: INFO: Waiting for pod downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:51:35.545: INFO: Pod downwardapi-volume-3a5e1b00-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:51:35.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2005" for this suite.
Jul 22 01:51:41.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:51:41.711: INFO: namespace downward-api-2005 deletion completed in 6.15935677s

â€¢ [SLOW TEST:10.334 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:51:41.711: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 01:51:41.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 version'
Jul 22 01:51:42.087: INFO: stderr: ""
Jul 22 01:51:42.087: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/arm64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/arm64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:51:42.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4130" for this suite.
Jul 22 01:51:48.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:51:48.266: INFO: namespace kubectl-4130 deletion completed in 6.171647828s

â€¢ [SLOW TEST:6.555 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:51:48.267: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 22 01:51:52.890: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1195 pod-service-account-44bd4727-ac23-11e9-906c-f6b46dea7a80 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 22 01:51:53.396: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1195 pod-service-account-44bd4727-ac23-11e9-906c-f6b46dea7a80 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 22 01:51:53.917: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-1195 pod-service-account-44bd4727-ac23-11e9-906c-f6b46dea7a80 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:51:54.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1195" for this suite.
Jul 22 01:52:00.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:52:00.620: INFO: namespace svcaccounts-1195 deletion completed in 6.197138089s

â€¢ [SLOW TEST:12.354 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:52:00.621: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-4bcc4cc1-ac23-11e9-906c-f6b46dea7a80
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:52:00.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2958" for this suite.
Jul 22 01:52:06.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:52:06.890: INFO: namespace configmap-2958 deletion completed in 6.17890441s

â€¢ [SLOW TEST:6.270 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:52:06.891: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jul 22 01:52:07.027: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8765" to be "success or failure"
Jul 22 01:52:07.036: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.368971ms
Jul 22 01:52:09.043: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015616544s
Jul 22 01:52:11.050: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022225513s
STEP: Saw pod success
Jul 22 01:52:11.050: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 22 01:52:11.055: INFO: Trying to get logs from node k8s1 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 22 01:52:11.094: INFO: Waiting for pod pod-host-path-test to disappear
Jul 22 01:52:11.099: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:52:11.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8765" for this suite.
Jul 22 01:52:17.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:52:17.298: INFO: namespace hostpath-8765 deletion completed in 6.191771404s

â€¢ [SLOW TEST:10.408 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:52:17.298: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:52:45.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7937" for this suite.
Jul 22 01:52:51.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:52:51.731: INFO: namespace namespaces-7937 deletion completed in 6.167579649s
STEP: Destroying namespace "nsdeletetest-227" for this suite.
Jul 22 01:52:51.735: INFO: Namespace nsdeletetest-227 was already deleted
STEP: Destroying namespace "nsdeletetest-4643" for this suite.
Jul 22 01:52:57.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:52:57.903: INFO: namespace nsdeletetest-4643 deletion completed in 6.168486357s

â€¢ [SLOW TEST:40.605 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:52:57.904: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-6defdaf0-ac23-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:52:58.007: INFO: Waiting up to 5m0s for pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80" in namespace "secrets-6013" to be "success or failure"
Jul 22 01:52:58.019: INFO: Pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.764876ms
Jul 22 01:53:00.026: INFO: Pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018602017s
Jul 22 01:53:02.033: INFO: Pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025116432s
Jul 22 01:53:04.042: INFO: Pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.034191091s
STEP: Saw pod success
Jul 22 01:53:04.042: INFO: Pod "pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:53:04.047: INFO: Trying to get logs from node k8s3 pod pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:53:04.077: INFO: Waiting for pod pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:53:04.081: INFO: Pod pod-secrets-6df29ba9-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:53:04.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6013" for this suite.
Jul 22 01:53:10.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:53:10.271: INFO: namespace secrets-6013 deletion completed in 6.182561268s

â€¢ [SLOW TEST:12.367 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:53:10.271: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4838
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul 22 01:53:10.378: INFO: Found 0 stateful pods, waiting for 3
Jul 22 01:53:20.387: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:53:20.388: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:53:20.388: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 22 01:53:30.385: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:53:30.385: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:53:30.385: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 01:53:30.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-4838 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:53:30.940: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:53:30.940: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:53:30.940: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 22 01:53:40.991: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 22 01:53:51.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-4838 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:53:51.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 01:53:51.538: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 01:53:51.538: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 01:54:01.570: INFO: Waiting for StatefulSet statefulset-4838/ss2 to complete update
Jul 22 01:54:01.570: INFO: Waiting for Pod statefulset-4838/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 01:54:01.570: INFO: Waiting for Pod statefulset-4838/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 01:54:01.570: INFO: Waiting for Pod statefulset-4838/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 01:54:11.582: INFO: Waiting for StatefulSet statefulset-4838/ss2 to complete update
Jul 22 01:54:11.582: INFO: Waiting for Pod statefulset-4838/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 01:54:11.582: INFO: Waiting for Pod statefulset-4838/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 01:54:21.582: INFO: Waiting for StatefulSet statefulset-4838/ss2 to complete update
Jul 22 01:54:21.583: INFO: Waiting for Pod statefulset-4838/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jul 22 01:54:31.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-4838 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 01:54:32.082: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 01:54:32.082: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 01:54:32.082: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 01:54:42.130: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 22 01:54:52.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-4838 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 01:54:52.619: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 01:54:52.619: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 01:54:52.619: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 01:55:12.651: INFO: Waiting for StatefulSet statefulset-4838/ss2 to complete update
Jul 22 01:55:12.651: INFO: Waiting for Pod statefulset-4838/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 22 01:55:22.666: INFO: Deleting all statefulset in ns statefulset-4838
Jul 22 01:55:22.670: INFO: Scaling statefulset ss2 to 0
Jul 22 01:55:42.696: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 01:55:42.701: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:55:42.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4838" for this suite.
Jul 22 01:55:50.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:55:50.895: INFO: namespace statefulset-4838 deletion completed in 8.166936262s

â€¢ [SLOW TEST:160.624 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:55:50.895: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 22 01:55:50.974: INFO: Waiting up to 5m0s for pod "pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80" in namespace "emptydir-6765" to be "success or failure"
Jul 22 01:55:50.979: INFO: Pod "pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560385ms
Jul 22 01:55:52.990: INFO: Pod "pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015877649s
Jul 22 01:55:54.997: INFO: Pod "pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022732513s
STEP: Saw pod success
Jul 22 01:55:54.997: INFO: Pod "pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:55:55.002: INFO: Trying to get logs from node k8s2 pod pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:55:55.040: INFO: Waiting for pod pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:55:55.044: INFO: Pod pod-d50a9b0c-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:55:55.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6765" for this suite.
Jul 22 01:56:01.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:56:01.224: INFO: namespace emptydir-6765 deletion completed in 6.173179124s

â€¢ [SLOW TEST:10.329 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:56:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-db327452-ac23-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:56:01.307: INFO: Waiting up to 5m0s for pod "pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80" in namespace "secrets-8597" to be "success or failure"
Jul 22 01:56:01.320: INFO: Pod "pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.166532ms
Jul 22 01:56:03.326: INFO: Pod "pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018190134s
Jul 22 01:56:05.331: INFO: Pod "pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023821721s
STEP: Saw pod success
Jul 22 01:56:05.331: INFO: Pod "pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:56:05.336: INFO: Trying to get logs from node k8s2 pod pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:56:05.371: INFO: Waiting for pod pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:56:05.375: INFO: Pod pod-secrets-db340955-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:56:05.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8597" for this suite.
Jul 22 01:56:11.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:56:11.605: INFO: namespace secrets-8597 deletion completed in 6.222940052s

â€¢ [SLOW TEST:10.380 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:56:11.605: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-crxm
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 01:56:11.719: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-crxm" in namespace "subpath-8228" to be "success or failure"
Jul 22 01:56:11.724: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Pending", Reason="", readiness=false. Elapsed: 5.010701ms
Jul 22 01:56:13.731: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011649571s
Jul 22 01:56:15.738: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018723119s
Jul 22 01:56:17.744: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 6.024591804s
Jul 22 01:56:19.754: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 8.034874663s
Jul 22 01:56:21.763: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 10.044331306s
Jul 22 01:56:23.770: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 12.050886175s
Jul 22 01:56:25.778: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 14.058797464s
Jul 22 01:56:27.786: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 16.066479607s
Jul 22 01:56:29.791: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 18.07217745s
Jul 22 01:56:31.798: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 20.078970132s
Jul 22 01:56:33.804: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Running", Reason="", readiness=true. Elapsed: 22.085123258s
Jul 22 01:56:35.810: INFO: Pod "pod-subpath-test-secret-crxm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.090969778s
STEP: Saw pod success
Jul 22 01:56:35.810: INFO: Pod "pod-subpath-test-secret-crxm" satisfied condition "success or failure"
Jul 22 01:56:35.815: INFO: Trying to get logs from node k8s3 pod pod-subpath-test-secret-crxm container test-container-subpath-secret-crxm: <nil>
STEP: delete the pod
Jul 22 01:56:35.853: INFO: Waiting for pod pod-subpath-test-secret-crxm to disappear
Jul 22 01:56:35.857: INFO: Pod pod-subpath-test-secret-crxm no longer exists
STEP: Deleting pod pod-subpath-test-secret-crxm
Jul 22 01:56:35.857: INFO: Deleting pod "pod-subpath-test-secret-crxm" in namespace "subpath-8228"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:56:35.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8228" for this suite.
Jul 22 01:56:41.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:56:42.069: INFO: namespace subpath-8228 deletion completed in 6.197658874s

â€¢ [SLOW TEST:30.464 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:56:42.070: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f38b129f-ac23-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 01:56:42.169: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80" in namespace "projected-1972" to be "success or failure"
Jul 22 01:56:42.175: INFO: Pod "pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.295965ms
Jul 22 01:56:44.182: INFO: Pod "pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012293093s
Jul 22 01:56:46.190: INFO: Pod "pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020458555s
STEP: Saw pod success
Jul 22 01:56:46.190: INFO: Pod "pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:56:46.194: INFO: Trying to get logs from node k8s2 pod pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 01:56:46.227: INFO: Waiting for pod pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:56:46.231: INFO: Pod pod-projected-secrets-f38c8d89-ac23-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:56:46.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1972" for this suite.
Jul 22 01:56:52.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:56:52.417: INFO: namespace projected-1972 deletion completed in 6.179041414s

â€¢ [SLOW TEST:10.348 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:56:52.418: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 01:56:52.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8454'
Jul 22 01:56:52.720: INFO: stderr: ""
Jul 22 01:56:52.720: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jul 22 01:56:52.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete pods e2e-test-nginx-pod --namespace=kubectl-8454'
Jul 22 01:56:54.303: INFO: stderr: ""
Jul 22 01:56:54.304: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:56:54.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8454" for this suite.
Jul 22 01:57:00.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:57:00.469: INFO: namespace kubectl-8454 deletion completed in 6.156954012s

â€¢ [SLOW TEST:8.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:57:00.469: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 01:57:05.110: INFO: Successfully updated pod "pod-update-activedeadlineseconds-fe8542fb-ac23-11e9-906c-f6b46dea7a80"
Jul 22 01:57:05.110: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-fe8542fb-ac23-11e9-906c-f6b46dea7a80" in namespace "pods-3977" to be "terminated due to deadline exceeded"
Jul 22 01:57:05.115: INFO: Pod "pod-update-activedeadlineseconds-fe8542fb-ac23-11e9-906c-f6b46dea7a80": Phase="Running", Reason="", readiness=true. Elapsed: 4.981451ms
Jul 22 01:57:07.121: INFO: Pod "pod-update-activedeadlineseconds-fe8542fb-ac23-11e9-906c-f6b46dea7a80": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010823616s
Jul 22 01:57:07.121: INFO: Pod "pod-update-activedeadlineseconds-fe8542fb-ac23-11e9-906c-f6b46dea7a80" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:57:07.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3977" for this suite.
Jul 22 01:57:13.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:57:13.313: INFO: namespace pods-3977 deletion completed in 6.185125871s

â€¢ [SLOW TEST:12.844 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:57:13.314: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 01:57:21.475: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:21.480: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:23.481: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:23.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:25.481: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:25.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:27.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:27.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:29.481: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:29.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:31.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:31.488: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:33.482: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:33.499: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:35.483: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:35.489: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:37.480: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:37.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:39.481: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:39.487: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:41.480: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:41.486: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 22 01:57:43.480: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 22 01:57:43.486: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:57:43.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7831" for this suite.
Jul 22 01:58:07.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:58:07.665: INFO: namespace container-lifecycle-hook-7831 deletion completed in 24.172426518s

â€¢ [SLOW TEST:54.351 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:58:07.665: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jul 22 01:58:07.743: INFO: Waiting up to 5m0s for pod "client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80" in namespace "containers-3189" to be "success or failure"
Jul 22 01:58:07.748: INFO: Pod "client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.472244ms
Jul 22 01:58:09.755: INFO: Pod "client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012742352s
Jul 22 01:58:11.761: INFO: Pod "client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0186987s
STEP: Saw pod success
Jul 22 01:58:11.761: INFO: Pod "client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:58:11.766: INFO: Trying to get logs from node k8s3 pod client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:58:11.795: INFO: Waiting for pod client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:58:11.799: INFO: Pod client-containers-269007d0-ac24-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:58:11.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3189" for this suite.
Jul 22 01:58:17.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:58:17.988: INFO: namespace containers-3189 deletion completed in 6.182092753s

â€¢ [SLOW TEST:10.323 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:58:17.989: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul 22 01:58:18.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-8705'
Jul 22 01:58:18.414: INFO: stderr: ""
Jul 22 01:58:18.414: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 01:58:18.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:18.611: INFO: stderr: ""
Jul 22 01:58:18.611: INFO: stdout: "update-demo-nautilus-g8g7q update-demo-nautilus-v6sqg "
Jul 22 01:58:18.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-g8g7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:18.811: INFO: stderr: ""
Jul 22 01:58:18.811: INFO: stdout: ""
Jul 22 01:58:18.811: INFO: update-demo-nautilus-g8g7q is created but not running
Jul 22 01:58:23.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:24.019: INFO: stderr: ""
Jul 22 01:58:24.019: INFO: stdout: "update-demo-nautilus-g8g7q update-demo-nautilus-v6sqg "
Jul 22 01:58:24.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-g8g7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:24.212: INFO: stderr: ""
Jul 22 01:58:24.212: INFO: stdout: "true"
Jul 22 01:58:24.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-g8g7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:24.406: INFO: stderr: ""
Jul 22 01:58:24.406: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 01:58:24.406: INFO: validating pod update-demo-nautilus-g8g7q
Jul 22 01:58:24.413: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 01:58:24.413: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 01:58:24.413: INFO: update-demo-nautilus-g8g7q is verified up and running
Jul 22 01:58:24.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:24.612: INFO: stderr: ""
Jul 22 01:58:24.612: INFO: stdout: "true"
Jul 22 01:58:24.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:24.800: INFO: stderr: ""
Jul 22 01:58:24.800: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 01:58:24.800: INFO: validating pod update-demo-nautilus-v6sqg
Jul 22 01:58:24.807: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 01:58:24.807: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 01:58:24.807: INFO: update-demo-nautilus-v6sqg is verified up and running
STEP: scaling down the replication controller
Jul 22 01:58:24.811: INFO: scanned /root for discovery docs: <nil>
Jul 22 01:58:24.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8705'
Jul 22 01:58:26.069: INFO: stderr: ""
Jul 22 01:58:26.069: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 01:58:26.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:26.262: INFO: stderr: ""
Jul 22 01:58:26.262: INFO: stdout: "update-demo-nautilus-g8g7q update-demo-nautilus-v6sqg "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 22 01:58:31.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:31.463: INFO: stderr: ""
Jul 22 01:58:31.463: INFO: stdout: "update-demo-nautilus-v6sqg "
Jul 22 01:58:31.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:31.658: INFO: stderr: ""
Jul 22 01:58:31.658: INFO: stdout: "true"
Jul 22 01:58:31.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:31.844: INFO: stderr: ""
Jul 22 01:58:31.844: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 01:58:31.844: INFO: validating pod update-demo-nautilus-v6sqg
Jul 22 01:58:31.850: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 01:58:31.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 01:58:31.850: INFO: update-demo-nautilus-v6sqg is verified up and running
STEP: scaling up the replication controller
Jul 22 01:58:31.854: INFO: scanned /root for discovery docs: <nil>
Jul 22 01:58:31.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8705'
Jul 22 01:58:33.101: INFO: stderr: ""
Jul 22 01:58:33.101: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 01:58:33.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:33.290: INFO: stderr: ""
Jul 22 01:58:33.290: INFO: stdout: "update-demo-nautilus-5hz84 update-demo-nautilus-v6sqg "
Jul 22 01:58:33.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-5hz84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:33.478: INFO: stderr: ""
Jul 22 01:58:33.478: INFO: stdout: ""
Jul 22 01:58:33.478: INFO: update-demo-nautilus-5hz84 is created but not running
Jul 22 01:58:38.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8705'
Jul 22 01:58:38.680: INFO: stderr: ""
Jul 22 01:58:38.680: INFO: stdout: "update-demo-nautilus-5hz84 update-demo-nautilus-v6sqg "
Jul 22 01:58:38.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-5hz84 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:38.876: INFO: stderr: ""
Jul 22 01:58:38.876: INFO: stdout: "true"
Jul 22 01:58:38.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-5hz84 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:39.091: INFO: stderr: ""
Jul 22 01:58:39.091: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 01:58:39.091: INFO: validating pod update-demo-nautilus-5hz84
Jul 22 01:58:39.101: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 01:58:39.101: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 01:58:39.101: INFO: update-demo-nautilus-5hz84 is verified up and running
Jul 22 01:58:39.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:39.299: INFO: stderr: ""
Jul 22 01:58:39.299: INFO: stdout: "true"
Jul 22 01:58:39.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-v6sqg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8705'
Jul 22 01:58:39.498: INFO: stderr: ""
Jul 22 01:58:39.498: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 01:58:39.498: INFO: validating pod update-demo-nautilus-v6sqg
Jul 22 01:58:39.504: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 01:58:39.504: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 01:58:39.504: INFO: update-demo-nautilus-v6sqg is verified up and running
STEP: using delete to clean up resources
Jul 22 01:58:39.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-8705'
Jul 22 01:58:39.699: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 01:58:39.699: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 01:58:39.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8705'
Jul 22 01:58:39.895: INFO: stderr: "No resources found.\n"
Jul 22 01:58:39.895: INFO: stdout: ""
Jul 22 01:58:39.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -l name=update-demo --namespace=kubectl-8705 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 01:58:40.156: INFO: stderr: ""
Jul 22 01:58:40.156: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:58:40.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8705" for this suite.
Jul 22 01:58:46.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:58:46.348: INFO: namespace kubectl-8705 deletion completed in 6.18357328s

â€¢ [SLOW TEST:28.359 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:58:46.349: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-3d9eb63f-ac24-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 01:58:46.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80" in namespace "configmap-8375" to be "success or failure"
Jul 22 01:58:46.438: INFO: Pod "pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.343935ms
Jul 22 01:58:48.445: INFO: Pod "pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012133692s
Jul 22 01:58:50.454: INFO: Pod "pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02125175s
STEP: Saw pod success
Jul 22 01:58:50.454: INFO: Pod "pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:58:50.459: INFO: Trying to get logs from node k8s2 pod pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 01:58:50.514: INFO: Waiting for pod pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:58:50.525: INFO: Pod pod-configmaps-3da033ca-ac24-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:58:50.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8375" for this suite.
Jul 22 01:58:56.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:58:56.726: INFO: namespace configmap-8375 deletion completed in 6.194426184s

â€¢ [SLOW TEST:10.377 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:58:56.726: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8219
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 01:58:56.808: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 01:59:20.963: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.117.180:8080/dial?request=hostName&protocol=udp&host=10.233.88.214&port=8081&tries=1'] Namespace:pod-network-test-8219 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:59:20.963: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:59:21.230: INFO: Waiting for endpoints: map[]
Jul 22 01:59:21.235: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.117.180:8080/dial?request=hostName&protocol=udp&host=10.233.117.179&port=8081&tries=1'] Namespace:pod-network-test-8219 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:59:21.235: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:59:21.522: INFO: Waiting for endpoints: map[]
Jul 22 01:59:21.527: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.117.180:8080/dial?request=hostName&protocol=udp&host=10.233.91.51&port=8081&tries=1'] Namespace:pod-network-test-8219 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 01:59:21.527: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 01:59:21.917: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:59:21.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8219" for this suite.
Jul 22 01:59:45.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:59:46.123: INFO: namespace pod-network-test-8219 deletion completed in 24.19763496s

â€¢ [SLOW TEST:49.397 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:59:46.123: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jul 22 01:59:46.212: INFO: Waiting up to 5m0s for pod "client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80" in namespace "containers-7664" to be "success or failure"
Jul 22 01:59:46.224: INFO: Pod "client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.91871ms
Jul 22 01:59:48.230: INFO: Pod "client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017424456s
Jul 22 01:59:50.238: INFO: Pod "client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02516135s
STEP: Saw pod success
Jul 22 01:59:50.238: INFO: Pod "client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 01:59:50.242: INFO: Trying to get logs from node k8s1 pod client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 01:59:50.281: INFO: Waiting for pod client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80 to disappear
Jul 22 01:59:50.288: INFO: Pod client-containers-614137ed-ac24-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 01:59:50.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7664" for this suite.
Jul 22 01:59:56.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 01:59:56.508: INFO: namespace containers-7664 deletion completed in 6.207564402s

â€¢ [SLOW TEST:10.385 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 01:59:56.509: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:00:00.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1740" for this suite.
Jul 22 02:00:06.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:00:06.875: INFO: namespace emptydir-wrapper-1740 deletion completed in 6.18010113s

â€¢ [SLOW TEST:10.366 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:00:06.875: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:00:07.020: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"05e01544-ac37-11e9-b4b6-6c92bf130c27", Controller:(*bool)(0x4001ca66da), BlockOwnerDeletion:(*bool)(0x4001ca66db)}}
Jul 22 02:00:07.029: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"05daee92-ac37-11e9-b4b6-6c92bf130c27", Controller:(*bool)(0x4001de882a), BlockOwnerDeletion:(*bool)(0x4001de882b)}}
Jul 22 02:00:07.057: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"05dcffb3-ac37-11e9-b4b6-6c92bf130c27", Controller:(*bool)(0x4001ca690a), BlockOwnerDeletion:(*bool)(0x4001ca690b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:00:12.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5220" for this suite.
Jul 22 02:00:18.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:00:18.291: INFO: namespace gc-5220 deletion completed in 6.195165882s

â€¢ [SLOW TEST:11.416 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:00:18.291: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 22 02:00:18.375: INFO: PodSpec: initContainers in spec.initContainers
Jul 22 02:01:11.967: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-746f2633-ac24-11e9-906c-f6b46dea7a80", GenerateName:"", Namespace:"init-container-2387", SelfLink:"/api/v1/namespaces/init-container-2387/pods/pod-init-746f2633-ac24-11e9-906c-f6b46dea7a80", UID:"0c2504df-ac37-11e9-b4b6-6c92bf130c27", ResourceVersion:"2300110", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63699365603, loc:(*time.Location)(0x81de100)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"375233758"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-l9jsr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0x4002ad6d00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9jsr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9jsr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-l9jsr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0x40025c6e78), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0x4000ed3da0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40025c6f00)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0x40025c6f20)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0x40025c6f28), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0x40025c6f2c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699357618, loc:(*time.Location)(0x81de100)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699357618, loc:(*time.Location)(0x81de100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699357618, loc:(*time.Location)(0x81de100)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699365603, loc:(*time.Location)(0x81de100)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.2.97.92", PodIP:"10.233.88.215", StartTime:(*v1.Time)(0x4002d3e6a0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4002473490)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0x4002473500)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:669f9b20ccd59ba113166e6eb7782e87d701cb57f8b64bece7269b0a4c20d1c4", ContainerID:"docker://f3e1518e70ccc0f6ffc2cea81c20e8eed8eb15f9797a0ed6f1ba294c955be488"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002d3e6e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0x4002d3e6c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:01:11.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2387" for this suite.
Jul 22 02:01:35.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:01:36.137: INFO: namespace init-container-2387 deletion completed in 24.160845202s

â€¢ [SLOW TEST:77.846 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:01:36.138: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4579.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4579.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4579.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4579.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.9.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.9.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.9.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.9.155_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4579.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4579.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4579.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4579.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4579.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4579.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 155.9.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.9.155_udp@PTR;check="$$(dig +tcp +noall +answer +search 155.9.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.9.155_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 02:01:42.283: INFO: Unable to read wheezy_udp@dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.301: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.307: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.313: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.351: INFO: Unable to read jessie_udp@dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.357: INFO: Unable to read jessie_tcp@dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.362: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.367: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local from pod dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80: the server could not find the requested resource (get pods dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80)
Jul 22 02:01:42.399: INFO: Lookups using dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80 failed for: [wheezy_udp@dns-test-service.dns-4579.svc.cluster.local wheezy_tcp@dns-test-service.dns-4579.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local jessie_udp@dns-test-service.dns-4579.svc.cluster.local jessie_tcp@dns-test-service.dns-4579.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-4579.svc.cluster.local]

Jul 22 02:01:47.525: INFO: DNS probes using dns-4579/dns-test-a2d7e8eb-ac24-11e9-906c-f6b46dea7a80 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:01:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4579" for this suite.
Jul 22 02:01:53.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:01:53.857: INFO: namespace dns-4579 deletion completed in 6.201464447s

â€¢ [SLOW TEST:17.719 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:01:53.857: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jul 22 02:01:53.950: INFO: Waiting up to 5m0s for pod "var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80" in namespace "var-expansion-6257" to be "success or failure"
Jul 22 02:01:53.955: INFO: Pod "var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.371039ms
Jul 22 02:01:55.962: INFO: Pod "var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01274443s
Jul 22 02:01:57.968: INFO: Pod "var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018810302s
STEP: Saw pod success
Jul 22 02:01:57.968: INFO: Pod "var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:01:57.973: INFO: Trying to get logs from node k8s2 pod var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 02:01:58.001: INFO: Waiting for pod var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:01:58.005: INFO: Pod var-expansion-ad6479ab-ac24-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:01:58.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6257" for this suite.
Jul 22 02:02:04.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:02:04.180: INFO: namespace var-expansion-6257 deletion completed in 6.167923239s

â€¢ [SLOW TEST:10.323 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:02:04.181: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 22 02:02:08.830: INFO: Successfully updated pod "annotationupdateb38a6dc5-ac24-11e9-906c-f6b46dea7a80"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:02:12.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6793" for this suite.
Jul 22 02:02:36.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:02:37.066: INFO: namespace downward-api-6793 deletion completed in 24.183703872s

â€¢ [SLOW TEST:32.886 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:02:37.067: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 22 02:02:37.169: INFO: Waiting up to 5m0s for pod "downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80" in namespace "downward-api-8708" to be "success or failure"
Jul 22 02:02:37.175: INFO: Pod "downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.764636ms
Jul 22 02:02:39.182: INFO: Pod "downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013024602s
Jul 22 02:02:41.188: INFO: Pod "downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018884727s
STEP: Saw pod success
Jul 22 02:02:41.188: INFO: Pod "downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:02:41.193: INFO: Trying to get logs from node k8s3 pod downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 02:02:41.225: INFO: Waiting for pod downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:02:41.233: INFO: Pod downward-api-c725d929-ac24-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:02:41.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8708" for this suite.
Jul 22 02:02:47.258: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:02:47.414: INFO: namespace downward-api-8708 deletion completed in 6.174510061s

â€¢ [SLOW TEST:10.348 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:02:47.415: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-cd510f2d-ac24-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:02:53.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4541" for this suite.
Jul 22 02:03:17.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:03:17.756: INFO: namespace configmap-4541 deletion completed in 24.176121705s

â€¢ [SLOW TEST:30.341 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:03:17.756: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0722 02:03:48.378191      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 02:03:48.378: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:03:48.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3327" for this suite.
Jul 22 02:03:54.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:03:54.607: INFO: namespace gc-3327 deletion completed in 6.222959979s

â€¢ [SLOW TEST:36.851 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:03:54.608: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 22 02:04:02.791: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 02:04:02.796: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 02:04:04.797: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 02:04:04.802: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 02:04:06.797: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 02:04:06.802: INFO: Pod pod-with-poststart-http-hook still exists
Jul 22 02:04:08.798: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 22 02:04:08.804: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:04:08.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7371" for this suite.
Jul 22 02:04:32.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:04:32.983: INFO: namespace container-lifecycle-hook-7371 deletion completed in 24.171579718s

â€¢ [SLOW TEST:38.375 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:04:32.983: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 02:04:33.064: INFO: Waiting up to 5m0s for pod "pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80" in namespace "emptydir-508" to be "success or failure"
Jul 22 02:04:33.070: INFO: Pod "pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.411556ms
Jul 22 02:04:35.076: INFO: Pod "pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01133408s
Jul 22 02:04:37.082: INFO: Pod "pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01802174s
STEP: Saw pod success
Jul 22 02:04:37.082: INFO: Pod "pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:04:37.087: INFO: Trying to get logs from node k8s1 pod pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:04:37.125: INFO: Waiting for pod pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:04:37.129: INFO: Pod pod-0c3aa0fe-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:04:37.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-508" for this suite.
Jul 22 02:04:43.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:04:43.318: INFO: namespace emptydir-508 deletion completed in 6.182388363s

â€¢ [SLOW TEST:10.335 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:04:43.319: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-126590ed-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:04:43.421: INFO: Waiting up to 5m0s for pod "pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80" in namespace "secrets-2835" to be "success or failure"
Jul 22 02:04:43.427: INFO: Pod "pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.997748ms
Jul 22 02:04:45.437: INFO: Pod "pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016500607s
Jul 22 02:04:47.446: INFO: Pod "pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02500842s
STEP: Saw pod success
Jul 22 02:04:47.446: INFO: Pod "pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:04:47.452: INFO: Trying to get logs from node k8s1 pod pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80 container secret-env-test: <nil>
STEP: delete the pod
Jul 22 02:04:47.494: INFO: Waiting for pod pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:04:47.499: INFO: Pod pod-secrets-12670d93-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:04:47.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2835" for this suite.
Jul 22 02:04:53.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:04:53.691: INFO: namespace secrets-2835 deletion completed in 6.185316355s

â€¢ [SLOW TEST:10.372 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:04:53.692: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 02:04:53.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1525'
Jul 22 02:04:53.969: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 02:04:53.969: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jul 22 02:04:55.987: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1525'
Jul 22 02:04:56.193: INFO: stderr: ""
Jul 22 02:04:56.193: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:04:56.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1525" for this suite.
Jul 22 02:05:02.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:05:02.371: INFO: namespace kubectl-1525 deletion completed in 6.169770752s

â€¢ [SLOW TEST:8.679 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:05:02.371: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-nft2g in namespace proxy-4597
I0722 02:05:02.475852      18 runners.go:184] Created replication controller with name: proxy-service-nft2g, namespace: proxy-4597, replica count: 1
I0722 02:05:03.527191      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 02:05:04.527591      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0722 02:05:05.528172      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:06.529848      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:07.530114      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:08.531379      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:09.531855      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:10.534376      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:11.534634      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0722 02:05:12.536862      18 runners.go:184] proxy-service-nft2g Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 22 02:05:12.541: INFO: setup took 10.106472404s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.25005ms)
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.113485ms)
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 9.53154ms)
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 10.451399ms)
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.264025ms)
Jul 22 02:05:12.552: INFO: (0) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.608201ms)
Jul 22 02:05:12.553: INFO: (0) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.531287ms)
Jul 22 02:05:12.554: INFO: (0) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.566474ms)
Jul 22 02:05:12.554: INFO: (0) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 11.730794ms)
Jul 22 02:05:12.564: INFO: (0) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 22.029594ms)
Jul 22 02:05:12.565: INFO: (0) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 22.977793ms)
Jul 22 02:05:12.619: INFO: (0) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 77.296889ms)
Jul 22 02:05:12.620: INFO: (0) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 77.670618ms)
Jul 22 02:05:12.622: INFO: (0) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 80.166857ms)
Jul 22 02:05:12.626: INFO: (0) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 84.098796ms)
Jul 22 02:05:12.632: INFO: (0) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 89.978786ms)
Jul 22 02:05:12.640: INFO: (1) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 7.978668ms)
Jul 22 02:05:12.641: INFO: (1) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.037254ms)
Jul 22 02:05:12.641: INFO: (1) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.733556ms)
Jul 22 02:05:12.641: INFO: (1) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.227986ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 8.931785ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 9.452478ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 9.042934ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 9.202986ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 9.535461ms)
Jul 22 02:05:12.642: INFO: (1) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 9.868673ms)
Jul 22 02:05:12.643: INFO: (1) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 10.597193ms)
Jul 22 02:05:12.645: INFO: (1) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 11.940376ms)
Jul 22 02:05:12.645: INFO: (1) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 12.290206ms)
Jul 22 02:05:12.645: INFO: (1) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.206638ms)
Jul 22 02:05:12.645: INFO: (1) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.80206ms)
Jul 22 02:05:12.645: INFO: (1) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.903742ms)
Jul 22 02:05:12.651: INFO: (2) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 5.254212ms)
Jul 22 02:05:12.653: INFO: (2) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 7.11252ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 8.075842ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 8.580112ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 8.345788ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 8.625266ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.786098ms)
Jul 22 02:05:12.654: INFO: (2) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.575779ms)
Jul 22 02:05:12.655: INFO: (2) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 9.45406ms)
Jul 22 02:05:12.656: INFO: (2) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 9.692848ms)
Jul 22 02:05:12.656: INFO: (2) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 10.526972ms)
Jul 22 02:05:12.657: INFO: (2) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.821707ms)
Jul 22 02:05:12.658: INFO: (2) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 11.683539ms)
Jul 22 02:05:12.658: INFO: (2) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 11.931449ms)
Jul 22 02:05:12.658: INFO: (2) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.164994ms)
Jul 22 02:05:12.658: INFO: (2) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.424128ms)
Jul 22 02:05:12.665: INFO: (3) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 6.723517ms)
Jul 22 02:05:12.666: INFO: (3) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 7.932994ms)
Jul 22 02:05:12.666: INFO: (3) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.351442ms)
Jul 22 02:05:12.667: INFO: (3) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 8.808176ms)
Jul 22 02:05:12.670: INFO: (3) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 11.761344ms)
Jul 22 02:05:12.670: INFO: (3) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 12.443801ms)
Jul 22 02:05:12.671: INFO: (3) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 12.2597ms)
Jul 22 02:05:12.671: INFO: (3) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 12.375161ms)
Jul 22 02:05:12.671: INFO: (3) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.150135ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 13.790147ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 14.12832ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.75626ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 14.276585ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 13.719383ms)
Jul 22 02:05:12.672: INFO: (3) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 14.214012ms)
Jul 22 02:05:12.673: INFO: (3) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 14.561935ms)
Jul 22 02:05:12.681: INFO: (4) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 7.890679ms)
Jul 22 02:05:12.682: INFO: (4) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 9.311623ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 9.151723ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 9.114217ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.006408ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 9.391399ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 9.787813ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 9.982488ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 9.500383ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 9.921367ms)
Jul 22 02:05:12.683: INFO: (4) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 9.565275ms)
Jul 22 02:05:12.684: INFO: (4) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 10.19324ms)
Jul 22 02:05:12.686: INFO: (4) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.172535ms)
Jul 22 02:05:12.686: INFO: (4) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 12.347753ms)
Jul 22 02:05:12.686: INFO: (4) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.352086ms)
Jul 22 02:05:12.686: INFO: (4) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 12.486095ms)
Jul 22 02:05:12.692: INFO: (5) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 5.485179ms)
Jul 22 02:05:12.693: INFO: (5) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 6.70666ms)
Jul 22 02:05:12.693: INFO: (5) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 6.373838ms)
Jul 22 02:05:12.693: INFO: (5) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 6.74059ms)
Jul 22 02:05:12.693: INFO: (5) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 6.539306ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 11.594143ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.411406ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 11.586906ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 11.499286ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 11.712508ms)
Jul 22 02:05:12.698: INFO: (5) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 11.742689ms)
Jul 22 02:05:12.699: INFO: (5) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.127493ms)
Jul 22 02:05:12.700: INFO: (5) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 14.02471ms)
Jul 22 02:05:12.702: INFO: (5) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 15.251174ms)
Jul 22 02:05:12.702: INFO: (5) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 15.321352ms)
Jul 22 02:05:12.702: INFO: (5) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 15.576911ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.004545ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.206132ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.290957ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 10.022875ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.200867ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.313945ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.211158ms)
Jul 22 02:05:12.712: INFO: (6) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 10.490507ms)
Jul 22 02:05:12.713: INFO: (6) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.313988ms)
Jul 22 02:05:12.713: INFO: (6) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 10.577498ms)
Jul 22 02:05:12.713: INFO: (6) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 10.705744ms)
Jul 22 02:05:12.714: INFO: (6) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 11.411579ms)
Jul 22 02:05:12.714: INFO: (6) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 11.873035ms)
Jul 22 02:05:12.714: INFO: (6) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 11.83761ms)
Jul 22 02:05:12.714: INFO: (6) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.141551ms)
Jul 22 02:05:12.715: INFO: (6) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 12.608576ms)
Jul 22 02:05:12.721: INFO: (7) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 6.518615ms)
Jul 22 02:05:12.723: INFO: (7) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.681491ms)
Jul 22 02:05:12.724: INFO: (7) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.240228ms)
Jul 22 02:05:12.724: INFO: (7) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 8.548415ms)
Jul 22 02:05:12.725: INFO: (7) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.222056ms)
Jul 22 02:05:12.726: INFO: (7) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.181107ms)
Jul 22 02:05:12.726: INFO: (7) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 10.802138ms)
Jul 22 02:05:12.726: INFO: (7) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 10.178311ms)
Jul 22 02:05:12.726: INFO: (7) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.390689ms)
Jul 22 02:05:12.726: INFO: (7) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.360377ms)
Jul 22 02:05:12.727: INFO: (7) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.009536ms)
Jul 22 02:05:12.727: INFO: (7) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.420617ms)
Jul 22 02:05:12.730: INFO: (7) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 14.362277ms)
Jul 22 02:05:12.730: INFO: (7) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 13.924459ms)
Jul 22 02:05:12.730: INFO: (7) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 14.469267ms)
Jul 22 02:05:12.730: INFO: (7) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 14.215312ms)
Jul 22 02:05:12.739: INFO: (8) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 7.843684ms)
Jul 22 02:05:12.739: INFO: (8) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.078182ms)
Jul 22 02:05:12.739: INFO: (8) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.414558ms)
Jul 22 02:05:12.739: INFO: (8) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 7.681119ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 7.617116ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 7.489758ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 7.346585ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 8.116316ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 9.953585ms)
Jul 22 02:05:12.740: INFO: (8) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 9.835783ms)
Jul 22 02:05:12.741: INFO: (8) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 10.618427ms)
Jul 22 02:05:12.742: INFO: (8) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 10.796093ms)
Jul 22 02:05:12.745: INFO: (8) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.886101ms)
Jul 22 02:05:12.745: INFO: (8) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 13.508567ms)
Jul 22 02:05:12.745: INFO: (8) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.966249ms)
Jul 22 02:05:12.745: INFO: (8) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.172322ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.474885ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 9.439651ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 9.01778ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 11.076872ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.517894ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 10.086423ms)
Jul 22 02:05:12.756: INFO: (9) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.713891ms)
Jul 22 02:05:12.757: INFO: (9) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 11.200805ms)
Jul 22 02:05:12.757: INFO: (9) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.790875ms)
Jul 22 02:05:12.757: INFO: (9) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 9.717071ms)
Jul 22 02:05:12.757: INFO: (9) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.035644ms)
Jul 22 02:05:12.757: INFO: (9) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.302462ms)
Jul 22 02:05:12.759: INFO: (9) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 11.464554ms)
Jul 22 02:05:12.760: INFO: (9) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.053675ms)
Jul 22 02:05:12.760: INFO: (9) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 13.729978ms)
Jul 22 02:05:12.760: INFO: (9) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 13.607193ms)
Jul 22 02:05:12.768: INFO: (10) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.112871ms)
Jul 22 02:05:12.768: INFO: (10) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 8.534548ms)
Jul 22 02:05:12.770: INFO: (10) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.293383ms)
Jul 22 02:05:12.771: INFO: (10) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.69855ms)
Jul 22 02:05:12.771: INFO: (10) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.911252ms)
Jul 22 02:05:12.771: INFO: (10) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 11.388504ms)
Jul 22 02:05:12.772: INFO: (10) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 11.744878ms)
Jul 22 02:05:12.772: INFO: (10) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 12.400013ms)
Jul 22 02:05:12.773: INFO: (10) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 12.509148ms)
Jul 22 02:05:12.773: INFO: (10) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 12.691625ms)
Jul 22 02:05:12.773: INFO: (10) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 12.451341ms)
Jul 22 02:05:12.774: INFO: (10) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 14.064122ms)
Jul 22 02:05:12.776: INFO: (10) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 15.591189ms)
Jul 22 02:05:12.776: INFO: (10) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 15.638314ms)
Jul 22 02:05:12.776: INFO: (10) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 16.014405ms)
Jul 22 02:05:12.776: INFO: (10) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 15.983464ms)
Jul 22 02:05:12.784: INFO: (11) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 7.674012ms)
Jul 22 02:05:12.786: INFO: (11) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.959864ms)
Jul 22 02:05:12.786: INFO: (11) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 9.591015ms)
Jul 22 02:05:12.786: INFO: (11) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 10.130428ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.83934ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.891254ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.833621ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.681455ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.880897ms)
Jul 22 02:05:12.787: INFO: (11) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 10.760972ms)
Jul 22 02:05:12.788: INFO: (11) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.538199ms)
Jul 22 02:05:12.788: INFO: (11) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.843548ms)
Jul 22 02:05:12.789: INFO: (11) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 12.812828ms)
Jul 22 02:05:12.790: INFO: (11) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 13.270991ms)
Jul 22 02:05:12.790: INFO: (11) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.460076ms)
Jul 22 02:05:12.791: INFO: (11) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 14.451175ms)
Jul 22 02:05:12.798: INFO: (12) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 7.106627ms)
Jul 22 02:05:12.800: INFO: (12) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 8.775915ms)
Jul 22 02:05:12.800: INFO: (12) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 8.770498ms)
Jul 22 02:05:12.800: INFO: (12) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 9.020141ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.427674ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.805497ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.978418ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.998764ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 11.309248ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 11.137778ms)
Jul 22 02:05:12.802: INFO: (12) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 11.603892ms)
Jul 22 02:05:12.803: INFO: (12) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.163998ms)
Jul 22 02:05:12.804: INFO: (12) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.758076ms)
Jul 22 02:05:12.804: INFO: (12) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 13.113886ms)
Jul 22 02:05:12.804: INFO: (12) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 13.469371ms)
Jul 22 02:05:12.804: INFO: (12) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.3101ms)
Jul 22 02:05:12.810: INFO: (13) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 5.590415ms)
Jul 22 02:05:12.816: INFO: (13) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.469321ms)
Jul 22 02:05:12.816: INFO: (13) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 11.619926ms)
Jul 22 02:05:12.817: INFO: (13) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 12.098261ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.183003ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 13.003863ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 13.287762ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.217735ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 13.146364ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 13.483282ms)
Jul 22 02:05:12.818: INFO: (13) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 13.669637ms)
Jul 22 02:05:12.819: INFO: (13) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 13.722526ms)
Jul 22 02:05:12.819: INFO: (13) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 13.718343ms)
Jul 22 02:05:12.819: INFO: (13) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 13.82035ms)
Jul 22 02:05:12.819: INFO: (13) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 14.451847ms)
Jul 22 02:05:12.819: INFO: (13) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 14.484586ms)
Jul 22 02:05:12.827: INFO: (14) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 7.39228ms)
Jul 22 02:05:12.827: INFO: (14) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 7.143027ms)
Jul 22 02:05:12.827: INFO: (14) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 7.298853ms)
Jul 22 02:05:12.827: INFO: (14) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 7.170522ms)
Jul 22 02:05:12.827: INFO: (14) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 7.517817ms)
Jul 22 02:05:12.828: INFO: (14) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 9.119677ms)
Jul 22 02:05:12.830: INFO: (14) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 11.145187ms)
Jul 22 02:05:12.830: INFO: (14) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.254993ms)
Jul 22 02:05:12.831: INFO: (14) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 11.154547ms)
Jul 22 02:05:12.831: INFO: (14) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 11.4246ms)
Jul 22 02:05:12.831: INFO: (14) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 11.223599ms)
Jul 22 02:05:12.832: INFO: (14) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.435871ms)
Jul 22 02:05:12.832: INFO: (14) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 12.294626ms)
Jul 22 02:05:12.832: INFO: (14) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 12.9266ms)
Jul 22 02:05:12.832: INFO: (14) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.807346ms)
Jul 22 02:05:12.832: INFO: (14) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 12.783448ms)
Jul 22 02:05:12.839: INFO: (15) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 6.361379ms)
Jul 22 02:05:12.839: INFO: (15) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 6.637738ms)
Jul 22 02:05:12.839: INFO: (15) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 7.060606ms)
Jul 22 02:05:12.839: INFO: (15) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 6.633404ms)
Jul 22 02:05:12.840: INFO: (15) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 7.791922ms)
Jul 22 02:05:12.841: INFO: (15) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.525837ms)
Jul 22 02:05:12.842: INFO: (15) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 8.880239ms)
Jul 22 02:05:12.842: INFO: (15) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 9.216485ms)
Jul 22 02:05:12.848: INFO: (15) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 14.839269ms)
Jul 22 02:05:12.848: INFO: (15) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 15.207992ms)
Jul 22 02:05:12.848: INFO: (15) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 15.743744ms)
Jul 22 02:05:12.850: INFO: (15) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 17.07566ms)
Jul 22 02:05:12.850: INFO: (15) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 17.157452ms)
Jul 22 02:05:12.850: INFO: (15) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 17.227673ms)
Jul 22 02:05:12.850: INFO: (15) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 17.954395ms)
Jul 22 02:05:12.850: INFO: (15) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 17.66222ms)
Jul 22 02:05:12.857: INFO: (16) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 7.025009ms)
Jul 22 02:05:12.860: INFO: (16) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 9.358185ms)
Jul 22 02:05:12.860: INFO: (16) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 9.423704ms)
Jul 22 02:05:12.861: INFO: (16) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.671748ms)
Jul 22 02:05:12.861: INFO: (16) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.735362ms)
Jul 22 02:05:12.862: INFO: (16) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 11.120704ms)
Jul 22 02:05:12.861: INFO: (16) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 10.822765ms)
Jul 22 02:05:12.862: INFO: (16) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.513218ms)
Jul 22 02:05:12.862: INFO: (16) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 11.930431ms)
Jul 22 02:05:12.862: INFO: (16) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 11.810095ms)
Jul 22 02:05:12.864: INFO: (16) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.006853ms)
Jul 22 02:05:12.864: INFO: (16) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 13.107971ms)
Jul 22 02:05:12.865: INFO: (16) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 14.047027ms)
Jul 22 02:05:12.865: INFO: (16) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 14.501767ms)
Jul 22 02:05:12.865: INFO: (16) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 14.679586ms)
Jul 22 02:05:12.865: INFO: (16) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 14.467512ms)
Jul 22 02:05:12.871: INFO: (17) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 5.93225ms)
Jul 22 02:05:12.874: INFO: (17) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 8.590361ms)
Jul 22 02:05:12.877: INFO: (17) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 10.947695ms)
Jul 22 02:05:12.877: INFO: (17) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.412576ms)
Jul 22 02:05:12.877: INFO: (17) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.546173ms)
Jul 22 02:05:12.877: INFO: (17) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.480912ms)
Jul 22 02:05:12.877: INFO: (17) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 11.998681ms)
Jul 22 02:05:12.878: INFO: (17) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 11.862527ms)
Jul 22 02:05:12.878: INFO: (17) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.076573ms)
Jul 22 02:05:12.878: INFO: (17) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 12.121704ms)
Jul 22 02:05:12.878: INFO: (17) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 12.379602ms)
Jul 22 02:05:12.878: INFO: (17) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 12.836705ms)
Jul 22 02:05:12.879: INFO: (17) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 13.694185ms)
Jul 22 02:05:12.879: INFO: (17) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 13.526485ms)
Jul 22 02:05:12.879: INFO: (17) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.853976ms)
Jul 22 02:05:12.879: INFO: (17) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 13.582775ms)
Jul 22 02:05:12.889: INFO: (18) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 9.786729ms)
Jul 22 02:05:12.890: INFO: (18) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.433589ms)
Jul 22 02:05:12.890: INFO: (18) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 10.576133ms)
Jul 22 02:05:12.890: INFO: (18) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 10.579449ms)
Jul 22 02:05:12.890: INFO: (18) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 10.310348ms)
Jul 22 02:05:12.890: INFO: (18) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 10.453803ms)
Jul 22 02:05:12.891: INFO: (18) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.409477ms)
Jul 22 02:05:12.891: INFO: (18) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.359189ms)
Jul 22 02:05:12.893: INFO: (18) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 13.007286ms)
Jul 22 02:05:12.893: INFO: (18) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.036319ms)
Jul 22 02:05:12.893: INFO: (18) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 13.031488ms)
Jul 22 02:05:12.893: INFO: (18) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 13.42437ms)
Jul 22 02:05:12.893: INFO: (18) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 13.796994ms)
Jul 22 02:05:12.895: INFO: (18) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 15.267468ms)
Jul 22 02:05:12.895: INFO: (18) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 15.418766ms)
Jul 22 02:05:12.895: INFO: (18) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 15.700692ms)
Jul 22 02:05:12.902: INFO: (19) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:460/proxy/: tls baz (200; 6.935829ms)
Jul 22 02:05:12.905: INFO: (19) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:462/proxy/: tls qux (200; 9.174473ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 11.86153ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname2/proxy/: bar (200; 11.552477ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c/proxy/rewriteme">test</a> (200; 11.593991ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname1/proxy/: foo (200; 12.034453ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">test<... (200; 11.787496ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:1080/proxy/rewriteme">... (200; 11.822726ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:160/proxy/: foo (200; 12.095683ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/: <a href="/api/v1/namespaces/proxy-4597/pods/https:proxy-service-nft2g-j6r9c:443/proxy/tlsrewritem... (200; 11.694958ms)
Jul 22 02:05:12.907: INFO: (19) /api/v1/namespaces/proxy-4597/pods/proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 11.958641ms)
Jul 22 02:05:12.908: INFO: (19) /api/v1/namespaces/proxy-4597/pods/http:proxy-service-nft2g-j6r9c:162/proxy/: bar (200; 12.498791ms)
Jul 22 02:05:12.908: INFO: (19) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname1/proxy/: tls baz (200; 12.493288ms)
Jul 22 02:05:12.908: INFO: (19) /api/v1/namespaces/proxy-4597/services/https:proxy-service-nft2g:tlsportname2/proxy/: tls qux (200; 12.769538ms)
Jul 22 02:05:12.909: INFO: (19) /api/v1/namespaces/proxy-4597/services/http:proxy-service-nft2g:portname1/proxy/: foo (200; 13.250062ms)
Jul 22 02:05:12.909: INFO: (19) /api/v1/namespaces/proxy-4597/services/proxy-service-nft2g:portname2/proxy/: bar (200; 13.413797ms)
STEP: deleting ReplicationController proxy-service-nft2g in namespace proxy-4597, will wait for the garbage collector to delete the pods
Jul 22 02:05:12.975: INFO: Deleting ReplicationController proxy-service-nft2g took: 9.660023ms
Jul 22 02:05:13.076: INFO: Terminating ReplicationController proxy-service-nft2g pods took: 100.434084ms
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:05:16.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4597" for this suite.
Jul 22 02:05:22.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:05:22.883: INFO: namespace proxy-4597 deletion completed in 6.19789384s

â€¢ [SLOW TEST:20.512 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:05:22.883: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0722 02:05:24.032684      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 22 02:05:24.032: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:05:24.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7683" for this suite.
Jul 22 02:05:30.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:05:30.209: INFO: namespace gc-7683 deletion completed in 6.170323859s

â€¢ [SLOW TEST:7.326 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:05:30.209: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-2e57ce47-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:05:30.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80" in namespace "configmap-1482" to be "success or failure"
Jul 22 02:05:30.302: INFO: Pod "pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.802311ms
Jul 22 02:05:32.308: INFO: Pod "pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010603523s
Jul 22 02:05:34.315: INFO: Pod "pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017367407s
STEP: Saw pod success
Jul 22 02:05:34.315: INFO: Pod "pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:05:34.320: INFO: Trying to get logs from node k8s1 pod pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 02:05:34.356: INFO: Waiting for pod pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:05:34.364: INFO: Pod pod-configmaps-2e593f54-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:05:34.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1482" for this suite.
Jul 22 02:05:40.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:05:40.571: INFO: namespace configmap-1482 deletion completed in 6.191611977s

â€¢ [SLOW TEST:10.362 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:05:40.571: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-2532/configmap-test-348a1ca5-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:05:40.695: INFO: Waiting up to 5m0s for pod "pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80" in namespace "configmap-2532" to be "success or failure"
Jul 22 02:05:40.700: INFO: Pod "pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.792279ms
Jul 22 02:05:42.706: INFO: Pod "pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01122124s
Jul 22 02:05:44.712: INFO: Pod "pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017288042s
STEP: Saw pod success
Jul 22 02:05:44.712: INFO: Pod "pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:05:44.717: INFO: Trying to get logs from node k8s2 pod pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80 container env-test: <nil>
STEP: delete the pod
Jul 22 02:05:44.744: INFO: Waiting for pod pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:05:44.749: INFO: Pod pod-configmaps-348b91e2-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:05:44.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2532" for this suite.
Jul 22 02:05:50.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:05:50.936: INFO: namespace configmap-2532 deletion completed in 6.180574039s

â€¢ [SLOW TEST:10.365 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:05:50.937: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:05:51.030: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80" in namespace "projected-1676" to be "success or failure"
Jul 22 02:05:51.038: INFO: Pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.603101ms
Jul 22 02:05:53.045: INFO: Pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015775098s
Jul 22 02:05:55.054: INFO: Pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023985948s
Jul 22 02:05:57.061: INFO: Pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031218569s
STEP: Saw pod success
Jul 22 02:05:57.061: INFO: Pod "downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:05:57.066: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:05:57.105: INFO: Waiting for pod downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:05:57.109: INFO: Pod downwardapi-volume-3ab2d09c-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:05:57.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1676" for this suite.
Jul 22 02:06:03.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:06:03.312: INFO: namespace projected-1676 deletion completed in 6.196648764s

â€¢ [SLOW TEST:12.375 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:06:03.313: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:06:07.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-825" for this suite.
Jul 22 02:06:49.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:06:49.609: INFO: namespace kubelet-test-825 deletion completed in 42.170283126s

â€¢ [SLOW TEST:46.296 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:06:49.609: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-5dab5134-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:06:49.708: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80" in namespace "projected-2259" to be "success or failure"
Jul 22 02:06:49.713: INFO: Pod "pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.63273ms
Jul 22 02:06:51.721: INFO: Pod "pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012762954s
Jul 22 02:06:53.727: INFO: Pod "pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01968806s
STEP: Saw pod success
Jul 22 02:06:53.728: INFO: Pod "pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:06:53.733: INFO: Trying to get logs from node k8s2 pod pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 02:06:53.795: INFO: Waiting for pod pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:06:53.800: INFO: Pod pod-projected-configmaps-5dacf1c9-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:06:53.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2259" for this suite.
Jul 22 02:06:59.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:06:59.980: INFO: namespace projected-2259 deletion completed in 6.173076849s

â€¢ [SLOW TEST:10.371 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:06:59.981: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:07:00.094: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 22 02:07:00.110: INFO: Number of nodes with available pods: 0
Jul 22 02:07:00.110: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 22 02:07:00.158: INFO: Number of nodes with available pods: 0
Jul 22 02:07:00.158: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:01.163: INFO: Number of nodes with available pods: 0
Jul 22 02:07:01.164: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:02.165: INFO: Number of nodes with available pods: 0
Jul 22 02:07:02.165: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:03.163: INFO: Number of nodes with available pods: 1
Jul 22 02:07:03.163: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 22 02:07:03.204: INFO: Number of nodes with available pods: 1
Jul 22 02:07:03.204: INFO: Number of running nodes: 0, number of available pods: 1
Jul 22 02:07:04.211: INFO: Number of nodes with available pods: 0
Jul 22 02:07:04.211: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 22 02:07:04.236: INFO: Number of nodes with available pods: 0
Jul 22 02:07:04.236: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:05.243: INFO: Number of nodes with available pods: 0
Jul 22 02:07:05.244: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:06.242: INFO: Number of nodes with available pods: 0
Jul 22 02:07:06.242: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:07.241: INFO: Number of nodes with available pods: 0
Jul 22 02:07:07.241: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:08.252: INFO: Number of nodes with available pods: 0
Jul 22 02:07:08.252: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:09.242: INFO: Number of nodes with available pods: 0
Jul 22 02:07:09.242: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:10.242: INFO: Number of nodes with available pods: 0
Jul 22 02:07:10.242: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:11.244: INFO: Number of nodes with available pods: 0
Jul 22 02:07:11.244: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:12.242: INFO: Number of nodes with available pods: 0
Jul 22 02:07:12.242: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:07:13.241: INFO: Number of nodes with available pods: 1
Jul 22 02:07:13.241: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2406, will wait for the garbage collector to delete the pods
Jul 22 02:07:13.314: INFO: Deleting DaemonSet.extensions daemon-set took: 9.664443ms
Jul 22 02:07:13.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.340332ms
Jul 22 02:07:19.120: INFO: Number of nodes with available pods: 0
Jul 22 02:07:19.121: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 02:07:19.125: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2406/daemonsets","resourceVersion":"2301793"},"items":null}

Jul 22 02:07:19.128: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2406/pods","resourceVersion":"2301793"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:07:19.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2406" for this suite.
Jul 22 02:07:25.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:07:25.346: INFO: namespace daemonsets-2406 deletion completed in 6.161127525s

â€¢ [SLOW TEST:25.365 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:07:25.346: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 22 02:07:25.436: INFO: Waiting up to 5m0s for pod "pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80" in namespace "emptydir-4539" to be "success or failure"
Jul 22 02:07:25.443: INFO: Pod "pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.229152ms
Jul 22 02:07:27.463: INFO: Pod "pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027386942s
Jul 22 02:07:29.469: INFO: Pod "pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033426618s
STEP: Saw pod success
Jul 22 02:07:29.469: INFO: Pod "pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:07:29.474: INFO: Trying to get logs from node k8s2 pod pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:07:29.513: INFO: Waiting for pod pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:07:29.517: INFO: Pod pod-72f7efcf-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:07:29.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4539" for this suite.
Jul 22 02:07:35.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:07:35.683: INFO: namespace emptydir-4539 deletion completed in 6.159048997s

â€¢ [SLOW TEST:10.337 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:07:35.684: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:07:35.764: INFO: (0) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.836639ms)
Jul 22 02:07:35.770: INFO: (1) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.470276ms)
Jul 22 02:07:35.777: INFO: (2) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.441676ms)
Jul 22 02:07:35.783: INFO: (3) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.344263ms)
Jul 22 02:07:35.791: INFO: (4) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.64097ms)
Jul 22 02:07:35.798: INFO: (5) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.41728ms)
Jul 22 02:07:35.805: INFO: (6) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.128098ms)
Jul 22 02:07:35.812: INFO: (7) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.299763ms)
Jul 22 02:07:35.819: INFO: (8) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.468933ms)
Jul 22 02:07:35.825: INFO: (9) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.670931ms)
Jul 22 02:07:35.832: INFO: (10) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.814127ms)
Jul 22 02:07:35.839: INFO: (11) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.643285ms)
Jul 22 02:07:35.845: INFO: (12) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.435047ms)
Jul 22 02:07:35.852: INFO: (13) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.00332ms)
Jul 22 02:07:35.866: INFO: (14) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 13.629749ms)
Jul 22 02:07:35.873: INFO: (15) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.755583ms)
Jul 22 02:07:35.880: INFO: (16) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 7.093713ms)
Jul 22 02:07:35.887: INFO: (17) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.972727ms)
Jul 22 02:07:35.894: INFO: (18) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.92155ms)
Jul 22 02:07:35.901: INFO: (19) /api/v1/nodes/k8s1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apache2/">apache2/</a>
<a href="ap... (200; 6.940075ms)
[AfterEach] version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:07:35.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8071" for this suite.
Jul 22 02:07:41.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:07:42.075: INFO: namespace proxy-8071 deletion completed in 6.166934129s

â€¢ [SLOW TEST:6.391 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:07:42.075: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 22 02:07:46.732: INFO: Successfully updated pod "annotationupdate7cf0454e-ac25-11e9-906c-f6b46dea7a80"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:07:48.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8999" for this suite.
Jul 22 02:08:12.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:08:12.939: INFO: namespace projected-8999 deletion completed in 24.169418452s

â€¢ [SLOW TEST:30.864 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:08:12.939: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jul 22 02:08:13.035: INFO: Waiting up to 5m0s for pod "client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80" in namespace "containers-3583" to be "success or failure"
Jul 22 02:08:13.044: INFO: Pod "client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 8.570839ms
Jul 22 02:08:15.051: INFO: Pod "client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01547131s
Jul 22 02:08:17.058: INFO: Pod "client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022080148s
STEP: Saw pod success
Jul 22 02:08:17.058: INFO: Pod "client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:08:17.062: INFO: Trying to get logs from node k8s2 pod client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:08:17.106: INFO: Waiting for pod client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:08:17.111: INFO: Pod client-containers-8f57bfc0-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:08:17.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3583" for this suite.
Jul 22 02:08:23.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:08:23.298: INFO: namespace containers-3583 deletion completed in 6.178934002s

â€¢ [SLOW TEST:10.359 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:08:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-9582ff71-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:08:23.405: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80" in namespace "projected-1061" to be "success or failure"
Jul 22 02:08:23.411: INFO: Pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.73218ms
Jul 22 02:08:25.417: INFO: Pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011902895s
Jul 22 02:08:27.423: INFO: Pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017663894s
Jul 22 02:08:29.429: INFO: Pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.024079443s
STEP: Saw pod success
Jul 22 02:08:29.429: INFO: Pod "pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:08:29.434: INFO: Trying to get logs from node k8s3 pod pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:08:29.474: INFO: Waiting for pod pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:08:29.478: INFO: Pod pod-projected-secrets-9585c591-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:08:29.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1061" for this suite.
Jul 22 02:08:35.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:08:35.652: INFO: namespace projected-1061 deletion completed in 6.167035572s

â€¢ [SLOW TEST:12.354 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:08:35.653: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-9cdf5256-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:08:35.746: INFO: Waiting up to 5m0s for pod "pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80" in namespace "secrets-1541" to be "success or failure"
Jul 22 02:08:35.751: INFO: Pod "pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.589007ms
Jul 22 02:08:37.757: INFO: Pod "pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01131809s
Jul 22 02:08:39.765: INFO: Pod "pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01960876s
STEP: Saw pod success
Jul 22 02:08:39.765: INFO: Pod "pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:08:39.770: INFO: Trying to get logs from node k8s1 pod pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:08:39.812: INFO: Waiting for pod pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:08:39.817: INFO: Pod pod-secrets-9ce0f79f-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:08:39.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1541" for this suite.
Jul 22 02:08:45.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:08:46.021: INFO: namespace secrets-1541 deletion completed in 6.188938569s

â€¢ [SLOW TEST:10.369 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:08:46.022: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-a30f7482-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:08:46.127: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80" in namespace "configmap-7596" to be "success or failure"
Jul 22 02:08:46.132: INFO: Pod "pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.884453ms
Jul 22 02:08:48.139: INFO: Pod "pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011980613s
Jul 22 02:08:50.145: INFO: Pod "pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018319332s
STEP: Saw pod success
Jul 22 02:08:50.145: INFO: Pod "pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:08:50.154: INFO: Trying to get logs from node k8s1 pod pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 02:08:50.191: INFO: Waiting for pod pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:08:50.196: INFO: Pod pod-configmaps-a3123ccd-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:08:50.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7596" for this suite.
Jul 22 02:08:56.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:08:56.379: INFO: namespace configmap-7596 deletion completed in 6.175806506s

â€¢ [SLOW TEST:10.358 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:08:56.380: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 22 02:08:56.477: INFO: Waiting up to 5m0s for pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80" in namespace "downward-api-5098" to be "success or failure"
Jul 22 02:08:56.482: INFO: Pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.187588ms
Jul 22 02:08:58.492: INFO: Pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015232198s
Jul 22 02:09:00.498: INFO: Pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.021309207s
Jul 22 02:09:02.506: INFO: Pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029254728s
STEP: Saw pod success
Jul 22 02:09:02.506: INFO: Pod "downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:09:02.511: INFO: Trying to get logs from node k8s3 pod downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 02:09:02.558: INFO: Waiting for pod downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:09:02.563: INFO: Pod downward-api-a93bc60a-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:09:02.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5098" for this suite.
Jul 22 02:09:08.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:09:08.774: INFO: namespace downward-api-5098 deletion completed in 6.198687927s

â€¢ [SLOW TEST:12.394 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:09:08.775: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-b0a03387-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:09:08.884: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80" in namespace "projected-7309" to be "success or failure"
Jul 22 02:09:08.892: INFO: Pod "pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.51344ms
Jul 22 02:09:10.899: INFO: Pod "pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015134958s
Jul 22 02:09:12.906: INFO: Pod "pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02212604s
STEP: Saw pod success
Jul 22 02:09:12.906: INFO: Pod "pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:09:12.911: INFO: Trying to get logs from node k8s1 pod pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 02:09:12.977: INFO: Waiting for pod pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:09:12.982: INFO: Pod pod-projected-configmaps-b0a1e187-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:09:12.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7309" for this suite.
Jul 22 02:09:19.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:09:19.172: INFO: namespace projected-7309 deletion completed in 6.183071042s

â€¢ [SLOW TEST:10.397 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:09:19.173: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:09:23.327: INFO: Waiting up to 5m0s for pod "client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80" in namespace "pods-5198" to be "success or failure"
Jul 22 02:09:23.332: INFO: Pod "client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.425769ms
Jul 22 02:09:25.339: INFO: Pod "client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012253333s
Jul 22 02:09:27.345: INFO: Pod "client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01841833s
STEP: Saw pod success
Jul 22 02:09:27.345: INFO: Pod "client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:09:27.350: INFO: Trying to get logs from node k8s3 pod client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80 container env3cont: <nil>
STEP: delete the pod
Jul 22 02:09:27.406: INFO: Waiting for pod client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:09:27.411: INFO: Pod client-envvars-b93de96d-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:09:27.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5198" for this suite.
Jul 22 02:10:09.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:10:09.586: INFO: namespace pods-5198 deletion completed in 42.162496674s

â€¢ [SLOW TEST:50.414 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:10:09.587: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:10:09.682: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80" in namespace "downward-api-2969" to be "success or failure"
Jul 22 02:10:09.687: INFO: Pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.648069ms
Jul 22 02:10:11.694: INFO: Pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012309796s
Jul 22 02:10:13.701: INFO: Pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019079055s
Jul 22 02:10:15.708: INFO: Pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026158624s
STEP: Saw pod success
Jul 22 02:10:15.708: INFO: Pod "downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:10:15.713: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:10:15.751: INFO: Waiting for pod downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:10:15.756: INFO: Pod downwardapi-volume-d4de5140-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:10:15.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2969" for this suite.
Jul 22 02:10:21.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:10:21.930: INFO: namespace downward-api-2969 deletion completed in 6.166890757s

â€¢ [SLOW TEST:12.343 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:10:21.930: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6102/configmap-test-dc3ac4eb-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:10:22.036: INFO: Waiting up to 5m0s for pod "pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80" in namespace "configmap-6102" to be "success or failure"
Jul 22 02:10:22.041: INFO: Pod "pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.285868ms
Jul 22 02:10:24.050: INFO: Pod "pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013843494s
Jul 22 02:10:26.056: INFO: Pod "pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02028407s
STEP: Saw pod success
Jul 22 02:10:26.056: INFO: Pod "pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:10:26.061: INFO: Trying to get logs from node k8s1 pod pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80 container env-test: <nil>
STEP: delete the pod
Jul 22 02:10:26.097: INFO: Waiting for pod pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:10:26.105: INFO: Pod pod-configmaps-dc3ce268-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:10:26.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6102" for this suite.
Jul 22 02:10:32.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:10:32.299: INFO: namespace configmap-6102 deletion completed in 6.185394728s

â€¢ [SLOW TEST:10.370 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:10:32.300: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-e2674de1-ac25-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:10:32.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80" in namespace "projected-4677" to be "success or failure"
Jul 22 02:10:32.402: INFO: Pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.119251ms
Jul 22 02:10:34.409: INFO: Pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012050924s
Jul 22 02:10:36.416: INFO: Pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018894261s
Jul 22 02:10:38.422: INFO: Pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025129956s
STEP: Saw pod success
Jul 22 02:10:38.423: INFO: Pod "pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:10:38.427: INFO: Trying to get logs from node k8s3 pod pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:10:38.468: INFO: Waiting for pod pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:10:38.473: INFO: Pod pod-projected-secrets-e26913a3-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:10:38.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4677" for this suite.
Jul 22 02:10:44.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:10:44.647: INFO: namespace projected-4677 deletion completed in 6.167547973s

â€¢ [SLOW TEST:12.348 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:10:44.648: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 22 02:10:49.302: INFO: Successfully updated pod "labelsupdatee9c23dd3-ac25-11e9-906c-f6b46dea7a80"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:10:53.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7898" for this suite.
Jul 22 02:11:17.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:11:17.527: INFO: namespace projected-7898 deletion completed in 24.172181102s

â€¢ [SLOW TEST:32.880 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:11:17.527: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jul 22 02:11:17.624: INFO: Waiting up to 5m0s for pod "client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80" in namespace "containers-3839" to be "success or failure"
Jul 22 02:11:17.636: INFO: Pod "client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 11.358322ms
Jul 22 02:11:19.643: INFO: Pod "client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018034263s
Jul 22 02:11:21.649: INFO: Pod "client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024018603s
STEP: Saw pod success
Jul 22 02:11:21.649: INFO: Pod "client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:11:21.653: INFO: Trying to get logs from node k8s2 pod client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:11:21.689: INFO: Waiting for pod client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:11:21.693: INFO: Pod client-containers-fd5e3ba2-ac25-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:11:21.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3839" for this suite.
Jul 22 02:11:27.717: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:11:27.880: INFO: namespace containers-3839 deletion completed in 6.181022197s

â€¢ [SLOW TEST:10.353 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:11:27.881: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jul 22 02:11:27.948: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-346668565 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:11:28.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2674" for this suite.
Jul 22 02:11:34.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:11:34.286: INFO: namespace kubectl-2674 deletion completed in 6.164339657s

â€¢ [SLOW TEST:6.405 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:11:34.286: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 22 02:11:34.391: INFO: Waiting up to 5m0s for pod "pod-075ae5df-ac26-11e9-906c-f6b46dea7a80" in namespace "emptydir-3242" to be "success or failure"
Jul 22 02:11:34.396: INFO: Pod "pod-075ae5df-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.284935ms
Jul 22 02:11:36.402: INFO: Pod "pod-075ae5df-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011716086s
Jul 22 02:11:38.409: INFO: Pod "pod-075ae5df-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018278385s
STEP: Saw pod success
Jul 22 02:11:38.409: INFO: Pod "pod-075ae5df-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:11:38.414: INFO: Trying to get logs from node k8s3 pod pod-075ae5df-ac26-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:11:38.453: INFO: Waiting for pod pod-075ae5df-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:11:38.458: INFO: Pod pod-075ae5df-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:11:38.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3242" for this suite.
Jul 22 02:11:44.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:11:44.649: INFO: namespace emptydir-3242 deletion completed in 6.184162869s

â€¢ [SLOW TEST:10.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:11:44.649: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:11:44.716: INFO: Creating deployment "nginx-deployment"
Jul 22 02:11:44.734: INFO: Waiting for observed generation 1
Jul 22 02:11:46.751: INFO: Waiting for all required pods to come up
Jul 22 02:11:46.758: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 22 02:11:52.783: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 22 02:11:52.792: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 22 02:11:52.811: INFO: Updating deployment nginx-deployment
Jul 22 02:11:52.811: INFO: Waiting for observed generation 2
Jul 22 02:11:54.826: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 22 02:11:54.831: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 22 02:11:54.835: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 22 02:11:54.850: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 22 02:11:54.850: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 22 02:11:54.854: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 22 02:11:54.864: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 22 02:11:54.864: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 22 02:11:54.882: INFO: Updating deployment nginx-deployment
Jul 22 02:11:54.882: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 22 02:11:54.908: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 22 02:11:54.973: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 22 02:11:55.146: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-5396,SelfLink:/apis/apps/v1/namespaces/deployment-5396/deployments/nginx-deployment,UID:85c3ab4e-ac38-11e9-b4b6-6c92bf130c27,ResourceVersion:2303338,Generation:3,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-07-22 04:24:05 +0000 UTC 2019-07-22 04:23:57 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-07-22 04:24:06 +0000 UTC 2019-07-22 04:24:06 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 22 02:11:55.189: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-5396,SelfLink:/apis/apps/v1/namespaces/deployment-5396/replicasets/nginx-deployment-5f9595f595,UID:8a379b36-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303365,Generation:3,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 85c3ab4e-ac38-11e9-b4b6-6c92bf130c27 0x4002bc9517 0x4002bc9518}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 02:11:55.189: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 22 02:11:55.189: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-5396,SelfLink:/apis/apps/v1/namespaces/deployment-5396/replicasets/nginx-deployment-6f478d8d8,UID:85c685b1-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303351,Generation:3,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 85c3ab4e-ac38-11e9-b4b6-6c92bf130c27 0x4002bc95e7 0x4002bc95e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 22 02:11:55.202: INFO: Pod "nginx-deployment-5f9595f595-2mlww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2mlww,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-2mlww,UID:8b66259c-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303343,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956337 0x4002956338}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029563b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029563d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.202: INFO: Pod "nginx-deployment-5f9595f595-7whhk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7whhk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-7whhk,UID:8b6618d1-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303344,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956457 0x4002956458}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029564d0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029564f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.203: INFO: Pod "nginx-deployment-5f9595f595-9blmd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9blmd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-9blmd,UID:8b6b42f2-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303350,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956577 0x4002956578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029565f0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.203: INFO: Pod "nginx-deployment-5f9595f595-d8fz8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-d8fz8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-d8fz8,UID:8b6b667b-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303355,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956697 0x4002956698}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956710} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.203: INFO: Pod "nginx-deployment-5f9595f595-dp9bk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dp9bk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-dp9bk,UID:8b6b4981-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303354,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x40029567b7 0x40029567b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956830} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.204: INFO: Pod "nginx-deployment-5f9595f595-hl88j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hl88j,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-hl88j,UID:8a4cba14-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303277,Generation:0,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x40029568d7 0x40029568d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956960} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:05 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:,StartTime:2019-07-22 04:24:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.204: INFO: Pod "nginx-deployment-5f9595f595-k9jff" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k9jff,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-k9jff,UID:8a495d03-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303281,Generation:0,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956a50 0x4002956a51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:53 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:53 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:,StartTime:2019-07-22 02:11:53 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.204: INFO: Pod "nginx-deployment-5f9595f595-lrmzs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lrmzs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-lrmzs,UID:8a38c852-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303273,Generation:0,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956bc0 0x4002956bc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956c40} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002956c60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:,StartTime:2019-07-22 02:11:52 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.205: INFO: Pod "nginx-deployment-5f9595f595-q4qrz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-q4qrz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-q4qrz,UID:8a39dadf-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303250,Generation:0,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002956d30 0x4002956d31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002956ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:,StartTime:2019-07-22 04:24:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.205: INFO: Pod "nginx-deployment-5f9595f595-qxd59" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-qxd59,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-qxd59,UID:8a39d762-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303256,Generation:0,CreationTimestamp:2019-07-22 04:24:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x40029570e0 0x40029570e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957160} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:04 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:,StartTime:2019-07-22 04:24:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.205: INFO: Pod "nginx-deployment-5f9595f595-wjt6z" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wjt6z,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-wjt6z,UID:8b6b4c03-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303356,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002957250 0x4002957251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029572e0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.206: INFO: Pod "nginx-deployment-5f9595f595-xlkcx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xlkcx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-xlkcx,UID:8b74c49b-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303363,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x4002957397 0x4002957398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957410} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.206: INFO: Pod "nginx-deployment-5f9595f595-z845l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-z845l,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-5f9595f595-z845l,UID:8b618b85-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303357,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 8a379b36-ac38-11e9-9d2b-6c92bf900680 0x40029574b7 0x40029574b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957530} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:,StartTime:2019-07-22 04:24:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.206: INFO: Pod "nginx-deployment-6f478d8d8-2qp6x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2qp6x,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-2qp6x,UID:8b5e7f1a-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303320,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957620 0x4002957621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029576a0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029576c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.207: INFO: Pod "nginx-deployment-6f478d8d8-4ft4g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4ft4g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-4ft4g,UID:85d33092-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303177,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957747 0x4002957748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029577d0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029577f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:10.233.91.72,StartTime:2019-07-22 04:23:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 04:24:01 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://2f8e1fc7d613ee8f6e9df738568d9cea26d055f89d11e4d7cde5d4a9129f0186}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.207: INFO: Pod "nginx-deployment-6f478d8d8-7jwh2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7jwh2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-7jwh2,UID:8b666265-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303349,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029578d7 0x40029578d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957960} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.207: INFO: Pod "nginx-deployment-6f478d8d8-8t2bz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8t2bz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-8t2bz,UID:8b620d74-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303332,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957a27 0x4002957a28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957ac0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957ae0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.208: INFO: Pod "nginx-deployment-6f478d8d8-9t9ks" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9t9ks,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-9t9ks,UID:8b5e5794-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303366,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957b67 0x4002957b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957be0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:,StartTime:2019-07-22 04:24:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.208: INFO: Pod "nginx-deployment-6f478d8d8-bphsg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bphsg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-bphsg,UID:8b5d205a-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303340,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957cd7 0x4002957cd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957d80} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957da0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:,StartTime:2019-07-22 04:24:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.208: INFO: Pod "nginx-deployment-6f478d8d8-dx2hf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dx2hf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-dx2hf,UID:8b6633f6-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303346,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957e67 0x4002957e68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4002957ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0x4002957f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.209: INFO: Pod "nginx-deployment-6f478d8d8-h2kpk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h2kpk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-h2kpk,UID:8b662ee0-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303345,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x4002957f87 0x4002957f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8000} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.209: INFO: Pod "nginx-deployment-6f478d8d8-jm8dl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jm8dl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-jm8dl,UID:85cac560-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303225,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f80a7 0x40029f80a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8120} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.228,StartTime:2019-07-22 02:11:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 02:11:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://dd7fa86ac973e72e9ac6e6e8c175e581b21174f1cd4216975d90e3e0d6eba679}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.209: INFO: Pod "nginx-deployment-6f478d8d8-kwzfr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kwzfr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-kwzfr,UID:85cea391-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303171,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8217 0x40029f8218}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8290} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f82b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:10.233.117.191,StartTime:2019-07-22 04:23:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 04:24:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://1199d37ecf83f8013347a1d14661a1f73f7e887793aaf873c2660ffdc4ff2968}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.210: INFO: Pod "nginx-deployment-6f478d8d8-mk5f9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-mk5f9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-mk5f9,UID:8b6202b3-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303333,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8387 0x40029f8388}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8400} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.210: INFO: Pod "nginx-deployment-6f478d8d8-n6lwc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-n6lwc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-n6lwc,UID:85d32491-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303167,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f84a7 0x40029f84a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8520} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:10.233.117.183,StartTime:2019-07-22 04:23:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 04:24:00 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://96f22f0f708b14a3cb13156910ecaf02c6adb1e29e4de5801e1aae0f893826bc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.210: INFO: Pod "nginx-deployment-6f478d8d8-nr998" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nr998,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-nr998,UID:8b61fc2a-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303331,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8617 0x40029f8618}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8690} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f86b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.211: INFO: Pod "nginx-deployment-6f478d8d8-pf2ms" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pf2ms,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-pf2ms,UID:85ce95e1-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303214,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8737 0x40029f8738}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f87b0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f87d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.230,StartTime:2019-07-22 02:11:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 02:11:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://0a27102080de07e62be20a28cd408bfd7d3269c3263501d3c8cad08133e9cf7e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.211: INFO: Pod "nginx-deployment-6f478d8d8-q98bc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q98bc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-q98bc,UID:8b665cc3-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303348,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f88b7 0x40029f88b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8930} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.211: INFO: Pod "nginx-deployment-6f478d8d8-r7fdv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-r7fdv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-r7fdv,UID:8b665b21-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303347,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f89d7 0x40029f89d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8a50} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.212: INFO: Pod "nginx-deployment-6f478d8d8-sb2ws" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-sb2ws,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-sb2ws,UID:85ceb551-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303222,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8af7 0x40029f8af8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8b70} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.231,StartTime:2019-07-22 02:11:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 02:11:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://192407b7bf2f4a9e4b2eda6c2565fa0b1baf81eb391d5d2eb34fbfe51c18c3bd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.212: INFO: Pod "nginx-deployment-6f478d8d8-wkgcl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wkgcl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-wkgcl,UID:85d335e6-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303228,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8c87 0x40029f8c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8d00} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:11:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.92,PodIP:10.233.88.229,StartTime:2019-07-22 02:11:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 02:11:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://8e120a639d678b46134e70f79814812b8b6df78088aa9266462f4815cf2e4cb8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.212: INFO: Pod "nginx-deployment-6f478d8d8-xh2xr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xh2xr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-xh2xr,UID:8b61ff82-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303330,Generation:0,CreationTimestamp:2019-07-22 04:24:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8e07 0x40029f8e08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8e90} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f8eb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 22 02:11:55.213: INFO: Pod "nginx-deployment-6f478d8d8-z8s88" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z8s88,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-5396,SelfLink:/api/v1/namespaces/deployment-5396/pods/nginx-deployment-6f478d8d8-z8s88,UID:85cacd37-ac38-11e9-9d2b-6c92bf900680,ResourceVersion:2303140,Generation:0,CreationTimestamp:2019-07-22 04:23:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 85c685b1-ac38-11e9-9d2b-6c92bf900680 0x40029f8f57 0x40029f8f58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d8r6s {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d8r6s,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-d8r6s true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40029f8ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40029f9020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:24:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:23:57 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:10.233.117.184,StartTime:2019-07-22 04:23:57 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 04:23:59 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://0966a2efe29b41e3f9a6665a888cffef42d875329184886a314aaec2885e07cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:11:55.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5396" for this suite.
Jul 22 02:12:03.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:12:03.922: INFO: namespace deployment-5396 deletion completed in 8.662638465s

â€¢ [SLOW TEST:19.272 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:12:03.922: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4812
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 02:12:04.055: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 02:12:34.314: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.88.237:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:12:34.314: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:12:34.602: INFO: Found all expected endpoints: [netserver-0]
Jul 22 02:12:34.607: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.117.202:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:12:34.607: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:12:34.917: INFO: Found all expected endpoints: [netserver-1]
Jul 22 02:12:34.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.91.76:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4812 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:12:34.923: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:12:35.299: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:12:35.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4812" for this suite.
Jul 22 02:12:59.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:12:59.510: INFO: namespace pod-network-test-4812 deletion completed in 24.2032106s

â€¢ [SLOW TEST:55.588 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:12:59.511: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-3a253056-ac26-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:12:59.604: INFO: Waiting up to 5m0s for pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80" in namespace "secrets-6717" to be "success or failure"
Jul 22 02:12:59.619: INFO: Pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 14.864879ms
Jul 22 02:13:01.626: INFO: Pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022105909s
Jul 22 02:13:03.632: INFO: Pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028329752s
Jul 22 02:13:05.644: INFO: Pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.039639263s
STEP: Saw pod success
Jul 22 02:13:05.644: INFO: Pod "pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:13:05.648: INFO: Trying to get logs from node k8s3 pod pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80 container secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:13:05.687: INFO: Waiting for pod pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:13:05.691: INFO: Pod pod-secrets-3a26b71e-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:13:05.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6717" for this suite.
Jul 22 02:13:11.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:13:11.869: INFO: namespace secrets-6717 deletion completed in 6.168345525s

â€¢ [SLOW TEST:12.358 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:13:11.870: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-41844a9f-ac26-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume configMaps
Jul 22 02:13:11.985: INFO: Waiting up to 5m0s for pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80" in namespace "configmap-9715" to be "success or failure"
Jul 22 02:13:11.993: INFO: Pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.812397ms
Jul 22 02:13:14.001: INFO: Pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01611358s
Jul 22 02:13:16.008: INFO: Pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022590254s
Jul 22 02:13:18.014: INFO: Pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.029027084s
STEP: Saw pod success
Jul 22 02:13:18.014: INFO: Pod "pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:13:18.019: INFO: Trying to get logs from node k8s3 pod pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 22 02:13:18.057: INFO: Waiting for pod pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:13:18.062: INFO: Pod pod-configmaps-4187cb35-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:13:18.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9715" for this suite.
Jul 22 02:13:24.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:13:24.248: INFO: namespace configmap-9715 deletion completed in 6.178919714s

â€¢ [SLOW TEST:12.379 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:13:24.249: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:13:24.316: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:13:30.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-188" for this suite.
Jul 22 02:13:36.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:13:36.656: INFO: namespace custom-resource-definition-188 deletion completed in 6.168411116s

â€¢ [SLOW TEST:12.407 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:13:36.656: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 22 02:13:36.740: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:13:52.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2727" for this suite.
Jul 22 02:13:58.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:13:58.252: INFO: namespace pods-2727 deletion completed in 6.168025535s

â€¢ [SLOW TEST:21.596 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:13:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jul 22 02:13:58.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-8643'
Jul 22 02:13:58.639: INFO: stderr: ""
Jul 22 02:13:58.639: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jul 22 02:13:59.647: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:13:59.647: INFO: Found 0 / 1
Jul 22 02:14:00.647: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:14:00.647: INFO: Found 0 / 1
Jul 22 02:14:01.649: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:14:01.649: INFO: Found 0 / 1
Jul 22 02:14:02.646: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:14:02.646: INFO: Found 1 / 1
Jul 22 02:14:02.646: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 02:14:02.652: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:14:02.652: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 22 02:14:02.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 logs redis-master-vt5z8 redis-master --namespace=kubectl-8643'
Jul 22 02:14:02.872: INFO: stderr: ""
Jul 22 02:14:02.872: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 04:26:03.289 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 04:26:03.289 # Server started, Redis version 3.2.12\n1:M 22 Jul 04:26:03.289 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 04:26:03.289 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 22 02:14:02.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 log redis-master-vt5z8 redis-master --namespace=kubectl-8643 --tail=1'
Jul 22 02:14:03.084: INFO: stderr: ""
Jul 22 02:14:03.084: INFO: stdout: "1:M 22 Jul 04:26:03.289 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 22 02:14:03.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 log redis-master-vt5z8 redis-master --namespace=kubectl-8643 --limit-bytes=1'
Jul 22 02:14:03.298: INFO: stderr: ""
Jul 22 02:14:03.299: INFO: stdout: " "
STEP: exposing timestamps
Jul 22 02:14:03.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 log redis-master-vt5z8 redis-master --namespace=kubectl-8643 --tail=1 --timestamps'
Jul 22 02:14:03.547: INFO: stderr: ""
Jul 22 02:14:03.547: INFO: stdout: "2019-07-22T04:26:03.289588545Z 1:M 22 Jul 04:26:03.289 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 22 02:14:06.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 log redis-master-vt5z8 redis-master --namespace=kubectl-8643 --since=1s'
Jul 22 02:14:06.264: INFO: stderr: ""
Jul 22 02:14:06.264: INFO: stdout: ""
Jul 22 02:14:06.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 log redis-master-vt5z8 redis-master --namespace=kubectl-8643 --since=24h'
Jul 22 02:14:06.473: INFO: stderr: ""
Jul 22 02:14:06.473: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 04:26:03.289 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 04:26:03.289 # Server started, Redis version 3.2.12\n1:M 22 Jul 04:26:03.289 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 04:26:03.289 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jul 22 02:14:06.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-8643'
Jul 22 02:14:06.693: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 02:14:06.693: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 22 02:14:06.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8643'
Jul 22 02:14:06.897: INFO: stderr: "No resources found.\n"
Jul 22 02:14:06.897: INFO: stdout: ""
Jul 22 02:14:06.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -l name=nginx --namespace=kubectl-8643 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 02:14:07.084: INFO: stderr: ""
Jul 22 02:14:07.084: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:14:07.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8643" for this suite.
Jul 22 02:14:13.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:14:13.294: INFO: namespace kubectl-8643 deletion completed in 6.200960814s

â€¢ [SLOW TEST:15.041 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:14:13.294: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-6620affa-ac26-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:14:13.395: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80" in namespace "projected-3136" to be "success or failure"
Jul 22 02:14:13.400: INFO: Pod "pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.216924ms
Jul 22 02:14:15.408: INFO: Pod "pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01321577s
Jul 22 02:14:17.415: INFO: Pod "pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019776381s
STEP: Saw pod success
Jul 22 02:14:17.415: INFO: Pod "pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:14:17.419: INFO: Trying to get logs from node k8s3 pod pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:14:17.462: INFO: Waiting for pod pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:14:17.467: INFO: Pod pod-projected-secrets-66236fe4-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:14:17.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3136" for this suite.
Jul 22 02:14:23.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:14:23.648: INFO: namespace projected-3136 deletion completed in 6.174074504s

â€¢ [SLOW TEST:10.354 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:14:23.648: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6c4c21a9-ac26-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6c4c21a9-ac26-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:15:40.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2147" for this suite.
Jul 22 02:16:04.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:16:04.658: INFO: namespace projected-2147 deletion completed in 24.182418443s

â€¢ [SLOW TEST:101.010 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:16:04.659: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 22 02:16:04.750: INFO: Waiting up to 5m0s for pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80" in namespace "downward-api-5033" to be "success or failure"
Jul 22 02:16:04.762: INFO: Pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 12.08381ms
Jul 22 02:16:06.771: INFO: Pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021223926s
Jul 22 02:16:08.779: INFO: Pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029200606s
Jul 22 02:16:10.788: INFO: Pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037439085s
STEP: Saw pod success
Jul 22 02:16:10.788: INFO: Pod "downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:16:10.793: INFO: Trying to get logs from node k8s3 pod downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 02:16:10.831: INFO: Waiting for pod downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:16:10.839: INFO: Pod downward-api-a8807186-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:16:10.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5033" for this suite.
Jul 22 02:16:16.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:16:17.014: INFO: namespace downward-api-5033 deletion completed in 6.16740914s

â€¢ [SLOW TEST:12.355 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:16:17.014: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul 22 02:16:21.678: INFO: Successfully updated pod "labelsupdateafe11b08-ac26-11e9-906c-f6b46dea7a80"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:16:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3143" for this suite.
Jul 22 02:16:47.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:16:47.889: INFO: namespace downward-api-3143 deletion completed in 24.166741778s

â€¢ [SLOW TEST:30.875 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:16:47.889: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 02:16:47.983: INFO: Waiting up to 5m0s for pod "pod-c24686f4-ac26-11e9-906c-f6b46dea7a80" in namespace "emptydir-9687" to be "success or failure"
Jul 22 02:16:47.997: INFO: Pod "pod-c24686f4-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 13.195895ms
Jul 22 02:16:50.003: INFO: Pod "pod-c24686f4-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01953529s
Jul 22 02:16:52.012: INFO: Pod "pod-c24686f4-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028308892s
STEP: Saw pod success
Jul 22 02:16:52.012: INFO: Pod "pod-c24686f4-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:16:52.017: INFO: Trying to get logs from node k8s1 pod pod-c24686f4-ac26-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:16:52.060: INFO: Waiting for pod pod-c24686f4-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:16:52.065: INFO: Pod pod-c24686f4-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:16:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9687" for this suite.
Jul 22 02:16:58.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:16:58.253: INFO: namespace emptydir-9687 deletion completed in 6.180242812s

â€¢ [SLOW TEST:10.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:16:58.253: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:17:03.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6719" for this suite.
Jul 22 02:17:27.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:17:27.570: INFO: namespace replication-controller-6719 deletion completed in 24.191099067s

â€¢ [SLOW TEST:29.317 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:17:27.570: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:17:27.659: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80" in namespace "downward-api-5005" to be "success or failure"
Jul 22 02:17:27.665: INFO: Pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.74856ms
Jul 22 02:17:29.683: INFO: Pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023530301s
Jul 22 02:17:31.692: INFO: Pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032744997s
Jul 22 02:17:33.698: INFO: Pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038929045s
STEP: Saw pod success
Jul 22 02:17:33.698: INFO: Pod "downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:17:33.703: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:17:33.739: INFO: Waiting for pod downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:17:33.743: INFO: Pod downwardapi-volume-d9ec0627-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:17:33.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5005" for this suite.
Jul 22 02:17:39.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:17:39.920: INFO: namespace downward-api-5005 deletion completed in 6.168499743s

â€¢ [SLOW TEST:12.349 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:17:39.921: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:17:40.020: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80" in namespace "downward-api-387" to be "success or failure"
Jul 22 02:17:40.027: INFO: Pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.70012ms
Jul 22 02:17:42.033: INFO: Pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013883648s
Jul 22 02:17:44.042: INFO: Pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022246476s
Jul 22 02:17:46.048: INFO: Pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028541695s
STEP: Saw pod success
Jul 22 02:17:46.048: INFO: Pod "downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:17:46.053: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:17:46.095: INFO: Waiting for pod downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:17:46.099: INFO: Pod downwardapi-volume-e14a0998-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:17:46.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-387" for this suite.
Jul 22 02:17:52.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:17:52.275: INFO: namespace downward-api-387 deletion completed in 6.167144471s

â€¢ [SLOW TEST:12.354 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:17:52.276: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 22 02:17:52.359: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 22 02:17:57.375: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:17:58.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8996" for this suite.
Jul 22 02:18:04.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:04.631: INFO: namespace replication-controller-8996 deletion completed in 6.216850126s

â€¢ [SLOW TEST:12.355 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:04.631: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f0073dbd-ac26-11e9-906c-f6b46dea7a80
STEP: Creating a pod to test consume secrets
Jul 22 02:18:04.748: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80" in namespace "projected-4043" to be "success or failure"
Jul 22 02:18:04.754: INFO: Pod "pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138642ms
Jul 22 02:18:06.760: INFO: Pod "pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011123487s
Jul 22 02:18:08.770: INFO: Pod "pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021263653s
STEP: Saw pod success
Jul 22 02:18:08.770: INFO: Pod "pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:18:08.774: INFO: Trying to get logs from node k8s1 pod pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 22 02:18:08.823: INFO: Waiting for pod pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:18:08.828: INFO: Pod pod-projected-secrets-f008e666-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:18:08.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4043" for this suite.
Jul 22 02:18:14.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:15.010: INFO: namespace projected-4043 deletion completed in 6.17392523s

â€¢ [SLOW TEST:10.379 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:15.010: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:18:15.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80" in namespace "projected-9160" to be "success or failure"
Jul 22 02:18:15.109: INFO: Pod "downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.797392ms
Jul 22 02:18:17.117: INFO: Pod "downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012223027s
Jul 22 02:18:19.123: INFO: Pod "downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018712423s
STEP: Saw pod success
Jul 22 02:18:19.123: INFO: Pod "downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:18:19.128: INFO: Trying to get logs from node k8s1 pod downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:18:19.165: INFO: Waiting for pod downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:18:19.170: INFO: Pod downwardapi-volume-f633991d-ac26-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:18:19.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9160" for this suite.
Jul 22 02:18:25.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:25.336: INFO: namespace projected-9160 deletion completed in 6.158321323s

â€¢ [SLOW TEST:10.326 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:25.336: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:18:29.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5480" for this suite.
Jul 22 02:18:35.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:35.620: INFO: namespace kubelet-test-5480 deletion completed in 6.174162155s

â€¢ [SLOW TEST:10.284 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:35.621: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 22 02:18:35.704: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:18:40.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-845" for this suite.
Jul 22 02:18:46.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:46.656: INFO: namespace init-container-845 deletion completed in 6.169769085s

â€¢ [SLOW TEST:11.035 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:46.657: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 22 02:18:46.743: INFO: Waiting up to 5m0s for pod "pod-090f25a2-ac27-11e9-906c-f6b46dea7a80" in namespace "emptydir-6889" to be "success or failure"
Jul 22 02:18:46.750: INFO: Pod "pod-090f25a2-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 7.438712ms
Jul 22 02:18:48.757: INFO: Pod "pod-090f25a2-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013935605s
Jul 22 02:18:50.763: INFO: Pod "pod-090f25a2-ac27-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02042485s
STEP: Saw pod success
Jul 22 02:18:50.763: INFO: Pod "pod-090f25a2-ac27-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:18:50.768: INFO: Trying to get logs from node k8s1 pod pod-090f25a2-ac27-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:18:50.818: INFO: Waiting for pod pod-090f25a2-ac27-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:18:50.824: INFO: Pod pod-090f25a2-ac27-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:18:50.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6889" for this suite.
Jul 22 02:18:56.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:18:57.001: INFO: namespace emptydir-6889 deletion completed in 6.169821042s

â€¢ [SLOW TEST:10.344 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:18:57.001: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:18:57.102: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 22 02:19:02.109: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 02:19:02.109: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 22 02:19:04.119: INFO: Creating deployment "test-rollover-deployment"
Jul 22 02:19:04.132: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 22 02:19:06.144: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 22 02:19:06.154: INFO: Ensure that both replica sets have 1 created replica
Jul 22 02:19:06.163: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 22 02:19:06.182: INFO: Updating deployment test-rollover-deployment
Jul 22 02:19:06.182: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 22 02:19:08.195: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 22 02:19:08.205: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 22 02:19:08.215: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:08.215: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366645, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:10.226: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:10.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366647, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:12.226: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:12.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366647, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:14.226: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:14.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366647, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:16.227: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:16.227: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366647, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:18.226: INFO: all replica sets need to contain the pod-template-hash label
Jul 22 02:19:18.226: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366647, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:20.237: INFO: 
Jul 22 02:19:20.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366657, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699366643, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:19:22.226: INFO: 
Jul 22 02:19:22.226: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 22 02:19:22.241: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-726,SelfLink:/apis/apps/v1/namespaces/deployment-726/deployments/test-rollover-deployment,UID:7785d242-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2305705,Generation:2,CreationTimestamp:2019-07-22 04:30:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-22 04:30:43 +0000 UTC 2019-07-22 04:30:43 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-22 04:30:57 +0000 UTC 2019-07-22 04:30:43 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 22 02:19:22.247: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-726,SelfLink:/apis/apps/v1/namespaces/deployment-726/replicasets/test-rollover-deployment-766b4d6c9d,UID:78a87057-ac39-11e9-9d2b-6c92bf900680,ResourceVersion:2305695,Generation:2,CreationTimestamp:2019-07-22 04:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7785d242-ac39-11e9-b4b6-6c92bf130c27 0x4001ca7997 0x4001ca7998}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 02:19:22.247: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 22 02:19:22.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-726,SelfLink:/apis/apps/v1/namespaces/deployment-726/replicasets/test-rollover-controller,UID:73a5abb7-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2305704,Generation:2,CreationTimestamp:2019-07-22 04:30:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7785d242-ac39-11e9-b4b6-6c92bf130c27 0x4001ca77c7 0x4001ca77c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 02:19:22.247: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-726,SelfLink:/apis/apps/v1/namespaces/deployment-726/replicasets/test-rollover-deployment-6455657675,UID:77898eca-ac39-11e9-9d2b-6c92bf900680,ResourceVersion:2305642,Generation:2,CreationTimestamp:2019-07-22 04:30:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7785d242-ac39-11e9-b4b6-6c92bf130c27 0x4001ca78b7 0x4001ca78b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 22 02:19:22.253: INFO: Pod "test-rollover-deployment-766b4d6c9d-q92vm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-q92vm,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-726,SelfLink:/api/v1/namespaces/deployment-726/pods/test-rollover-deployment-766b4d6c9d-q92vm,UID:78b37a4e-ac39-11e9-9d2b-6c92bf900680,ResourceVersion:2305668,Generation:0,CreationTimestamp:2019-07-22 04:30:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 78a87057-ac39-11e9-9d2b-6c92bf900680 0x40000e6917 0x40000e6918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r6jtf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r6jtf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-r6jtf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x40000e6ca0} {node.kubernetes.io/unreachable Exists  NoExecute 0x40000e6d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:30:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:30:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:30:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:30:44 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.91,PodIP:10.233.117.209,StartTime:2019-07-22 04:30:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-22 04:30:47 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:3719faa3a44c8ca688772f854f75904514f84774f80c0936c440d399f707cb50 docker://e23425c846039fb052e6d3232c0d0789dba79d4423f60606e18a0ad18c550f0a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:19:22.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-726" for this suite.
Jul 22 02:19:30.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:19:30.436: INFO: namespace deployment-726 deletion completed in 8.17645385s

â€¢ [SLOW TEST:33.435 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:19:30.438: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7807
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 02:19:30.498: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 02:19:58.699: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.88.249:8080/dial?request=hostName&protocol=http&host=10.233.117.210&port=8080&tries=1'] Namespace:pod-network-test-7807 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:19:58.699: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:19:59.034: INFO: Waiting for endpoints: map[]
Jul 22 02:19:59.041: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.88.249:8080/dial?request=hostName&protocol=http&host=10.233.88.248&port=8080&tries=1'] Namespace:pod-network-test-7807 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:19:59.041: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:19:59.417: INFO: Waiting for endpoints: map[]
Jul 22 02:19:59.422: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.88.249:8080/dial?request=hostName&protocol=http&host=10.233.91.87&port=8080&tries=1'] Namespace:pod-network-test-7807 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:19:59.422: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:19:59.730: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:19:59.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7807" for this suite.
Jul 22 02:20:23.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:20:23.916: INFO: namespace pod-network-test-7807 deletion completed in 24.1772619s

â€¢ [SLOW TEST:53.478 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:20:23.916: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 22 02:20:24.044: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5551,SelfLink:/api/v1/namespaces/watch-5551/configmaps/e2e-watch-test-resource-version,UID:a37674eb-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306006,Generation:0,CreationTimestamp:2019-07-22 04:31:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 02:20:24.044: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5551,SelfLink:/api/v1/namespaces/watch-5551/configmaps/e2e-watch-test-resource-version,UID:a37674eb-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306007,Generation:0,CreationTimestamp:2019-07-22 04:31:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:20:24.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5551" for this suite.
Jul 22 02:20:30.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:20:30.215: INFO: namespace watch-5551 deletion completed in 6.163588081s

â€¢ [SLOW TEST:6.299 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:20:30.215: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4831.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4831.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 22 02:20:36.359: INFO: DNS probes using dns-4831/dns-test-46c8c476-ac27-11e9-906c-f6b46dea7a80 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:20:36.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4831" for this suite.
Jul 22 02:20:42.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:20:42.578: INFO: namespace dns-4831 deletion completed in 6.184286474s

â€¢ [SLOW TEST:12.363 seconds]
[sig-network] DNS
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:20:42.578: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jul 22 02:20:43.189: INFO: created pod pod-service-account-defaultsa
Jul 22 02:20:43.190: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 22 02:20:43.205: INFO: created pod pod-service-account-mountsa
Jul 22 02:20:43.205: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 22 02:20:43.229: INFO: created pod pod-service-account-nomountsa
Jul 22 02:20:43.229: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 22 02:20:43.254: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 22 02:20:43.254: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 22 02:20:43.275: INFO: created pod pod-service-account-mountsa-mountspec
Jul 22 02:20:43.275: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 22 02:20:43.298: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 22 02:20:43.298: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 22 02:20:43.311: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 22 02:20:43.311: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 22 02:20:43.336: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 22 02:20:43.336: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 22 02:20:43.359: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 22 02:20:43.359: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:20:43.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6321" for this suite.
Jul 22 02:21:07.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:21:07.647: INFO: namespace svcaccounts-6321 deletion completed in 24.271242696s

â€¢ [SLOW TEST:25.069 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:21:07.648: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jul 22 02:21:07.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 api-versions'
Jul 22 02:21:07.873: INFO: stderr: ""
Jul 22 02:21:07.874: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:21:07.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-911" for this suite.
Jul 22 02:21:13.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:21:14.057: INFO: namespace kubectl-911 deletion completed in 6.175332135s

â€¢ [SLOW TEST:6.409 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:21:14.058: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 22 02:21:14.172: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-643,SelfLink:/api/v1/namespaces/watch-643/configmaps/e2e-watch-test-watch-closed,UID:bf0de7bc-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306352,Generation:0,CreationTimestamp:2019-07-22 04:32:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 22 02:21:14.172: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-643,SelfLink:/api/v1/namespaces/watch-643/configmaps/e2e-watch-test-watch-closed,UID:bf0de7bc-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306355,Generation:0,CreationTimestamp:2019-07-22 04:32:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 22 02:21:14.195: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-643,SelfLink:/api/v1/namespaces/watch-643/configmaps/e2e-watch-test-watch-closed,UID:bf0de7bc-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306356,Generation:0,CreationTimestamp:2019-07-22 04:32:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 22 02:21:14.195: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-643,SelfLink:/api/v1/namespaces/watch-643/configmaps/e2e-watch-test-watch-closed,UID:bf0de7bc-ac39-11e9-b4b6-6c92bf130c27,ResourceVersion:2306357,Generation:0,CreationTimestamp:2019-07-22 04:32:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:21:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-643" for this suite.
Jul 22 02:21:20.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:21:20.381: INFO: namespace watch-643 deletion completed in 6.179112147s

â€¢ [SLOW TEST:6.323 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:21:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul 22 02:21:20.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-1888'
Jul 22 02:21:20.773: INFO: stderr: ""
Jul 22 02:21:20.773: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 02:21:21.781: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:21.781: INFO: Found 0 / 1
Jul 22 02:21:22.780: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:22.780: INFO: Found 0 / 1
Jul 22 02:21:23.795: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:23.795: INFO: Found 0 / 1
Jul 22 02:21:24.779: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:24.780: INFO: Found 1 / 1
Jul 22 02:21:24.780: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 22 02:21:24.784: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:24.784: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 02:21:24.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 patch pod redis-master-5g9rv --namespace=kubectl-1888 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 22 02:21:24.990: INFO: stderr: ""
Jul 22 02:21:24.990: INFO: stdout: "pod/redis-master-5g9rv patched\n"
STEP: checking annotations
Jul 22 02:21:24.997: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:21:24.997: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:21:24.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1888" for this suite.
Jul 22 02:21:49.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:21:49.179: INFO: namespace kubectl-1888 deletion completed in 24.173984469s

â€¢ [SLOW TEST:28.797 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:21:49.180: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2515
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 22 02:21:49.262: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 22 02:22:15.489: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.91.90 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:22:15.489: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:22:16.771: INFO: Found all expected endpoints: [netserver-0]
Jul 22 02:22:16.777: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.117.214 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:22:16.777: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:22:18.056: INFO: Found all expected endpoints: [netserver-1]
Jul 22 02:22:18.061: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.88.1 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 22 02:22:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
Jul 22 02:22:19.349: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:22:19.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2515" for this suite.
Jul 22 02:22:43.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:22:43.530: INFO: namespace pod-network-test-2515 deletion completed in 24.172772635s

â€¢ [SLOW TEST:54.350 seconds]
[sig-network] Networking
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:22:43.531: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-tnxp
STEP: Creating a pod to test atomic-volume-subpath
Jul 22 02:22:43.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-tnxp" in namespace "subpath-8662" to be "success or failure"
Jul 22 02:22:43.639: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Pending", Reason="", readiness=false. Elapsed: 15.291257ms
Jul 22 02:22:45.645: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021126992s
Jul 22 02:22:47.652: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 4.028234777s
Jul 22 02:22:49.659: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 6.034867456s
Jul 22 02:22:51.666: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 8.041905041s
Jul 22 02:22:53.674: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 10.049912015s
Jul 22 02:22:55.682: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 12.058267498s
Jul 22 02:22:57.691: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 14.066937863s
Jul 22 02:22:59.697: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 16.072781484s
Jul 22 02:23:01.703: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 18.079127512s
Jul 22 02:23:03.710: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 20.086240194s
Jul 22 02:23:05.717: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 22.093438503s
Jul 22 02:23:07.723: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Running", Reason="", readiness=true. Elapsed: 24.099545895s
Jul 22 02:23:09.730: INFO: Pod "pod-subpath-test-downwardapi-tnxp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.106079578s
STEP: Saw pod success
Jul 22 02:23:09.730: INFO: Pod "pod-subpath-test-downwardapi-tnxp" satisfied condition "success or failure"
Jul 22 02:23:09.735: INFO: Trying to get logs from node k8s3 pod pod-subpath-test-downwardapi-tnxp container test-container-subpath-downwardapi-tnxp: <nil>
STEP: delete the pod
Jul 22 02:23:09.773: INFO: Waiting for pod pod-subpath-test-downwardapi-tnxp to disappear
Jul 22 02:23:09.777: INFO: Pod pod-subpath-test-downwardapi-tnxp no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-tnxp
Jul 22 02:23:09.777: INFO: Deleting pod "pod-subpath-test-downwardapi-tnxp" in namespace "subpath-8662"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:23:09.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8662" for this suite.
Jul 22 02:23:15.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:23:15.975: INFO: namespace subpath-8662 deletion completed in 6.181688728s

â€¢ [SLOW TEST:32.444 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:23:15.975: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6180
Jul 22 02:23:20.102: INFO: Started pod liveness-http in namespace container-probe-6180
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 02:23:20.106: INFO: Initial restart count of pod liveness-http is 0
Jul 22 02:23:40.177: INFO: Restart count of pod container-probe-6180/liveness-http is now 1 (20.07113839s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:23:40.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6180" for this suite.
Jul 22 02:23:46.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:23:46.384: INFO: namespace container-probe-6180 deletion completed in 6.172382045s

â€¢ [SLOW TEST:30.409 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:23:46.384: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jul 22 02:23:46.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 cluster-info'
Jul 22 02:23:46.655: INFO: stderr: ""
Jul 22 02:23:46.655: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\x1b[0;32mcoredns\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/coredns:dns/proxy\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:23:46.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3069" for this suite.
Jul 22 02:23:52.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:23:52.852: INFO: namespace kubectl-3069 deletion completed in 6.188148799s

â€¢ [SLOW TEST:6.468 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:23:52.853: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:23:52.945: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80" in namespace "projected-9100" to be "success or failure"
Jul 22 02:23:52.951: INFO: Pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.503141ms
Jul 22 02:23:54.959: INFO: Pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013650949s
Jul 22 02:23:56.967: INFO: Pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022167068s
Jul 22 02:23:58.973: INFO: Pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028208983s
STEP: Saw pod success
Jul 22 02:23:58.973: INFO: Pod "downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:23:58.978: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:23:59.015: INFO: Waiting for pod downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:23:59.020: INFO: Pod downwardapi-volume-bf921292-ac27-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:23:59.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9100" for this suite.
Jul 22 02:24:05.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:24:05.206: INFO: namespace projected-9100 deletion completed in 6.179282036s

â€¢ [SLOW TEST:12.354 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:24:05.208: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jul 22 02:24:05.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-7447'
Jul 22 02:24:05.820: INFO: stderr: ""
Jul 22 02:24:05.820: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 02:24:05.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Jul 22 02:24:06.013: INFO: stderr: ""
Jul 22 02:24:06.013: INFO: stdout: "update-demo-nautilus-jbcjq update-demo-nautilus-kph9v "
Jul 22 02:24:06.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-jbcjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:06.198: INFO: stderr: ""
Jul 22 02:24:06.199: INFO: stdout: ""
Jul 22 02:24:06.199: INFO: update-demo-nautilus-jbcjq is created but not running
Jul 22 02:24:11.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Jul 22 02:24:11.393: INFO: stderr: ""
Jul 22 02:24:11.393: INFO: stdout: "update-demo-nautilus-jbcjq update-demo-nautilus-kph9v "
Jul 22 02:24:11.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-jbcjq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:11.579: INFO: stderr: ""
Jul 22 02:24:11.579: INFO: stdout: "true"
Jul 22 02:24:11.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-jbcjq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:11.760: INFO: stderr: ""
Jul 22 02:24:11.760: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 02:24:11.760: INFO: validating pod update-demo-nautilus-jbcjq
Jul 22 02:24:11.767: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 02:24:11.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 02:24:11.768: INFO: update-demo-nautilus-jbcjq is verified up and running
Jul 22 02:24:11.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-kph9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:11.949: INFO: stderr: ""
Jul 22 02:24:11.949: INFO: stdout: "true"
Jul 22 02:24:11.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-kph9v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:12.132: INFO: stderr: ""
Jul 22 02:24:12.132: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 02:24:12.133: INFO: validating pod update-demo-nautilus-kph9v
Jul 22 02:24:12.139: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 02:24:12.139: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 02:24:12.139: INFO: update-demo-nautilus-kph9v is verified up and running
STEP: rolling-update to new replication controller
Jul 22 02:24:12.143: INFO: scanned /root for discovery docs: <nil>
Jul 22 02:24:12.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7447'
Jul 22 02:24:31.389: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 22 02:24:31.389: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 02:24:31.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7447'
Jul 22 02:24:31.582: INFO: stderr: ""
Jul 22 02:24:31.582: INFO: stdout: "update-demo-kitten-c2fpd update-demo-kitten-sfsp6 "
Jul 22 02:24:31.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-kitten-c2fpd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:31.766: INFO: stderr: ""
Jul 22 02:24:31.766: INFO: stdout: "true"
Jul 22 02:24:31.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-kitten-c2fpd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:31.957: INFO: stderr: ""
Jul 22 02:24:31.957: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 22 02:24:31.957: INFO: validating pod update-demo-kitten-c2fpd
Jul 22 02:24:31.964: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 22 02:24:31.964: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 22 02:24:31.964: INFO: update-demo-kitten-c2fpd is verified up and running
Jul 22 02:24:31.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-kitten-sfsp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:32.147: INFO: stderr: ""
Jul 22 02:24:32.147: INFO: stdout: "true"
Jul 22 02:24:32.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-kitten-sfsp6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7447'
Jul 22 02:24:32.337: INFO: stderr: ""
Jul 22 02:24:32.338: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 22 02:24:32.338: INFO: validating pod update-demo-kitten-sfsp6
Jul 22 02:24:32.349: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 22 02:24:32.349: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 22 02:24:32.349: INFO: update-demo-kitten-sfsp6 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:24:32.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7447" for this suite.
Jul 22 02:24:56.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:24:56.528: INFO: namespace kubectl-7447 deletion completed in 24.171403582s

â€¢ [SLOW TEST:51.320 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:24:56.528: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:24:56.631: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80" in namespace "downward-api-9464" to be "success or failure"
Jul 22 02:24:56.636: INFO: Pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.335246ms
Jul 22 02:24:58.643: INFO: Pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011796389s
Jul 22 02:25:00.650: INFO: Pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019313566s
Jul 22 02:25:02.659: INFO: Pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027953012s
STEP: Saw pod success
Jul 22 02:25:02.659: INFO: Pod "downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:25:02.664: INFO: Trying to get logs from node k8s3 pod downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:25:02.704: INFO: Waiting for pod downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:25:02.709: INFO: Pod downwardapi-volume-e587c80c-ac27-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:25:02.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9464" for this suite.
Jul 22 02:25:08.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:25:08.914: INFO: namespace downward-api-9464 deletion completed in 6.197810035s

â€¢ [SLOW TEST:12.386 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:25:08.914: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8070
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul 22 02:25:09.020: INFO: Found 0 stateful pods, waiting for 3
Jul 22 02:25:19.027: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:19.027: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:19.027: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jul 22 02:25:29.034: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:29.034: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:29.034: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 22 02:25:29.073: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 22 02:25:39.117: INFO: Updating stateful set ss2
Jul 22 02:25:39.138: INFO: Waiting for Pod statefulset-8070/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jul 22 02:25:49.254: INFO: Found 2 stateful pods, waiting for 3
Jul 22 02:25:59.278: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:59.278: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:25:59.278: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 22 02:25:59.321: INFO: Updating stateful set ss2
Jul 22 02:25:59.377: INFO: Waiting for Pod statefulset-8070/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul 22 02:26:09.417: INFO: Updating stateful set ss2
Jul 22 02:26:09.438: INFO: Waiting for StatefulSet statefulset-8070/ss2 to complete update
Jul 22 02:26:09.438: INFO: Waiting for Pod statefulset-8070/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 22 02:26:19.449: INFO: Deleting all statefulset in ns statefulset-8070
Jul 22 02:26:19.453: INFO: Scaling statefulset ss2 to 0
Jul 22 02:26:39.475: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 02:26:39.479: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:26:39.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8070" for this suite.
Jul 22 02:26:47.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:26:47.712: INFO: namespace statefulset-8070 deletion completed in 8.207544126s

â€¢ [SLOW TEST:98.797 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:26:47.712: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-2310
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2310 to expose endpoints map[]
Jul 22 02:26:47.803: INFO: Get endpoints failed (6.500978ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul 22 02:26:48.818: INFO: successfully validated that service endpoint-test2 in namespace services-2310 exposes endpoints map[] (1.02156382s elapsed)
STEP: Creating pod pod1 in namespace services-2310
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2310 to expose endpoints map[pod1:[80]]
Jul 22 02:26:52.891: INFO: successfully validated that service endpoint-test2 in namespace services-2310 exposes endpoints map[pod1:[80]] (4.062006022s elapsed)
STEP: Creating pod pod2 in namespace services-2310
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2310 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 22 02:26:56.983: INFO: Unexpected endpoints: found map[773360fb-ac3a-11e9-b4b6-6c92bf130c27:[80]], expected map[pod1:[80] pod2:[80]] (4.080232529s elapsed, will retry)
Jul 22 02:26:57.999: INFO: successfully validated that service endpoint-test2 in namespace services-2310 exposes endpoints map[pod1:[80] pod2:[80]] (5.095836366s elapsed)
STEP: Deleting pod pod1 in namespace services-2310
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2310 to expose endpoints map[pod2:[80]]
Jul 22 02:26:59.036: INFO: successfully validated that service endpoint-test2 in namespace services-2310 exposes endpoints map[pod2:[80]] (1.021431284s elapsed)
STEP: Deleting pod pod2 in namespace services-2310
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-2310 to expose endpoints map[]
Jul 22 02:26:59.069: INFO: successfully validated that service endpoint-test2 in namespace services-2310 exposes endpoints map[] (5.325929ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:26:59.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2310" for this suite.
Jul 22 02:27:05.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:27:05.304: INFO: namespace services-2310 deletion completed in 6.182545363s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:17.592 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:27:05.306: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8720
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8720
STEP: Deleting pre-stop pod
Jul 22 02:27:18.474: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:27:18.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8720" for this suite.
Jul 22 02:28:00.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:28:00.693: INFO: namespace prestop-8720 deletion completed in 42.193072149s

â€¢ [SLOW TEST:55.387 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:28:00.694: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul 22 02:28:00.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80" in namespace "projected-6017" to be "success or failure"
Jul 22 02:28:00.791: INFO: Pod "downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.432746ms
Jul 22 02:28:02.805: INFO: Pod "downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019789639s
Jul 22 02:28:04.812: INFO: Pod "downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026853311s
STEP: Saw pod success
Jul 22 02:28:04.812: INFO: Pod "downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:28:04.817: INFO: Trying to get logs from node k8s2 pod downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80 container client-container: <nil>
STEP: delete the pod
Jul 22 02:28:04.854: INFO: Waiting for pod downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:28:04.859: INFO: Pod downwardapi-volume-534c9105-ac28-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:28:04.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6017" for this suite.
Jul 22 02:28:10.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:28:11.072: INFO: namespace projected-6017 deletion completed in 6.206018802s

â€¢ [SLOW TEST:10.378 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:28:11.072: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul 22 02:28:11.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-4688'
Jul 22 02:28:11.459: INFO: stderr: ""
Jul 22 02:28:11.459: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 22 02:28:11.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4688'
Jul 22 02:28:11.660: INFO: stderr: ""
Jul 22 02:28:11.660: INFO: stdout: "update-demo-nautilus-9k8gv update-demo-nautilus-qljb5 "
Jul 22 02:28:11.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-9k8gv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4688'
Jul 22 02:28:11.850: INFO: stderr: ""
Jul 22 02:28:11.850: INFO: stdout: ""
Jul 22 02:28:11.850: INFO: update-demo-nautilus-9k8gv is created but not running
Jul 22 02:28:16.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4688'
Jul 22 02:28:17.046: INFO: stderr: ""
Jul 22 02:28:17.046: INFO: stdout: "update-demo-nautilus-9k8gv update-demo-nautilus-qljb5 "
Jul 22 02:28:17.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-9k8gv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4688'
Jul 22 02:28:17.234: INFO: stderr: ""
Jul 22 02:28:17.234: INFO: stdout: "true"
Jul 22 02:28:17.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-9k8gv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4688'
Jul 22 02:28:17.418: INFO: stderr: ""
Jul 22 02:28:17.418: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 02:28:17.418: INFO: validating pod update-demo-nautilus-9k8gv
Jul 22 02:28:17.426: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 02:28:17.426: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 02:28:17.426: INFO: update-demo-nautilus-9k8gv is verified up and running
Jul 22 02:28:17.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-qljb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4688'
Jul 22 02:28:17.607: INFO: stderr: ""
Jul 22 02:28:17.607: INFO: stdout: "true"
Jul 22 02:28:17.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods update-demo-nautilus-qljb5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4688'
Jul 22 02:28:17.793: INFO: stderr: ""
Jul 22 02:28:17.793: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 22 02:28:17.793: INFO: validating pod update-demo-nautilus-qljb5
Jul 22 02:28:17.802: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 22 02:28:17.802: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 22 02:28:17.802: INFO: update-demo-nautilus-qljb5 is verified up and running
STEP: using delete to clean up resources
Jul 22 02:28:17.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete --grace-period=0 --force -f - --namespace=kubectl-4688'
Jul 22 02:28:18.004: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 22 02:28:18.004: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 22 02:28:18.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4688'
Jul 22 02:28:18.212: INFO: stderr: "No resources found.\n"
Jul 22 02:28:18.212: INFO: stdout: ""
Jul 22 02:28:18.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -l name=update-demo --namespace=kubectl-4688 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 22 02:28:18.455: INFO: stderr: ""
Jul 22 02:28:18.455: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:28:18.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4688" for this suite.
Jul 22 02:28:24.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:28:24.635: INFO: namespace kubectl-4688 deletion completed in 6.171871816s

â€¢ [SLOW TEST:13.563 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:28:24.636: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 22 02:28:34.816: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:34.821: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:36.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:36.827: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:38.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:38.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:40.824: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:40.830: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:42.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:42.827: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:44.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:44.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:46.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:46.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:48.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:48.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:50.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:50.827: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:52.821: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:52.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:54.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:54.828: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 22 02:28:56.822: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 22 02:28:56.830: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:28:56.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9866" for this suite.
Jul 22 02:29:20.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:29:21.081: INFO: namespace container-lifecycle-hook-9866 deletion completed in 24.228816139s

â€¢ [SLOW TEST:56.445 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:29:21.081: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:29:21.181: INFO: Create a RollingUpdate DaemonSet
Jul 22 02:29:21.192: INFO: Check that daemon pods launch on every node of the cluster
Jul 22 02:29:21.215: INFO: Number of nodes with available pods: 0
Jul 22 02:29:21.215: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:29:22.230: INFO: Number of nodes with available pods: 0
Jul 22 02:29:22.231: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:29:23.229: INFO: Number of nodes with available pods: 0
Jul 22 02:29:23.229: INFO: Node k8s1 is running more than one daemon pod
Jul 22 02:29:24.232: INFO: Number of nodes with available pods: 2
Jul 22 02:29:24.232: INFO: Node k8s3 is running more than one daemon pod
Jul 22 02:29:25.228: INFO: Number of nodes with available pods: 2
Jul 22 02:29:25.228: INFO: Node k8s3 is running more than one daemon pod
Jul 22 02:29:26.231: INFO: Number of nodes with available pods: 3
Jul 22 02:29:26.231: INFO: Number of running nodes: 3, number of available pods: 3
Jul 22 02:29:26.231: INFO: Update the DaemonSet to trigger a rollout
Jul 22 02:29:26.243: INFO: Updating DaemonSet daemon-set
Jul 22 02:29:31.273: INFO: Roll back the DaemonSet before rollout is complete
Jul 22 02:29:31.286: INFO: Updating DaemonSet daemon-set
Jul 22 02:29:31.286: INFO: Make sure DaemonSet rollback is complete
Jul 22 02:29:31.300: INFO: Wrong image for pod: daemon-set-f6l7m. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 02:29:31.300: INFO: Pod daemon-set-f6l7m is not available
Jul 22 02:29:32.313: INFO: Wrong image for pod: daemon-set-f6l7m. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 22 02:29:32.313: INFO: Pod daemon-set-f6l7m is not available
Jul 22 02:29:33.315: INFO: Pod daemon-set-npg6v is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4883, will wait for the garbage collector to delete the pods
Jul 22 02:29:33.404: INFO: Deleting DaemonSet.extensions daemon-set took: 14.639979ms
Jul 22 02:29:33.805: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.844777ms
Jul 22 02:29:37.310: INFO: Number of nodes with available pods: 0
Jul 22 02:29:37.310: INFO: Number of running nodes: 0, number of available pods: 0
Jul 22 02:29:37.315: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4883/daemonsets","resourceVersion":"2308749"},"items":null}

Jul 22 02:29:37.319: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4883/pods","resourceVersion":"2308749"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:29:37.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4883" for this suite.
Jul 22 02:29:43.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:29:43.531: INFO: namespace daemonsets-4883 deletion completed in 6.18136936s

â€¢ [SLOW TEST:22.450 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:29:43.532: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5829
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-5829
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5829
Jul 22 02:29:43.632: INFO: Found 0 stateful pods, waiting for 1
Jul 22 02:29:53.639: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 22 02:29:53.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 02:29:54.137: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 02:29:54.137: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 02:29:54.137: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 02:29:54.144: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 22 02:30:04.150: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 02:30:04.150: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 02:30:04.171: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Jul 22 02:30:04.171: INFO: ss-0  k8s1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:43 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  }]
Jul 22 02:30:04.171: INFO: 
Jul 22 02:30:04.171: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 22 02:30:05.179: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992694565s
Jul 22 02:30:06.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985224854s
Jul 22 02:30:07.194: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977159028s
Jul 22 02:30:08.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970288292s
Jul 22 02:30:09.209: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.96316529s
Jul 22 02:30:10.219: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.951332669s
Jul 22 02:30:11.226: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.944734976s
Jul 22 02:30:12.245: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.926176108s
Jul 22 02:30:13.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 917.681433ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5829
Jul 22 02:30:14.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 02:30:14.728: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 22 02:30:14.728: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 02:30:14.728: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 02:30:14.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 02:30:15.223: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 02:30:15.223: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 02:30:15.223: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 02:30:15.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 22 02:30:15.805: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 22 02:30:15.806: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 22 02:30:15.806: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 22 02:30:15.813: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:30:15.813: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 22 02:30:15.813: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 22 02:30:15.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 02:30:16.315: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 02:30:16.316: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 02:30:16.316: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 02:30:16.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 02:30:16.853: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 02:30:16.853: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 02:30:16.853: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 02:30:16.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 exec --namespace=statefulset-5829 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 22 02:30:17.347: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 22 02:30:17.347: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 22 02:30:17.347: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 22 02:30:17.347: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 02:30:17.353: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 22 02:30:27.365: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 02:30:27.365: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 02:30:27.365: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 22 02:30:27.388: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Jul 22 02:30:27.388: INFO: ss-0  k8s1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  }]
Jul 22 02:30:27.388: INFO: ss-1  k8s2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:27.388: INFO: ss-2  k8s3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:27.388: INFO: 
Jul 22 02:30:27.388: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 02:30:28.396: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Jul 22 02:30:28.396: INFO: ss-0  k8s1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  }]
Jul 22 02:30:28.396: INFO: ss-1  k8s2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:28.396: INFO: ss-2  k8s3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:28.396: INFO: 
Jul 22 02:30:28.396: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 02:30:29.403: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Jul 22 02:30:29.403: INFO: ss-0  k8s1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  }]
Jul 22 02:30:29.403: INFO: ss-1  k8s2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:29.403: INFO: ss-2  k8s3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:29.403: INFO: 
Jul 22 02:30:29.403: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 02:30:30.412: INFO: POD   NODE  PHASE    GRACE  CONDITIONS
Jul 22 02:30:30.412: INFO: ss-0  k8s1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:33 +0000 UTC  }]
Jul 22 02:30:30.412: INFO: ss-1  k8s2  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:41:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:30.412: INFO: ss-2  k8s3  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-22 02:30:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:40:52 +0000 UTC  }]
Jul 22 02:30:30.412: INFO: 
Jul 22 02:30:30.412: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 22 02:30:31.417: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.969352354s
Jul 22 02:30:32.423: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.963704268s
Jul 22 02:30:33.429: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.95784738s
Jul 22 02:30:34.434: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.952248629s
Jul 22 02:30:35.440: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.946668013s
Jul 22 02:30:36.446: INFO: Verifying statefulset ss doesn't scale past 0 for another 940.231932ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5829
Jul 22 02:30:37.453: INFO: Scaling statefulset ss to 0
Jul 22 02:30:37.467: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul 22 02:30:37.471: INFO: Deleting all statefulset in ns statefulset-5829
Jul 22 02:30:37.475: INFO: Scaling statefulset ss to 0
Jul 22 02:30:37.489: INFO: Waiting for statefulset status.replicas updated to 0
Jul 22 02:30:37.493: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:30:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5829" for this suite.
Jul 22 02:30:45.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:30:45.700: INFO: namespace statefulset-5829 deletion completed in 8.181796078s

â€¢ [SLOW TEST:62.169 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:30:45.701: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:30:45.776: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:30:50.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-151" for this suite.
Jul 22 02:31:32.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:31:32.291: INFO: namespace pods-151 deletion completed in 42.171391886s

â€¢ [SLOW TEST:46.590 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:31:32.292: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4039
Jul 22 02:31:36.411: INFO: Started pod liveness-http in namespace container-probe-4039
STEP: checking the pod's current state and verifying that restartCount is present
Jul 22 02:31:36.415: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:35:37.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4039" for this suite.
Jul 22 02:35:43.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:35:43.490: INFO: namespace container-probe-4039 deletion completed in 6.173771553s

â€¢ [SLOW TEST:251.198 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:35:43.490: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jul 22 02:35:45.236: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 22 02:35:47.319: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:35:49.326: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:35:51.327: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699367567, loc:(*time.Location)(0x81de100)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 22 02:36:06.141: INFO: Waited 12.799208679s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:36:06.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3075" for this suite.
Jul 22 02:36:12.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:36:12.911: INFO: namespace aggregator-3075 deletion completed in 6.272416924s

â€¢ [SLOW TEST:29.421 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:36:12.911: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:36:13.013: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 22 02:36:18.020: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 22 02:36:18.020: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul 22 02:36:18.050: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3048,SelfLink:/apis/apps/v1/namespaces/deployment-3048/deployments/test-cleanup-deployment,UID:b05fda03-ac3b-11e9-b4b6-6c92bf130c27,ResourceVersion:2310080,Generation:1,CreationTimestamp:2019-07-22 04:46:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul 22 02:36:18.058: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jul 22 02:36:18.058: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul 22 02:36:18.058: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-3048,SelfLink:/apis/apps/v1/namespaces/deployment-3048/replicasets/test-cleanup-controller,UID:ad9bbaa8-ac3b-11e9-b4b6-6c92bf130c27,ResourceVersion:2310081,Generation:1,CreationTimestamp:2019-07-22 04:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b05fda03-ac3b-11e9-b4b6-6c92bf130c27 0x4002055a47 0x4002055a48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 22 02:36:18.080: INFO: Pod "test-cleanup-controller-lv22v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-lv22v,GenerateName:test-cleanup-controller-,Namespace:deployment-3048,SelfLink:/api/v1/namespaces/deployment-3048/pods/test-cleanup-controller-lv22v,UID:ad9de33a-ac3b-11e9-9d2b-6c92bf900680,ResourceVersion:2310074,Generation:0,CreationTimestamp:2019-07-22 04:46:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller ad9bbaa8-ac3b-11e9-b4b6-6c92bf130c27 0x4000348e67 0x4000348e68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4cbrz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4cbrz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4cbrz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0x4000348f10} {node.kubernetes.io/unreachable Exists  NoExecute 0x4000348f60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:46:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:46:35 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:46:35 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-22 04:46:32 +0000 UTC  }],Message:,Reason:,HostIP:100.2.97.90,PodIP:10.233.91.106,StartTime:2019-07-22 04:46:32 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-22 04:46:34 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker://sha256:4229b69b3ac4ca493123375ce9236dbf8eac859289ce74aea8a0aedc8dd96afb docker://5d686a47f6b64ccb8e83daf7cf333bf82e2eb40c9d8664137c848ef41821259f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:36:18.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3048" for this suite.
Jul 22 02:36:24.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:36:24.335: INFO: namespace deployment-3048 deletion completed in 6.228184098s

â€¢ [SLOW TEST:11.423 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:36:24.335: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul 22 02:36:24.399: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:36:32.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-399" for this suite.
Jul 22 02:36:56.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:36:56.838: INFO: namespace init-container-399 deletion completed in 24.207952462s

â€¢ [SLOW TEST:32.503 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:36:56.838: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 22 02:36:56.932: INFO: Waiting up to 5m0s for pod "pod-92de0073-ac29-11e9-906c-f6b46dea7a80" in namespace "emptydir-8832" to be "success or failure"
Jul 22 02:36:56.937: INFO: Pod "pod-92de0073-ac29-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624648ms
Jul 22 02:36:58.944: INFO: Pod "pod-92de0073-ac29-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011989701s
Jul 22 02:37:00.950: INFO: Pod "pod-92de0073-ac29-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018782107s
STEP: Saw pod success
Jul 22 02:37:00.950: INFO: Pod "pod-92de0073-ac29-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:37:00.956: INFO: Trying to get logs from node k8s1 pod pod-92de0073-ac29-11e9-906c-f6b46dea7a80 container test-container: <nil>
STEP: delete the pod
Jul 22 02:37:00.993: INFO: Waiting for pod pod-92de0073-ac29-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:37:00.999: INFO: Pod pod-92de0073-ac29-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:37:00.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8832" for this suite.
Jul 22 02:37:07.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:37:07.177: INFO: namespace emptydir-8832 deletion completed in 6.170473387s

â€¢ [SLOW TEST:10.339 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:37:07.177: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 22 02:37:07.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6112'
Jul 22 02:37:07.531: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 22 02:37:07.531: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul 22 02:37:07.540: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jul 22 02:37:07.547: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul 22 02:37:07.574: INFO: scanned /root for discovery docs: <nil>
Jul 22 02:37:07.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-6112'
Jul 22 02:37:23.514: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 22 02:37:23.514: INFO: stdout: "Created e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862\nScaling up e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 22 02:37:23.514: INFO: stdout: "Created e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862\nScaling up e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 22 02:37:23.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-6112'
Jul 22 02:37:23.725: INFO: stderr: ""
Jul 22 02:37:23.725: INFO: stdout: "e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862-xm9cx "
Jul 22 02:37:23.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862-xm9cx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6112'
Jul 22 02:37:23.952: INFO: stderr: ""
Jul 22 02:37:23.952: INFO: stdout: "true"
Jul 22 02:37:23.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 get pods e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862-xm9cx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6112'
Jul 22 02:37:24.141: INFO: stderr: ""
Jul 22 02:37:24.141: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 22 02:37:24.141: INFO: e2e-test-nginx-rc-7b867e999a7e8e2b2c14083b9c932862-xm9cx is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jul 22 02:37:24.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 delete rc e2e-test-nginx-rc --namespace=kubectl-6112'
Jul 22 02:37:24.390: INFO: stderr: ""
Jul 22 02:37:24.391: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:37:24.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6112" for this suite.
Jul 22 02:37:30.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:37:30.579: INFO: namespace kubectl-6112 deletion completed in 6.18091198s

â€¢ [SLOW TEST:23.402 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:37:30.579: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:37:30.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-442" for this suite.
Jul 22 02:37:36.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:37:36.840: INFO: namespace services-442 deletion completed in 6.168707877s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:6.261 seconds]
[sig-network] Services
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:37:36.841: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul 22 02:37:36.970: INFO: Waiting up to 5m0s for pod "downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80" in namespace "downward-api-1295" to be "success or failure"
Jul 22 02:37:36.976: INFO: Pod "downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083458ms
Jul 22 02:37:38.982: INFO: Pod "downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011271927s
Jul 22 02:37:40.992: INFO: Pod "downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02142139s
STEP: Saw pod success
Jul 22 02:37:40.992: INFO: Pod "downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80" satisfied condition "success or failure"
Jul 22 02:37:41.007: INFO: Trying to get logs from node k8s1 pod downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80 container dapi-container: <nil>
STEP: delete the pod
Jul 22 02:37:41.049: INFO: Waiting for pod downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80 to disappear
Jul 22 02:37:41.055: INFO: Pod downward-api-aaba6504-ac29-11e9-906c-f6b46dea7a80 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:37:41.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1295" for this suite.
Jul 22 02:37:47.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:37:47.258: INFO: namespace downward-api-1295 deletion completed in 6.195970321s

â€¢ [SLOW TEST:10.418 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:37:47.259: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul 22 02:37:47.323: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 22 02:37:47.336: INFO: Waiting for terminating namespaces to be deleted...
Jul 22 02:37:47.342: INFO: 
Logging pods the kubelet thinks is on node k8s1 before test
Jul 22 02:37:47.360: INFO: local-path-provisioner-546d678b86-qlhkz from local-path-storage started at 2019-07-12 06:09:22 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container local-path-provisioner ready: true, restart count 1
Jul 22 02:37:47.360: INFO: prometheus-adapter-d7ff7d6cd-b9689 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jul 22 02:37:47.360: INFO: kube-state-metrics-7d47cbc8c5-w92wp from monitoring started at 2019-07-19 06:26:32 +0000 UTC (4 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container addon-resizer ready: true, restart count 0
Jul 22 02:37:47.360: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jul 22 02:37:47.360: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jul 22 02:37:47.360: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jul 22 02:37:47.360: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-75w7m from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 22 02:37:47.360: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 22 02:37:47.360: INFO: kube-proxy-5vpfs from kube-system started at 2019-07-11 19:32:18 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 02:37:47.360: INFO: listening-dragon-rabbitmq-ha-0 from default started at 2019-07-17 07:12:45 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container rabbitmq-ha ready: true, restart count 1
Jul 22 02:37:47.360: INFO: nginx-deployment-6f478d8d8-t2dws from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.360: INFO: nginx-deployment-6f478d8d8-cb865 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.360: INFO: kube-scheduler-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.360: INFO: nginx-deployment-6f478d8d8-8kpvr from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.360: INFO: ingress-nginx-controller-bhvc9 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container ingress-nginx-controller ready: true, restart count 6
Jul 22 02:37:47.360: INFO: coredns-848c785ddb-rghg5 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container coredns ready: true, restart count 0
Jul 22 02:37:47.360: INFO: nginx-deployment-6f478d8d8-v5r52 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.360: INFO: kube-controller-manager-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.360: INFO: kube-apiserver-k8s1 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.360: INFO: node-exporter-dtt5q from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 02:37:47.360: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 02:37:47.360: INFO: etcdtest2-57bf45d5c8-9xj4s from default started at 2019-07-22 01:41:31 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container etcdtest2 ready: true, restart count 24
Jul 22 02:37:47.360: INFO: calico-node-qfw58 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 02:37:47.360: INFO: nginx-deployment-6f478d8d8-znlw8 from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.360: INFO: tiller-deploy-5f4c64779f-jrkt8 from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.360: INFO: 	Container tiller ready: true, restart count 0
Jul 22 02:37:47.360: INFO: 
Logging pods the kubelet thinks is on node k8s2 before test
Jul 22 02:37:47.379: INFO: nginx-deployment-6f478d8d8-p4pgc from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.379: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.379: INFO: alertmanager-main-0 from monitoring started at 2019-07-19 06:26:32 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.379: INFO: 	Container alertmanager ready: true, restart count 0
Jul 22 02:37:47.379: INFO: 	Container config-reloader ready: true, restart count 0
Jul 22 02:37:47.379: INFO: node-exporter-97jbc from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.379: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jul 22 02:37:47.380: INFO: 	Container node-exporter ready: true, restart count 0
Jul 22 02:37:47.380: INFO: coredns-848c785ddb-r66tb from kube-system started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container coredns ready: true, restart count 0
Jul 22 02:37:47.380: INFO: nginx-deployment-6f478d8d8-smzdp from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.380: INFO: kube-apiserver-k8s2 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.380: INFO: ingress-nginx-controller-6drg8 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container ingress-nginx-controller ready: true, restart count 10
Jul 22 02:37:47.380: INFO: grafana-5ccccd76c8-nkcbl from monitoring started at 2019-07-11 20:05:14 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container grafana ready: true, restart count 0
Jul 22 02:37:47.380: INFO: prometheus-operator-9695d44c6-kjvfp from monitoring started at 2019-07-11 20:16:54 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container prometheus-operator ready: true, restart count 0
Jul 22 02:37:47.380: INFO: calico-node-zjpdk from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container calico-node ready: true, restart count 0
Jul 22 02:37:47.380: INFO: calico-kube-controllers-5bc4f6d669-l2mgl from kube-system started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jul 22 02:37:47.380: INFO: dns-autoscaler-554d755c54-qnnsl from kube-system started at 2019-07-11 19:34:10 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container autoscaler ready: true, restart count 0
Jul 22 02:37:47.380: INFO: prometheus-k8s-0 from monitoring started at 2019-07-11 20:05:28 +0000 UTC (3 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 02:37:47.380: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 02:37:47.380: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 22 02:37:47.380: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-9htdd from heptio-sonobuoy started at 2019-07-22 03:21:11 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 22 02:37:47.380: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 22 02:37:47.380: INFO: kube-proxy-dlvss from kube-system started at 2019-07-11 19:32:23 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container kube-proxy ready: true, restart count 0
Jul 22 02:37:47.380: INFO: nginx-deployment-6f478d8d8-vb77p from deployment-9981 started at 2019-07-18 03:35:26 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.380: INFO: nginx-deployment-6f478d8d8-xrrg6 from deployment-9981 started at 2019-07-19 06:26:32 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.380: INFO: listening-dragon-rabbitmq-ha-1 from default started at 2019-07-17 07:16:09 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 02:37:47.380: INFO: nginx-deployment-6f478d8d8-pvn55 from deployment-9981 started at 2019-07-18 03:35:25 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.380: INFO: 	Container nginx ready: true, restart count 0
Jul 22 02:37:47.380: INFO: 
Logging pods the kubelet thinks is on node k8s3 before test
Jul 22 02:37:47.395: INFO: kube-apiserver-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.395: INFO: kube-scheduler-k8s3 from kube-system started at <nil> (0 container statuses recorded)
Jul 22 02:37:47.395: INFO: kube-proxy-lmr2d from kube-system started at 2019-07-11 19:32:28 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.395: INFO: 	Container kube-proxy ready: true, restart count 1
Jul 22 02:37:47.395: INFO: node-exporter-hg26v from monitoring started at 2019-07-11 20:05:16 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.395: INFO: 	Container kube-rbac-proxy ready: true, restart count 1
Jul 22 02:37:47.395: INFO: 	Container node-exporter ready: true, restart count 1
Jul 22 02:37:47.395: INFO: sonobuoy-systemd-logs-daemon-set-d59ef0e112294f58-dk2k4 from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.395: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jul 22 02:37:47.395: INFO: 	Container systemd-logs ready: true, restart count 1
Jul 22 02:37:47.395: INFO: ingress-nginx-controller-vhvx7 from ingress-nginx started at 2019-07-12 06:07:59 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.395: INFO: 	Container ingress-nginx-controller ready: true, restart count 3
Jul 22 02:37:47.395: INFO: prometheus-k8s-1 from monitoring started at 2019-07-18 22:51:45 +0000 UTC (3 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container prometheus ready: true, restart count 1
Jul 22 02:37:47.396: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Jul 22 02:37:47.396: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Jul 22 02:37:47.396: INFO: testetcdxyys-769d48b897-fdl8m from default started at 2019-07-18 23:02:11 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container testetcdxyys ready: true, restart count 0
Jul 22 02:37:47.396: INFO: sonobuoy-e2e-job-3b2fad7799ec403c from heptio-sonobuoy started at 2019-07-22 01:03:45 +0000 UTC (2 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container e2e ready: true, restart count 0
Jul 22 02:37:47.396: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 22 02:37:47.396: INFO: calico-node-wqns7 from kube-system started at 2019-07-11 19:32:30 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container calico-node ready: true, restart count 1
Jul 22 02:37:47.396: INFO: listening-dragon-rabbitmq-ha-2 from default started at 2019-07-18 22:52:25 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container rabbitmq-ha ready: true, restart count 0
Jul 22 02:37:47.396: INFO: etcdtest-cbfd679c7-9cksx from default started at 2019-07-21 23:08:38 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container etcdtest ready: false, restart count 0
Jul 22 02:37:47.396: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-22 01:03:40 +0000 UTC (1 container statuses recorded)
Jul 22 02:37:47.396: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b3a1547fa7c194], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:37:48.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6295" for this suite.
Jul 22 02:37:54.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:37:54.652: INFO: namespace sched-pred-6295 deletion completed in 6.174313155s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.394 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:37:54.653: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:38:54.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2321" for this suite.
Jul 22 02:39:18.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:39:18.955: INFO: namespace container-probe-2321 deletion completed in 24.192861119s

â€¢ [SLOW TEST:84.302 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:39:18.956: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul 22 02:39:43.059: INFO: Container started at 2019-07-22 02:39:22 +0000 UTC, pod became ready at 2019-07-22 02:39:42 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:39:43.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9418" for this suite.
Jul 22 02:40:07.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:40:07.260: INFO: namespace container-probe-9418 deletion completed in 24.194574956s

â€¢ [SLOW TEST:48.305 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:40:07.261: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul 22 02:40:07.328: INFO: namespace kubectl-9203
Jul 22 02:40:07.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 create -f - --namespace=kubectl-9203'
Jul 22 02:40:07.627: INFO: stderr: ""
Jul 22 02:40:07.628: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 22 02:40:08.635: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:40:08.635: INFO: Found 0 / 1
Jul 22 02:40:09.635: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:40:09.635: INFO: Found 0 / 1
Jul 22 02:40:10.633: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:40:10.633: INFO: Found 0 / 1
Jul 22 02:40:11.635: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:40:11.635: INFO: Found 1 / 1
Jul 22 02:40:11.635: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 22 02:40:11.640: INFO: Selector matched 1 pods for map[app:redis]
Jul 22 02:40:11.640: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 22 02:40:11.640: INFO: wait on redis-master startup in kubectl-9203 
Jul 22 02:40:11.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 logs redis-master-9pz66 redis-master --namespace=kubectl-9203'
Jul 22 02:40:11.849: INFO: stderr: ""
Jul 22 02:40:11.849: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jul 04:50:11.376 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jul 04:50:11.376 # Server started, Redis version 3.2.12\n1:M 22 Jul 04:50:11.376 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jul 04:50:11.376 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 22 02:40:11.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9203'
Jul 22 02:40:12.077: INFO: stderr: ""
Jul 22 02:40:12.077: INFO: stdout: "service/rm2 exposed\n"
Jul 22 02:40:12.086: INFO: Service rm2 in namespace kubectl-9203 found.
STEP: exposing service
Jul 22 02:40:14.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-346668565 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9203'
Jul 22 02:40:14.332: INFO: stderr: ""
Jul 22 02:40:14.332: INFO: stdout: "service/rm3 exposed\n"
Jul 22 02:40:14.339: INFO: Service rm3 in namespace kubectl-9203 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:40:16.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9203" for this suite.
Jul 22 02:40:40.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:40:40.542: INFO: namespace kubectl-9203 deletion completed in 24.171171435s

â€¢ [SLOW TEST:33.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:40:40.542: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-18354a05-ac2a-11e9-906c-f6b46dea7a80
STEP: Creating configMap with name cm-test-opt-upd-18354a8f-ac2a-11e9-906c-f6b46dea7a80
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-18354a05-ac2a-11e9-906c-f6b46dea7a80
STEP: Updating configmap cm-test-opt-upd-18354a8f-ac2a-11e9-906c-f6b46dea7a80
STEP: Creating configMap with name cm-test-opt-create-18354ada-ac2a-11e9-906c-f6b46dea7a80
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:42:03.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2348" for this suite.
Jul 22 02:42:27.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:42:27.587: INFO: namespace projected-2348 deletion completed in 24.159473986s

â€¢ [SLOW TEST:107.045 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul 22 02:42:27.587: INFO: >>> kubeConfig: /tmp/kubeconfig-346668565
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 22 02:42:32.220: INFO: Successfully updated pod "pod-update-58002dd2-ac2a-11e9-906c-f6b46dea7a80"
STEP: verifying the updated pod is in kubernetes
Jul 22 02:42:32.230: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul 22 02:42:32.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-12" for this suite.
Jul 22 02:42:56.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 22 02:42:56.420: INFO: namespace pods-12 deletion completed in 24.183262762s

â€¢ [SLOW TEST:28.833 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.1-beta.0.44+b7394102d6ef77/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJul 22 02:42:56.421: INFO: Running AfterSuite actions on all nodes
Jul 22 02:42:56.421: INFO: Running AfterSuite actions on node 1
Jul 22 02:42:56.421: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5943.349 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h39m7.183146137s
Test Suite Passed
