I0622 05:09:33.887031      18 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-834690535
I0622 05:09:33.887824      18 e2e.go:240] Starting e2e run "eb9d5a15-94ab-11e9-8f59-1e22372c056e" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1561180172 - Will randomize all specs
Will run 204 of 3585 specs

Jun 22 05:09:34.263: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:09:34.270: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 05:09:34.569: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 05:09:34.767: INFO: 37 / 37 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 05:09:34.767: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jun 22 05:09:34.767: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 05:09:34.794: INFO: 7 / 7 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 22 05:09:34.794: INFO: e2e test version: v1.14.3
Jun 22 05:09:34.795: INFO: kube-apiserver version: v1.14.3
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:09:34.795: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
Jun 22 05:09:36.443: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 22 05:09:36.469: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6779
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jun 22 05:09:36.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 api-versions'
Jun 22 05:09:37.555: INFO: stderr: ""
Jun 22 05:09:37.555: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.istio.io/v1alpha1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\nconfig.istio.io/v1alpha2\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncustom.metrics.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.istio.io/v1alpha3\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nrbac.istio.io/v1alpha1\nrds.iop.com/v1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:09:37.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6779" for this suite.
Jun 22 05:09:47.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:09:49.310: INFO: namespace kubectl-6779 deletion completed in 11.745036616s

• [SLOW TEST:14.514 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:09:49.310: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4853
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:09:50.829: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e" in namespace "projected-4853" to be "success or failure"
Jun 22 05:09:50.835: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.60871ms
Jun 22 05:09:52.873: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043831559s
Jun 22 05:09:54.896: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067342129s
Jun 22 05:09:56.951: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.121628533s
Jun 22 05:09:58.956: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.127492043s
STEP: Saw pod success
Jun 22 05:09:58.957: INFO: Pod "downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:09:58.961: INFO: Trying to get logs from node slave7 pod downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 05:09:59.171: INFO: Waiting for pod downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e to disappear
Jun 22 05:09:59.180: INFO: Pod downwardapi-volume-f677ee1b-94ab-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:09:59.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4853" for this suite.
Jun 22 05:10:11.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:10:13.126: INFO: namespace projected-4853 deletion completed in 13.925971367s

• [SLOW TEST:23.816 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:10:13.126: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7948
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 05:10:14.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7948'
Jun 22 05:10:20.325: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 05:10:20.325: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jun 22 05:10:25.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7948'
Jun 22 05:10:25.452: INFO: stderr: ""
Jun 22 05:10:25.452: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:10:25.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7948" for this suite.
Jun 22 05:12:00.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:12:01.794: INFO: namespace kubectl-7948 deletion completed in 1m36.332729965s

• [SLOW TEST:108.669 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:12:01.795: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1256
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-44fd1eea-94ac-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 05:12:02.777: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e" in namespace "projected-1256" to be "success or failure"
Jun 22 05:12:03.058: INFO: Pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 280.84062ms
Jun 22 05:12:05.076: INFO: Pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298588106s
Jun 22 05:12:07.421: INFO: Pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644158118s
Jun 22 05:12:09.438: INFO: Pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.661134741s
STEP: Saw pod success
Jun 22 05:12:09.438: INFO: Pod "pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:12:09.457: INFO: Trying to get logs from node slave8 pod pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:12:09.868: INFO: Waiting for pod pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e to disappear
Jun 22 05:12:10.001: INFO: Pod pod-projected-secrets-45150f61-94ac-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:12:10.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1256" for this suite.
Jun 22 05:12:24.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:12:26.120: INFO: namespace projected-1256 deletion completed in 16.018210547s

• [SLOW TEST:24.326 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:12:26.121: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4769
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:12:27.008: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e" in namespace "downward-api-4769" to be "success or failure"
Jun 22 05:12:27.016: INFO: Pod "downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.101386ms
Jun 22 05:12:29.029: INFO: Pod "downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021272219s
Jun 22 05:12:31.445: INFO: Pod "downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.436503505s
STEP: Saw pod success
Jun 22 05:12:31.445: INFO: Pod "downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:12:31.497: INFO: Trying to get logs from node slave6 pod downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 05:12:32.880: INFO: Waiting for pod downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e to disappear
Jun 22 05:12:33.274: INFO: Pod downwardapi-volume-53954b7b-94ac-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:12:33.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4769" for this suite.
Jun 22 05:12:45.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:12:47.501: INFO: namespace downward-api-4769 deletion completed in 14.210507554s

• [SLOW TEST:21.381 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:12:47.502: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-8929
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8929
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:12:48.721: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:13:20.238: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.203.187 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8929 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:13:20.238: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:13:21.675: INFO: Found all expected endpoints: [netserver-0]
Jun 22 05:13:21.842: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.17.189 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8929 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:13:21.842: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:13:23.238: INFO: Found all expected endpoints: [netserver-1]
Jun 22 05:13:23.247: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.190.186 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8929 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:13:23.247: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:13:24.496: INFO: Found all expected endpoints: [netserver-2]
Jun 22 05:13:24.567: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.151.174.220 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8929 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:13:24.567: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:13:25.820: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:13:25.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8929" for this suite.
Jun 22 05:14:10.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:14:11.765: INFO: namespace pod-network-test-8929 deletion completed in 45.774294728s

• [SLOW TEST:84.263 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:14:11.765: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9154
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-939848af-94ac-11e9-8f59-1e22372c056e
STEP: Creating configMap with name cm-test-opt-upd-939848f7-94ac-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-939848af-94ac-11e9-8f59-1e22372c056e
STEP: Updating configmap cm-test-opt-upd-939848f7-94ac-11e9-8f59-1e22372c056e
STEP: Creating configMap with name cm-test-opt-create-9398490a-94ac-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:15:55.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9154" for this suite.
Jun 22 05:16:35.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:16:37.084: INFO: namespace projected-9154 deletion completed in 41.962811598s

• [SLOW TEST:145.319 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:16:37.085: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7642
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 22 05:16:39.017: INFO: namespace kubectl-7642
Jun 22 05:16:39.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7642'
Jun 22 05:16:40.020: INFO: stderr: ""
Jun 22 05:16:40.020: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 05:16:41.293: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:41.294: INFO: Found 0 / 1
Jun 22 05:16:42.045: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:42.045: INFO: Found 0 / 1
Jun 22 05:16:43.129: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:43.129: INFO: Found 0 / 1
Jun 22 05:16:44.209: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:44.209: INFO: Found 0 / 1
Jun 22 05:16:45.825: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:45.825: INFO: Found 1 / 1
Jun 22 05:16:45.825: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 05:16:45.832: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 05:16:45.832: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 05:16:45.832: INFO: wait on redis-master startup in kubectl-7642 
Jun 22 05:16:45.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 logs redis-master-g6p5k redis-master --namespace=kubectl-7642'
Jun 22 05:16:45.999: INFO: stderr: ""
Jun 22 05:16:45.999: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 05:16:44.071 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 05:16:44.071 # Server started, Redis version 3.2.12\n1:M 22 Jun 05:16:44.071 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 05:16:44.071 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 22 05:16:45.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7642'
Jun 22 05:16:46.763: INFO: stderr: ""
Jun 22 05:16:46.763: INFO: stdout: "service/rm2 exposed\n"
Jun 22 05:16:46.772: INFO: Service rm2 in namespace kubectl-7642 found.
STEP: exposing service
Jun 22 05:16:48.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7642'
Jun 22 05:16:49.637: INFO: stderr: ""
Jun 22 05:16:49.637: INFO: stdout: "service/rm3 exposed\n"
Jun 22 05:16:49.968: INFO: Service rm3 in namespace kubectl-7642 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:16:52.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7642" for this suite.
Jun 22 05:17:32.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:17:34.953: INFO: namespace kubectl-7642 deletion completed in 42.730290397s

• [SLOW TEST:57.868 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:17:34.953: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7490
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:18:35.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7490" for this suite.
Jun 22 05:19:14.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:19:17.678: INFO: namespace container-probe-7490 deletion completed in 41.889354725s

• [SLOW TEST:102.725 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:19:17.678: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-493ff842-94ad-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 05:19:19.337: INFO: Waiting up to 5m0s for pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e" in namespace "secrets-4597" to be "success or failure"
Jun 22 05:19:19.637: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 300.009987ms
Jun 22 05:19:21.884: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.546726903s
Jun 22 05:19:24.170: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.832739316s
Jun 22 05:19:26.175: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.837796765s
Jun 22 05:19:28.206: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.869074516s
STEP: Saw pod success
Jun 22 05:19:28.206: INFO: Pod "pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:19:28.214: INFO: Trying to get logs from node slave5 pod pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:19:28.741: INFO: Waiting for pod pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e to disappear
Jun 22 05:19:28.746: INFO: Pod pod-secrets-49517734-94ad-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:19:28.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4597" for this suite.
Jun 22 05:19:41.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:19:44.100: INFO: namespace secrets-4597 deletion completed in 15.336175939s

• [SLOW TEST:26.422 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:19:44.101: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-242
Jun 22 05:19:53.622: INFO: Started pod liveness-exec in namespace container-probe-242
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 05:19:53.627: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:23:55.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-242" for this suite.
Jun 22 05:24:07.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:24:09.521: INFO: namespace container-probe-242 deletion completed in 13.968795225s

• [SLOW TEST:265.420 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:24:09.521: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7874
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-lwn9
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 05:24:11.331: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-lwn9" in namespace "subpath-7874" to be "success or failure"
Jun 22 05:24:11.574: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Pending", Reason="", readiness=false. Elapsed: 242.932082ms
Jun 22 05:24:13.935: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604077498s
Jun 22 05:24:16.029: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.698758913s
Jun 22 05:24:18.057: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 6.726086906s
Jun 22 05:24:20.116: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 8.785026117s
Jun 22 05:24:22.228: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 10.897615774s
Jun 22 05:24:24.336: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 13.005570123s
Jun 22 05:24:26.343: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 15.012615229s
Jun 22 05:24:28.508: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 17.176833018s
Jun 22 05:24:30.636: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 19.305406664s
Jun 22 05:24:32.645: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 21.313810289s
Jun 22 05:24:34.653: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Running", Reason="", readiness=true. Elapsed: 23.3227103s
Jun 22 05:24:37.077: INFO: Pod "pod-subpath-test-projected-lwn9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 25.74590736s
STEP: Saw pod success
Jun 22 05:24:37.077: INFO: Pod "pod-subpath-test-projected-lwn9" satisfied condition "success or failure"
Jun 22 05:24:37.220: INFO: Trying to get logs from node slave6 pod pod-subpath-test-projected-lwn9 container test-container-subpath-projected-lwn9: <nil>
STEP: delete the pod
Jun 22 05:24:37.547: INFO: Waiting for pod pod-subpath-test-projected-lwn9 to disappear
Jun 22 05:24:37.552: INFO: Pod pod-subpath-test-projected-lwn9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-lwn9
Jun 22 05:24:37.552: INFO: Deleting pod "pod-subpath-test-projected-lwn9" in namespace "subpath-7874"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:24:37.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7874" for this suite.
Jun 22 05:24:47.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:24:49.642: INFO: namespace subpath-7874 deletion completed in 12.053529344s

• [SLOW TEST:40.121 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:24:49.644: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 05:24:58.858: INFO: Waiting up to 5m0s for pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e" in namespace "pods-8981" to be "success or failure"
Jun 22 05:24:58.979: INFO: Pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 121.409504ms
Jun 22 05:25:01.066: INFO: Pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208259018s
Jun 22 05:25:03.072: INFO: Pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.214149704s
Jun 22 05:25:05.113: INFO: Pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.255499819s
STEP: Saw pod success
Jun 22 05:25:05.117: INFO: Pod "client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:25:05.130: INFO: Trying to get logs from node slave5 pod client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e container env3cont: <nil>
STEP: delete the pod
Jun 22 05:25:05.739: INFO: Waiting for pod client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e to disappear
Jun 22 05:25:05.770: INFO: Pod client-envvars-13a395c2-94ae-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:25:05.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8981" for this suite.
Jun 22 05:25:55.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:25:57.620: INFO: namespace pods-8981 deletion completed in 51.839148182s

• [SLOW TEST:67.977 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:25:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9601
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 22 05:25:58.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-9601'
Jun 22 05:26:04.897: INFO: stderr: ""
Jun 22 05:26:04.897: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 05:26:04.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9601'
Jun 22 05:26:05.430: INFO: stderr: ""
Jun 22 05:26:05.430: INFO: stdout: "update-demo-nautilus-qqd54 update-demo-nautilus-rk85j "
Jun 22 05:26:05.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-qqd54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9601'
Jun 22 05:26:05.891: INFO: stderr: ""
Jun 22 05:26:05.891: INFO: stdout: ""
Jun 22 05:26:05.891: INFO: update-demo-nautilus-qqd54 is created but not running
Jun 22 05:26:10.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9601'
Jun 22 05:26:11.036: INFO: stderr: ""
Jun 22 05:26:11.036: INFO: stdout: "update-demo-nautilus-qqd54 update-demo-nautilus-rk85j "
Jun 22 05:26:11.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-qqd54 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9601'
Jun 22 05:26:11.324: INFO: stderr: ""
Jun 22 05:26:11.324: INFO: stdout: "true"
Jun 22 05:26:11.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-qqd54 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9601'
Jun 22 05:26:11.543: INFO: stderr: ""
Jun 22 05:26:11.543: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:26:11.543: INFO: validating pod update-demo-nautilus-qqd54
Jun 22 05:26:11.687: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:26:11.687: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:26:11.687: INFO: update-demo-nautilus-qqd54 is verified up and running
Jun 22 05:26:11.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-rk85j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9601'
Jun 22 05:26:11.892: INFO: stderr: ""
Jun 22 05:26:11.892: INFO: stdout: "true"
Jun 22 05:26:11.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-rk85j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9601'
Jun 22 05:26:12.082: INFO: stderr: ""
Jun 22 05:26:12.082: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 05:26:12.082: INFO: validating pod update-demo-nautilus-rk85j
Jun 22 05:26:12.094: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 05:26:12.094: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 05:26:12.094: INFO: update-demo-nautilus-rk85j is verified up and running
STEP: using delete to clean up resources
Jun 22 05:26:12.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-9601'
Jun 22 05:26:12.382: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:26:12.382: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 05:26:12.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9601'
Jun 22 05:26:12.584: INFO: stderr: "No resources found.\n"
Jun 22 05:26:12.584: INFO: stdout: ""
Jun 22 05:26:12.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=update-demo --namespace=kubectl-9601 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 05:26:12.751: INFO: stderr: ""
Jun 22 05:26:12.751: INFO: stdout: "update-demo-nautilus-qqd54\nupdate-demo-nautilus-rk85j\n"
Jun 22 05:26:13.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9601'
Jun 22 05:26:13.365: INFO: stderr: "No resources found.\n"
Jun 22 05:26:13.366: INFO: stdout: ""
Jun 22 05:26:13.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=update-demo --namespace=kubectl-9601 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 05:26:13.636: INFO: stderr: ""
Jun 22 05:26:13.636: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:26:13.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9601" for this suite.
Jun 22 05:26:49.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:26:52.771: INFO: namespace kubectl-9601 deletion completed in 39.110627096s

• [SLOW TEST:55.150 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:26:52.771: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6100
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8593
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2864
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:27:11.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6100" for this suite.
Jun 22 05:27:23.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:27:26.013: INFO: namespace namespaces-6100 deletion completed in 14.739178384s
STEP: Destroying namespace "nsdeletetest-8593" for this suite.
Jun 22 05:27:26.085: INFO: Namespace nsdeletetest-8593 was already deleted
STEP: Destroying namespace "nsdeletetest-2864" for this suite.
Jun 22 05:27:38.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:27:40.230: INFO: namespace nsdeletetest-2864 deletion completed in 14.145644701s

• [SLOW TEST:47.459 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:27:40.231: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 22 05:27:41.243: INFO: Waiting up to 5m0s for pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e" in namespace "downward-api-4207" to be "success or failure"
Jun 22 05:27:41.552: INFO: Pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 308.610813ms
Jun 22 05:27:43.557: INFO: Pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313437945s
Jun 22 05:27:45.749: INFO: Pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505805048s
Jun 22 05:27:47.762: INFO: Pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.518548451s
STEP: Saw pod success
Jun 22 05:27:47.762: INFO: Pod "downward-api-747da47c-94ae-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:27:47.824: INFO: Trying to get logs from node slave7 pod downward-api-747da47c-94ae-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 05:27:48.303: INFO: Waiting for pod downward-api-747da47c-94ae-11e9-8f59-1e22372c056e to disappear
Jun 22 05:27:48.591: INFO: Pod downward-api-747da47c-94ae-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:27:48.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4207" for this suite.
Jun 22 05:27:58.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:28:00.485: INFO: namespace downward-api-4207 deletion completed in 11.864336836s

• [SLOW TEST:20.254 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:28:00.485: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-480
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
Jun 22 05:28:14.324: INFO: 5 pods remaining
Jun 22 05:28:14.324: INFO: 5 pods has nil DeletionTimestamp
Jun 22 05:28:14.324: INFO: 
STEP: Gathering metrics
W0622 05:28:19.329905      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 05:28:19.329: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:28:19.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-480" for this suite.
Jun 22 05:28:51.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:28:53.324: INFO: namespace gc-480 deletion completed in 33.831166805s

• [SLOW TEST:52.839 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:28:53.326: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3070
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 05:28:55.818: INFO: Create a RollingUpdate DaemonSet
Jun 22 05:28:55.974: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 05:28:56.197: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:56.197: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:56.197: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:56.428: INFO: Number of nodes with available pods: 0
Jun 22 05:28:56.428: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:28:57.746: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:57.746: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:57.746: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:58.039: INFO: Number of nodes with available pods: 0
Jun 22 05:28:58.039: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:28:58.502: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:58.502: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:58.502: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:58.602: INFO: Number of nodes with available pods: 0
Jun 22 05:28:58.602: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:28:59.549: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:59.549: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:59.549: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:28:59.854: INFO: Number of nodes with available pods: 0
Jun 22 05:28:59.854: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:29:00.548: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:00.548: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:00.548: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:00.561: INFO: Number of nodes with available pods: 0
Jun 22 05:29:00.561: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:29:01.761: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:01.761: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:01.761: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:02.168: INFO: Number of nodes with available pods: 1
Jun 22 05:29:02.168: INFO: Node slave5 is running more than one daemon pod
Jun 22 05:29:02.580: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:02.580: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:02.580: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:02.587: INFO: Number of nodes with available pods: 2
Jun 22 05:29:02.587: INFO: Node slave6 is running more than one daemon pod
Jun 22 05:29:03.436: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:03.436: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:03.436: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:03.440: INFO: Number of nodes with available pods: 3
Jun 22 05:29:03.440: INFO: Node slave6 is running more than one daemon pod
Jun 22 05:29:04.510: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:04.510: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:04.510: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:04.524: INFO: Number of nodes with available pods: 4
Jun 22 05:29:04.524: INFO: Number of running nodes: 4, number of available pods: 4
Jun 22 05:29:04.524: INFO: Update the DaemonSet to trigger a rollout
Jun 22 05:29:04.837: INFO: Updating DaemonSet daemon-set
Jun 22 05:29:12.383: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 05:29:12.707: INFO: Updating DaemonSet daemon-set
Jun 22 05:29:12.707: INFO: Make sure DaemonSet rollback is complete
Jun 22 05:29:12.730: INFO: Wrong image for pod: daemon-set-ll4sq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 05:29:12.730: INFO: Pod daemon-set-ll4sq is not available
Jun 22 05:29:12.879: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:12.879: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:12.879: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:13.884: INFO: Wrong image for pod: daemon-set-ll4sq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 05:29:13.884: INFO: Pod daemon-set-ll4sq is not available
Jun 22 05:29:13.889: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:13.889: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:13.889: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:14.885: INFO: Wrong image for pod: daemon-set-ll4sq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 05:29:14.885: INFO: Pod daemon-set-ll4sq is not available
Jun 22 05:29:14.892: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:14.892: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:14.892: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:15.935: INFO: Wrong image for pod: daemon-set-ll4sq. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 05:29:15.935: INFO: Pod daemon-set-ll4sq is not available
Jun 22 05:29:16.529: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:16.529: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:16.529: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:17.121: INFO: Pod daemon-set-mmmbs is not available
Jun 22 05:29:17.133: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:17.133: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 05:29:17.133: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3070, will wait for the garbage collector to delete the pods
Jun 22 05:29:17.381: INFO: Deleting DaemonSet.extensions daemon-set took: 179.081944ms
Jun 22 05:29:18.481: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.100214803s
Jun 22 05:29:35.885: INFO: Number of nodes with available pods: 0
Jun 22 05:29:35.885: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 05:29:35.890: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3070/daemonsets","resourceVersion":"648218"},"items":null}

Jun 22 05:29:35.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3070/pods","resourceVersion":"648218"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:29:35.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3070" for this suite.
Jun 22 05:29:50.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:29:52.364: INFO: namespace daemonsets-3070 deletion completed in 16.434711328s

• [SLOW TEST:59.038 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:29:52.365: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-987
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:29:53.290: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:30:26.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.190.195:8080/dial?request=hostName&protocol=http&host=10.151.190.194&port=8080&tries=1'] Namespace:pod-network-test-987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:30:26.859: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:30:27.195: INFO: Waiting for endpoints: map[]
Jun 22 05:30:27.366: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.190.195:8080/dial?request=hostName&protocol=http&host=10.151.17.198&port=8080&tries=1'] Namespace:pod-network-test-987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:30:27.366: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:30:28.173: INFO: Waiting for endpoints: map[]
Jun 22 05:30:28.229: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.190.195:8080/dial?request=hostName&protocol=http&host=10.151.174.227&port=8080&tries=1'] Namespace:pod-network-test-987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:30:28.229: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:30:28.587: INFO: Waiting for endpoints: map[]
Jun 22 05:30:28.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.190.195:8080/dial?request=hostName&protocol=http&host=10.151.203.194&port=8080&tries=1'] Namespace:pod-network-test-987 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:30:28.707: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:30:28.981: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:30:28.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-987" for this suite.
Jun 22 05:31:07.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:31:09.301: INFO: namespace pod-network-test-987 deletion completed in 39.916842627s

• [SLOW TEST:76.936 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:31:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-f13b6077-94ae-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 05:31:10.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e" in namespace "configmap-4651" to be "success or failure"
Jun 22 05:31:11.209: INFO: Pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 227.236857ms
Jun 22 05:31:13.220: INFO: Pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.238558972s
Jun 22 05:31:15.225: INFO: Pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.243530599s
Jun 22 05:31:17.280: INFO: Pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.298545506s
STEP: Saw pod success
Jun 22 05:31:17.280: INFO: Pod "pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:31:17.288: INFO: Trying to get logs from node slave5 pod pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:31:17.844: INFO: Waiting for pod pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e to disappear
Jun 22 05:31:17.848: INFO: Pod pod-configmaps-f169cf81-94ae-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:31:17.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4651" for this suite.
Jun 22 05:31:28.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:31:29.848: INFO: namespace configmap-4651 deletion completed in 11.980772356s

• [SLOW TEST:20.543 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:31:29.848: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1911
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-fd8c0343-94ae-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 05:31:31.147: INFO: Waiting up to 5m0s for pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e" in namespace "secrets-1911" to be "success or failure"
Jun 22 05:31:31.202: INFO: Pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 54.659705ms
Jun 22 05:31:33.278: INFO: Pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.1304928s
Jun 22 05:31:35.282: INFO: Pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.135079052s
Jun 22 05:31:37.288: INFO: Pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.140539639s
STEP: Saw pod success
Jun 22 05:31:37.288: INFO: Pod "pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:31:37.291: INFO: Trying to get logs from node slave8 pod pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:31:37.581: INFO: Waiting for pod pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e to disappear
Jun 22 05:31:37.636: INFO: Pod pod-secrets-fd971cbf-94ae-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:31:37.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1911" for this suite.
Jun 22 05:31:51.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:31:53.576: INFO: namespace secrets-1911 deletion completed in 15.911731038s

• [SLOW TEST:23.728 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:31:53.577: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-1059
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1059
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1059 to expose endpoints map[]
Jun 22 05:31:55.246: INFO: Get endpoints failed (290.517373ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 22 05:31:56.306: INFO: successfully validated that service multi-endpoint-test in namespace services-1059 exposes endpoints map[] (1.349635286s elapsed)
STEP: Creating pod pod1 in namespace services-1059
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1059 to expose endpoints map[pod1:[100]]
Jun 22 05:32:01.237: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.714761851s elapsed, will retry)
Jun 22 05:32:03.307: INFO: successfully validated that service multi-endpoint-test in namespace services-1059 exposes endpoints map[pod1:[100]] (6.784577365s elapsed)
STEP: Creating pod pod2 in namespace services-1059
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1059 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 22 05:32:08.600: INFO: Unexpected endpoints: found map[0cb23da5-94af-11e9-a9b0-fa163e4f9fd7:[100]], expected map[pod1:[100] pod2:[101]] (5.140666091s elapsed, will retry)
Jun 22 05:32:09.863: INFO: successfully validated that service multi-endpoint-test in namespace services-1059 exposes endpoints map[pod1:[100] pod2:[101]] (6.403949635s elapsed)
STEP: Deleting pod pod1 in namespace services-1059
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1059 to expose endpoints map[pod2:[101]]
Jun 22 05:32:10.667: INFO: successfully validated that service multi-endpoint-test in namespace services-1059 exposes endpoints map[pod2:[101]] (703.869884ms elapsed)
STEP: Deleting pod pod2 in namespace services-1059
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1059 to expose endpoints map[]
Jun 22 05:32:12.606: INFO: successfully validated that service multi-endpoint-test in namespace services-1059 exposes endpoints map[] (1.419127419s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:32:13.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1059" for this suite.
Jun 22 05:32:46.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:32:48.729: INFO: namespace services-1059 deletion completed in 34.752169144s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:55.153 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:32:48.731: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 05:32:49.630: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 05:32:49.695: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 22 05:32:54.863: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 05:32:54.863: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 05:32:55.003: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 05:32:55.271: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 05:32:57.281: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 05:32:57.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778375, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:32:59.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778375, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:33:01.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778376, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778381, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696778375, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 05:33:03.348: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 22 05:33:03.775: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2061,SelfLink:/apis/apps/v1/namespaces/deployment-2061/deployments/test-rolling-update-deployment,UID:2f96ddc8-94af-11e9-a9b0-fa163e4f9fd7,ResourceVersion:649090,Generation:1,CreationTimestamp:2019-06-22 05:32:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-22 05:32:56 +0000 UTC 2019-06-22 05:32:56 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-22 05:33:02 +0000 UTC 2019-06-22 05:32:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 05:33:03.792: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2061,SelfLink:/apis/apps/v1/namespaces/deployment-2061/replicasets/test-rolling-update-deployment-67599b4d9,UID:2fc5a525-94af-11e9-82ac-fa163e446741,ResourceVersion:649075,Generation:1,CreationTimestamp:2019-06-22 05:32:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2f96ddc8-94af-11e9-a9b0-fa163e4f9fd7 0xc0026b99f0 0xc0026b99f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 05:33:03.792: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 05:33:03.792: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2061,SelfLink:/apis/apps/v1/namespaces/deployment-2061/replicasets/test-rolling-update-controller,UID:2c786e0b-94af-11e9-a9b0-fa163e4f9fd7,ResourceVersion:649089,Generation:2,CreationTimestamp:2019-06-22 05:32:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 2f96ddc8-94af-11e9-a9b0-fa163e4f9fd7 0xc0026b9927 0xc0026b9928}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 05:33:03.972: INFO: Pod "test-rolling-update-deployment-67599b4d9-9wcvw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-9wcvw,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2061,SelfLink:/api/v1/namespaces/deployment-2061/pods/test-rolling-update-deployment-67599b4d9-9wcvw,UID:2fc7f566-94af-11e9-82ac-fa163e446741,ResourceVersion:649074,Generation:0,CreationTimestamp:2019-06-22 05:32:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 2fc5a525-94af-11e9-82ac-fa163e446741 0xc00256a270 0xc00256a271}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9xxnl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9xxnl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9xxnl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00256a2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00256a300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 05:32:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 05:33:00 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 05:33:00 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 05:32:55 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:10.151.203.196,StartTime:2019-06-22 05:32:55 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-22 05:32:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://288ac9df5a30df94e7dfd000211a40c6d5a5bc871af583816fd809bc9aeda59a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:33:03.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2061" for this suite.
Jun 22 05:33:18.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:33:20.274: INFO: namespace deployment-2061 deletion completed in 16.293751421s

• [SLOW TEST:31.544 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:33:20.275: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-288
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0622 05:34:03.458027      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 05:34:03.458: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:34:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-288" for this suite.
Jun 22 05:34:37.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:34:39.619: INFO: namespace gc-288 deletion completed in 36.155484599s

• [SLOW TEST:79.345 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:34:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6623
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:34:42.859: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e" in namespace "downward-api-6623" to be "success or failure"
Jun 22 05:34:43.245: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 385.647628ms
Jun 22 05:34:45.338: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.479167577s
Jun 22 05:34:47.398: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.539109965s
Jun 22 05:34:49.460: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.601070742s
Jun 22 05:34:51.494: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.635271464s
STEP: Saw pod success
Jun 22 05:34:51.494: INFO: Pod "downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:34:51.593: INFO: Trying to get logs from node slave5 pod downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 05:34:52.530: INFO: Waiting for pod downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e to disappear
Jun 22 05:34:52.541: INFO: Pod downwardapi-volume-6f7219b9-94af-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:34:52.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6623" for this suite.
Jun 22 05:35:07.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:35:09.829: INFO: namespace downward-api-6623 deletion completed in 16.960126343s

• [SLOW TEST:30.209 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:35:09.829: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3170
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-80d0adbd-94af-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:35:21.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3170" for this suite.
Jun 22 05:35:56.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:35:58.586: INFO: namespace configmap-3170 deletion completed in 36.749798155s

• [SLOW TEST:48.757 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:35:58.587: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-470
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-9dc783f9-94af-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 05:36:00.099: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e" in namespace "projected-470" to be "success or failure"
Jun 22 05:36:00.367: INFO: Pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 267.69805ms
Jun 22 05:36:02.376: INFO: Pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.277026231s
Jun 22 05:36:05.208: INFO: Pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109384495s
Jun 22 05:36:07.376: INFO: Pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.276669218s
STEP: Saw pod success
Jun 22 05:36:07.376: INFO: Pod "pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:36:07.586: INFO: Trying to get logs from node slave6 pod pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:36:09.241: INFO: Waiting for pod pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e to disappear
Jun 22 05:36:09.522: INFO: Pod pod-projected-secrets-9de268fd-94af-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:36:09.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-470" for this suite.
Jun 22 05:36:24.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:36:26.275: INFO: namespace projected-470 deletion completed in 16.397241316s

• [SLOW TEST:27.688 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:36:26.276: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6065
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jun 22 05:36:28.061: INFO: Waiting up to 5m0s for pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e" in namespace "containers-6065" to be "success or failure"
Jun 22 05:36:28.314: INFO: Pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 253.508024ms
Jun 22 05:36:30.332: INFO: Pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.271066048s
Jun 22 05:36:32.409: INFO: Pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.348225146s
Jun 22 05:36:34.611: INFO: Pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.55053941s
STEP: Saw pod success
Jun 22 05:36:34.611: INFO: Pod "client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:36:34.628: INFO: Trying to get logs from node slave7 pod client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 05:36:35.112: INFO: Waiting for pod client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e to disappear
Jun 22 05:36:35.154: INFO: Pod client-containers-ae6fad4c-94af-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:36:35.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6065" for this suite.
Jun 22 05:36:49.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:36:53.255: INFO: namespace containers-6065 deletion completed in 18.090927523s

• [SLOW TEST:26.980 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:36:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5947
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 05:36:56.098: INFO: Waiting up to 5m0s for pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e" in namespace "emptydir-5947" to be "success or failure"
Jun 22 05:36:56.193: INFO: Pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 94.81065ms
Jun 22 05:36:58.237: INFO: Pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.138819525s
Jun 22 05:37:00.497: INFO: Pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399077153s
Jun 22 05:37:03.159: INFO: Pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.060857466s
STEP: Saw pod success
Jun 22 05:37:03.159: INFO: Pod "pod-beec09b4-94af-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:37:03.646: INFO: Trying to get logs from node slave5 pod pod-beec09b4-94af-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 05:37:04.104: INFO: Waiting for pod pod-beec09b4-94af-11e9-8f59-1e22372c056e to disappear
Jun 22 05:37:04.119: INFO: Pod pod-beec09b4-94af-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:37:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5947" for this suite.
Jun 22 05:37:18.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:37:20.332: INFO: namespace emptydir-5947 deletion completed in 16.19240563s

• [SLOW TEST:27.076 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:37:20.333: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-2411
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:37:31.412: INFO: Pod name wrapped-volume-race-d4359eb5-94af-11e9-8f59-1e22372c056e: Found 0 pods out of 5
Jun 22 05:37:36.481: INFO: Pod name wrapped-volume-race-d4359eb5-94af-11e9-8f59-1e22372c056e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d4359eb5-94af-11e9-8f59-1e22372c056e in namespace emptydir-wrapper-2411, will wait for the garbage collector to delete the pods
Jun 22 05:37:57.542: INFO: Deleting ReplicationController wrapped-volume-race-d4359eb5-94af-11e9-8f59-1e22372c056e took: 291.831825ms
Jun 22 05:37:58.742: INFO: Terminating ReplicationController wrapped-volume-race-d4359eb5-94af-11e9-8f59-1e22372c056e pods took: 1.200262607s
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:38:46.311: INFO: Pod name wrapped-volume-race-0067361c-94b0-11e9-8f59-1e22372c056e: Found 0 pods out of 5
Jun 22 05:38:51.464: INFO: Pod name wrapped-volume-race-0067361c-94b0-11e9-8f59-1e22372c056e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0067361c-94b0-11e9-8f59-1e22372c056e in namespace emptydir-wrapper-2411, will wait for the garbage collector to delete the pods
Jun 22 05:39:18.267: INFO: Deleting ReplicationController wrapped-volume-race-0067361c-94b0-11e9-8f59-1e22372c056e took: 328.62421ms
Jun 22 05:39:20.367: INFO: Terminating ReplicationController wrapped-volume-race-0067361c-94b0-11e9-8f59-1e22372c056e pods took: 2.100264285s
STEP: Creating RC which spawns configmap-volume pods
Jun 22 05:40:06.329: INFO: Pod name wrapped-volume-race-3047c99d-94b0-11e9-8f59-1e22372c056e: Found 0 pods out of 5
Jun 22 05:40:12.117: INFO: Pod name wrapped-volume-race-3047c99d-94b0-11e9-8f59-1e22372c056e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-3047c99d-94b0-11e9-8f59-1e22372c056e in namespace emptydir-wrapper-2411, will wait for the garbage collector to delete the pods
Jun 22 05:40:39.022: INFO: Deleting ReplicationController wrapped-volume-race-3047c99d-94b0-11e9-8f59-1e22372c056e took: 700.842076ms
Jun 22 05:40:40.523: INFO: Terminating ReplicationController wrapped-volume-race-3047c99d-94b0-11e9-8f59-1e22372c056e pods took: 1.501355159s
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:41:31.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2411" for this suite.
Jun 22 05:42:17.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:42:20.367: INFO: namespace emptydir-wrapper-2411 deletion completed in 49.246793804s

• [SLOW TEST:300.035 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:42:20.367: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7170
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-8116785f-94b0-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 05:42:21.596: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e" in namespace "projected-7170" to be "success or failure"
Jun 22 05:42:21.883: INFO: Pod "pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 287.170862ms
Jun 22 05:42:23.972: INFO: Pod "pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376228184s
Jun 22 05:42:26.073: INFO: Pod "pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.477054355s
STEP: Saw pod success
Jun 22 05:42:26.073: INFO: Pod "pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:42:26.076: INFO: Trying to get logs from node slave8 pod pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 05:42:26.685: INFO: Waiting for pod pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e to disappear
Jun 22 05:42:26.904: INFO: Pod pod-projected-secrets-8128cb33-94b0-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:42:26.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7170" for this suite.
Jun 22 05:42:43.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:42:45.110: INFO: namespace projected-7170 deletion completed in 17.781771714s

• [SLOW TEST:24.743 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:42:45.111: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7776
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-8ff8c17c-94b0-11e9-8f59-1e22372c056e
STEP: Creating configMap with name cm-test-opt-upd-8ff8c1d1-94b0-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-8ff8c17c-94b0-11e9-8f59-1e22372c056e
STEP: Updating configmap cm-test-opt-upd-8ff8c1d1-94b0-11e9-8f59-1e22372c056e
STEP: Creating configMap with name cm-test-opt-create-8ff8c1e9-94b0-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:44:21.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7776" for this suite.
Jun 22 05:44:51.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:44:52.963: INFO: namespace configmap-7776 deletion completed in 31.680790573s

• [SLOW TEST:127.852 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:44:52.963: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-482
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 05:44:54.174: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"dc23869c-94b0-11e9-a9b0-fa163e4f9fd7", Controller:(*bool)(0xc00218f2d6), BlockOwnerDeletion:(*bool)(0xc00218f2d7)}}
Jun 22 05:44:54.216: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"dc0f0d6b-94b0-11e9-a9b0-fa163e4f9fd7", Controller:(*bool)(0xc0023444d6), BlockOwnerDeletion:(*bool)(0xc0023444d7)}}
Jun 22 05:44:54.224: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dc219214-94b0-11e9-a9b0-fa163e4f9fd7", Controller:(*bool)(0xc0023446a6), BlockOwnerDeletion:(*bool)(0xc0023446a7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:44:59.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-482" for this suite.
Jun 22 05:45:13.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:45:15.583: INFO: namespace gc-482 deletion completed in 16.17382506s

• [SLOW TEST:22.620 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:45:15.584: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3384
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jun 22 05:45:16.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-3384'
Jun 22 05:45:22.997: INFO: stderr: ""
Jun 22 05:45:22.997: INFO: stdout: "pod/pause created\n"
Jun 22 05:45:22.997: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 05:45:22.997: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3384" to be "running and ready"
Jun 22 05:45:23.206: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 209.489641ms
Jun 22 05:45:25.211: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.214091887s
Jun 22 05:45:27.531: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.533667321s
Jun 22 05:45:29.540: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.543047931s
Jun 22 05:45:29.540: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 05:45:29.540: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 22 05:45:29.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 label pods pause testing-label=testing-label-value --namespace=kubectl-3384'
Jun 22 05:45:29.807: INFO: stderr: ""
Jun 22 05:45:29.807: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 22 05:45:29.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pod pause -L testing-label --namespace=kubectl-3384'
Jun 22 05:45:30.212: INFO: stderr: ""
Jun 22 05:45:30.212: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          8s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 22 05:45:30.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 label pods pause testing-label- --namespace=kubectl-3384'
Jun 22 05:45:30.658: INFO: stderr: ""
Jun 22 05:45:30.658: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 22 05:45:30.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pod pause -L testing-label --namespace=kubectl-3384'
Jun 22 05:45:30.844: INFO: stderr: ""
Jun 22 05:45:30.844: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          8s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jun 22 05:45:30.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-3384'
Jun 22 05:45:32.572: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 05:45:32.572: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 05:45:32.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=pause --no-headers --namespace=kubectl-3384'
Jun 22 05:45:33.342: INFO: stderr: "No resources found.\n"
Jun 22 05:45:33.342: INFO: stdout: ""
Jun 22 05:45:33.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=pause --namespace=kubectl-3384 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 05:45:33.569: INFO: stderr: ""
Jun 22 05:45:33.569: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:45:33.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3384" for this suite.
Jun 22 05:45:49.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:45:51.369: INFO: namespace kubectl-3384 deletion completed in 17.794254834s

• [SLOW TEST:35.785 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:45:51.370: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-5684
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:46:36.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5684" for this suite.
Jun 22 05:46:50.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:46:52.550: INFO: namespace container-runtime-5684 deletion completed in 16.24828404s

• [SLOW TEST:61.180 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:46:52.551: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9241
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jun 22 05:46:53.708: INFO: Waiting up to 5m0s for pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e" in namespace "containers-9241" to be "success or failure"
Jun 22 05:46:53.769: INFO: Pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 61.118413ms
Jun 22 05:46:55.801: INFO: Pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092455209s
Jun 22 05:46:57.811: INFO: Pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.102952378s
Jun 22 05:46:59.967: INFO: Pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.258817565s
STEP: Saw pod success
Jun 22 05:46:59.967: INFO: Pod "client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:46:59.993: INFO: Trying to get logs from node slave6 pod client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 05:47:01.152: INFO: Waiting for pod client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e to disappear
Jun 22 05:47:01.159: INFO: Pod client-containers-2379b48e-94b1-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:47:01.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9241" for this suite.
Jun 22 05:47:15.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:47:18.281: INFO: namespace containers-9241 deletion completed in 17.056510235s

• [SLOW TEST:25.731 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:47:18.286: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8881
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jun 22 05:47:27.233: INFO: Pod pod-hostip-33153e09-94b1-11e9-8f59-1e22372c056e has hostIP: 192.168.202.68
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:47:27.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8881" for this suite.
Jun 22 05:48:04.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:48:05.781: INFO: namespace pods-8881 deletion completed in 38.534096449s

• [SLOW TEST:47.495 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:48:05.782: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5133
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 05:48:06.679: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 05:48:36.245: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.174.254:8080/dial?request=hostName&protocol=udp&host=10.151.203.203&port=8081&tries=1'] Namespace:pod-network-test-5133 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:48:36.245: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:48:36.390: INFO: Waiting for endpoints: map[]
Jun 22 05:48:36.430: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.174.254:8080/dial?request=hostName&protocol=udp&host=10.151.17.205&port=8081&tries=1'] Namespace:pod-network-test-5133 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:48:36.430: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:48:36.593: INFO: Waiting for endpoints: map[]
Jun 22 05:48:36.598: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.174.254:8080/dial?request=hostName&protocol=udp&host=10.151.174.253&port=8081&tries=1'] Namespace:pod-network-test-5133 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:48:36.598: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:48:36.756: INFO: Waiting for endpoints: map[]
Jun 22 05:48:36.760: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.151.174.254:8080/dial?request=hostName&protocol=udp&host=10.151.190.204&port=8081&tries=1'] Namespace:pod-network-test-5133 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 05:48:36.760: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 05:48:36.900: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:48:36.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5133" for this suite.
Jun 22 05:49:19.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:49:21.047: INFO: namespace pod-network-test-5133 deletion completed in 44.139208845s

• [SLOW TEST:75.265 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:49:21.047: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2768
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-7c0d98df-94b1-11e9-8f59-1e22372c056e
STEP: Creating secret with name s-test-opt-upd-7c0d993e-94b1-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7c0d98df-94b1-11e9-8f59-1e22372c056e
STEP: Updating secret s-test-opt-upd-7c0d993e-94b1-11e9-8f59-1e22372c056e
STEP: Creating secret with name s-test-opt-create-7c0d9958-94b1-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:50:53.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2768" for this suite.
Jun 22 05:51:29.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:51:31.970: INFO: namespace projected-2768 deletion completed in 38.361884931s

• [SLOW TEST:130.923 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:51:31.970: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-5141
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5141
I0622 05:51:33.593768      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5141, replica count: 1
I0622 05:51:34.644250      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 05:51:35.645592      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 05:51:36.645834      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 05:51:37.646094      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 05:51:38.646308      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 05:51:39.181: INFO: Created: latency-svc-tzlgb
Jun 22 05:51:39.449: INFO: Got endpoints: latency-svc-tzlgb [702.746311ms]
Jun 22 05:51:39.772: INFO: Created: latency-svc-rw597
Jun 22 05:51:40.390: INFO: Got endpoints: latency-svc-rw597 [940.334375ms]
Jun 22 05:51:40.399: INFO: Created: latency-svc-xvqm9
Jun 22 05:51:40.459: INFO: Got endpoints: latency-svc-xvqm9 [1.009246657s]
Jun 22 05:51:40.637: INFO: Created: latency-svc-f9k95
Jun 22 05:51:40.915: INFO: Got endpoints: latency-svc-f9k95 [1.464958415s]
Jun 22 05:51:41.307: INFO: Created: latency-svc-p86qd
Jun 22 05:51:41.451: INFO: Got endpoints: latency-svc-p86qd [2.001301871s]
Jun 22 05:51:41.723: INFO: Created: latency-svc-scczv
Jun 22 05:51:42.065: INFO: Got endpoints: latency-svc-scczv [2.614900765s]
Jun 22 05:51:42.303: INFO: Created: latency-svc-gzcnk
Jun 22 05:51:42.556: INFO: Got endpoints: latency-svc-gzcnk [3.105938598s]
Jun 22 05:51:42.998: INFO: Created: latency-svc-v845p
Jun 22 05:51:43.233: INFO: Got endpoints: latency-svc-v845p [3.782741005s]
Jun 22 05:51:43.508: INFO: Created: latency-svc-xrcqc
Jun 22 05:51:43.647: INFO: Got endpoints: latency-svc-xrcqc [4.197184219s]
Jun 22 05:51:43.679: INFO: Created: latency-svc-44h46
Jun 22 05:51:43.937: INFO: Got endpoints: latency-svc-44h46 [4.486122696s]
Jun 22 05:51:44.649: INFO: Created: latency-svc-jq4d5
Jun 22 05:51:44.882: INFO: Got endpoints: latency-svc-jq4d5 [5.431233625s]
Jun 22 05:51:44.911: INFO: Created: latency-svc-nrq85
Jun 22 05:51:45.129: INFO: Got endpoints: latency-svc-nrq85 [5.679388934s]
Jun 22 05:51:45.180: INFO: Created: latency-svc-2fm7l
Jun 22 05:51:45.730: INFO: Got endpoints: latency-svc-2fm7l [6.279231644s]
Jun 22 05:51:46.073: INFO: Created: latency-svc-cwldv
Jun 22 05:51:46.702: INFO: Got endpoints: latency-svc-cwldv [7.251494367s]
Jun 22 05:51:46.916: INFO: Created: latency-svc-9tcww
Jun 22 05:51:47.283: INFO: Got endpoints: latency-svc-9tcww [7.832862851s]
Jun 22 05:51:47.463: INFO: Created: latency-svc-sk9vt
Jun 22 05:51:47.797: INFO: Got endpoints: latency-svc-sk9vt [8.346753064s]
Jun 22 05:51:48.264: INFO: Created: latency-svc-vc98d
Jun 22 05:51:48.791: INFO: Got endpoints: latency-svc-vc98d [8.400766602s]
Jun 22 05:51:49.046: INFO: Created: latency-svc-xt5hd
Jun 22 05:51:49.290: INFO: Got endpoints: latency-svc-xt5hd [8.831676044s]
Jun 22 05:51:49.306: INFO: Created: latency-svc-9rdtk
Jun 22 05:51:49.630: INFO: Got endpoints: latency-svc-9rdtk [8.715293011s]
Jun 22 05:51:49.631: INFO: Created: latency-svc-7rzs8
Jun 22 05:51:50.030: INFO: Got endpoints: latency-svc-7rzs8 [8.578701297s]
Jun 22 05:51:50.131: INFO: Created: latency-svc-4v8qh
Jun 22 05:51:50.484: INFO: Got endpoints: latency-svc-4v8qh [8.419031205s]
Jun 22 05:51:50.494: INFO: Created: latency-svc-sbppq
Jun 22 05:51:50.815: INFO: Got endpoints: latency-svc-sbppq [8.258390562s]
Jun 22 05:51:51.070: INFO: Created: latency-svc-x2t2s
Jun 22 05:51:51.363: INFO: Got endpoints: latency-svc-x2t2s [8.130240324s]
Jun 22 05:51:51.626: INFO: Created: latency-svc-5fc68
Jun 22 05:51:52.111: INFO: Got endpoints: latency-svc-5fc68 [8.463779165s]
Jun 22 05:51:52.172: INFO: Created: latency-svc-qc47q
Jun 22 05:51:52.324: INFO: Got endpoints: latency-svc-qc47q [8.385630537s]
Jun 22 05:51:52.743: INFO: Created: latency-svc-lklw5
Jun 22 05:51:53.271: INFO: Got endpoints: latency-svc-lklw5 [8.388938117s]
Jun 22 05:51:53.394: INFO: Created: latency-svc-h6cws
Jun 22 05:51:53.586: INFO: Got endpoints: latency-svc-h6cws [8.457052343s]
Jun 22 05:51:53.718: INFO: Created: latency-svc-2kmrh
Jun 22 05:51:54.245: INFO: Got endpoints: latency-svc-2kmrh [8.515267073s]
Jun 22 05:51:54.437: INFO: Created: latency-svc-7str6
Jun 22 05:51:54.783: INFO: Got endpoints: latency-svc-7str6 [8.080927886s]
Jun 22 05:51:54.785: INFO: Created: latency-svc-ms7dr
Jun 22 05:51:55.315: INFO: Created: latency-svc-g5zrt
Jun 22 05:51:55.318: INFO: Got endpoints: latency-svc-ms7dr [8.034504255s]
Jun 22 05:51:55.821: INFO: Got endpoints: latency-svc-g5zrt [8.023336873s]
Jun 22 05:51:55.947: INFO: Created: latency-svc-2djl5
Jun 22 05:51:56.399: INFO: Got endpoints: latency-svc-2djl5 [7.608230639s]
Jun 22 05:51:56.609: INFO: Created: latency-svc-x6c2j
Jun 22 05:51:57.303: INFO: Got endpoints: latency-svc-x6c2j [8.012479004s]
Jun 22 05:51:57.309: INFO: Created: latency-svc-6qxnx
Jun 22 05:51:57.513: INFO: Got endpoints: latency-svc-6qxnx [7.882251103s]
Jun 22 05:51:57.976: INFO: Created: latency-svc-5hbxv
Jun 22 05:51:58.324: INFO: Created: latency-svc-kvwck
Jun 22 05:51:58.325: INFO: Got endpoints: latency-svc-5hbxv [8.294519649s]
Jun 22 05:51:58.648: INFO: Got endpoints: latency-svc-kvwck [8.163722052s]
Jun 22 05:51:59.034: INFO: Created: latency-svc-xzhvf
Jun 22 05:51:59.262: INFO: Got endpoints: latency-svc-xzhvf [8.447184849s]
Jun 22 05:51:59.631: INFO: Created: latency-svc-qvsq6
Jun 22 05:52:00.099: INFO: Got endpoints: latency-svc-qvsq6 [8.735934324s]
Jun 22 05:52:00.325: INFO: Created: latency-svc-spxg4
Jun 22 05:52:00.668: INFO: Got endpoints: latency-svc-spxg4 [8.556426183s]
Jun 22 05:52:01.127: INFO: Created: latency-svc-5lmx4
Jun 22 05:52:01.415: INFO: Got endpoints: latency-svc-5lmx4 [9.09152086s]
Jun 22 05:52:01.515: INFO: Created: latency-svc-7prm2
Jun 22 05:52:01.779: INFO: Got endpoints: latency-svc-7prm2 [8.508103659s]
Jun 22 05:52:02.363: INFO: Created: latency-svc-jx82w
Jun 22 05:52:03.285: INFO: Got endpoints: latency-svc-jx82w [9.699613749s]
Jun 22 05:52:03.292: INFO: Created: latency-svc-nx95p
Jun 22 05:52:03.831: INFO: Got endpoints: latency-svc-nx95p [9.585890312s]
Jun 22 05:52:05.346: INFO: Created: latency-svc-z5ktf
Jun 22 05:52:05.346: INFO: Created: latency-svc-8z5tw
Jun 22 05:52:05.948: INFO: Got endpoints: latency-svc-8z5tw [10.629707542s]
Jun 22 05:52:05.954: INFO: Got endpoints: latency-svc-z5ktf [11.170655241s]
Jun 22 05:52:06.427: INFO: Created: latency-svc-r2tf4
Jun 22 05:52:06.915: INFO: Got endpoints: latency-svc-r2tf4 [11.094202834s]
Jun 22 05:52:07.209: INFO: Created: latency-svc-975hs
Jun 22 05:52:07.695: INFO: Got endpoints: latency-svc-975hs [11.296116392s]
Jun 22 05:52:07.907: INFO: Created: latency-svc-49pct
Jun 22 05:52:07.966: INFO: Got endpoints: latency-svc-49pct [10.66297049s]
Jun 22 05:52:08.257: INFO: Created: latency-svc-slj55
Jun 22 05:52:08.663: INFO: Got endpoints: latency-svc-slj55 [11.14997206s]
Jun 22 05:52:08.679: INFO: Created: latency-svc-lcrml
Jun 22 05:52:09.060: INFO: Got endpoints: latency-svc-lcrml [10.734928247s]
Jun 22 05:52:09.132: INFO: Created: latency-svc-hs5m5
Jun 22 05:52:09.384: INFO: Got endpoints: latency-svc-hs5m5 [10.73606058s]
Jun 22 05:52:09.930: INFO: Created: latency-svc-phlxl
Jun 22 05:52:10.017: INFO: Got endpoints: latency-svc-phlxl [10.753901013s]
Jun 22 05:52:10.177: INFO: Created: latency-svc-kwl2w
Jun 22 05:52:10.313: INFO: Got endpoints: latency-svc-kwl2w [10.213440669s]
Jun 22 05:52:10.591: INFO: Created: latency-svc-d9wdm
Jun 22 05:52:10.928: INFO: Got endpoints: latency-svc-d9wdm [10.260117538s]
Jun 22 05:52:11.042: INFO: Created: latency-svc-2lm2g
Jun 22 05:52:11.415: INFO: Got endpoints: latency-svc-2lm2g [9.9995266s]
Jun 22 05:52:11.895: INFO: Created: latency-svc-d8b79
Jun 22 05:52:12.336: INFO: Got endpoints: latency-svc-d8b79 [10.556848073s]
Jun 22 05:52:12.974: INFO: Created: latency-svc-wsw6f
Jun 22 05:52:13.227: INFO: Got endpoints: latency-svc-wsw6f [9.9415326s]
Jun 22 05:52:13.791: INFO: Created: latency-svc-2jks7
Jun 22 05:52:14.151: INFO: Got endpoints: latency-svc-2jks7 [10.319528554s]
Jun 22 05:52:14.438: INFO: Created: latency-svc-lj5kk
Jun 22 05:52:14.862: INFO: Got endpoints: latency-svc-lj5kk [8.91442102s]
Jun 22 05:52:15.095: INFO: Created: latency-svc-nkd7t
Jun 22 05:52:15.618: INFO: Got endpoints: latency-svc-nkd7t [9.664425367s]
Jun 22 05:52:15.734: INFO: Created: latency-svc-vkcqx
Jun 22 05:52:16.134: INFO: Got endpoints: latency-svc-vkcqx [9.219219122s]
Jun 22 05:52:16.688: INFO: Created: latency-svc-j69tm
Jun 22 05:52:17.063: INFO: Got endpoints: latency-svc-j69tm [9.367757534s]
Jun 22 05:52:17.242: INFO: Created: latency-svc-5gmzs
Jun 22 05:52:17.417: INFO: Got endpoints: latency-svc-5gmzs [9.4503985s]
Jun 22 05:52:17.965: INFO: Created: latency-svc-f9mgl
Jun 22 05:52:17.997: INFO: Got endpoints: latency-svc-f9mgl [9.334614051s]
Jun 22 05:52:18.253: INFO: Created: latency-svc-qmjvw
Jun 22 05:52:18.314: INFO: Got endpoints: latency-svc-qmjvw [9.25475158s]
Jun 22 05:52:18.319: INFO: Created: latency-svc-8zwb8
Jun 22 05:52:18.467: INFO: Got endpoints: latency-svc-8zwb8 [9.082601635s]
Jun 22 05:52:18.741: INFO: Created: latency-svc-rqblj
Jun 22 05:52:19.089: INFO: Got endpoints: latency-svc-rqblj [9.072708737s]
Jun 22 05:52:19.286: INFO: Created: latency-svc-ngnh2
Jun 22 05:52:19.872: INFO: Got endpoints: latency-svc-ngnh2 [9.558879212s]
Jun 22 05:52:19.914: INFO: Created: latency-svc-hf4qw
Jun 22 05:52:20.165: INFO: Got endpoints: latency-svc-hf4qw [9.236878853s]
Jun 22 05:52:20.198: INFO: Created: latency-svc-dbqdh
Jun 22 05:52:20.833: INFO: Got endpoints: latency-svc-dbqdh [9.418188823s]
Jun 22 05:52:21.288: INFO: Created: latency-svc-8b8mx
Jun 22 05:52:21.609: INFO: Got endpoints: latency-svc-8b8mx [9.272657347s]
Jun 22 05:52:21.889: INFO: Created: latency-svc-srxfb
Jun 22 05:52:22.090: INFO: Got endpoints: latency-svc-srxfb [8.862736234s]
Jun 22 05:52:22.629: INFO: Created: latency-svc-r9xqs
Jun 22 05:52:23.385: INFO: Got endpoints: latency-svc-r9xqs [9.233793025s]
Jun 22 05:52:23.474: INFO: Created: latency-svc-x7f84
Jun 22 05:52:23.756: INFO: Got endpoints: latency-svc-x7f84 [8.893318328s]
Jun 22 05:52:24.536: INFO: Created: latency-svc-xzsdp
Jun 22 05:52:24.963: INFO: Got endpoints: latency-svc-xzsdp [9.344471863s]
Jun 22 05:52:25.563: INFO: Created: latency-svc-nb4bj
Jun 22 05:52:25.918: INFO: Got endpoints: latency-svc-nb4bj [9.783974036s]
Jun 22 05:52:26.225: INFO: Created: latency-svc-7sdf5
Jun 22 05:52:26.603: INFO: Got endpoints: latency-svc-7sdf5 [9.539377097s]
Jun 22 05:52:27.068: INFO: Created: latency-svc-8dn5m
Jun 22 05:52:27.292: INFO: Got endpoints: latency-svc-8dn5m [9.875647564s]
Jun 22 05:52:27.616: INFO: Created: latency-svc-kkmfq
Jun 22 05:52:28.318: INFO: Got endpoints: latency-svc-kkmfq [10.320724704s]
Jun 22 05:52:28.552: INFO: Created: latency-svc-4n7fm
Jun 22 05:52:29.097: INFO: Got endpoints: latency-svc-4n7fm [10.782788668s]
Jun 22 05:52:29.412: INFO: Created: latency-svc-zb7z9
Jun 22 05:52:29.666: INFO: Got endpoints: latency-svc-zb7z9 [11.199305323s]
Jun 22 05:52:30.300: INFO: Created: latency-svc-sfklc
Jun 22 05:52:30.752: INFO: Got endpoints: latency-svc-sfklc [11.662416066s]
Jun 22 05:52:30.927: INFO: Created: latency-svc-6mkgn
Jun 22 05:52:31.495: INFO: Got endpoints: latency-svc-6mkgn [11.622983318s]
Jun 22 05:52:31.495: INFO: Created: latency-svc-b9zrx
Jun 22 05:52:31.674: INFO: Got endpoints: latency-svc-b9zrx [11.508813134s]
Jun 22 05:52:32.353: INFO: Created: latency-svc-nwnnc
Jun 22 05:52:32.817: INFO: Got endpoints: latency-svc-nwnnc [11.983912469s]
Jun 22 05:52:33.596: INFO: Created: latency-svc-l8qkz
Jun 22 05:52:34.231: INFO: Got endpoints: latency-svc-l8qkz [12.62244826s]
Jun 22 05:52:34.232: INFO: Created: latency-svc-lw977
Jun 22 05:52:34.681: INFO: Got endpoints: latency-svc-lw977 [12.590809793s]
Jun 22 05:52:35.131: INFO: Created: latency-svc-6lknw
Jun 22 05:52:35.462: INFO: Created: latency-svc-dznr2
Jun 22 05:52:35.545: INFO: Got endpoints: latency-svc-6lknw [12.160495559s]
Jun 22 05:52:35.782: INFO: Got endpoints: latency-svc-dznr2 [12.025927157s]
Jun 22 05:52:36.445: INFO: Created: latency-svc-7lmbm
Jun 22 05:52:37.277: INFO: Created: latency-svc-smwkd
Jun 22 05:52:37.279: INFO: Got endpoints: latency-svc-7lmbm [12.316123917s]
Jun 22 05:52:37.958: INFO: Created: latency-svc-qg5kw
Jun 22 05:52:37.959: INFO: Got endpoints: latency-svc-smwkd [12.040516871s]
Jun 22 05:52:38.320: INFO: Got endpoints: latency-svc-qg5kw [11.717478947s]
Jun 22 05:52:38.549: INFO: Created: latency-svc-4mkvt
Jun 22 05:52:38.655: INFO: Got endpoints: latency-svc-4mkvt [11.362480214s]
Jun 22 05:52:39.425: INFO: Created: latency-svc-lccws
Jun 22 05:52:39.741: INFO: Created: latency-svc-tfrfr
Jun 22 05:52:39.742: INFO: Got endpoints: latency-svc-lccws [11.423417036s]
Jun 22 05:52:40.071: INFO: Got endpoints: latency-svc-tfrfr [10.974065101s]
Jun 22 05:52:40.243: INFO: Created: latency-svc-njd9b
Jun 22 05:52:40.757: INFO: Created: latency-svc-6vtdj
Jun 22 05:52:40.761: INFO: Got endpoints: latency-svc-njd9b [11.094757846s]
Jun 22 05:52:41.175: INFO: Got endpoints: latency-svc-6vtdj [10.42272298s]
Jun 22 05:52:41.209: INFO: Created: latency-svc-ftn57
Jun 22 05:52:41.602: INFO: Got endpoints: latency-svc-ftn57 [10.107012441s]
Jun 22 05:52:42.194: INFO: Created: latency-svc-dgmkm
Jun 22 05:52:42.693: INFO: Got endpoints: latency-svc-dgmkm [11.019441128s]
Jun 22 05:52:42.796: INFO: Created: latency-svc-f96zd
Jun 22 05:52:43.073: INFO: Got endpoints: latency-svc-f96zd [10.255384162s]
Jun 22 05:52:43.616: INFO: Created: latency-svc-vzxzd
Jun 22 05:52:44.257: INFO: Got endpoints: latency-svc-vzxzd [10.026136229s]
Jun 22 05:52:44.490: INFO: Created: latency-svc-c6w87
Jun 22 05:52:44.803: INFO: Got endpoints: latency-svc-c6w87 [10.122184886s]
Jun 22 05:52:45.060: INFO: Created: latency-svc-m5gf9
Jun 22 05:52:45.282: INFO: Got endpoints: latency-svc-m5gf9 [9.736264766s]
Jun 22 05:52:45.338: INFO: Created: latency-svc-hz5cq
Jun 22 05:52:45.692: INFO: Got endpoints: latency-svc-hz5cq [9.910382628s]
Jun 22 05:52:46.185: INFO: Created: latency-svc-ng26d
Jun 22 05:52:46.436: INFO: Got endpoints: latency-svc-ng26d [9.156691951s]
Jun 22 05:52:46.484: INFO: Created: latency-svc-kzt7k
Jun 22 05:52:46.760: INFO: Got endpoints: latency-svc-kzt7k [8.80076556s]
Jun 22 05:52:47.148: INFO: Created: latency-svc-lxl8q
Jun 22 05:52:47.205: INFO: Got endpoints: latency-svc-lxl8q [8.884762555s]
Jun 22 05:52:47.496: INFO: Created: latency-svc-sqtjr
Jun 22 05:52:48.058: INFO: Got endpoints: latency-svc-sqtjr [9.402632066s]
Jun 22 05:52:48.282: INFO: Created: latency-svc-xhfp4
Jun 22 05:52:48.584: INFO: Created: latency-svc-8xrbh
Jun 22 05:52:48.590: INFO: Got endpoints: latency-svc-xhfp4 [8.847939232s]
Jun 22 05:52:48.911: INFO: Got endpoints: latency-svc-8xrbh [8.83924654s]
Jun 22 05:52:48.919: INFO: Created: latency-svc-qd4vb
Jun 22 05:52:49.215: INFO: Got endpoints: latency-svc-qd4vb [8.45367725s]
Jun 22 05:52:49.427: INFO: Created: latency-svc-k5wcz
Jun 22 05:52:49.638: INFO: Got endpoints: latency-svc-k5wcz [8.462291497s]
Jun 22 05:52:49.658: INFO: Created: latency-svc-gbv9p
Jun 22 05:52:50.005: INFO: Got endpoints: latency-svc-gbv9p [8.403006047s]
Jun 22 05:52:50.019: INFO: Created: latency-svc-m7h8f
Jun 22 05:52:50.283: INFO: Got endpoints: latency-svc-m7h8f [7.589328486s]
Jun 22 05:52:50.288: INFO: Created: latency-svc-942vv
Jun 22 05:52:50.840: INFO: Got endpoints: latency-svc-942vv [7.767460293s]
Jun 22 05:52:50.854: INFO: Created: latency-svc-jbxwq
Jun 22 05:52:51.298: INFO: Got endpoints: latency-svc-jbxwq [7.040954888s]
Jun 22 05:52:51.614: INFO: Created: latency-svc-4nlrq
Jun 22 05:52:52.056: INFO: Got endpoints: latency-svc-4nlrq [7.252556931s]
Jun 22 05:52:52.262: INFO: Created: latency-svc-p2dpr
Jun 22 05:52:52.600: INFO: Got endpoints: latency-svc-p2dpr [7.318123307s]
Jun 22 05:52:52.847: INFO: Created: latency-svc-stn9t
Jun 22 05:52:53.317: INFO: Got endpoints: latency-svc-stn9t [7.624518885s]
Jun 22 05:52:53.647: INFO: Created: latency-svc-9ss5z
Jun 22 05:52:54.411: INFO: Got endpoints: latency-svc-9ss5z [7.975508785s]
Jun 22 05:52:54.422: INFO: Created: latency-svc-m5dmb
Jun 22 05:52:54.729: INFO: Got endpoints: latency-svc-m5dmb [7.968978851s]
Jun 22 05:52:54.970: INFO: Created: latency-svc-4jt5v
Jun 22 05:52:55.283: INFO: Got endpoints: latency-svc-4jt5v [8.077523339s]
Jun 22 05:52:55.610: INFO: Created: latency-svc-4w9qs
Jun 22 05:52:55.998: INFO: Got endpoints: latency-svc-4w9qs [7.940324508s]
Jun 22 05:52:56.194: INFO: Created: latency-svc-9bqt4
Jun 22 05:52:56.590: INFO: Got endpoints: latency-svc-9bqt4 [8.000664742s]
Jun 22 05:52:56.902: INFO: Created: latency-svc-c8t9f
Jun 22 05:52:57.399: INFO: Got endpoints: latency-svc-c8t9f [8.488381013s]
Jun 22 05:52:57.936: INFO: Created: latency-svc-ggz6v
Jun 22 05:52:57.948: INFO: Got endpoints: latency-svc-ggz6v [8.733484887s]
Jun 22 05:52:58.198: INFO: Created: latency-svc-pzgd2
Jun 22 05:52:58.595: INFO: Got endpoints: latency-svc-pzgd2 [8.956860215s]
Jun 22 05:52:58.817: INFO: Created: latency-svc-2m4nt
Jun 22 05:52:59.176: INFO: Got endpoints: latency-svc-2m4nt [9.171043764s]
Jun 22 05:52:59.217: INFO: Created: latency-svc-7vknt
Jun 22 05:52:59.485: INFO: Got endpoints: latency-svc-7vknt [9.201839769s]
Jun 22 05:52:59.723: INFO: Created: latency-svc-7h55v
Jun 22 05:53:00.061: INFO: Got endpoints: latency-svc-7h55v [9.220963999s]
Jun 22 05:53:00.413: INFO: Created: latency-svc-2ddgh
Jun 22 05:53:00.739: INFO: Got endpoints: latency-svc-2ddgh [9.440534922s]
Jun 22 05:53:00.862: INFO: Created: latency-svc-2ddc6
Jun 22 05:53:01.032: INFO: Got endpoints: latency-svc-2ddc6 [8.976019386s]
Jun 22 05:53:01.217: INFO: Created: latency-svc-m4hvb
Jun 22 05:53:01.729: INFO: Got endpoints: latency-svc-m4hvb [9.128830264s]
Jun 22 05:53:01.931: INFO: Created: latency-svc-t2v2k
Jun 22 05:53:02.363: INFO: Got endpoints: latency-svc-t2v2k [9.045722158s]
Jun 22 05:53:02.872: INFO: Created: latency-svc-n2xj4
Jun 22 05:53:03.501: INFO: Got endpoints: latency-svc-n2xj4 [9.089155638s]
Jun 22 05:53:03.545: INFO: Created: latency-svc-h7qls
Jun 22 05:53:04.337: INFO: Got endpoints: latency-svc-h7qls [9.607921498s]
Jun 22 05:53:04.704: INFO: Created: latency-svc-dfg95
Jun 22 05:53:05.647: INFO: Got endpoints: latency-svc-dfg95 [10.364035208s]
Jun 22 05:53:05.665: INFO: Created: latency-svc-v2xwv
Jun 22 05:53:05.899: INFO: Got endpoints: latency-svc-v2xwv [9.901061562s]
Jun 22 05:53:05.899: INFO: Created: latency-svc-wckzl
Jun 22 05:53:06.246: INFO: Got endpoints: latency-svc-wckzl [9.655994024s]
Jun 22 05:53:06.523: INFO: Created: latency-svc-zjlkf
Jun 22 05:53:06.863: INFO: Got endpoints: latency-svc-zjlkf [9.463466092s]
Jun 22 05:53:07.153: INFO: Created: latency-svc-zgg2g
Jun 22 05:53:07.546: INFO: Got endpoints: latency-svc-zgg2g [9.59744737s]
Jun 22 05:53:08.708: INFO: Created: latency-svc-rz4wx
Jun 22 05:53:09.525: INFO: Got endpoints: latency-svc-rz4wx [10.930142609s]
Jun 22 05:53:09.754: INFO: Created: latency-svc-zr59w
Jun 22 05:53:10.027: INFO: Got endpoints: latency-svc-zr59w [10.85056254s]
Jun 22 05:53:10.073: INFO: Created: latency-svc-ww85p
Jun 22 05:53:10.576: INFO: Got endpoints: latency-svc-ww85p [11.091737902s]
Jun 22 05:53:10.883: INFO: Created: latency-svc-gskvg
Jun 22 05:53:11.236: INFO: Got endpoints: latency-svc-gskvg [11.174700191s]
Jun 22 05:53:11.255: INFO: Created: latency-svc-zksn2
Jun 22 05:53:11.767: INFO: Got endpoints: latency-svc-zksn2 [11.028321151s]
Jun 22 05:53:11.846: INFO: Created: latency-svc-95bmz
Jun 22 05:53:12.488: INFO: Got endpoints: latency-svc-95bmz [11.456300636s]
Jun 22 05:53:12.864: INFO: Created: latency-svc-p9zsl
Jun 22 05:53:13.191: INFO: Got endpoints: latency-svc-p9zsl [11.462038744s]
Jun 22 05:53:13.585: INFO: Created: latency-svc-f4nzp
Jun 22 05:53:13.838: INFO: Got endpoints: latency-svc-f4nzp [11.475157233s]
Jun 22 05:53:14.497: INFO: Created: latency-svc-zjgfj
Jun 22 05:53:14.845: INFO: Got endpoints: latency-svc-zjgfj [11.34459938s]
Jun 22 05:53:15.248: INFO: Created: latency-svc-ng4td
Jun 22 05:53:15.353: INFO: Got endpoints: latency-svc-ng4td [11.016202124s]
Jun 22 05:53:15.948: INFO: Created: latency-svc-wt7xz
Jun 22 05:53:16.121: INFO: Got endpoints: latency-svc-wt7xz [10.473717717s]
Jun 22 05:53:16.156: INFO: Created: latency-svc-qnp46
Jun 22 05:53:16.451: INFO: Got endpoints: latency-svc-qnp46 [10.551585461s]
Jun 22 05:53:16.766: INFO: Created: latency-svc-h66rt
Jun 22 05:53:17.047: INFO: Got endpoints: latency-svc-h66rt [10.800363662s]
Jun 22 05:53:17.089: INFO: Created: latency-svc-4w4pw
Jun 22 05:53:17.474: INFO: Got endpoints: latency-svc-4w4pw [10.610961119s]
Jun 22 05:53:18.182: INFO: Created: latency-svc-hrc6r
Jun 22 05:53:18.514: INFO: Got endpoints: latency-svc-hrc6r [10.968338546s]
Jun 22 05:53:18.586: INFO: Created: latency-svc-sm62j
Jun 22 05:53:19.032: INFO: Got endpoints: latency-svc-sm62j [9.507010635s]
Jun 22 05:53:19.624: INFO: Created: latency-svc-m99gw
Jun 22 05:53:20.452: INFO: Got endpoints: latency-svc-m99gw [10.424906171s]
Jun 22 05:53:20.516: INFO: Created: latency-svc-qmknr
Jun 22 05:53:20.678: INFO: Got endpoints: latency-svc-qmknr [10.101583614s]
Jun 22 05:53:21.083: INFO: Created: latency-svc-h2jns
Jun 22 05:53:21.424: INFO: Got endpoints: latency-svc-h2jns [10.187608365s]
Jun 22 05:53:21.659: INFO: Created: latency-svc-sfsht
Jun 22 05:53:21.888: INFO: Got endpoints: latency-svc-sfsht [10.120392878s]
Jun 22 05:53:21.895: INFO: Created: latency-svc-8lbk7
Jun 22 05:53:22.661: INFO: Got endpoints: latency-svc-8lbk7 [10.173307671s]
Jun 22 05:53:22.676: INFO: Created: latency-svc-wpgll
Jun 22 05:53:23.290: INFO: Got endpoints: latency-svc-wpgll [10.09880359s]
Jun 22 05:53:23.457: INFO: Created: latency-svc-jxscr
Jun 22 05:53:23.895: INFO: Got endpoints: latency-svc-jxscr [10.056936889s]
Jun 22 05:53:23.971: INFO: Created: latency-svc-whwtt
Jun 22 05:53:24.538: INFO: Got endpoints: latency-svc-whwtt [9.692291612s]
Jun 22 05:53:24.636: INFO: Created: latency-svc-2kc98
Jun 22 05:53:24.900: INFO: Got endpoints: latency-svc-2kc98 [9.546428923s]
Jun 22 05:53:25.921: INFO: Created: latency-svc-6s9k4
Jun 22 05:53:26.371: INFO: Got endpoints: latency-svc-6s9k4 [10.250738637s]
Jun 22 05:53:26.771: INFO: Created: latency-svc-h77ht
Jun 22 05:53:27.107: INFO: Got endpoints: latency-svc-h77ht [10.655794109s]
Jun 22 05:53:27.351: INFO: Created: latency-svc-hqpmv
Jun 22 05:53:27.571: INFO: Got endpoints: latency-svc-hqpmv [1.199925156s]
Jun 22 05:53:27.644: INFO: Created: latency-svc-wp84v
Jun 22 05:53:27.897: INFO: Got endpoints: latency-svc-wp84v [10.850337735s]
Jun 22 05:53:27.909: INFO: Created: latency-svc-4x89x
Jun 22 05:53:28.422: INFO: Got endpoints: latency-svc-4x89x [10.948284623s]
Jun 22 05:53:28.423: INFO: Created: latency-svc-tpdtb
Jun 22 05:53:28.963: INFO: Got endpoints: latency-svc-tpdtb [10.448364948s]
Jun 22 05:53:28.977: INFO: Created: latency-svc-vxvp7
Jun 22 05:53:29.435: INFO: Got endpoints: latency-svc-vxvp7 [10.403063899s]
Jun 22 05:53:29.493: INFO: Created: latency-svc-t75nm
Jun 22 05:53:30.001: INFO: Got endpoints: latency-svc-t75nm [9.548995802s]
Jun 22 05:53:30.235: INFO: Created: latency-svc-dq5zk
Jun 22 05:53:31.039: INFO: Got endpoints: latency-svc-dq5zk [10.360879003s]
Jun 22 05:53:31.044: INFO: Created: latency-svc-7bmsf
Jun 22 05:53:31.318: INFO: Got endpoints: latency-svc-7bmsf [9.893839336s]
Jun 22 05:53:31.842: INFO: Created: latency-svc-jbmcv
Jun 22 05:53:32.104: INFO: Got endpoints: latency-svc-jbmcv [10.215673019s]
Jun 22 05:53:32.888: INFO: Created: latency-svc-wzrld
Jun 22 05:53:33.303: INFO: Got endpoints: latency-svc-wzrld [10.641108142s]
Jun 22 05:53:33.585: INFO: Created: latency-svc-gfnb8
Jun 22 05:53:34.198: INFO: Created: latency-svc-5t9v8
Jun 22 05:53:34.198: INFO: Got endpoints: latency-svc-gfnb8 [10.908115327s]
Jun 22 05:53:34.516: INFO: Got endpoints: latency-svc-5t9v8 [10.620306834s]
Jun 22 05:53:34.745: INFO: Created: latency-svc-bdqnp
Jun 22 05:53:34.934: INFO: Got endpoints: latency-svc-bdqnp [10.39600311s]
Jun 22 05:53:35.222: INFO: Created: latency-svc-clb54
Jun 22 05:53:35.429: INFO: Got endpoints: latency-svc-clb54 [10.529537892s]
Jun 22 05:53:35.789: INFO: Created: latency-svc-9b77q
Jun 22 05:53:36.169: INFO: Got endpoints: latency-svc-9b77q [9.062834844s]
Jun 22 05:53:36.170: INFO: Created: latency-svc-df25d
Jun 22 05:53:36.526: INFO: Got endpoints: latency-svc-df25d [8.954863327s]
Jun 22 05:53:36.617: INFO: Created: latency-svc-m7kpb
Jun 22 05:53:36.969: INFO: Got endpoints: latency-svc-m7kpb [9.071616057s]
Jun 22 05:53:37.028: INFO: Created: latency-svc-4djqm
Jun 22 05:53:37.459: INFO: Got endpoints: latency-svc-4djqm [9.03676818s]
Jun 22 05:53:37.695: INFO: Created: latency-svc-2df49
Jun 22 05:53:37.905: INFO: Got endpoints: latency-svc-2df49 [8.941715917s]
Jun 22 05:53:38.131: INFO: Created: latency-svc-zscm2
Jun 22 05:53:38.717: INFO: Got endpoints: latency-svc-zscm2 [9.28188772s]
Jun 22 05:53:38.816: INFO: Created: latency-svc-v9697
Jun 22 05:53:39.027: INFO: Got endpoints: latency-svc-v9697 [9.025969815s]
Jun 22 05:53:39.525: INFO: Created: latency-svc-47xch
Jun 22 05:53:39.589: INFO: Created: latency-svc-g6wbw
Jun 22 05:53:39.719: INFO: Got endpoints: latency-svc-47xch [8.679768617s]
Jun 22 05:53:39.720: INFO: Got endpoints: latency-svc-g6wbw [8.402524262s]
Jun 22 05:53:39.978: INFO: Created: latency-svc-vjbpk
Jun 22 05:53:40.049: INFO: Got endpoints: latency-svc-vjbpk [7.945111782s]
Jun 22 05:53:40.069: INFO: Created: latency-svc-2cflj
Jun 22 05:53:40.659: INFO: Got endpoints: latency-svc-2cflj [7.356239773s]
Jun 22 05:53:40.920: INFO: Created: latency-svc-4bx7t
Jun 22 05:53:41.709: INFO: Created: latency-svc-g8gnk
Jun 22 05:53:41.709: INFO: Got endpoints: latency-svc-4bx7t [7.511287556s]
Jun 22 05:53:42.228: INFO: Got endpoints: latency-svc-g8gnk [7.712539121s]
Jun 22 05:53:43.325: INFO: Created: latency-svc-jxrkh
Jun 22 05:53:43.873: INFO: Got endpoints: latency-svc-jxrkh [8.939561323s]
Jun 22 05:53:44.439: INFO: Created: latency-svc-dlf2r
Jun 22 05:53:44.601: INFO: Got endpoints: latency-svc-dlf2r [9.172096394s]
Jun 22 05:53:44.982: INFO: Created: latency-svc-nqn2n
Jun 22 05:53:45.514: INFO: Got endpoints: latency-svc-nqn2n [9.344411358s]
Jun 22 05:53:45.738: INFO: Created: latency-svc-zdsk5
Jun 22 05:53:46.158: INFO: Got endpoints: latency-svc-zdsk5 [9.631235639s]
Jun 22 05:53:46.468: INFO: Created: latency-svc-qblwr
Jun 22 05:53:46.666: INFO: Got endpoints: latency-svc-qblwr [9.697007237s]
Jun 22 05:53:46.937: INFO: Created: latency-svc-vptvw
Jun 22 05:53:47.228: INFO: Got endpoints: latency-svc-vptvw [9.768881357s]
Jun 22 05:53:47.228: INFO: Latencies: [940.334375ms 1.009246657s 1.199925156s 1.464958415s 2.001301871s 2.614900765s 3.105938598s 3.782741005s 4.197184219s 4.486122696s 5.431233625s 5.679388934s 6.279231644s 7.040954888s 7.251494367s 7.252556931s 7.318123307s 7.356239773s 7.511287556s 7.589328486s 7.608230639s 7.624518885s 7.712539121s 7.767460293s 7.832862851s 7.882251103s 7.940324508s 7.945111782s 7.968978851s 7.975508785s 8.000664742s 8.012479004s 8.023336873s 8.034504255s 8.077523339s 8.080927886s 8.130240324s 8.163722052s 8.258390562s 8.294519649s 8.346753064s 8.385630537s 8.388938117s 8.400766602s 8.402524262s 8.403006047s 8.419031205s 8.447184849s 8.45367725s 8.457052343s 8.462291497s 8.463779165s 8.488381013s 8.508103659s 8.515267073s 8.556426183s 8.578701297s 8.679768617s 8.715293011s 8.733484887s 8.735934324s 8.80076556s 8.831676044s 8.83924654s 8.847939232s 8.862736234s 8.884762555s 8.893318328s 8.91442102s 8.939561323s 8.941715917s 8.954863327s 8.956860215s 8.976019386s 9.025969815s 9.03676818s 9.045722158s 9.062834844s 9.071616057s 9.072708737s 9.082601635s 9.089155638s 9.09152086s 9.128830264s 9.156691951s 9.171043764s 9.172096394s 9.201839769s 9.219219122s 9.220963999s 9.233793025s 9.236878853s 9.25475158s 9.272657347s 9.28188772s 9.334614051s 9.344411358s 9.344471863s 9.367757534s 9.402632066s 9.418188823s 9.440534922s 9.4503985s 9.463466092s 9.507010635s 9.539377097s 9.546428923s 9.548995802s 9.558879212s 9.585890312s 9.59744737s 9.607921498s 9.631235639s 9.655994024s 9.664425367s 9.692291612s 9.697007237s 9.699613749s 9.736264766s 9.768881357s 9.783974036s 9.875647564s 9.893839336s 9.901061562s 9.910382628s 9.9415326s 9.9995266s 10.026136229s 10.056936889s 10.09880359s 10.101583614s 10.107012441s 10.120392878s 10.122184886s 10.173307671s 10.187608365s 10.213440669s 10.215673019s 10.250738637s 10.255384162s 10.260117538s 10.319528554s 10.320724704s 10.360879003s 10.364035208s 10.39600311s 10.403063899s 10.42272298s 10.424906171s 10.448364948s 10.473717717s 10.529537892s 10.551585461s 10.556848073s 10.610961119s 10.620306834s 10.629707542s 10.641108142s 10.655794109s 10.66297049s 10.734928247s 10.73606058s 10.753901013s 10.782788668s 10.800363662s 10.850337735s 10.85056254s 10.908115327s 10.930142609s 10.948284623s 10.968338546s 10.974065101s 11.016202124s 11.019441128s 11.028321151s 11.091737902s 11.094202834s 11.094757846s 11.14997206s 11.170655241s 11.174700191s 11.199305323s 11.296116392s 11.34459938s 11.362480214s 11.423417036s 11.456300636s 11.462038744s 11.475157233s 11.508813134s 11.622983318s 11.662416066s 11.717478947s 11.983912469s 12.025927157s 12.040516871s 12.160495559s 12.316123917s 12.590809793s 12.62244826s]
Jun 22 05:53:47.228: INFO: 50 %ile: 9.418188823s
Jun 22 05:53:47.228: INFO: 90 %ile: 11.174700191s
Jun 22 05:53:47.228: INFO: 99 %ile: 12.590809793s
Jun 22 05:53:47.229: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:53:47.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5141" for this suite.
Jun 22 05:57:50.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:57:52.127: INFO: namespace svc-latency-5141 deletion completed in 4m4.602022676s

• [SLOW TEST:380.157 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:57:52.127: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3981
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 05:57:53.885: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:58:02.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3981" for this suite.
Jun 22 05:58:56.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:58:58.684: INFO: namespace pods-3981 deletion completed in 56.06687729s

• [SLOW TEST:66.557 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:58:58.684: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7801
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 05:58:59.773: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e" in namespace "downward-api-7801" to be "success or failure"
Jun 22 05:58:59.780: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.864539ms
Jun 22 05:59:01.791: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017966175s
Jun 22 05:59:03.967: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.19431037s
Jun 22 05:59:06.136: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.363503382s
Jun 22 05:59:08.236: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.463519355s
STEP: Saw pod success
Jun 22 05:59:08.236: INFO: Pod "downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:59:08.246: INFO: Trying to get logs from node slave8 pod downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 05:59:08.290: INFO: Waiting for pod downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e to disappear
Jun 22 05:59:08.308: INFO: Pod downwardapi-volume-d417d0f9-94b2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:59:08.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7801" for this suite.
Jun 22 05:59:20.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:59:22.068: INFO: namespace downward-api-7801 deletion completed in 13.746906098s

• [SLOW TEST:23.383 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:59:22.068: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-935
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-e25bc861-94b2-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 05:59:24.193: INFO: Waiting up to 5m0s for pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e" in namespace "configmap-935" to be "success or failure"
Jun 22 05:59:24.246: INFO: Pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 52.774311ms
Jun 22 05:59:26.251: INFO: Pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057630602s
Jun 22 05:59:28.357: INFO: Pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163353331s
Jun 22 05:59:30.583: INFO: Pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.389718045s
STEP: Saw pod success
Jun 22 05:59:30.583: INFO: Pod "pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 05:59:30.809: INFO: Trying to get logs from node slave6 pod pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 05:59:31.306: INFO: Waiting for pod pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e to disappear
Jun 22 05:59:31.386: INFO: Pod pod-configmaps-e2982198-94b2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:59:31.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-935" for this suite.
Jun 22 05:59:45.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 05:59:49.452: INFO: namespace configmap-935 deletion completed in 18.060678293s

• [SLOW TEST:27.384 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 05:59:49.453: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3308
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jun 22 05:59:50.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 cluster-info'
Jun 22 05:59:56.359: INFO: stderr: ""
Jun 22 05:59:56.359: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443\x1b[0m\n\x1b[0;32mkube-dns\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mMetrics-server\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\x1b[0m\n\x1b[0;32mtrigger\x1b[0m is running at \x1b[0;33mhttps://10.150.0.1:443/api/v1/namespaces/kube-system/services/trigger/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 05:59:56.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3308" for this suite.
Jun 22 06:00:10.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:00:12.900: INFO: namespace kubectl-3308 deletion completed in 16.375410144s

• [SLOW TEST:23.447 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:00:12.901: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-868
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-868
Jun 22 06:00:21.124: INFO: Started pod liveness-http in namespace container-probe-868
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 06:00:21.127: INFO: Initial restart count of pod liveness-http is 0
Jun 22 06:00:35.825: INFO: Restart count of pod container-probe-868/liveness-http is now 1 (14.697316295s elapsed)
Jun 22 06:00:56.204: INFO: Restart count of pod container-probe-868/liveness-http is now 2 (35.076334757s elapsed)
Jun 22 06:01:16.790: INFO: Restart count of pod container-probe-868/liveness-http is now 3 (55.662478685s elapsed)
Jun 22 06:01:35.239: INFO: Restart count of pod container-probe-868/liveness-http is now 4 (1m14.11186324s elapsed)
Jun 22 06:02:36.517: INFO: Restart count of pod container-probe-868/liveness-http is now 5 (2m15.389093486s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:02:36.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-868" for this suite.
Jun 22 06:02:49.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:02:51.378: INFO: namespace container-probe-868 deletion completed in 14.303412662s

• [SLOW TEST:158.477 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:02:51.379: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-6665
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jun 22 06:02:54.228: INFO: created pod pod-service-account-defaultsa
Jun 22 06:02:54.228: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 06:02:54.636: INFO: created pod pod-service-account-mountsa
Jun 22 06:02:54.636: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 06:02:54.903: INFO: created pod pod-service-account-nomountsa
Jun 22 06:02:54.903: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 06:02:55.221: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 06:02:55.221: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 06:02:55.990: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 06:02:55.991: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 06:02:56.027: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 06:02:56.027: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 06:02:56.073: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 06:02:56.073: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 06:02:56.553: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 06:02:56.553: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 06:02:56.786: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 06:02:56.786: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:02:56.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6665" for this suite.
Jun 22 06:03:13.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:03:15.439: INFO: namespace svcaccounts-6665 deletion completed in 18.216980773s

• [SLOW TEST:24.060 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:03:15.440: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:03:17.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5739" for this suite.
Jun 22 06:03:27.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:03:29.502: INFO: namespace services-5739 deletion completed in 12.144131459s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:14.063 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:03:29.504: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5226
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 06:03:30.881: INFO: Waiting up to 5m0s for pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e" in namespace "emptydir-5226" to be "success or failure"
Jun 22 06:03:30.889: INFO: Pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.348101ms
Jun 22 06:03:32.913: INFO: Pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032389388s
Jun 22 06:03:34.928: INFO: Pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.047430126s
Jun 22 06:03:36.999: INFO: Pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.11796737s
STEP: Saw pod success
Jun 22 06:03:36.999: INFO: Pod "pod-75a84fa1-94b3-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:03:37.068: INFO: Trying to get logs from node slave8 pod pod-75a84fa1-94b3-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:03:37.408: INFO: Waiting for pod pod-75a84fa1-94b3-11e9-8f59-1e22372c056e to disappear
Jun 22 06:03:37.415: INFO: Pod pod-75a84fa1-94b3-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:03:37.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5226" for this suite.
Jun 22 06:03:53.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:03:55.219: INFO: namespace emptydir-5226 deletion completed in 17.795714567s

• [SLOW TEST:25.716 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:03:55.222: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 22 06:03:56.254: INFO: Waiting up to 5m0s for pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e" in namespace "downward-api-9119" to be "success or failure"
Jun 22 06:03:56.262: INFO: Pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.538821ms
Jun 22 06:03:58.297: INFO: Pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042713382s
Jun 22 06:04:00.458: INFO: Pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.204549031s
Jun 22 06:04:02.469: INFO: Pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.214760159s
STEP: Saw pod success
Jun 22 06:04:02.469: INFO: Pod "downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:04:02.520: INFO: Trying to get logs from node slave6 pod downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:04:03.323: INFO: Waiting for pod downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e to disappear
Jun 22 06:04:03.347: INFO: Pod downward-api-84ef77f0-94b3-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:04:03.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9119" for this suite.
Jun 22 06:04:15.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:04:17.277: INFO: namespace downward-api-9119 deletion completed in 13.912380803s

• [SLOW TEST:22.055 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:04:17.277: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7847
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 06:04:33.667: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:33.682: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:35.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:35.864: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:37.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:37.714: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:39.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:39.933: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:41.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:41.688: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:43.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:43.687: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:45.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:45.708: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 06:04:47.683: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 06:04:47.749: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:04:47.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7847" for this suite.
Jun 22 06:05:19.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:05:21.597: INFO: namespace container-lifecycle-hook-7847 deletion completed in 33.815286328s

• [SLOW TEST:64.320 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:05:21.598: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5739
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:05:23.531: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e" in namespace "projected-5739" to be "success or failure"
Jun 22 06:05:23.558: INFO: Pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 26.737631ms
Jun 22 06:05:25.733: INFO: Pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.20219411s
Jun 22 06:05:27.741: INFO: Pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.210095753s
Jun 22 06:05:29.753: INFO: Pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.221927872s
STEP: Saw pod success
Jun 22 06:05:29.753: INFO: Pod "downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:05:29.771: INFO: Trying to get logs from node slave8 pod downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:05:30.128: INFO: Waiting for pod downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e to disappear
Jun 22 06:05:30.147: INFO: Pod downwardapi-volume-b89b3f99-94b3-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:05:30.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5739" for this suite.
Jun 22 06:05:42.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:05:43.957: INFO: namespace projected-5739 deletion completed in 13.755299872s

• [SLOW TEST:22.359 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:05:43.957: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9041
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:05:45.557: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:05:52.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9041" for this suite.
Jun 22 06:06:04.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:06:06.800: INFO: namespace custom-resource-definition-9041 deletion completed in 14.6217397s

• [SLOW TEST:22.843 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:06:06.802: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jun 22 06:06:09.762: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 06:06:13.224: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780370, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:15.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780370, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:17.502: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780370, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780369, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 06:06:21.473: INFO: Waited 2.156875318s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:06:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3652" for this suite.
Jun 22 06:06:42.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:06:44.615: INFO: namespace aggregator-3652 deletion completed in 14.599439089s

• [SLOW TEST:37.813 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:06:44.615: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9114
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 22 06:06:45.855: INFO: PodSpec: initContainers in spec.initContainers
Jun 22 06:07:37.498: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-ea112400-94b3-11e9-8f59-1e22372c056e", GenerateName:"", Namespace:"init-container-9114", SelfLink:"/api/v1/namespaces/init-container-9114/pods/pod-init-ea112400-94b3-11e9-8f59-1e22372c056e", UID:"ea3213c1-94b3-11e9-a9b0-fa163e4f9fd7", ResourceVersion:"658373", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63696780406, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"855954448"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-wtvmw", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc000c0a880), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wtvmw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wtvmw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-wtvmw", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002daaab8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"slave7", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001f4c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002daab40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002daab60)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002daab68), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002daab6c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780406, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780406, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780406, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696780406, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.202.68", PodIP:"10.151.17.210", StartTime:(*v1.Time)(0xc00100afe0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d15b90)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d15c70)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker://sha256:758ec7f3a1ee85f8f08399b55641bfb13e8c1109287ddc5e22b68c3d653152ee", ContainerID:"docker://9e1ddd890c5a99d24a62c27e715c12a222e71902666bbb4eae62efca2a21a0b9"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00100b020), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00100b000), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:07:37.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9114" for this suite.
Jun 22 06:08:11.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:08:14.188: INFO: namespace init-container-9114 deletion completed in 36.470656381s

• [SLOW TEST:89.572 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:08:14.188: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:08:15.804: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 06:08:15.972: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:15.972: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:15.972: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:16.113: INFO: Number of nodes with available pods: 0
Jun 22 06:08:16.113: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:17.340: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:17.340: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:17.340: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:17.866: INFO: Number of nodes with available pods: 0
Jun 22 06:08:17.866: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:18.181: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:18.181: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:18.181: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:18.366: INFO: Number of nodes with available pods: 0
Jun 22 06:08:18.366: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:19.266: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:19.266: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:19.266: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:19.275: INFO: Number of nodes with available pods: 0
Jun 22 06:08:19.275: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:20.384: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:20.384: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:20.384: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:20.401: INFO: Number of nodes with available pods: 0
Jun 22 06:08:20.401: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:21.210: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:21.210: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:21.210: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:21.219: INFO: Number of nodes with available pods: 0
Jun 22 06:08:21.219: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:22.629: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:22.629: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:22.629: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:22.682: INFO: Number of nodes with available pods: 2
Jun 22 06:08:22.682: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:23.376: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:23.376: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:23.376: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:23.898: INFO: Number of nodes with available pods: 3
Jun 22 06:08:23.898: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:24.411: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:24.411: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:24.411: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:24.430: INFO: Number of nodes with available pods: 3
Jun 22 06:08:24.430: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:08:25.327: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:25.327: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:25.327: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:25.342: INFO: Number of nodes with available pods: 4
Jun 22 06:08:25.342: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 22 06:08:26.858: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:26.858: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:26.858: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:26.858: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:26.993: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:26.993: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:26.993: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:28.109: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:28.109: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:28.109: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:28.109: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:28.453: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:28.453: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:28.453: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:29.342: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:29.342: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:29.342: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:29.342: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:29.350: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:29.350: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:29.350: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:30.016: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:30.016: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:30.016: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:30.016: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:30.022: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:30.022: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:30.022: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:31.002: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:31.002: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:31.002: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:31.002: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:31.010: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:31.010: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:31.010: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:32.222: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:32.222: INFO: Pod daemon-set-926l6 is not available
Jun 22 06:08:32.222: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:32.222: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:32.222: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:32.234: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:32.234: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:32.234: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:33.118: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:33.118: INFO: Pod daemon-set-926l6 is not available
Jun 22 06:08:33.118: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:33.122: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:33.122: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:33.158: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:33.158: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:33.158: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:34.227: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:34.227: INFO: Pod daemon-set-926l6 is not available
Jun 22 06:08:34.227: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:34.227: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:34.227: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:34.242: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:34.242: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:34.242: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:35.049: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:35.049: INFO: Pod daemon-set-926l6 is not available
Jun 22 06:08:35.049: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:35.049: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:35.049: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:35.339: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:35.340: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:35.340: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:36.093: INFO: Wrong image for pod: daemon-set-926l6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:36.094: INFO: Pod daemon-set-926l6 is not available
Jun 22 06:08:36.094: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:36.094: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:36.094: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:36.099: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:36.099: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:36.099: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:37.049: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:37.049: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:37.049: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:37.049: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:37.249: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:37.249: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:37.249: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:38.029: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:38.029: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:38.029: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:38.029: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:38.038: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:38.038: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:38.038: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:39.060: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:39.060: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:39.060: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:39.060: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:39.066: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:39.066: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:39.066: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:40.190: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:40.190: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:40.190: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:40.190: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:40.200: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:40.200: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:40.200: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:41.031: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:41.031: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:41.031: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:41.031: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:41.041: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:41.041: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:41.041: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:42.075: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:42.076: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:42.076: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:42.076: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:42.090: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:42.091: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:42.091: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:43.420: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:43.420: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:43.422: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:43.422: INFO: Pod daemon-set-xxwwd is not available
Jun 22 06:08:43.597: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:43.597: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:43.597: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:44.154: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:44.155: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:44.155: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:44.423: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:44.423: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:44.423: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:45.121: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:45.122: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:45.122: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:45.451: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:45.451: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:45.451: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:46.209: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:46.209: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:46.209: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:46.250: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:46.250: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:46.250: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:47.131: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:47.131: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:47.131: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:47.144: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:47.144: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:47.145: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:48.023: INFO: Wrong image for pod: daemon-set-gjcl6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:48.023: INFO: Pod daemon-set-gjcl6 is not available
Jun 22 06:08:48.023: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:48.023: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:48.042: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:48.042: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:48.042: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:49.227: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:49.227: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:49.235: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:49.235: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:49.235: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:50.055: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:50.055: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:50.055: INFO: Pod daemon-set-rl9pw is not available
Jun 22 06:08:50.067: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:50.067: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:50.067: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:50.999: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:50.999: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:50.999: INFO: Pod daemon-set-rl9pw is not available
Jun 22 06:08:51.013: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:51.013: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:51.013: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:52.001: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:52.001: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:52.001: INFO: Pod daemon-set-rl9pw is not available
Jun 22 06:08:52.010: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:52.010: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:52.010: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:53.264: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:53.264: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:53.264: INFO: Pod daemon-set-rl9pw is not available
Jun 22 06:08:53.343: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:53.343: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:53.343: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:53.999: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:53.999: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:53.999: INFO: Pod daemon-set-rl9pw is not available
Jun 22 06:08:54.007: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:54.007: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:54.007: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:55.109: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:55.109: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:55.494: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:55.495: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:55.495: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:56.108: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:56.108: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:56.363: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:56.363: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:56.363: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:56.998: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:56.998: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:57.005: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:57.005: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:57.006: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:58.016: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:58.016: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:58.016: INFO: Pod daemon-set-p754z is not available
Jun 22 06:08:58.157: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:58.157: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:58.157: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:59.041: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:59.041: INFO: Wrong image for pod: daemon-set-p754z. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:08:59.041: INFO: Pod daemon-set-p754z is not available
Jun 22 06:08:59.051: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:59.051: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:59.051: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:08:59.999: INFO: Pod daemon-set-l5gbv is not available
Jun 22 06:08:59.999: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:00.005: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:00.005: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:00.005: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:01.030: INFO: Pod daemon-set-l5gbv is not available
Jun 22 06:09:01.030: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:01.047: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:01.047: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:01.047: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:02.050: INFO: Pod daemon-set-l5gbv is not available
Jun 22 06:09:02.050: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:02.062: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:02.062: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:02.062: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:03.106: INFO: Pod daemon-set-l5gbv is not available
Jun 22 06:09:03.106: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:03.119: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:03.119: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:03.119: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:04.100: INFO: Pod daemon-set-l5gbv is not available
Jun 22 06:09:04.100: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:04.148: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:04.148: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:04.148: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:05.202: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:05.255: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:05.255: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:05.255: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:06.116: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:06.141: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:06.141: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:06.141: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:07.090: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:07.096: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:07.096: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:07.096: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:08.119: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:08.119: INFO: Pod daemon-set-mrct6 is not available
Jun 22 06:09:08.126: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:08.126: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:08.126: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:09.054: INFO: Wrong image for pod: daemon-set-mrct6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 06:09:09.054: INFO: Pod daemon-set-mrct6 is not available
Jun 22 06:09:09.083: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:09.083: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:09.083: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.034: INFO: Pod daemon-set-rp4qt is not available
Jun 22 06:09:10.040: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.040: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.040: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 22 06:09:10.586: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.587: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.587: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:10.871: INFO: Number of nodes with available pods: 3
Jun 22 06:09:10.871: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:11.884: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:11.884: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:11.884: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:11.891: INFO: Number of nodes with available pods: 3
Jun 22 06:09:11.891: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:13.136: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:13.137: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:13.137: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:13.356: INFO: Number of nodes with available pods: 3
Jun 22 06:09:13.356: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:14.129: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.129: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.129: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.135: INFO: Number of nodes with available pods: 3
Jun 22 06:09:14.136: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:14.881: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.881: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.881: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:14.888: INFO: Number of nodes with available pods: 3
Jun 22 06:09:14.888: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:15.880: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:15.880: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:15.881: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:15.886: INFO: Number of nodes with available pods: 3
Jun 22 06:09:15.886: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:09:17.063: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:17.063: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:17.063: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:09:17.288: INFO: Number of nodes with available pods: 4
Jun 22 06:09:17.288: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-865, will wait for the garbage collector to delete the pods
Jun 22 06:09:17.807: INFO: Deleting DaemonSet.extensions daemon-set took: 178.845963ms
Jun 22 06:09:19.007: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.200286935s
Jun 22 06:09:25.824: INFO: Number of nodes with available pods: 0
Jun 22 06:09:25.824: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 06:09:25.833: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-865/daemonsets","resourceVersion":"658892"},"items":null}

Jun 22 06:09:25.837: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-865/pods","resourceVersion":"658892"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:09:25.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-865" for this suite.
Jun 22 06:10:06.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:10:08.405: INFO: namespace daemonsets-865 deletion completed in 42.538705667s

• [SLOW TEST:114.217 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:10:08.406: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:10:10.441: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 22 06:10:11.054: INFO: Number of nodes with available pods: 0
Jun 22 06:10:11.054: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 22 06:10:12.197: INFO: Number of nodes with available pods: 0
Jun 22 06:10:12.197: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:13.462: INFO: Number of nodes with available pods: 0
Jun 22 06:10:13.462: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:14.226: INFO: Number of nodes with available pods: 0
Jun 22 06:10:14.226: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:15.205: INFO: Number of nodes with available pods: 0
Jun 22 06:10:15.205: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:16.230: INFO: Number of nodes with available pods: 0
Jun 22 06:10:16.230: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:17.260: INFO: Number of nodes with available pods: 0
Jun 22 06:10:17.260: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:18.417: INFO: Number of nodes with available pods: 1
Jun 22 06:10:18.417: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 22 06:10:19.277: INFO: Number of nodes with available pods: 1
Jun 22 06:10:19.277: INFO: Number of running nodes: 0, number of available pods: 1
Jun 22 06:10:20.446: INFO: Number of nodes with available pods: 0
Jun 22 06:10:20.446: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 22 06:10:20.738: INFO: Number of nodes with available pods: 0
Jun 22 06:10:20.738: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:22.008: INFO: Number of nodes with available pods: 0
Jun 22 06:10:22.008: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:23.121: INFO: Number of nodes with available pods: 0
Jun 22 06:10:23.121: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:24.059: INFO: Number of nodes with available pods: 0
Jun 22 06:10:24.059: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:24.745: INFO: Number of nodes with available pods: 0
Jun 22 06:10:24.745: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:25.836: INFO: Number of nodes with available pods: 0
Jun 22 06:10:25.836: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:26.744: INFO: Number of nodes with available pods: 0
Jun 22 06:10:26.744: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:27.744: INFO: Number of nodes with available pods: 0
Jun 22 06:10:27.744: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:28.839: INFO: Number of nodes with available pods: 0
Jun 22 06:10:28.839: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:29.972: INFO: Number of nodes with available pods: 0
Jun 22 06:10:29.972: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:31.214: INFO: Number of nodes with available pods: 0
Jun 22 06:10:31.214: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:31.768: INFO: Number of nodes with available pods: 0
Jun 22 06:10:31.768: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:33.090: INFO: Number of nodes with available pods: 0
Jun 22 06:10:33.090: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:33.968: INFO: Number of nodes with available pods: 0
Jun 22 06:10:33.968: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:34.747: INFO: Number of nodes with available pods: 0
Jun 22 06:10:34.747: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:35.761: INFO: Number of nodes with available pods: 0
Jun 22 06:10:35.761: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:37.075: INFO: Number of nodes with available pods: 0
Jun 22 06:10:37.075: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:38.120: INFO: Number of nodes with available pods: 0
Jun 22 06:10:38.120: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:38.884: INFO: Number of nodes with available pods: 0
Jun 22 06:10:38.884: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:39.744: INFO: Number of nodes with available pods: 0
Jun 22 06:10:39.744: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:40.799: INFO: Number of nodes with available pods: 0
Jun 22 06:10:40.799: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:42.381: INFO: Number of nodes with available pods: 0
Jun 22 06:10:42.381: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:42.794: INFO: Number of nodes with available pods: 0
Jun 22 06:10:42.794: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:10:43.775: INFO: Number of nodes with available pods: 1
Jun 22 06:10:43.775: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3037, will wait for the garbage collector to delete the pods
Jun 22 06:10:44.173: INFO: Deleting DaemonSet.extensions daemon-set took: 207.264514ms
Jun 22 06:10:45.774: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.600297072s
Jun 22 06:10:51.681: INFO: Number of nodes with available pods: 0
Jun 22 06:10:51.681: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 06:10:51.689: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3037/daemonsets","resourceVersion":"659286"},"items":null}

Jun 22 06:10:51.694: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3037/pods","resourceVersion":"659286"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:10:51.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3037" for this suite.
Jun 22 06:11:08.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:11:10.215: INFO: namespace daemonsets-3037 deletion completed in 18.292698726s

• [SLOW TEST:61.809 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:11:10.215: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-88a44b95-94b4-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 06:11:12.353: INFO: Waiting up to 5m0s for pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e" in namespace "secrets-8534" to be "success or failure"
Jun 22 06:11:12.809: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 455.825411ms
Jun 22 06:11:14.814: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.461167201s
Jun 22 06:11:16.941: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.587334333s
Jun 22 06:11:19.022: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.668811616s
Jun 22 06:11:21.035: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.681750417s
STEP: Saw pod success
Jun 22 06:11:21.035: INFO: Pod "pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:11:21.042: INFO: Trying to get logs from node slave5 pod pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:11:21.732: INFO: Waiting for pod pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e to disappear
Jun 22 06:11:21.989: INFO: Pod pod-secrets-88beca01-94b4-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:11:21.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8534" for this suite.
Jun 22 06:11:38.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:11:40.330: INFO: namespace secrets-8534 deletion completed in 18.330380504s

• [SLOW TEST:30.115 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:11:40.331: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6686
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-6686
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6686
Jun 22 06:11:42.956: INFO: Found 0 stateful pods, waiting for 1
Jun 22 06:11:53.164: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 22 06:11:53.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:11:53.945: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:11:53.945: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:11:53.945: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 06:11:53.971: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 06:12:03.979: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 06:12:03.979: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:12:04.494: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:04.494: INFO: ss-0  slave8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:04.494: INFO: ss-1          Pending         []
Jun 22 06:12:04.494: INFO: 
Jun 22 06:12:04.494: INFO: StatefulSet ss has not reached scale 3, at 2
Jun 22 06:12:05.928: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.657269884s
Jun 22 06:12:06.946: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.222612744s
Jun 22 06:12:08.165: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.205164447s
Jun 22 06:12:09.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986129506s
Jun 22 06:12:10.184: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979244339s
Jun 22 06:12:11.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967564463s
Jun 22 06:12:12.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.717312094s
Jun 22 06:12:13.537: INFO: Verifying statefulset ss doesn't scale past 3 for another 619.780387ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6686
Jun 22 06:12:14.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:12:15.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 06:12:15.312: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 06:12:15.312: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 06:12:15.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:12:16.117: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 06:12:16.117: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 06:12:16.117: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 06:12:16.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:12:16.874: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 06:12:16.874: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 06:12:16.874: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 06:12:16.938: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:12:16.938: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:12:16.938: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 22 06:12:16.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:12:17.504: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:12:17.504: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:12:17.504: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 06:12:17.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:12:18.178: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:12:18.178: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:12:18.178: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 06:12:18.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:12:18.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:12:18.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:12:18.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 06:12:18.771: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:12:18.811: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 22 06:12:28.826: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 06:12:28.826: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 06:12:28.826: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 06:12:29.064: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:29.064: INFO: ss-0  slave8  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:29.064: INFO: ss-1  slave6  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:29.064: INFO: ss-2  slave7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:29.064: INFO: 
Jun 22 06:12:29.064: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:30.131: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:30.131: INFO: ss-0  slave8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:30.131: INFO: ss-1  slave6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:30.131: INFO: ss-2  slave7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:30.131: INFO: 
Jun 22 06:12:30.131: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:31.326: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:31.326: INFO: ss-0  slave8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:31.326: INFO: ss-1  slave6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:31.326: INFO: ss-2  slave7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:31.326: INFO: 
Jun 22 06:12:31.326: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:32.560: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:32.561: INFO: ss-0  slave8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:32.561: INFO: ss-1  slave6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:32.561: INFO: ss-2  slave7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:32.561: INFO: 
Jun 22 06:12:32.561: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:33.623: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:33.623: INFO: ss-0  slave8  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:33.623: INFO: ss-1  slave6  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:33.623: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:33.623: INFO: 
Jun 22 06:12:33.623: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:34.636: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:34.637: INFO: ss-0  slave8  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:11:43 +0000 UTC  }]
Jun 22 06:12:34.637: INFO: ss-1  slave6  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:18 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:34.637: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:34.637: INFO: 
Jun 22 06:12:34.637: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 06:12:35.652: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:35.652: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:35.652: INFO: 
Jun 22 06:12:35.652: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 22 06:12:36.662: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:36.662: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:36.662: INFO: 
Jun 22 06:12:36.662: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 22 06:12:37.718: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:37.718: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:37.718: INFO: 
Jun 22 06:12:37.718: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 22 06:12:38.820: INFO: POD   NODE    PHASE    GRACE  CONDITIONS
Jun 22 06:12:38.820: INFO: ss-2  slave7  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:12:04 +0000 UTC  }]
Jun 22 06:12:38.820: INFO: 
Jun 22 06:12:38.820: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6686
Jun 22 06:12:39.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:12:39.979: INFO: rc: 1
Jun 22 06:12:39.979: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc00247f020 exit status 1 <nil> <nil> true [0xc00309ddc0 0xc00309ddd8 0xc00309ddf0] [0xc00309ddc0 0xc00309ddd8 0xc00309ddf0] [0xc00309ddd0 0xc00309dde8] [0x9c00a0 0x9c00a0] 0xc00330e6c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Jun 22 06:12:49.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:12:50.109: INFO: rc: 1
Jun 22 06:12:50.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001d80ea0 exit status 1 <nil> <nil> true [0xc00097e4b0 0xc00097e4c8 0xc00097e4e0] [0xc00097e4b0 0xc00097e4c8 0xc00097e4e0] [0xc00097e4c0 0xc00097e4d8] [0x9c00a0 0x9c00a0] 0xc0030188a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:00.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:00.188: INFO: rc: 1
Jun 22 06:13:00.188: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001d811d0 exit status 1 <nil> <nil> true [0xc00097e4e8 0xc00097e500 0xc00097e518] [0xc00097e4e8 0xc00097e500 0xc00097e518] [0xc00097e4f8 0xc00097e510] [0x9c00a0 0x9c00a0] 0xc003018f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:10.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:10.293: INFO: rc: 1
Jun 22 06:13:10.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00288a1e0 exit status 1 <nil> <nil> true [0xc000697f50 0xc000697fb0 0xc000697ff0] [0xc000697f50 0xc000697fb0 0xc000697ff0] [0xc000697f80 0xc000697fe8] [0x9c00a0 0x9c00a0] 0xc001545980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:20.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:20.417: INFO: rc: 1
Jun 22 06:13:20.417: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00247f650 exit status 1 <nil> <nil> true [0xc00309ddf8 0xc00309de10 0xc00309de28] [0xc00309ddf8 0xc00309de10 0xc00309de28] [0xc00309de08 0xc00309de20] [0x9c00a0 0x9c00a0] 0xc00330eb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:30.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:30.529: INFO: rc: 1
Jun 22 06:13:30.529: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280c4b0 exit status 1 <nil> <nil> true [0xc00026c0e0 0xc00026c2f0 0xc00026c540] [0xc00026c0e0 0xc00026c2f0 0xc00026c540] [0xc00026c230 0xc00026c4c8] [0x9c00a0 0x9c00a0] 0xc002484ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:40.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:41.350: INFO: rc: 1
Jun 22 06:13:41.350: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002562300 exit status 1 <nil> <nil> true [0xc0005c7238 0xc0005c7470 0xc0005c7588] [0xc0005c7238 0xc0005c7470 0xc0005c7588] [0xc0005c7428 0xc0005c7568] [0x9c00a0 0x9c00a0] 0xc002ee62a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:13:51.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:13:51.516: INFO: rc: 1
Jun 22 06:13:51.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002562660 exit status 1 <nil> <nil> true [0xc0005c75d8 0xc0005c7e90 0xc000010508] [0xc0005c75d8 0xc0005c7e90 0xc000010508] [0xc0005c7de8 0xc000010028] [0x9c00a0 0x9c00a0] 0xc002ee6600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:01.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:01.613: INFO: rc: 1
Jun 22 06:14:01.613: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982300 exit status 1 <nil> <nil> true [0xc000696068 0xc000696150 0xc000696280] [0xc000696068 0xc000696150 0xc000696280] [0xc0006960c8 0xc000696250] [0x9c00a0 0x9c00a0] 0xc0033f87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:11.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:11.703: INFO: rc: 1
Jun 22 06:14:11.703: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982630 exit status 1 <nil> <nil> true [0xc000696298 0xc0006963f0 0xc000696488] [0xc000696298 0xc0006963f0 0xc000696488] [0xc000696370 0xc000696408] [0x9c00a0 0x9c00a0] 0xc0033f8d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:21.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:21.792: INFO: rc: 1
Jun 22 06:14:21.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019829c0 exit status 1 <nil> <nil> true [0xc0006964e8 0xc0006965b8 0xc0006966a0] [0xc0006964e8 0xc0006965b8 0xc0006966a0] [0xc000696558 0xc000696640] [0x9c00a0 0x9c00a0] 0xc0033f9080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:31.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:32.064: INFO: rc: 1
Jun 22 06:14:32.064: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280c840 exit status 1 <nil> <nil> true [0xc00026c618 0xc00026c948 0xc00026cd30] [0xc00026c618 0xc00026c948 0xc00026cd30] [0xc00026c7e0 0xc00026cb70] [0x9c00a0 0x9c00a0] 0xc00317c060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:42.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:42.159: INFO: rc: 1
Jun 22 06:14:42.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982d50 exit status 1 <nil> <nil> true [0xc0006966e8 0xc0006967b0 0xc000696840] [0xc0006966e8 0xc0006967b0 0xc000696840] [0xc000696738 0xc000696820] [0x9c00a0 0x9c00a0] 0xc0033f93e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:14:52.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:14:52.249: INFO: rc: 1
Jun 22 06:14:52.249: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280cba0 exit status 1 <nil> <nil> true [0xc00026ce90 0xc00026d100 0xc00026d1f8] [0xc00026ce90 0xc00026d100 0xc00026d1f8] [0xc00026d090 0xc00026d148] [0x9c00a0 0x9c00a0] 0xc00317cde0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:02.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:02.380: INFO: rc: 1
Jun 22 06:15:02.380: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001983500 exit status 1 <nil> <nil> true [0xc0006968b8 0xc000696938 0xc000696a20] [0xc0006968b8 0xc000696938 0xc000696a20] [0xc000696920 0xc000696a18] [0x9c00a0 0x9c00a0] 0xc0033f97a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:12.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:12.625: INFO: rc: 1
Jun 22 06:15:12.625: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280cf60 exit status 1 <nil> <nil> true [0xc00026d280 0xc00026d338 0xc00026d4d0] [0xc00026d280 0xc00026d338 0xc00026d4d0] [0xc00026d310 0xc00026d438] [0x9c00a0 0x9c00a0] 0xc00317d8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:22.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:22.792: INFO: rc: 1
Jun 22 06:15:22.792: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280d290 exit status 1 <nil> <nil> true [0xc00026d4e8 0xc00026d698 0xc00026d8c8] [0xc00026d4e8 0xc00026d698 0xc00026d8c8] [0xc00026d5f8 0xc00026d8a8] [0x9c00a0 0x9c00a0] 0xc0026120c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:32.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:33.101: INFO: rc: 1
Jun 22 06:15:33.101: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982330 exit status 1 <nil> <nil> true [0xc0005c73a0 0xc0005c7558 0xc0005c75d8] [0xc0005c73a0 0xc0005c7558 0xc0005c75d8] [0xc0005c7470 0xc0005c7588] [0x9c00a0 0x9c00a0] 0xc00317cae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:43.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:43.286: INFO: rc: 1
Jun 22 06:15:43.286: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002562330 exit status 1 <nil> <nil> true [0xc000696068 0xc000696150 0xc000696280] [0xc000696068 0xc000696150 0xc000696280] [0xc0006960c8 0xc000696250] [0x9c00a0 0x9c00a0] 0xc002484ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:15:53.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:15:53.663: INFO: rc: 1
Jun 22 06:15:53.663: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280c4e0 exit status 1 <nil> <nil> true [0xc00026c000 0xc00026c230 0xc00026c4c8] [0xc00026c000 0xc00026c230 0xc00026c4c8] [0xc00026c1c0 0xc00026c3c8] [0x9c00a0 0x9c00a0] 0xc0033f87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:03.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:03.789: INFO: rc: 1
Jun 22 06:16:03.789: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0019826c0 exit status 1 <nil> <nil> true [0xc0005c7820 0xc0005c7fd8 0xc000010560] [0xc0005c7820 0xc0005c7fd8 0xc000010560] [0xc0005c7e90 0xc000010508] [0x9c00a0 0x9c00a0] 0xc00317d740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:13.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:13.898: INFO: rc: 1
Jun 22 06:16:13.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982a50 exit status 1 <nil> <nil> true [0xc000010578 0xc000010718 0xc000010868] [0xc000010578 0xc000010718 0xc000010868] [0xc000010690 0xc0000107c0] [0x9c00a0 0x9c00a0] 0xc00317dec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:23.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:24.085: INFO: rc: 1
Jun 22 06:16:24.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001982db0 exit status 1 <nil> <nil> true [0xc0000108e0 0xc000010988 0xc000010b38] [0xc0000108e0 0xc000010988 0xc000010b38] [0xc000010918 0xc000010a98] [0x9c00a0 0x9c00a0] 0xc002612a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:34.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:34.245: INFO: rc: 1
Jun 22 06:16:34.245: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00280c870 exit status 1 <nil> <nil> true [0xc00026c540 0xc00026c7e0 0xc00026cb70] [0xc00026c540 0xc00026c7e0 0xc00026cb70] [0xc00026c660 0xc00026c988] [0x9c00a0 0x9c00a0] 0xc0033f8d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:44.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:44.399: INFO: rc: 1
Jun 22 06:16:44.399: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001983560 exit status 1 <nil> <nil> true [0xc000010c88 0xc000010db8 0xc000011120] [0xc000010c88 0xc000010db8 0xc000011120] [0xc000010d48 0xc0000110e0] [0x9c00a0 0x9c00a0] 0xc002ee6000 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:16:54.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:16:54.736: INFO: rc: 1
Jun 22 06:16:54.736: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001983890 exit status 1 <nil> <nil> true [0xc000011130 0xc000011370 0xc0000113f8] [0xc000011130 0xc000011370 0xc0000113f8] [0xc0000112f0 0xc0000113c8] [0x9c00a0 0x9c00a0] 0xc002ee6360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:17:04.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:17:04.875: INFO: rc: 1
Jun 22 06:17:04.875: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001983bf0 exit status 1 <nil> <nil> true [0xc000011460 0xc0000115c8 0xc000011748] [0xc000011460 0xc0000115c8 0xc000011748] [0xc000011598 0xc000011630] [0x9c00a0 0x9c00a0] 0xc002ee66c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:17:14.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:17:15.018: INFO: rc: 1
Jun 22 06:17:15.018: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a884b0 exit status 1 <nil> <nil> true [0xc00097e000 0xc00097e018 0xc00097e030] [0xc00097e000 0xc00097e018 0xc00097e030] [0xc00097e010 0xc00097e028] [0x9c00a0 0x9c00a0] 0xc0025a0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:17:25.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:17:25.130: INFO: rc: 1
Jun 22 06:17:25.131: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a887e0 exit status 1 <nil> <nil> true [0xc00097e038 0xc00097e050 0xc00097e068] [0xc00097e038 0xc00097e050 0xc00097e068] [0xc00097e048 0xc00097e060] [0x9c00a0 0x9c00a0] 0xc0025a0cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:17:35.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:17:35.308: INFO: rc: 1
Jun 22 06:17:35.308: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002a88450 exit status 1 <nil> <nil> true [0xc0005c73a0 0xc0005c7558 0xc0005c75d8] [0xc0005c73a0 0xc0005c7558 0xc0005c75d8] [0xc0005c7470 0xc0005c7588] [0x9c00a0 0x9c00a0] 0xc002612a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1

Jun 22 06:17:45.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-6686 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:17:45.379: INFO: rc: 1
Jun 22 06:17:45.379: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Jun 22 06:17:45.379: INFO: Scaling statefulset ss to 0
Jun 22 06:17:45.391: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 22 06:17:45.397: INFO: Deleting all statefulset in ns statefulset-6686
Jun 22 06:17:45.401: INFO: Scaling statefulset ss to 0
Jun 22 06:17:45.429: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:17:45.556: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:17:45.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6686" for this suite.
Jun 22 06:18:00.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:18:02.557: INFO: namespace statefulset-6686 deletion completed in 16.741053107s

• [SLOW TEST:382.228 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:18:02.562: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1038
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 06:18:03.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-1038'
Jun 22 06:18:08.872: INFO: stderr: ""
Jun 22 06:18:08.872: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 22 06:18:18.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pod e2e-test-nginx-pod --namespace=kubectl-1038 -o json'
Jun 22 06:18:19.084: INFO: stderr: ""
Jun 22 06:18:19.084: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-06-22T06:18:08Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-1038\",\n        \"resourceVersion\": \"660668\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-1038/pods/e2e-test-nginx-pod\",\n        \"uid\": \"80fce028-94b5-11e9-9ddd-fa163e2c9a22\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-n8bvb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"slave5\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-n8bvb\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-n8bvb\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T06:18:09Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T06:18:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T06:18:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T06:18:08Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://5f601c5a73f12f28d7ad015f61de6065c9a04ce79ee9a6f3583a434e328e0195\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-22T06:18:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.202.67\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.151.174.12\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-22T06:18:09Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 22 06:18:19.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 replace -f - --namespace=kubectl-1038'
Jun 22 06:18:19.985: INFO: stderr: ""
Jun 22 06:18:19.985: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jun 22 06:18:20.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete pods e2e-test-nginx-pod --namespace=kubectl-1038'
Jun 22 06:18:26.739: INFO: stderr: ""
Jun 22 06:18:26.739: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:18:26.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1038" for this suite.
Jun 22 06:18:39.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:18:42.053: INFO: namespace kubectl-1038 deletion completed in 15.303872763s

• [SLOW TEST:39.492 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:18:42.054: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4016
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jun 22 06:18:43.528: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-834690535 proxy --unix-socket=/tmp/kubectl-proxy-unix749949422/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:18:43.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4016" for this suite.
Jun 22 06:18:52.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:18:54.821: INFO: namespace kubectl-4016 deletion completed in 11.151469751s

• [SLOW TEST:12.767 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:18:54.822: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-9d61eeea-94b5-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:18:56.775: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e" in namespace "projected-9257" to be "success or failure"
Jun 22 06:18:56.824: INFO: Pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.574949ms
Jun 22 06:18:58.830: INFO: Pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054329376s
Jun 22 06:19:00.836: INFO: Pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.060684499s
Jun 22 06:19:02.855: INFO: Pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.079262349s
STEP: Saw pod success
Jun 22 06:19:02.855: INFO: Pod "pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:19:02.872: INFO: Trying to get logs from node slave8 pod pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:19:03.453: INFO: Waiting for pod pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e to disappear
Jun 22 06:19:03.460: INFO: Pod pod-projected-configmaps-9d72e73c-94b5-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:19:03.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9257" for this suite.
Jun 22 06:19:16.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:19:18.179: INFO: namespace projected-9257 deletion completed in 14.573056684s

• [SLOW TEST:23.357 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:19:18.179: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 22 06:19:19.428: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:19:32.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5414" for this suite.
Jun 22 06:20:08.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:20:10.645: INFO: namespace init-container-5414 deletion completed in 38.359406873s

• [SLOW TEST:52.466 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:20:10.646: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2035
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 06:20:11.169: INFO: Waiting up to 5m0s for pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e" in namespace "emptydir-2035" to be "success or failure"
Jun 22 06:20:11.173: INFO: Pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.78605ms
Jun 22 06:20:13.200: INFO: Pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030318201s
Jun 22 06:20:15.466: INFO: Pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.296591958s
Jun 22 06:20:17.482: INFO: Pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.312673442s
STEP: Saw pod success
Jun 22 06:20:17.482: INFO: Pod "pod-c9fcad95-94b5-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:20:17.486: INFO: Trying to get logs from node slave7 pod pod-c9fcad95-94b5-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:20:17.647: INFO: Waiting for pod pod-c9fcad95-94b5-11e9-8f59-1e22372c056e to disappear
Jun 22 06:20:17.843: INFO: Pod pod-c9fcad95-94b5-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:20:17.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2035" for this suite.
Jun 22 06:20:30.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:20:32.194: INFO: namespace emptydir-2035 deletion completed in 14.342285163s

• [SLOW TEST:21.549 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:20:32.195: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5252
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-d7295759-94b5-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 06:20:33.547: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e" in namespace "projected-5252" to be "success or failure"
Jun 22 06:20:33.582: INFO: Pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 35.053109ms
Jun 22 06:20:35.593: INFO: Pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045914639s
Jun 22 06:20:37.958: INFO: Pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.410756205s
Jun 22 06:20:40.270: INFO: Pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.722900505s
STEP: Saw pod success
Jun 22 06:20:40.270: INFO: Pod "pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:20:40.420: INFO: Trying to get logs from node slave5 pod pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:20:41.354: INFO: Waiting for pod pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e to disappear
Jun 22 06:20:41.359: INFO: Pod pod-projected-secrets-d72c07c4-94b5-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:20:41.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5252" for this suite.
Jun 22 06:20:52.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:20:53.769: INFO: namespace projected-5252 deletion completed in 12.39146205s

• [SLOW TEST:21.574 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:20:53.769: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4372/configmap-test-e3fba2c6-94b5-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:20:54.867: INFO: Waiting up to 5m0s for pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e" in namespace "configmap-4372" to be "success or failure"
Jun 22 06:20:54.872: INFO: Pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.019409ms
Jun 22 06:20:56.878: INFO: Pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010885735s
Jun 22 06:20:58.882: INFO: Pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014742925s
Jun 22 06:21:00.887: INFO: Pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019503398s
STEP: Saw pod success
Jun 22 06:21:00.887: INFO: Pod "pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:21:00.890: INFO: Trying to get logs from node slave8 pod pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e container env-test: <nil>
STEP: delete the pod
Jun 22 06:21:01.075: INFO: Waiting for pod pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e to disappear
Jun 22 06:21:01.083: INFO: Pod pod-configmaps-e41b5cfd-94b5-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:21:01.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4372" for this suite.
Jun 22 06:21:11.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:21:13.088: INFO: namespace configmap-4372 deletion completed in 11.999204827s

• [SLOW TEST:19.319 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:21:13.088: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4374
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e
Jun 22 06:21:14.326: INFO: Pod name my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e: Found 0 pods out of 1
Jun 22 06:21:19.332: INFO: Pod name my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e: Found 1 pods out of 1
Jun 22 06:21:19.332: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e" are running
Jun 22 06:21:21.341: INFO: Pod "my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e-jdkc5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:21:14 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:21:14 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:21:14 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:21:14 +0000 UTC Reason: Message:}])
Jun 22 06:21:21.341: INFO: Trying to dial the pod
Jun 22 06:21:26.509: INFO: Controller my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e: Got expected result from replica 1 [my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e-jdkc5]: "my-hostname-basic-efa4edb5-94b5-11e9-8f59-1e22372c056e-jdkc5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:21:26.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4374" for this suite.
Jun 22 06:21:40.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:21:42.391: INFO: namespace replication-controller-4374 deletion completed in 15.875192783s

• [SLOW TEST:29.304 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:21:42.392: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4143
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 06:21:44.745: INFO: Waiting up to 5m0s for pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e" in namespace "emptydir-4143" to be "success or failure"
Jun 22 06:21:44.857: INFO: Pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 112.175551ms
Jun 22 06:21:46.862: INFO: Pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116998948s
Jun 22 06:21:48.940: INFO: Pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194962167s
Jun 22 06:21:51.245: INFO: Pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.500416644s
STEP: Saw pod success
Jun 22 06:21:51.245: INFO: Pod "pod-01a8c65e-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:21:51.366: INFO: Trying to get logs from node slave5 pod pod-01a8c65e-94b6-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:21:51.625: INFO: Waiting for pod pod-01a8c65e-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:21:51.630: INFO: Pod pod-01a8c65e-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:21:51.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4143" for this suite.
Jun 22 06:22:05.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:22:07.939: INFO: namespace emptydir-4143 deletion completed in 16.285669049s

• [SLOW TEST:25.548 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:22:07.943: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5693
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:22:09.448: INFO: Waiting up to 5m0s for pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e" in namespace "downward-api-5693" to be "success or failure"
Jun 22 06:22:09.458: INFO: Pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 9.548329ms
Jun 22 06:22:11.463: INFO: Pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015104688s
Jun 22 06:22:13.729: INFO: Pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.280596275s
Jun 22 06:22:15.875: INFO: Pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.427182803s
STEP: Saw pod success
Jun 22 06:22:15.875: INFO: Pod "downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:22:15.911: INFO: Trying to get logs from node slave5 pod downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:22:16.860: INFO: Waiting for pod downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:22:16.868: INFO: Pod downwardapi-volume-108a1f3d-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:22:16.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5693" for this suite.
Jun 22 06:22:29.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:22:31.214: INFO: namespace downward-api-5693 deletion completed in 14.340226464s

• [SLOW TEST:23.271 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:22:31.214: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4426
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:22:33.365: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e" in namespace "projected-4426" to be "success or failure"
Jun 22 06:22:33.821: INFO: Pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 455.518216ms
Jun 22 06:22:35.829: INFO: Pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.463715535s
Jun 22 06:22:38.068: INFO: Pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702666539s
Jun 22 06:22:40.101: INFO: Pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.735572816s
STEP: Saw pod success
Jun 22 06:22:40.101: INFO: Pod "downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:22:40.112: INFO: Trying to get logs from node slave8 pod downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:22:40.246: INFO: Waiting for pod downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:22:40.258: INFO: Pod downwardapi-volume-1e79fdb3-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:22:40.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4426" for this suite.
Jun 22 06:22:50.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:22:52.227: INFO: namespace projected-4426 deletion completed in 11.963272619s

• [SLOW TEST:21.014 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:22:52.228: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-817
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:23:00.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-817" for this suite.
Jun 22 06:23:48.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:23:50.587: INFO: namespace kubelet-test-817 deletion completed in 49.976882468s

• [SLOW TEST:58.359 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:23:50.587: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-2325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 06:24:04.447: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:04.459: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:06.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:06.547: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:08.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:08.465: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:10.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:10.735: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:12.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:12.465: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:14.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:14.472: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:16.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:16.540: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:18.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:18.690: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:20.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:20.559: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:22.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:22.530: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:24.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:24.675: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 06:24:26.459: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 06:24:26.468: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:24:26.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2325" for this suite.
Jun 22 06:24:56.549: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:24:58.234: INFO: namespace container-lifecycle-hook-2325 deletion completed in 31.755754473s

• [SLOW TEST:67.647 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:24:58.236: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9072
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 06:24:59.337: INFO: Waiting up to 5m0s for pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e" in namespace "emptydir-9072" to be "success or failure"
Jun 22 06:24:59.363: INFO: Pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 25.366892ms
Jun 22 06:25:01.459: INFO: Pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121591161s
Jun 22 06:25:03.753: INFO: Pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.415877239s
Jun 22 06:25:05.791: INFO: Pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.453951861s
STEP: Saw pod success
Jun 22 06:25:05.791: INFO: Pod "pod-75b2811a-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:25:05.831: INFO: Trying to get logs from node slave8 pod pod-75b2811a-94b6-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:25:06.225: INFO: Waiting for pod pod-75b2811a-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:25:06.301: INFO: Pod pod-75b2811a-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:25:06.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9072" for this suite.
Jun 22 06:25:17.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:25:19.561: INFO: namespace emptydir-9072 deletion completed in 13.051953908s

• [SLOW TEST:21.325 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:25:19.561: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6322
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:25:20.879: INFO: Waiting up to 5m0s for pod "downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e" in namespace "downward-api-6322" to be "success or failure"
Jun 22 06:25:20.885: INFO: Pod "downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.514359ms
Jun 22 06:25:22.960: INFO: Pod "downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.081021696s
Jun 22 06:25:25.081: INFO: Pod "downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.202198788s
STEP: Saw pod success
Jun 22 06:25:25.081: INFO: Pod "downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:25:25.132: INFO: Trying to get logs from node slave6 pod downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:25:25.937: INFO: Waiting for pod downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:25:26.002: INFO: Pod downwardapi-volume-827f1249-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:25:26.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6322" for this suite.
Jun 22 06:25:38.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:25:40.827: INFO: namespace downward-api-6322 deletion completed in 14.491887364s

• [SLOW TEST:21.266 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:25:40.828: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7887
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jun 22 06:25:41.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7887'
Jun 22 06:25:42.522: INFO: stderr: ""
Jun 22 06:25:42.522: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:25:42.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:25:43.125: INFO: stderr: ""
Jun 22 06:25:43.125: INFO: stdout: "update-demo-nautilus-ht8pw "
STEP: Replicas for name=update-demo: expected=2 actual=1
Jun 22 06:25:48.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:25:48.311: INFO: stderr: ""
Jun 22 06:25:48.311: INFO: stdout: "update-demo-nautilus-ht8pw update-demo-nautilus-xbvrd "
Jun 22 06:25:48.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-ht8pw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:25:48.618: INFO: stderr: ""
Jun 22 06:25:48.618: INFO: stdout: ""
Jun 22 06:25:48.618: INFO: update-demo-nautilus-ht8pw is created but not running
Jun 22 06:25:53.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:25:53.745: INFO: stderr: ""
Jun 22 06:25:53.745: INFO: stdout: "update-demo-nautilus-ht8pw update-demo-nautilus-xbvrd "
Jun 22 06:25:53.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-ht8pw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:25:54.045: INFO: stderr: ""
Jun 22 06:25:54.045: INFO: stdout: "true"
Jun 22 06:25:54.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-ht8pw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:25:54.301: INFO: stderr: ""
Jun 22 06:25:54.301: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:25:54.301: INFO: validating pod update-demo-nautilus-ht8pw
Jun 22 06:25:54.372: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:25:54.372: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:25:54.372: INFO: update-demo-nautilus-ht8pw is verified up and running
Jun 22 06:25:54.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:25:54.559: INFO: stderr: ""
Jun 22 06:25:54.559: INFO: stdout: "true"
Jun 22 06:25:54.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:25:54.743: INFO: stderr: ""
Jun 22 06:25:54.743: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:25:54.743: INFO: validating pod update-demo-nautilus-xbvrd
Jun 22 06:25:54.778: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:25:54.778: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:25:54.778: INFO: update-demo-nautilus-xbvrd is verified up and running
STEP: scaling down the replication controller
Jun 22 06:25:54.780: INFO: scanned /root for discovery docs: <nil>
Jun 22 06:25:54.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7887'
Jun 22 06:25:56.550: INFO: stderr: ""
Jun 22 06:25:56.550: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:25:56.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:25:56.785: INFO: stderr: ""
Jun 22 06:25:56.785: INFO: stdout: "update-demo-nautilus-ht8pw update-demo-nautilus-xbvrd "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 06:26:01.785: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:26:02.268: INFO: stderr: ""
Jun 22 06:26:02.268: INFO: stdout: "update-demo-nautilus-xbvrd "
Jun 22 06:26:02.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:02.645: INFO: stderr: ""
Jun 22 06:26:02.645: INFO: stdout: "true"
Jun 22 06:26:02.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:02.855: INFO: stderr: ""
Jun 22 06:26:02.855: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:26:02.855: INFO: validating pod update-demo-nautilus-xbvrd
Jun 22 06:26:02.869: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:26:02.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:26:02.869: INFO: update-demo-nautilus-xbvrd is verified up and running
STEP: scaling up the replication controller
Jun 22 06:26:02.872: INFO: scanned /root for discovery docs: <nil>
Jun 22 06:26:02.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7887'
Jun 22 06:26:04.332: INFO: stderr: ""
Jun 22 06:26:04.332: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:26:04.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:26:04.651: INFO: stderr: ""
Jun 22 06:26:04.651: INFO: stdout: "update-demo-nautilus-s6mfp update-demo-nautilus-xbvrd "
Jun 22 06:26:04.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-s6mfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:04.834: INFO: stderr: ""
Jun 22 06:26:04.834: INFO: stdout: ""
Jun 22 06:26:04.834: INFO: update-demo-nautilus-s6mfp is created but not running
Jun 22 06:26:09.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7887'
Jun 22 06:26:10.103: INFO: stderr: ""
Jun 22 06:26:10.103: INFO: stdout: "update-demo-nautilus-s6mfp update-demo-nautilus-xbvrd "
Jun 22 06:26:10.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-s6mfp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:10.234: INFO: stderr: ""
Jun 22 06:26:10.234: INFO: stdout: "true"
Jun 22 06:26:10.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-s6mfp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:10.628: INFO: stderr: ""
Jun 22 06:26:10.628: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:26:10.628: INFO: validating pod update-demo-nautilus-s6mfp
Jun 22 06:26:10.635: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:26:10.635: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:26:10.635: INFO: update-demo-nautilus-s6mfp is verified up and running
Jun 22 06:26:10.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:10.789: INFO: stderr: ""
Jun 22 06:26:10.789: INFO: stdout: "true"
Jun 22 06:26:10.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-xbvrd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7887'
Jun 22 06:26:11.045: INFO: stderr: ""
Jun 22 06:26:11.045: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:26:11.045: INFO: validating pod update-demo-nautilus-xbvrd
Jun 22 06:26:11.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:26:11.053: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:26:11.053: INFO: update-demo-nautilus-xbvrd is verified up and running
STEP: using delete to clean up resources
Jun 22 06:26:11.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7887'
Jun 22 06:26:11.209: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 06:26:11.209: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 06:26:11.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7887'
Jun 22 06:26:11.683: INFO: stderr: "No resources found.\n"
Jun 22 06:26:11.683: INFO: stdout: ""
Jun 22 06:26:11.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=update-demo --namespace=kubectl-7887 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:26:12.022: INFO: stderr: ""
Jun 22 06:26:12.022: INFO: stdout: "update-demo-nautilus-s6mfp\nupdate-demo-nautilus-xbvrd\n"
Jun 22 06:26:12.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7887'
Jun 22 06:26:12.676: INFO: stderr: "No resources found.\n"
Jun 22 06:26:12.676: INFO: stdout: ""
Jun 22 06:26:12.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=update-demo --namespace=kubectl-7887 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:26:13.026: INFO: stderr: ""
Jun 22 06:26:13.026: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:26:13.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7887" for this suite.
Jun 22 06:26:55.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:26:57.175: INFO: namespace kubectl-7887 deletion completed in 44.071327845s

• [SLOW TEST:76.347 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:26:57.175: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9731
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 06:26:58.705: INFO: Waiting up to 5m0s for pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e" in namespace "emptydir-9731" to be "success or failure"
Jun 22 06:26:58.953: INFO: Pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 248.037174ms
Jun 22 06:27:01.236: INFO: Pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530683996s
Jun 22 06:27:03.875: INFO: Pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.169699217s
Jun 22 06:27:05.991: INFO: Pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.285708377s
STEP: Saw pod success
Jun 22 06:27:05.991: INFO: Pod "pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:27:06.003: INFO: Trying to get logs from node slave6 pod pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:27:06.801: INFO: Waiting for pod pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e to disappear
Jun 22 06:27:06.815: INFO: Pod pod-bcbc7d10-94b6-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:27:06.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9731" for this suite.
Jun 22 06:27:17.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:27:19.255: INFO: namespace emptydir-9731 deletion completed in 12.433962289s

• [SLOW TEST:22.080 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:27:19.256: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9457
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 22 06:27:21.080: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:27:33.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9457" for this suite.
Jun 22 06:27:50.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:27:52.149: INFO: namespace init-container-9457 deletion completed in 18.203565039s

• [SLOW TEST:32.893 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:27:52.149: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4190
Jun 22 06:28:02.274: INFO: Started pod liveness-exec in namespace container-probe-4190
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 06:28:02.577: INFO: Initial restart count of pod liveness-exec is 0
Jun 22 06:28:50.922: INFO: Restart count of pod container-probe-4190/liveness-exec is now 1 (48.344776121s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:28:51.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4190" for this suite.
Jun 22 06:29:07.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:29:10.765: INFO: namespace container-probe-4190 deletion completed in 19.381390732s

• [SLOW TEST:78.617 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:29:10.767: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8128
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-0c45bcf9-94b7-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 06:29:12.117: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e" in namespace "projected-8128" to be "success or failure"
Jun 22 06:29:12.314: INFO: Pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 197.433931ms
Jun 22 06:29:14.322: INFO: Pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205277229s
Jun 22 06:29:16.373: INFO: Pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.255795419s
Jun 22 06:29:18.501: INFO: Pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.384747444s
STEP: Saw pod success
Jun 22 06:29:18.502: INFO: Pod "pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:29:18.767: INFO: Trying to get logs from node slave8 pod pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:29:19.392: INFO: Waiting for pod pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e to disappear
Jun 22 06:29:19.712: INFO: Pod pod-projected-secrets-0c70cc5e-94b7-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:29:19.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8128" for this suite.
Jun 22 06:29:29.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:29:31.639: INFO: namespace projected-8128 deletion completed in 11.918711821s

• [SLOW TEST:20.873 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:29:31.639: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6507
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:29:33.560: INFO: Waiting up to 5m0s for pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e" in namespace "projected-6507" to be "success or failure"
Jun 22 06:29:33.697: INFO: Pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 136.658068ms
Jun 22 06:29:35.735: INFO: Pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.174753044s
Jun 22 06:29:37.743: INFO: Pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e": Phase="Running", Reason="", readiness=true. Elapsed: 4.182482785s
Jun 22 06:29:39.751: INFO: Pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.190318792s
STEP: Saw pod success
Jun 22 06:29:39.751: INFO: Pod "downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:29:39.758: INFO: Trying to get logs from node slave6 pod downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:29:40.104: INFO: Waiting for pod downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e to disappear
Jun 22 06:29:40.266: INFO: Pod downwardapi-volume-191ba8f3-94b7-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:29:40.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6507" for this suite.
Jun 22 06:29:50.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:29:52.037: INFO: namespace projected-6507 deletion completed in 11.765274148s

• [SLOW TEST:20.398 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:29:52.039: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3974/configmap-test-2512a3f3-94b7-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:29:54.610: INFO: Waiting up to 5m0s for pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e" in namespace "configmap-3974" to be "success or failure"
Jun 22 06:29:54.640: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 30.012158ms
Jun 22 06:29:56.652: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042237794s
Jun 22 06:29:58.707: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096953333s
Jun 22 06:30:00.819: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209462988s
Jun 22 06:30:02.835: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.225295424s
STEP: Saw pod success
Jun 22 06:30:02.835: INFO: Pod "pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:30:02.854: INFO: Trying to get logs from node slave7 pod pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e container env-test: <nil>
STEP: delete the pod
Jun 22 06:30:03.352: INFO: Waiting for pod pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e to disappear
Jun 22 06:30:03.390: INFO: Pod pod-configmaps-2553bb31-94b7-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:30:03.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3974" for this suite.
Jun 22 06:30:19.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:30:21.743: INFO: namespace configmap-3974 deletion completed in 18.346696358s

• [SLOW TEST:29.703 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:30:21.744: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4609
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jun 22 06:30:23.058: INFO: Waiting up to 5m0s for pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e" in namespace "var-expansion-4609" to be "success or failure"
Jun 22 06:30:23.064: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033958ms
Jun 22 06:30:25.109: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051491493s
Jun 22 06:30:27.171: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113230675s
Jun 22 06:30:29.452: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.393987096s
Jun 22 06:30:31.469: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.411438215s
STEP: Saw pod success
Jun 22 06:30:31.469: INFO: Pod "var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:30:31.752: INFO: Trying to get logs from node slave5 pod var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:30:32.645: INFO: Waiting for pod var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e to disappear
Jun 22 06:30:32.649: INFO: Pod var-expansion-369b032b-94b7-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:30:32.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4609" for this suite.
Jun 22 06:30:45.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:30:48.174: INFO: namespace var-expansion-4609 deletion completed in 15.497907541s

• [SLOW TEST:26.438 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:30:48.182: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jun 22 06:30:50.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 --namespace=kubectl-2335 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 22 06:31:05.796: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 22 06:31:05.796: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:31:07.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2335" for this suite.
Jun 22 06:31:24.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:31:25.988: INFO: namespace kubectl-2335 deletion completed in 17.931188358s

• [SLOW TEST:37.806 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:31:25.989: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1918
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0622 06:32:00.233853      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 06:32:00.233: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:32:00.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1918" for this suite.
Jun 22 06:32:18.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:32:20.046: INFO: namespace gc-1918 deletion completed in 19.806495889s

• [SLOW TEST:54.057 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:32:20.046: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9242
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 22 06:32:30.397: INFO: Successfully updated pod "annotationupdate7d5aeb77-94b7-11e9-8f59-1e22372c056e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:32:30.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9242" for this suite.
Jun 22 06:33:09.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:33:11.357: INFO: namespace downward-api-9242 deletion completed in 40.648372251s

• [SLOW TEST:51.311 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:33:11.357: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6636
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 06:33:13.638: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:13.639: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:13.639: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:13.657: INFO: Number of nodes with available pods: 0
Jun 22 06:33:13.657: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:15.293: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:15.293: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:15.293: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:15.391: INFO: Number of nodes with available pods: 0
Jun 22 06:33:15.391: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:16.060: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.060: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.060: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.120: INFO: Number of nodes with available pods: 0
Jun 22 06:33:16.120: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:16.664: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.664: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.664: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:16.676: INFO: Number of nodes with available pods: 0
Jun 22 06:33:16.676: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:17.733: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:17.733: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:17.733: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:17.895: INFO: Number of nodes with available pods: 0
Jun 22 06:33:17.895: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:18.761: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:18.761: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:18.761: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:19.172: INFO: Number of nodes with available pods: 0
Jun 22 06:33:19.172: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:20.054: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:20.054: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:20.054: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:20.316: INFO: Number of nodes with available pods: 1
Jun 22 06:33:20.316: INFO: Node slave5 is running more than one daemon pod
Jun 22 06:33:20.894: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:20.894: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:20.894: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:21.146: INFO: Number of nodes with available pods: 3
Jun 22 06:33:21.146: INFO: Node slave8 is running more than one daemon pod
Jun 22 06:33:21.910: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:21.910: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:21.910: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:22.108: INFO: Number of nodes with available pods: 4
Jun 22 06:33:22.108: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 22 06:33:22.386: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:22.386: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:22.386: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 06:33:22.542: INFO: Number of nodes with available pods: 4
Jun 22 06:33:22.542: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6636, will wait for the garbage collector to delete the pods
Jun 22 06:33:25.439: INFO: Deleting DaemonSet.extensions daemon-set took: 320.789158ms
Jun 22 06:33:26.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400284518s
Jun 22 06:33:31.965: INFO: Number of nodes with available pods: 0
Jun 22 06:33:31.965: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 06:33:31.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6636/daemonsets","resourceVersion":"664193"},"items":null}

Jun 22 06:33:31.982: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6636/pods","resourceVersion":"664193"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:33:32.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6636" for this suite.
Jun 22 06:33:50.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:33:52.371: INFO: namespace daemonsets-6636 deletion completed in 20.299096719s

• [SLOW TEST:41.014 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:33:52.371: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4834
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:33:54.183: INFO: (0) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.599283ms)
Jun 22 06:33:54.192: INFO: (1) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.198149ms)
Jun 22 06:33:54.198: INFO: (2) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.06122ms)
Jun 22 06:33:54.208: INFO: (3) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.464994ms)
Jun 22 06:33:54.214: INFO: (4) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.863206ms)
Jun 22 06:33:54.221: INFO: (5) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.008116ms)
Jun 22 06:33:54.230: INFO: (6) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.980053ms)
Jun 22 06:33:54.235: INFO: (7) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.168607ms)
Jun 22 06:33:54.240: INFO: (8) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.802582ms)
Jun 22 06:33:54.244: INFO: (9) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.111995ms)
Jun 22 06:33:54.248: INFO: (10) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.348873ms)
Jun 22 06:33:54.253: INFO: (11) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.477332ms)
Jun 22 06:33:54.258: INFO: (12) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.765063ms)
Jun 22 06:33:54.262: INFO: (13) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.948054ms)
Jun 22 06:33:54.413: INFO: (14) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 151.842735ms)
Jun 22 06:33:54.421: INFO: (15) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.4829ms)
Jun 22 06:33:54.428: INFO: (16) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.682403ms)
Jun 22 06:33:54.434: INFO: (17) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.492502ms)
Jun 22 06:33:54.441: INFO: (18) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.920019ms)
Jun 22 06:33:54.446: INFO: (19) /api/v1/nodes/slave5/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.479061ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:33:54.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4834" for this suite.
Jun 22 06:34:06.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:34:08.075: INFO: namespace proxy-4834 deletion completed in 13.621021346s

• [SLOW TEST:15.704 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:34:08.076: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 06:34:20.551: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:20.563: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:22.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:22.742: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:24.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:24.581: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:26.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:26.789: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:28.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:28.704: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:30.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:30.605: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:32.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:32.675: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:34.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:34.584: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:36.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:36.571: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 06:34:38.563: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 06:34:38.861: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:34:38.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3503" for this suite.
Jun 22 06:35:11.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:35:13.343: INFO: namespace container-lifecycle-hook-3503 deletion completed in 34.46750989s

• [SLOW TEST:65.267 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:35:13.343: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5558
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e4d5e53b-94b7-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-e4d5e53b-94b7-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:36:35.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5558" for this suite.
Jun 22 06:37:07.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:37:09.148: INFO: namespace projected-5558 deletion completed in 34.107668046s

• [SLOW TEST:115.805 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:37:09.149: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5149
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 06:37:10.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5149'
Jun 22 06:37:11.139: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:37:11.140: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jun 22 06:37:13.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5149'
Jun 22 06:37:13.999: INFO: stderr: ""
Jun 22 06:37:13.999: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:37:13.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5149" for this suite.
Jun 22 06:37:26.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:37:29.012: INFO: namespace kubectl-5149 deletion completed in 14.981505012s

• [SLOW TEST:19.864 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:37:29.013: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-356dbc73-94b8-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:37:30.485: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e" in namespace "projected-7998" to be "success or failure"
Jun 22 06:37:30.874: INFO: Pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 388.734133ms
Jun 22 06:37:32.997: INFO: Pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.512030226s
Jun 22 06:37:35.104: INFO: Pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.618931888s
Jun 22 06:37:37.191: INFO: Pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.705656096s
STEP: Saw pod success
Jun 22 06:37:37.191: INFO: Pod "pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:37:37.223: INFO: Trying to get logs from node slave8 pod pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:37:37.791: INFO: Waiting for pod pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:37:37.799: INFO: Pod pod-projected-configmaps-3589a7eb-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:37:37.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7998" for this suite.
Jun 22 06:37:50.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:37:51.644: INFO: namespace projected-7998 deletion completed in 13.837927627s

• [SLOW TEST:22.632 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:37:51.645: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2190
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:37:52.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2190" for this suite.
Jun 22 06:38:27.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:38:30.178: INFO: namespace pods-2190 deletion completed in 37.131331274s

• [SLOW TEST:38.534 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:38:30.179: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:38:31.258: INFO: Creating ReplicaSet my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e
Jun 22 06:38:31.363: INFO: Pod name my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e: Found 0 pods out of 1
Jun 22 06:38:36.370: INFO: Pod name my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e: Found 1 pods out of 1
Jun 22 06:38:36.370: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e" is running
Jun 22 06:38:38.391: INFO: Pod "my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e-kgfvl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:38:32 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:38:32 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:38:32 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 06:38:32 +0000 UTC Reason: Message:}])
Jun 22 06:38:38.391: INFO: Trying to dial the pod
Jun 22 06:38:43.669: INFO: Controller my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e: Got expected result from replica 1 [my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e-kgfvl]: "my-hostname-basic-59c67fc8-94b8-11e9-8f59-1e22372c056e-kgfvl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:38:43.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4158" for this suite.
Jun 22 06:39:00.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:39:02.889: INFO: namespace replicaset-4158 deletion completed in 19.046068668s

• [SLOW TEST:32.710 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:39:02.890: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2611
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jun 22 06:39:03.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-2611'
Jun 22 06:39:04.600: INFO: stderr: ""
Jun 22 06:39:04.600: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jun 22 06:39:05.686: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:05.686: INFO: Found 0 / 1
Jun 22 06:39:06.615: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:06.615: INFO: Found 0 / 1
Jun 22 06:39:07.611: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:07.611: INFO: Found 0 / 1
Jun 22 06:39:08.734: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:08.735: INFO: Found 0 / 1
Jun 22 06:39:09.632: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:09.632: INFO: Found 0 / 1
Jun 22 06:39:10.666: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:10.666: INFO: Found 0 / 1
Jun 22 06:39:11.622: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:11.622: INFO: Found 1 / 1
Jun 22 06:39:11.623: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 06:39:11.843: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:39:11.843: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 22 06:39:11.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 logs redis-master-hkjlg redis-master --namespace=kubectl-2611'
Jun 22 06:39:12.506: INFO: stderr: ""
Jun 22 06:39:12.507: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 06:39:10.056 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 06:39:10.056 # Server started, Redis version 3.2.12\n1:M 22 Jun 06:39:10.056 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 06:39:10.056 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 22 06:39:12.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 log redis-master-hkjlg redis-master --namespace=kubectl-2611 --tail=1'
Jun 22 06:39:12.889: INFO: stderr: ""
Jun 22 06:39:12.889: INFO: stdout: "1:M 22 Jun 06:39:10.056 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 22 06:39:12.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 log redis-master-hkjlg redis-master --namespace=kubectl-2611 --limit-bytes=1'
Jun 22 06:39:13.111: INFO: stderr: ""
Jun 22 06:39:13.111: INFO: stdout: " "
STEP: exposing timestamps
Jun 22 06:39:13.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 log redis-master-hkjlg redis-master --namespace=kubectl-2611 --tail=1 --timestamps'
Jun 22 06:39:13.332: INFO: stderr: ""
Jun 22 06:39:13.332: INFO: stdout: "2019-06-22T06:39:10.056965406Z 1:M 22 Jun 06:39:10.056 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 22 06:39:15.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 log redis-master-hkjlg redis-master --namespace=kubectl-2611 --since=1s'
Jun 22 06:39:16.124: INFO: stderr: ""
Jun 22 06:39:16.124: INFO: stdout: ""
Jun 22 06:39:16.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 log redis-master-hkjlg redis-master --namespace=kubectl-2611 --since=24h'
Jun 22 06:39:16.359: INFO: stderr: ""
Jun 22 06:39:16.359: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 06:39:10.056 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 06:39:10.056 # Server started, Redis version 3.2.12\n1:M 22 Jun 06:39:10.056 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 06:39:10.056 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jun 22 06:39:16.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-2611'
Jun 22 06:39:16.560: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 06:39:16.560: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 22 06:39:16.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2611'
Jun 22 06:39:16.755: INFO: stderr: "No resources found.\n"
Jun 22 06:39:16.755: INFO: stdout: ""
Jun 22 06:39:16.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -l name=nginx --namespace=kubectl-2611 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 06:39:16.908: INFO: stderr: ""
Jun 22 06:39:16.908: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:39:16.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2611" for this suite.
Jun 22 06:39:46.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:39:48.620: INFO: namespace kubectl-2611 deletion completed in 31.679870515s

• [SLOW TEST:45.730 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:39:48.621: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jun 22 06:39:49.717: INFO: Waiting up to 5m0s for pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e" in namespace "containers-8613" to be "success or failure"
Jun 22 06:39:49.726: INFO: Pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.418845ms
Jun 22 06:39:51.731: INFO: Pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013034176s
Jun 22 06:39:54.264: INFO: Pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.546119617s
Jun 22 06:39:56.377: INFO: Pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.659732309s
STEP: Saw pod success
Jun 22 06:39:56.377: INFO: Pod "client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:39:56.387: INFO: Trying to get logs from node slave8 pod client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:39:56.990: INFO: Waiting for pod client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:39:57.012: INFO: Pod client-containers-8870cb09-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:39:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8613" for this suite.
Jun 22 06:40:07.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:40:09.257: INFO: namespace containers-8613 deletion completed in 11.857729846s

• [SLOW TEST:20.637 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:40:09.258: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 06:40:10.396: INFO: Waiting up to 5m0s for pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e" in namespace "emptydir-3194" to be "success or failure"
Jun 22 06:40:10.428: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 32.249036ms
Jun 22 06:40:12.439: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043649706s
Jun 22 06:40:14.445: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048841907s
Jun 22 06:40:16.451: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.054948494s
Jun 22 06:40:18.639: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.243485488s
STEP: Saw pod success
Jun 22 06:40:18.639: INFO: Pod "pod-94c7a079-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:40:18.825: INFO: Trying to get logs from node slave6 pod pod-94c7a079-94b8-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:40:18.892: INFO: Waiting for pod pod-94c7a079-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:40:18.905: INFO: Pod pod-94c7a079-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:40:18.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3194" for this suite.
Jun 22 06:40:29.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:40:31.179: INFO: namespace emptydir-3194 deletion completed in 12.264665212s

• [SLOW TEST:21.920 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:40:31.179: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9289
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 22 06:40:32.903: INFO: Waiting up to 5m0s for pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e" in namespace "emptydir-9289" to be "success or failure"
Jun 22 06:40:32.939: INFO: Pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 36.382843ms
Jun 22 06:40:34.997: INFO: Pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.093933147s
Jun 22 06:40:37.016: INFO: Pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112610215s
Jun 22 06:40:39.111: INFO: Pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.208218776s
STEP: Saw pod success
Jun 22 06:40:39.111: INFO: Pod "pod-a2135928-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:40:39.121: INFO: Trying to get logs from node slave7 pod pod-a2135928-94b8-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:40:39.309: INFO: Waiting for pod pod-a2135928-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:40:39.337: INFO: Pod pod-a2135928-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:40:39.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9289" for this suite.
Jun 22 06:40:49.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:40:51.861: INFO: namespace emptydir-9289 deletion completed in 12.510938029s

• [SLOW TEST:20.682 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:40:51.861: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1606
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1606.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1606.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1606.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1606.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 06:41:02.591: INFO: DNS probes using dns-1606/dns-test-ae9db258-94b8-11e9-8f59-1e22372c056e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:41:02.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1606" for this suite.
Jun 22 06:41:15.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:41:17.097: INFO: namespace dns-1606 deletion completed in 14.070878507s

• [SLOW TEST:25.236 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:41:17.098: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-2336
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:41:17.860: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 06:41:22.897: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 06:41:24.942: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 22 06:41:25.185: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2336,SelfLink:/apis/apps/v1/namespaces/deployment-2336/deployments/test-cleanup-deployment,UID:c16f6a1f-94b8-11e9-a9b0-fa163e4f9fd7,ResourceVersion:665941,Generation:1,CreationTimestamp:2019-06-22 06:41:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 06:41:25.195: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 22 06:41:25.195: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 22 06:41:25.195: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-2336,SelfLink:/apis/apps/v1/namespaces/deployment-2336/replicasets/test-cleanup-controller,UID:bd2d1208-94b8-11e9-a9b0-fa163e4f9fd7,ResourceVersion:665942,Generation:1,CreationTimestamp:2019-06-22 06:41:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment c16f6a1f-94b8-11e9-a9b0-fa163e4f9fd7 0xc0034f4d47 0xc0034f4d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 06:41:25.202: INFO: Pod "test-cleanup-controller-r8nk8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-r8nk8,GenerateName:test-cleanup-controller-,Namespace:deployment-2336,SelfLink:/api/v1/namespaces/deployment-2336/pods/test-cleanup-controller-r8nk8,UID:bd1916b0-94b8-11e9-82ac-fa163e446741,ResourceVersion:665932,Generation:0,CreationTimestamp:2019-06-22 06:41:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller bd2d1208-94b8-11e9-a9b0-fa163e4f9fd7 0xc0034f52e7 0xc0034f52e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-r7jcz {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-r7jcz,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-r7jcz true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034f5360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0034f5380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:41:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:41:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:41:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 06:41:17 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:10.151.203.224,StartTime:2019-06-22 06:41:18 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 06:41:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fdaae662f32855b395109dab298f5091656e3e8b24eeff023789ddbb99d6a39f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:41:25.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2336" for this suite.
Jun 22 06:41:39.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:41:41.702: INFO: namespace deployment-2336 deletion completed in 16.201204049s

• [SLOW TEST:24.604 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:41:41.703: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5779
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cbff5452-94b8-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:41:43.534: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e" in namespace "projected-5779" to be "success or failure"
Jun 22 06:41:43.750: INFO: Pod "pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 215.84025ms
Jun 22 06:41:45.854: INFO: Pod "pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319356148s
Jun 22 06:41:47.930: INFO: Pod "pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.395286161s
STEP: Saw pod success
Jun 22 06:41:47.930: INFO: Pod "pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:41:47.938: INFO: Trying to get logs from node slave7 pod pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:41:48.516: INFO: Waiting for pod pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:41:48.535: INFO: Pod pod-projected-configmaps-cc075343-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:41:48.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5779" for this suite.
Jun 22 06:42:00.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:42:02.563: INFO: namespace projected-5779 deletion completed in 14.005486269s

• [SLOW TEST:20.861 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:42:02.564: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9112
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:42:04.654: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e" in namespace "downward-api-9112" to be "success or failure"
Jun 22 06:42:04.756: INFO: Pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 102.325153ms
Jun 22 06:42:06.780: INFO: Pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.125875715s
Jun 22 06:42:08.847: INFO: Pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.192886311s
Jun 22 06:42:10.893: INFO: Pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.239350255s
STEP: Saw pod success
Jun 22 06:42:10.893: INFO: Pod "downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:42:10.900: INFO: Trying to get logs from node slave5 pod downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:42:10.963: INFO: Waiting for pod downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:42:10.973: INFO: Pod downwardapi-volume-d8c1bb5e-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:42:10.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9112" for this suite.
Jun 22 06:42:23.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:42:25.643: INFO: namespace downward-api-9112 deletion completed in 14.653334573s

• [SLOW TEST:23.081 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:42:25.645: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-906
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 22 06:42:27.613: INFO: Waiting up to 5m0s for pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e" in namespace "downward-api-906" to be "success or failure"
Jun 22 06:42:27.627: INFO: Pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.995087ms
Jun 22 06:42:29.643: INFO: Pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030631922s
Jun 22 06:42:31.731: INFO: Pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118636552s
Jun 22 06:42:33.785: INFO: Pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.172502925s
STEP: Saw pod success
Jun 22 06:42:33.785: INFO: Pod "downward-api-e6739183-94b8-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:42:33.811: INFO: Trying to get logs from node slave8 pod downward-api-e6739183-94b8-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:42:35.233: INFO: Waiting for pod downward-api-e6739183-94b8-11e9-8f59-1e22372c056e to disappear
Jun 22 06:42:35.502: INFO: Pod downward-api-e6739183-94b8-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:42:35.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-906" for this suite.
Jun 22 06:42:49.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:42:51.469: INFO: namespace downward-api-906 deletion completed in 15.926031958s

• [SLOW TEST:25.825 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:42:51.470: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7690
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jun 22 06:42:53.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7690'
Jun 22 06:42:59.460: INFO: stderr: ""
Jun 22 06:42:59.460: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:42:59.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7690'
Jun 22 06:42:59.619: INFO: stderr: ""
Jun 22 06:42:59.619: INFO: stdout: "update-demo-nautilus-chm74 "
STEP: Replicas for name=update-demo: expected=2 actual=1
Jun 22 06:43:04.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7690'
Jun 22 06:43:04.863: INFO: stderr: ""
Jun 22 06:43:04.863: INFO: stdout: "update-demo-nautilus-chm74 update-demo-nautilus-lhk4p "
Jun 22 06:43:04.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-chm74 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:05.392: INFO: stderr: ""
Jun 22 06:43:05.392: INFO: stdout: "true"
Jun 22 06:43:05.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-chm74 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:05.626: INFO: stderr: ""
Jun 22 06:43:05.626: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:43:05.626: INFO: validating pod update-demo-nautilus-chm74
Jun 22 06:43:05.764: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:43:05.764: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:43:05.764: INFO: update-demo-nautilus-chm74 is verified up and running
Jun 22 06:43:05.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-lhk4p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:06.186: INFO: stderr: ""
Jun 22 06:43:06.186: INFO: stdout: "true"
Jun 22 06:43:06.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-nautilus-lhk4p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:06.351: INFO: stderr: ""
Jun 22 06:43:06.351: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 06:43:06.351: INFO: validating pod update-demo-nautilus-lhk4p
Jun 22 06:43:06.362: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 06:43:06.362: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 06:43:06.362: INFO: update-demo-nautilus-lhk4p is verified up and running
STEP: rolling-update to new replication controller
Jun 22 06:43:06.367: INFO: scanned /root for discovery docs: <nil>
Jun 22 06:43:06.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7690'
Jun 22 06:43:35.581: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 06:43:35.581: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 06:43:35.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7690'
Jun 22 06:43:35.814: INFO: stderr: ""
Jun 22 06:43:35.814: INFO: stdout: "update-demo-kitten-bkk44 update-demo-kitten-z89nt "
Jun 22 06:43:35.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-kitten-bkk44 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:36.141: INFO: stderr: ""
Jun 22 06:43:36.141: INFO: stdout: "true"
Jun 22 06:43:36.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-kitten-bkk44 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:36.399: INFO: stderr: ""
Jun 22 06:43:36.399: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 06:43:36.399: INFO: validating pod update-demo-kitten-bkk44
Jun 22 06:43:36.493: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 06:43:36.493: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 06:43:36.493: INFO: update-demo-kitten-bkk44 is verified up and running
Jun 22 06:43:36.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-kitten-z89nt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:36.650: INFO: stderr: ""
Jun 22 06:43:36.650: INFO: stdout: "true"
Jun 22 06:43:36.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods update-demo-kitten-z89nt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7690'
Jun 22 06:43:36.914: INFO: stderr: ""
Jun 22 06:43:36.914: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 06:43:36.914: INFO: validating pod update-demo-kitten-z89nt
Jun 22 06:43:36.998: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 06:43:36.998: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 06:43:36.998: INFO: update-demo-kitten-z89nt is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:43:36.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7690" for this suite.
Jun 22 06:44:23.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:44:25.041: INFO: namespace kubectl-7690 deletion completed in 47.75848814s

• [SLOW TEST:93.571 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:44:25.042: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4024
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:44:26.311: INFO: (0) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 29.647448ms)
Jun 22 06:44:26.316: INFO: (1) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.732922ms)
Jun 22 06:44:26.320: INFO: (2) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.457469ms)
Jun 22 06:44:26.328: INFO: (3) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.798009ms)
Jun 22 06:44:26.333: INFO: (4) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.825074ms)
Jun 22 06:44:26.393: INFO: (5) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 59.656309ms)
Jun 22 06:44:26.398: INFO: (6) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.915127ms)
Jun 22 06:44:26.405: INFO: (7) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.972791ms)
Jun 22 06:44:26.410: INFO: (8) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.124908ms)
Jun 22 06:44:26.433: INFO: (9) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 22.458194ms)
Jun 22 06:44:26.439: INFO: (10) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.039081ms)
Jun 22 06:44:26.444: INFO: (11) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.82093ms)
Jun 22 06:44:26.453: INFO: (12) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.556656ms)
Jun 22 06:44:26.493: INFO: (13) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 40.149537ms)
Jun 22 06:44:26.512: INFO: (14) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 18.708541ms)
Jun 22 06:44:26.627: INFO: (15) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 115.212085ms)
Jun 22 06:44:26.647: INFO: (16) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 20.15811ms)
Jun 22 06:44:26.660: INFO: (17) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 12.13561ms)
Jun 22 06:44:26.869: INFO: (18) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 209.540875ms)
Jun 22 06:44:26.876: INFO: (19) /api/v1/nodes/slave5:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.456978ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:44:26.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4024" for this suite.
Jun 22 06:44:39.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:44:40.933: INFO: namespace proxy-4024 deletion completed in 14.052352412s

• [SLOW TEST:15.891 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:44:40.933: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9333
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 06:44:42.456: INFO: Waiting up to 5m0s for pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e" in namespace "emptydir-9333" to be "success or failure"
Jun 22 06:44:42.462: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.693386ms
Jun 22 06:44:44.578: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.121312519s
Jun 22 06:44:46.843: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.386838407s
Jun 22 06:44:48.852: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.395199337s
Jun 22 06:44:50.893: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.43681232s
STEP: Saw pod success
Jun 22 06:44:50.893: INFO: Pod "pod-36d8a38d-94b9-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:44:51.301: INFO: Trying to get logs from node slave6 pod pod-36d8a38d-94b9-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 06:44:51.738: INFO: Waiting for pod pod-36d8a38d-94b9-11e9-8f59-1e22372c056e to disappear
Jun 22 06:44:51.754: INFO: Pod pod-36d8a38d-94b9-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:44:51.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9333" for this suite.
Jun 22 06:45:05.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:45:07.756: INFO: namespace emptydir-9333 deletion completed in 15.994025848s

• [SLOW TEST:26.822 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:45:07.756: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2847
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 22 06:45:19.841: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2847 pod-service-account-4841a1c9-94b9-11e9-8f59-1e22372c056e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 22 06:45:20.063: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2847 pod-service-account-4841a1c9-94b9-11e9-8f59-1e22372c056e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 22 06:45:20.396: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2847 pod-service-account-4841a1c9-94b9-11e9-8f59-1e22372c056e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:45:21.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2847" for this suite.
Jun 22 06:45:37.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:45:39.226: INFO: namespace svcaccounts-2847 deletion completed in 18.039726034s

• [SLOW TEST:31.470 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:45:39.226: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9038
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-5993ce79-94b9-11e9-8f59-1e22372c056e
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:45:40.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9038" for this suite.
Jun 22 06:45:52.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:45:55.501: INFO: namespace configmap-9038 deletion completed in 15.030567717s

• [SLOW TEST:16.274 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:45:55.502: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1186
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:45:56.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 version --client'
Jun 22 06:45:56.619: INFO: stderr: ""
Jun 22 06:45:56.619: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jun 22 06:45:56.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-1186'
Jun 22 06:45:57.402: INFO: stderr: ""
Jun 22 06:45:57.402: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 22 06:45:57.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-1186'
Jun 22 06:45:58.629: INFO: stderr: ""
Jun 22 06:45:58.630: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 06:45:59.766: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:45:59.766: INFO: Found 0 / 1
Jun 22 06:46:00.808: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:00.808: INFO: Found 0 / 1
Jun 22 06:46:01.719: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:01.719: INFO: Found 0 / 1
Jun 22 06:46:02.746: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:02.746: INFO: Found 0 / 1
Jun 22 06:46:03.658: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:03.658: INFO: Found 0 / 1
Jun 22 06:46:04.990: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:04.990: INFO: Found 1 / 1
Jun 22 06:46:04.990: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 06:46:05.019: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:46:05.019: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 06:46:05.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 describe pod redis-master-799xp --namespace=kubectl-1186'
Jun 22 06:46:05.259: INFO: stderr: ""
Jun 22 06:46:05.259: INFO: stdout: "Name:               redis-master-799xp\nNamespace:          kubectl-1186\nPriority:           0\nPriorityClassName:  <none>\nNode:               slave5/192.168.202.67\nStart Time:         Sat, 22 Jun 2019 06:45:57 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.151.174.23\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f48e5d5dc487ccc74ccc0e6ed53729bd64cea305e8b0e70e0ac4c15e85f76c20\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 22 Jun 2019 06:46:02 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-mpwv6 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-mpwv6:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-mpwv6\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  7s    default-scheduler  Successfully assigned kubectl-1186/redis-master-799xp to slave5\n  Normal  Pulled     4s    kubelet, slave5    Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, slave5    Created container redis-master\n  Normal  Started    3s    kubelet, slave5    Started container redis-master\n"
Jun 22 06:46:05.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 describe rc redis-master --namespace=kubectl-1186'
Jun 22 06:46:05.639: INFO: stderr: ""
Jun 22 06:46:05.639: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-1186\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  8s    replication-controller  Created pod: redis-master-799xp\n"
Jun 22 06:46:05.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 describe service redis-master --namespace=kubectl-1186'
Jun 22 06:46:06.225: INFO: stderr: ""
Jun 22 06:46:06.226: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-1186\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.150.34.166\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.151.174.23:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 06:46:06.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 describe node master1'
Jun 22 06:46:06.822: INFO: stderr: ""
Jun 22 06:46:06.822: INFO: stdout: "Name:               master1\nRoles:              master,monitor,node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=true\n                    node-role.kubernetes.io/monitor=true\n                    node-role.kubernetes.io/node=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\nCreationTimestamp:  Wed, 19 Jun 2019 08:17:00 +0000\nTaints:             pro=master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 22 Jun 2019 06:45:47 +0000   Wed, 19 Jun 2019 08:17:00 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 22 Jun 2019 06:45:47 +0000   Wed, 19 Jun 2019 08:17:00 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 22 Jun 2019 06:45:47 +0000   Wed, 19 Jun 2019 08:17:00 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 22 Jun 2019 06:45:47 +0000   Fri, 21 Jun 2019 16:25:13 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.202.23\n  Hostname:    master1\nCapacity:\n cpu:                4\n ephemeral-storage:  206292644Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8174804Ki\n pods:               50\nAllocatable:\n cpu:                3\n ephemeral-storage:  195806884Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             6650516Ki\n pods:               50\nSystem Info:\n Machine ID:                 49412d1590bf7dac851aca3f5b75395c\n System UUID:                40EEFCEB-31C3-4FD2-A9D8-E0F76682770B\n Boot ID:                    ed1e7627-5f88-442e-a28d-44a1acead4a2\n Kernel Version:             4.4.0-116-generic\n OS Image:                   Ubuntu 16.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.1\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nNon-terminated Pods:         (12 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-5h98g    0 (0%)        0 (0%)      0 (0%)           0 (0%)         96m\n  kube-system                calico-node-z8kb9                                          150m (5%)     1 (33%)     64Mi (0%)        4Gi (63%)      2d22h\n  kube-system                kube-apiserver-master1                                     100m (3%)     2 (66%)     256Mi (3%)       6Gi (94%)      2d22h\n  kube-system                kube-controller-manager-master1                            100m (3%)     2 (66%)     100Mi (1%)       6Gi (94%)      2d22h\n  kube-system                kube-dns-master1                                           800m (26%)    2 (66%)     512Mi (7%)       6Gi (94%)      2d22h\n  kube-system                kube-proxy-master1                                         150m (5%)     500m (16%)  64M (0%)         2G (29%)       2d22h\n  kube-system                kube-scheduler-master1                                     80m (2%)      2 (66%)     170Mi (2%)       6Gi (94%)      2d22h\n  kube-system                nginx-proxy-master1                                        25m (0%)      300m (10%)  32M (0%)         512M (7%)      2d22h\n  kube-system                resource-reserver-master1                                  800m (26%)    800m (26%)  512Mi (7%)       512Mi (7%)     2d22h\n  monitoring                 kube-state-metrics-7f59b96454-v7d4q                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d22h\n  monitoring                 node-exporter-sjpd2                                        100m (3%)     200m (6%)   30Mi (0%)        50Mi (0%)      2d22h\n  monitoring                 prometheus-adapter-557df4fb4b-s2qlg                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         2d5h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                2305m (76%)      10800m (360%)\n  memory             1777206Ki (26%)  32388741Ki (487%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Jun 22 06:46:06.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 describe namespace kubectl-1186'
Jun 22 06:46:07.086: INFO: stderr: ""
Jun 22 06:46:07.086: INFO: stdout: "Name:         kubectl-1186\nLabels:       e2e-framework=kubectl\n              e2e-run=eb9d5a15-94ab-11e9-8f59-1e22372c056e\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:46:07.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1186" for this suite.
Jun 22 06:46:41.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:46:43.382: INFO: namespace kubectl-1186 deletion completed in 36.289249827s

• [SLOW TEST:47.880 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:46:43.383: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3728
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jun 22 06:46:45.062: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-834690535 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:46:45.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3728" for this suite.
Jun 22 06:46:57.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:46:59.979: INFO: namespace kubectl-3728 deletion completed in 14.711094635s

• [SLOW TEST:16.596 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:46:59.983: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-3490
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 06:47:01.814: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 06:47:06.954: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:47:07.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3490" for this suite.
Jun 22 06:47:24.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:47:27.517: INFO: namespace replication-controller-3490 deletion completed in 20.143106484s

• [SLOW TEST:27.534 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:47:27.519: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3861
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 22 06:47:30.105: INFO: Waiting up to 5m0s for pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e" in namespace "downward-api-3861" to be "success or failure"
Jun 22 06:47:30.300: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 194.756605ms
Jun 22 06:47:32.308: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203191994s
Jun 22 06:47:34.403: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.297882748s
Jun 22 06:47:37.279: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.174544683s
Jun 22 06:47:39.300: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.19519438s
STEP: Saw pod success
Jun 22 06:47:39.300: INFO: Pod "downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:47:39.415: INFO: Trying to get logs from node slave5 pod downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:47:41.285: INFO: Waiting for pod downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e to disappear
Jun 22 06:47:41.420: INFO: Pod downward-api-9ab5ac3e-94b9-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:47:41.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3861" for this suite.
Jun 22 06:47:56.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:47:59.275: INFO: namespace downward-api-3861 deletion completed in 17.44761349s

• [SLOW TEST:31.756 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:47:59.276: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-42pt
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 06:48:01.007: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-42pt" in namespace "subpath-7735" to be "success or failure"
Jun 22 06:48:01.288: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Pending", Reason="", readiness=false. Elapsed: 281.225053ms
Jun 22 06:48:03.405: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.397796705s
Jun 22 06:48:05.550: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Pending", Reason="", readiness=false. Elapsed: 4.543016504s
Jun 22 06:48:08.026: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Pending", Reason="", readiness=false. Elapsed: 7.019184092s
Jun 22 06:48:10.040: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 9.032807541s
Jun 22 06:48:12.050: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 11.042751767s
Jun 22 06:48:14.429: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 13.421756954s
Jun 22 06:48:16.652: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 15.645238923s
Jun 22 06:48:18.675: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 17.667512288s
Jun 22 06:48:21.033: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 20.026352618s
Jun 22 06:48:23.039: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 22.032049639s
Jun 22 06:48:25.075: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 24.067490065s
Jun 22 06:48:27.087: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Running", Reason="", readiness=true. Elapsed: 26.07980335s
Jun 22 06:48:29.248: INFO: Pod "pod-subpath-test-configmap-42pt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.241119862s
STEP: Saw pod success
Jun 22 06:48:29.248: INFO: Pod "pod-subpath-test-configmap-42pt" satisfied condition "success or failure"
Jun 22 06:48:29.619: INFO: Trying to get logs from node slave8 pod pod-subpath-test-configmap-42pt container test-container-subpath-configmap-42pt: <nil>
STEP: delete the pod
Jun 22 06:48:30.770: INFO: Waiting for pod pod-subpath-test-configmap-42pt to disappear
Jun 22 06:48:30.890: INFO: Pod pod-subpath-test-configmap-42pt no longer exists
STEP: Deleting pod pod-subpath-test-configmap-42pt
Jun 22 06:48:30.890: INFO: Deleting pod "pod-subpath-test-configmap-42pt" in namespace "subpath-7735"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:48:31.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7735" for this suite.
Jun 22 06:48:58.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:49:00.473: INFO: namespace subpath-7735 deletion completed in 29.249340358s

• [SLOW TEST:61.197 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:49:00.474: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9846
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 06:49:12.354: INFO: Successfully updated pod "pod-update-d24ea378-94b9-11e9-8f59-1e22372c056e"
STEP: verifying the updated pod is in kubernetes
Jun 22 06:49:12.666: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:49:12.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9846" for this suite.
Jun 22 06:49:47.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:49:48.981: INFO: namespace pods-9846 deletion completed in 36.25954293s

• [SLOW TEST:48.507 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:49:48.981: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5805
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:49:49.744: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e" in namespace "projected-5805" to be "success or failure"
Jun 22 06:49:49.835: INFO: Pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 90.713649ms
Jun 22 06:49:51.990: INFO: Pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.245621566s
Jun 22 06:49:54.127: INFO: Pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382331928s
Jun 22 06:49:56.137: INFO: Pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.393175454s
STEP: Saw pod success
Jun 22 06:49:56.137: INFO: Pod "downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:49:56.144: INFO: Trying to get logs from node slave6 pod downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:49:56.449: INFO: Waiting for pod downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e to disappear
Jun 22 06:49:56.575: INFO: Pod downwardapi-volume-ee15bc85-94b9-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:49:56.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5805" for this suite.
Jun 22 06:50:10.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:50:12.799: INFO: namespace projected-5805 deletion completed in 16.057754104s

• [SLOW TEST:23.817 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:50:12.799: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6796
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 06:50:14.886: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e" in namespace "projected-6796" to be "success or failure"
Jun 22 06:50:15.178: INFO: Pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 292.033557ms
Jun 22 06:50:17.185: INFO: Pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.298911921s
Jun 22 06:50:19.190: INFO: Pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.303833407s
Jun 22 06:50:21.206: INFO: Pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.320184192s
STEP: Saw pod success
Jun 22 06:50:21.206: INFO: Pod "downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:50:21.211: INFO: Trying to get logs from node slave5 pod downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 06:50:22.542: INFO: Waiting for pod downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e to disappear
Jun 22 06:50:22.549: INFO: Pod downwardapi-volume-fcf92675-94b9-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:50:22.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6796" for this suite.
Jun 22 06:50:36.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:50:38.587: INFO: namespace projected-6796 deletion completed in 16.032081427s

• [SLOW TEST:25.789 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:50:38.588: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8577
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0622 06:50:50.957834      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 06:50:50.957: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:50:50.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8577" for this suite.
Jun 22 06:51:05.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:51:06.943: INFO: namespace gc-8577 deletion completed in 15.980365547s

• [SLOW TEST:28.356 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:51:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6525
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6525
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6525
STEP: Creating statefulset with conflicting port in namespace statefulset-6525
STEP: Waiting until pod test-pod will start running in namespace statefulset-6525
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6525
Jun 22 06:51:18.685: INFO: Observed stateful pod in namespace: statefulset-6525, name: ss-0, uid: 227d48a9-94ba-11e9-82ac-fa163e446741, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 06:51:18.769: INFO: Observed stateful pod in namespace: statefulset-6525, name: ss-0, uid: 227d48a9-94ba-11e9-82ac-fa163e446741, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 06:51:18.804: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6525
STEP: Removing pod with conflicting port in namespace statefulset-6525
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6525 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 22 06:51:28.417: INFO: Deleting all statefulset in ns statefulset-6525
Jun 22 06:51:28.474: INFO: Scaling statefulset ss to 0
Jun 22 06:51:39.151: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 06:51:39.246: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:51:40.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6525" for this suite.
Jun 22 06:51:58.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:51:59.761: INFO: namespace statefulset-6525 deletion completed in 19.694583066s

• [SLOW TEST:52.817 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:51:59.777: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9048
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-3c29c77b-94ba-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 06:52:00.795: INFO: Waiting up to 5m0s for pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e" in namespace "secrets-9048" to be "success or failure"
Jun 22 06:52:01.057: INFO: Pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 261.981563ms
Jun 22 06:52:03.064: INFO: Pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.269103483s
Jun 22 06:52:05.073: INFO: Pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277814659s
Jun 22 06:52:07.303: INFO: Pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.507866842s
STEP: Saw pod success
Jun 22 06:52:07.303: INFO: Pod "pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:52:07.333: INFO: Trying to get logs from node slave7 pod pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:52:07.506: INFO: Waiting for pod pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e to disappear
Jun 22 06:52:07.510: INFO: Pod pod-secrets-3c447c94-94ba-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:52:07.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9048" for this suite.
Jun 22 06:52:19.533: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:52:21.156: INFO: namespace secrets-9048 deletion completed in 13.640833562s

• [SLOW TEST:21.380 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:52:21.157: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1438
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-4920ef25-94ba-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:52:22.736: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e" in namespace "projected-1438" to be "success or failure"
Jun 22 06:52:23.118: INFO: Pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 381.890763ms
Jun 22 06:52:25.361: INFO: Pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.624904236s
Jun 22 06:52:27.403: INFO: Pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.667602587s
Jun 22 06:52:29.616: INFO: Pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.880307152s
STEP: Saw pod success
Jun 22 06:52:29.616: INFO: Pod "pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:52:29.630: INFO: Trying to get logs from node slave7 pod pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:52:30.240: INFO: Waiting for pod pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e to disappear
Jun 22 06:52:30.247: INFO: Pod pod-projected-configmaps-495a768b-94ba-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:52:30.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1438" for this suite.
Jun 22 06:52:48.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:52:50.150: INFO: namespace projected-1438 deletion completed in 19.897671685s

• [SLOW TEST:28.994 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:52:50.151: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4170
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 22 06:52:59.318: INFO: Successfully updated pod "labelsupdate5adbb26a-94ba-11e9-8f59-1e22372c056e"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:52:59.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4170" for this suite.
Jun 22 06:53:35.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:53:37.422: INFO: namespace downward-api-4170 deletion completed in 37.772855591s

• [SLOW TEST:47.271 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:53:37.424: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5439
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-76c066fe-94ba-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 06:53:39.687: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e" in namespace "projected-5439" to be "success or failure"
Jun 22 06:53:39.818: INFO: Pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 131.531952ms
Jun 22 06:53:41.891: INFO: Pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203841941s
Jun 22 06:53:44.050: INFO: Pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363460478s
Jun 22 06:53:46.321: INFO: Pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.634013451s
STEP: Saw pod success
Jun 22 06:53:46.321: INFO: Pod "pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:53:46.334: INFO: Trying to get logs from node slave6 pod pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 06:53:46.527: INFO: Waiting for pod pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e to disappear
Jun 22 06:53:46.784: INFO: Pod pod-projected-configmaps-76e88915-94ba-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:53:46.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5439" for this suite.
Jun 22 06:53:59.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:54:00.951: INFO: namespace projected-5439 deletion completed in 14.098195436s

• [SLOW TEST:23.528 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:54:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 06:54:02.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5437'
Jun 22 06:54:09.076: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 06:54:09.076: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jun 22 06:54:09.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete jobs e2e-test-nginx-job --namespace=kubectl-5437'
Jun 22 06:54:09.995: INFO: stderr: ""
Jun 22 06:54:09.996: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:54:09.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5437" for this suite.
Jun 22 06:54:22.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:54:23.904: INFO: namespace kubectl-5437 deletion completed in 13.872440896s

• [SLOW TEST:22.952 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:54:23.905: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-3576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 22 06:54:25.026: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 06:54:25.242: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 06:54:25.259: INFO: 
Logging pods the kubelet thinks is on node slave5 before test
Jun 22 06:54:25.271: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-crs8x from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:54:25.271: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 06:54:25.271: INFO: istio-cleanup-secrets-lnzkk from istio-system started at 2019-06-21 15:02:51 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container hyperkube ready: false, restart count 0
Jun 22 06:54:25.271: INFO: sonobuoy-e2e-job-c1f815093e224ac6 from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container e2e ready: true, restart count 0
Jun 22 06:54:25.271: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 06:54:25.271: INFO: calico-node-lsmzz from kube-system started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 06:54:25.271: INFO: tiller-deploy-cdd5d96ff-lqrtt from kube-system started at 2019-06-21 15:02:13 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container tiller ready: true, restart count 0
Jun 22 06:54:25.271: INFO: kube-proxy-slave5 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 06:54:25.271: INFO: node-exporter-ghrg9 from monitoring started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.271: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 06:54:25.271: INFO: 
Logging pods the kubelet thinks is on node slave6 before test
Jun 22 06:54:25.283: INFO: kube-proxy-slave6 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 06:54:25.283: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-vc4bj from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 06:54:25.284: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:54:25.284: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 06:54:25.284: INFO: node-exporter-dxldj from monitoring started at 2019-06-21 14:46:43 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.284: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 06:54:25.284: INFO: calico-node-5954w from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.284: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 06:54:25.284: INFO: db-daemon-server-7859f8794b-hkc8d from kube-system started at 2019-06-21 15:02:18 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.284: INFO: 	Container db-daemon-server ready: true, restart count 0
Jun 22 06:54:25.284: INFO: 
Logging pods the kubelet thinks is on node slave7 before test
Jun 22 06:54:25.302: INFO: kube-proxy-slave7 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 06:54:25.302: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-scz7v from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 06:54:25.302: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:54:25.302: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 06:54:25.302: INFO: node-exporter-fr8lm from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.302: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 06:54:25.302: INFO: calico-node-lm5w9 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.302: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 06:54:25.303: INFO: 
Logging pods the kubelet thinks is on node slave8 before test
Jun 22 06:54:25.399: INFO: calico-kube-controllers-594b85bfc9-ngjs5 from kube-system started at 2019-06-21 15:02:32 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 06:54:25.399: INFO: trigger-d85ff74f-kswkh from kube-system started at 2019-06-21 15:02:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container trigger ready: true, restart count 0
Jun 22 06:54:25.399: INFO: kube-proxy-slave8 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 06:54:25.399: INFO: node-exporter-n2mhc from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 06:54:25.399: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 05:09:13 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 06:54:25.399: INFO: calico-node-j5pm7 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 06:54:25.399: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-cckqp from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 06:54:25.399: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 06:54:25.399: INFO: 	Container systemd-logs ready: true, restart count 1
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15aa72d1b467fbbe], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:54:26.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3576" for this suite.
Jun 22 06:54:39.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:54:42.696: INFO: namespace sched-pred-3576 deletion completed in 15.706748611s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:18.792 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:54:42.699: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3550
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 06:54:44.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 version'
Jun 22 06:54:44.508: INFO: stderr: ""
Jun 22 06:54:44.508: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:36:19Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:54:44.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3550" for this suite.
Jun 22 06:54:56.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:54:58.857: INFO: namespace kubectl-3550 deletion completed in 14.053325029s

• [SLOW TEST:16.159 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:54:58.858: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jun 22 06:55:00.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-2569'
Jun 22 06:55:01.187: INFO: stderr: ""
Jun 22 06:55:01.187: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 06:55:02.399: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:02.399: INFO: Found 0 / 1
Jun 22 06:55:03.197: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:03.197: INFO: Found 0 / 1
Jun 22 06:55:04.231: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:04.231: INFO: Found 0 / 1
Jun 22 06:55:05.672: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:05.672: INFO: Found 0 / 1
Jun 22 06:55:06.412: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:06.412: INFO: Found 0 / 1
Jun 22 06:55:07.216: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:07.216: INFO: Found 0 / 1
Jun 22 06:55:08.302: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:08.302: INFO: Found 1 / 1
Jun 22 06:55:08.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 22 06:55:08.307: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:08.307: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 06:55:08.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 patch pod redis-master-bz7jx --namespace=kubectl-2569 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 06:55:08.661: INFO: stderr: ""
Jun 22 06:55:08.661: INFO: stdout: "pod/redis-master-bz7jx patched\n"
STEP: checking annotations
Jun 22 06:55:08.699: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 06:55:08.699: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:55:08.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2569" for this suite.
Jun 22 06:55:41.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:55:42.728: INFO: namespace kubectl-2569 deletion completed in 33.979899502s

• [SLOW TEST:43.871 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:55:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4336
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-4025
STEP: Creating secret with name secret-test-c196d6c3-94ba-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 06:55:46.245: INFO: Waiting up to 5m0s for pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e" in namespace "secrets-4336" to be "success or failure"
Jun 22 06:55:46.259: INFO: Pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 13.401478ms
Jun 22 06:55:48.434: INFO: Pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.189049731s
Jun 22 06:55:50.542: INFO: Pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.296846354s
Jun 22 06:55:52.560: INFO: Pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.31443443s
STEP: Saw pod success
Jun 22 06:55:52.560: INFO: Pod "pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:55:52.565: INFO: Trying to get logs from node slave8 pod pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 06:55:53.668: INFO: Waiting for pod pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e to disappear
Jun 22 06:55:53.687: INFO: Pod pod-secrets-c2aad14b-94ba-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:55:53.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4336" for this suite.
Jun 22 06:56:06.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:56:07.910: INFO: namespace secrets-4336 deletion completed in 14.183598761s
STEP: Destroying namespace "secret-namespace-4025" for this suite.
Jun 22 06:56:20.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:56:21.719: INFO: namespace secret-namespace-4025 deletion completed in 13.808804609s

• [SLOW TEST:38.989 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:56:21.719: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1054
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jun 22 06:56:22.525: INFO: Waiting up to 5m0s for pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e" in namespace "var-expansion-1054" to be "success or failure"
Jun 22 06:56:22.811: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 286.432179ms
Jun 22 06:56:24.831: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.306217271s
Jun 22 06:56:27.124: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.599032108s
Jun 22 06:56:29.203: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.678388113s
Jun 22 06:56:31.208: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.68369318s
STEP: Saw pod success
Jun 22 06:56:31.208: INFO: Pod "var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 06:56:31.214: INFO: Trying to get logs from node slave6 pod var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 06:56:31.441: INFO: Waiting for pod var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e to disappear
Jun 22 06:56:31.453: INFO: Pod var-expansion-d8372acd-94ba-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 06:56:31.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1054" for this suite.
Jun 22 06:56:46.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 06:56:47.692: INFO: namespace var-expansion-1054 deletion completed in 16.07719518s

• [SLOW TEST:25.973 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 06:56:47.693: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3627
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3627
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 22 06:56:49.272: INFO: Found 0 stateful pods, waiting for 3
Jun 22 06:56:59.286: INFO: Found 2 stateful pods, waiting for 3
Jun 22 06:57:09.282: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:57:09.282: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:57:09.282: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 06:57:09.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-3627 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:57:09.690: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:57:09.690: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:57:09.690: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 22 06:57:20.104: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 22 06:57:30.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-3627 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:57:31.018: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 06:57:31.018: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 06:57:31.018: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 06:57:42.076: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:57:42.076: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:57:42.076: INFO: Waiting for Pod statefulset-3627/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:57:42.076: INFO: Waiting for Pod statefulset-3627/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:57:52.601: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:57:52.601: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:57:52.601: INFO: Waiting for Pod statefulset-3627/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:58:02.094: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:58:02.094: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:58:02.094: INFO: Waiting for Pod statefulset-3627/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:58:12.717: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:58:12.717: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 06:58:22.359: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
STEP: Rolling back to a previous revision
Jun 22 06:58:32.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-3627 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 06:58:32.373: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 06:58:32.374: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 06:58:32.374: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 06:58:43.928: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 22 06:58:54.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-3627 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 06:58:54.940: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 06:58:54.940: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 06:58:54.940: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 06:59:05.600: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:59:05.600: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:05.600: INFO: Waiting for Pod statefulset-3627/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:05.600: INFO: Waiting for Pod statefulset-3627/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:15.615: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:59:15.615: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:15.615: INFO: Waiting for Pod statefulset-3627/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:25.615: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:59:25.615: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Jun 22 06:59:35.627: INFO: Waiting for StatefulSet statefulset-3627/ss2 to complete update
Jun 22 06:59:35.627: INFO: Waiting for Pod statefulset-3627/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 22 06:59:45.622: INFO: Deleting all statefulset in ns statefulset-3627
Jun 22 06:59:45.626: INFO: Scaling statefulset ss2 to 0
Jun 22 07:00:15.691: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 07:00:15.720: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:00:16.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3627" for this suite.
Jun 22 07:00:42.453: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:00:44.228: INFO: namespace statefulset-3627 deletion completed in 28.068018997s

• [SLOW TEST:236.536 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:00:44.228: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-844
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:00:46.493: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e" in namespace "projected-844" to be "success or failure"
Jun 22 07:00:46.508: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.862493ms
Jun 22 07:00:48.606: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.112620639s
Jun 22 07:00:50.764: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.271406221s
Jun 22 07:00:52.770: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.276608027s
Jun 22 07:00:54.971: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.477874034s
STEP: Saw pod success
Jun 22 07:00:54.971: INFO: Pod "downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:00:55.001: INFO: Trying to get logs from node slave5 pod downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:00:56.362: INFO: Waiting for pod downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e to disappear
Jun 22 07:00:56.375: INFO: Pod downwardapi-volume-7576455c-94bb-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:00:56.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-844" for this suite.
Jun 22 07:01:08.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:01:11.950: INFO: namespace projected-844 deletion completed in 15.564118085s

• [SLOW TEST:27.721 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:01:11.950: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3474
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-85ac02b7-94bb-11e9-8f59-1e22372c056e
STEP: Creating secret with name secret-projected-all-test-volume-85ac029e-94bb-11e9-8f59-1e22372c056e
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 07:01:14.018: INFO: Waiting up to 5m0s for pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e" in namespace "projected-3474" to be "success or failure"
Jun 22 07:01:14.553: INFO: Pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 535.199274ms
Jun 22 07:01:16.660: INFO: Pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.641713427s
Jun 22 07:01:18.988: INFO: Pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.970001952s
Jun 22 07:01:20.999: INFO: Pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.980740732s
STEP: Saw pod success
Jun 22 07:01:20.999: INFO: Pod "projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:01:21.009: INFO: Trying to get logs from node slave8 pod projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 07:01:22.030: INFO: Waiting for pod projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e to disappear
Jun 22 07:01:22.655: INFO: Pod projected-volume-85ac0267-94bb-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:01:22.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3474" for this suite.
Jun 22 07:01:39.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:01:41.662: INFO: namespace projected-3474 deletion completed in 18.894590478s

• [SLOW TEST:29.712 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:01:41.663: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8799
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Jun 22 07:01:53.499: INFO: 10 pods remaining
Jun 22 07:01:53.499: INFO: 10 pods has nil DeletionTimestamp
Jun 22 07:01:53.499: INFO: 
Jun 22 07:01:54.467: INFO: 10 pods remaining
Jun 22 07:01:54.467: INFO: 0 pods has nil DeletionTimestamp
Jun 22 07:01:54.467: INFO: 
Jun 22 07:01:55.637: INFO: 0 pods remaining
Jun 22 07:01:55.637: INFO: 0 pods has nil DeletionTimestamp
Jun 22 07:01:55.637: INFO: 
Jun 22 07:01:57.107: INFO: 0 pods remaining
Jun 22 07:01:57.107: INFO: 0 pods has nil DeletionTimestamp
Jun 22 07:01:57.107: INFO: 
STEP: Gathering metrics
W0622 07:01:59.078642      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 07:01:59.078: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:01:59.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8799" for this suite.
Jun 22 07:02:26.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:02:28.234: INFO: namespace gc-8799 deletion completed in 28.634516438s

• [SLOW TEST:46.572 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:02:28.235: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3732
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:02:30.314: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e" in namespace "downward-api-3732" to be "success or failure"
Jun 22 07:02:30.711: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 396.485123ms
Jun 22 07:02:33.154: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.839585611s
Jun 22 07:02:35.258: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.94370685s
Jun 22 07:02:37.263: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.948951759s
Jun 22 07:02:39.272: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.958175475s
Jun 22 07:02:42.408: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.0939007s
Jun 22 07:02:45.319: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 15.004529142s
Jun 22 07:02:47.324: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 17.009898823s
STEP: Saw pod success
Jun 22 07:02:47.324: INFO: Pod "downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:02:47.332: INFO: Trying to get logs from node slave5 pod downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:02:47.770: INFO: Waiting for pod downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e to disappear
Jun 22 07:02:47.776: INFO: Pod downwardapi-volume-b334051b-94bb-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:02:47.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3732" for this suite.
Jun 22 07:03:02.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:03:06.239: INFO: namespace downward-api-3732 deletion completed in 18.457401708s

• [SLOW TEST:38.004 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:03:06.240: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1489
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:03:07.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e" in namespace "projected-1489" to be "success or failure"
Jun 22 07:03:07.989: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 90.155943ms
Jun 22 07:03:10.056: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.157796127s
Jun 22 07:03:12.110: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.211384535s
Jun 22 07:03:14.153: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.25424648s
Jun 22 07:03:16.563: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.665021704s
STEP: Saw pod success
Jun 22 07:03:16.563: INFO: Pod "downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:03:16.976: INFO: Trying to get logs from node slave8 pod downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:03:17.666: INFO: Waiting for pod downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e to disappear
Jun 22 07:03:17.928: INFO: Pod downwardapi-volume-c997ae87-94bb-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:03:17.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1489" for this suite.
Jun 22 07:03:34.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:03:36.236: INFO: namespace projected-1489 deletion completed in 18.301260982s

• [SLOW TEST:29.997 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:03:36.237: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1343
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:03:37.441: INFO: Waiting up to 5m0s for pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e" in namespace "downward-api-1343" to be "success or failure"
Jun 22 07:03:37.453: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.735227ms
Jun 22 07:03:39.943: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501796102s
Jun 22 07:03:42.053: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.611752504s
Jun 22 07:03:44.261: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.819978351s
Jun 22 07:03:46.361: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.919930744s
STEP: Saw pod success
Jun 22 07:03:46.361: INFO: Pod "downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:03:46.496: INFO: Trying to get logs from node slave6 pod downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:03:47.196: INFO: Waiting for pod downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e to disappear
Jun 22 07:03:47.550: INFO: Pod downwardapi-volume-db6dcfb0-94bb-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:03:47.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1343" for this suite.
Jun 22 07:04:00.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:04:02.251: INFO: namespace downward-api-1343 deletion completed in 14.529209668s

• [SLOW TEST:26.014 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:04:02.251: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7240
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 22 07:04:03.829: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 07:04:03.895: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 07:04:03.924: INFO: 
Logging pods the kubelet thinks is on node slave5 before test
Jun 22 07:04:03.942: INFO: istio-cleanup-secrets-lnzkk from istio-system started at 2019-06-21 15:02:51 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.942: INFO: 	Container hyperkube ready: false, restart count 0
Jun 22 07:04:03.942: INFO: sonobuoy-e2e-job-c1f815093e224ac6 from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 07:04:03.942: INFO: 	Container e2e ready: true, restart count 0
Jun 22 07:04:03.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 07:04:03.942: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-crs8x from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:04:03.942: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 07:04:03.942: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 07:04:03.942: INFO: calico-node-lsmzz from kube-system started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.942: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:04:03.942: INFO: tiller-deploy-cdd5d96ff-lqrtt from kube-system started at 2019-06-21 15:02:13 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.942: INFO: 	Container tiller ready: true, restart count 0
Jun 22 07:04:03.942: INFO: kube-proxy-slave5 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:04:03.942: INFO: node-exporter-ghrg9 from monitoring started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.943: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:04:03.943: INFO: 
Logging pods the kubelet thinks is on node slave6 before test
Jun 22 07:04:03.983: INFO: kube-proxy-slave6 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:04:03.983: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-vc4bj from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 07:04:03.983: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 07:04:03.983: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 07:04:03.983: INFO: node-exporter-dxldj from monitoring started at 2019-06-21 14:46:43 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.983: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:04:03.983: INFO: calico-node-5954w from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.983: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:04:03.983: INFO: db-daemon-server-7859f8794b-hkc8d from kube-system started at 2019-06-21 15:02:18 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:03.983: INFO: 	Container db-daemon-server ready: true, restart count 0
Jun 22 07:04:03.983: INFO: 
Logging pods the kubelet thinks is on node slave7 before test
Jun 22 07:04:04.025: INFO: node-exporter-fr8lm from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.025: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:04:04.025: INFO: calico-node-lm5w9 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.025: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:04:04.025: INFO: kube-proxy-slave7 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:04:04.025: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-scz7v from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:04:04.025: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 07:04:04.025: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 07:04:04.025: INFO: 
Logging pods the kubelet thinks is on node slave8 before test
Jun 22 07:04:04.062: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-cckqp from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 07:04:04.062: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 07:04:04.062: INFO: kube-proxy-slave8 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:04:04.062: INFO: calico-kube-controllers-594b85bfc9-ngjs5 from kube-system started at 2019-06-21 15:02:32 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 07:04:04.062: INFO: trigger-d85ff74f-kswkh from kube-system started at 2019-06-21 15:02:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container trigger ready: true, restart count 0
Jun 22 07:04:04.062: INFO: calico-node-j5pm7 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:04:04.062: INFO: node-exporter-n2mhc from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:04:04.062: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 05:09:13 +0000 UTC (1 container statuses recorded)
Jun 22 07:04:04.062: INFO: 	Container kube-sonobuoy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node slave5
STEP: verifying the node has the label node slave6
STEP: verifying the node has the label node slave7
STEP: verifying the node has the label node slave8
Jun 22 07:04:05.883: INFO: Pod sonobuoy requesting resource cpu=0m on Node slave8
Jun 22 07:04:05.883: INFO: Pod sonobuoy-e2e-job-c1f815093e224ac6 requesting resource cpu=0m on Node slave5
Jun 22 07:04:05.883: INFO: Pod sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-cckqp requesting resource cpu=0m on Node slave8
Jun 22 07:04:05.883: INFO: Pod sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-crs8x requesting resource cpu=0m on Node slave5
Jun 22 07:04:05.883: INFO: Pod sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-scz7v requesting resource cpu=0m on Node slave7
Jun 22 07:04:05.883: INFO: Pod sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-vc4bj requesting resource cpu=0m on Node slave6
Jun 22 07:04:05.883: INFO: Pod istio-cleanup-secrets-lnzkk requesting resource cpu=0m on Node slave5
Jun 22 07:04:05.883: INFO: Pod calico-kube-controllers-594b85bfc9-ngjs5 requesting resource cpu=30m on Node slave8
Jun 22 07:04:05.883: INFO: Pod calico-node-5954w requesting resource cpu=150m on Node slave6
Jun 22 07:04:05.883: INFO: Pod calico-node-j5pm7 requesting resource cpu=150m on Node slave8
Jun 22 07:04:05.883: INFO: Pod calico-node-lm5w9 requesting resource cpu=150m on Node slave7
Jun 22 07:04:05.883: INFO: Pod calico-node-lsmzz requesting resource cpu=150m on Node slave5
Jun 22 07:04:05.883: INFO: Pod db-daemon-server-7859f8794b-hkc8d requesting resource cpu=0m on Node slave6
Jun 22 07:04:05.883: INFO: Pod kube-proxy-slave5 requesting resource cpu=150m on Node slave5
Jun 22 07:04:05.883: INFO: Pod kube-proxy-slave6 requesting resource cpu=150m on Node slave6
Jun 22 07:04:05.884: INFO: Pod kube-proxy-slave7 requesting resource cpu=150m on Node slave7
Jun 22 07:04:05.884: INFO: Pod kube-proxy-slave8 requesting resource cpu=150m on Node slave8
Jun 22 07:04:05.884: INFO: Pod tiller-deploy-cdd5d96ff-lqrtt requesting resource cpu=0m on Node slave5
Jun 22 07:04:05.884: INFO: Pod trigger-d85ff74f-kswkh requesting resource cpu=100m on Node slave8
Jun 22 07:04:05.884: INFO: Pod node-exporter-dxldj requesting resource cpu=100m on Node slave6
Jun 22 07:04:05.884: INFO: Pod node-exporter-fr8lm requesting resource cpu=100m on Node slave7
Jun 22 07:04:05.884: INFO: Pod node-exporter-ghrg9 requesting resource cpu=100m on Node slave5
Jun 22 07:04:05.884: INFO: Pod node-exporter-n2mhc requesting resource cpu=100m on Node slave8
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e.15aa73590df2b3a1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7240/filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e to slave5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e.15aa7359e49f167c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e.15aa7359f2597ec6], Reason = [Created], Message = [Created container filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e.15aa735a20ba551d], Reason = [Started], Message = [Started container filler-pod-ec7bcbf1-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e.15aa73590e61067c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7240/filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e to slave6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e.15aa735a38ad5e63], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e.15aa735a740616b6], Reason = [Created], Message = [Created container filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e.15aa735ab288ca49], Reason = [Started], Message = [Started container filler-pod-ecac8a81-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e.15aa735913d3c04b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7240/filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e to slave7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e.15aa735a3558a17f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e.15aa735a5a33cfa9], Reason = [Created], Message = [Created container filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e.15aa735a6fa8efc6], Reason = [Started], Message = [Started container filler-pod-ecf7108d-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e.15aa7359264ed578], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7240/filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e to slave8]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e.15aa735a1b976c8c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e.15aa735a319ef261], Reason = [Created], Message = [Created container filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e.15aa735a3f90c4a1], Reason = [Started], Message = [Started container filler-pod-ed292848-94bb-11e9-8f59-1e22372c056e]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15aa735b1e21e30c], Reason = [FailedScheduling], Message = [0/7 nodes are available: 7 Insufficient cpu.]
STEP: removing the label node off the node slave5
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave6
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave7
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node slave8
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:04:18.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7240" for this suite.
Jun 22 07:04:42.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:04:45.040: INFO: namespace sched-pred-7240 deletion completed in 26.592994925s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:42.789 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:04:45.041: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 07:04:53.362: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-04fbb0e8-94bc-11e9-8f59-1e22372c056e,GenerateName:,Namespace:events-6715,SelfLink:/api/v1/namespaces/events-6715/pods/send-events-04fbb0e8-94bc-11e9-8f59-1e22372c056e,UID:0526d4f8-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:671691,Generation:0,CreationTimestamp:2019-06-22 07:04:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 987703692,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pg5vv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pg5vv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-pg5vv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033a27f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033a2810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:04:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:04:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:04:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:04:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:10.151.17.236,StartTime:2019-06-22 07:04:47 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-22 07:04:50 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker://sha256:c6b8a28d5611cf83d297661ddd7f40672d286eafd7eb3852267e634e8eee0948 docker://beb32fb1815c8b5f1ce661d21231a8704a8681e100d26496353b3e2a92d69030}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 22 07:04:55.368: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 07:04:57.499: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:04:57.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6715" for this suite.
Jun 22 07:05:47.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:05:49.513: INFO: namespace events-6715 deletion completed in 51.917210449s

• [SLOW TEST:64.473 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:05:49.514: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6117
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 07:05:51.633: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6117,SelfLink:/api/v1/namespaces/watch-6117/configmaps/e2e-watch-test-resource-version,UID:2acd3282-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:671862,Generation:0,CreationTimestamp:2019-06-22 07:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 07:05:51.633: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6117,SelfLink:/api/v1/namespaces/watch-6117/configmaps/e2e-watch-test-resource-version,UID:2acd3282-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:671865,Generation:0,CreationTimestamp:2019-06-22 07:05:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:05:51.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6117" for this suite.
Jun 22 07:06:01.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:06:06.182: INFO: namespace watch-6117 deletion completed in 14.528147536s

• [SLOW TEST:16.668 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:06:06.183: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-4290
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jun 22 07:06:08.350: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4290" to be "success or failure"
Jun 22 07:06:08.712: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 362.507429ms
Jun 22 07:06:10.815: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465829527s
Jun 22 07:06:12.934: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584753937s
Jun 22 07:06:15.200: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.850736856s
Jun 22 07:06:17.458: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 9.108848863s
Jun 22 07:06:19.463: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 11.113269392s
STEP: Saw pod success
Jun 22 07:06:19.463: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 22 07:06:19.467: INFO: Trying to get logs from node slave5 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 22 07:06:20.037: INFO: Waiting for pod pod-host-path-test to disappear
Jun 22 07:06:20.061: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:06:20.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4290" for this suite.
Jun 22 07:06:38.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:06:39.721: INFO: namespace hostpath-4290 deletion completed in 19.653363302s

• [SLOW TEST:33.538 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:06:39.721: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7944
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 22 07:06:42.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672039,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 07:06:42.733: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672040,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 07:06:42.733: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672041,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 22 07:06:53.462: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672068,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 07:06:53.462: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672069,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 22 07:06:53.462: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7944,SelfLink:/api/v1/namespaces/watch-7944/configmaps/e2e-watch-test-label-changed,UID:49df2392-94bc-11e9-a9b0-fa163e4f9fd7,ResourceVersion:672070,Generation:0,CreationTimestamp:2019-06-22 07:06:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:06:53.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7944" for this suite.
Jun 22 07:07:04.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:07:07.456: INFO: namespace watch-7944 deletion completed in 13.738741322s

• [SLOW TEST:27.736 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:07:07.457: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7956
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 07:07:09.040: INFO: Waiting up to 5m0s for pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e" in namespace "emptydir-7956" to be "success or failure"
Jun 22 07:07:09.579: INFO: Pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 539.267086ms
Jun 22 07:07:11.713: INFO: Pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673072739s
Jun 22 07:07:14.183: INFO: Pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.142838398s
Jun 22 07:07:16.199: INFO: Pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 7.159264241s
STEP: Saw pod success
Jun 22 07:07:16.200: INFO: Pod "pod-59668de2-94bc-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:07:16.208: INFO: Trying to get logs from node slave8 pod pod-59668de2-94bc-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:07:16.593: INFO: Waiting for pod pod-59668de2-94bc-11e9-8f59-1e22372c056e to disappear
Jun 22 07:07:16.609: INFO: Pod pod-59668de2-94bc-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:07:16.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7956" for this suite.
Jun 22 07:07:30.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:07:33.152: INFO: namespace emptydir-7956 deletion completed in 16.51622082s

• [SLOW TEST:25.695 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:07:33.155: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:07:46.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4405" for this suite.
Jun 22 07:07:59.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:08:01.738: INFO: namespace emptydir-wrapper-4405 deletion completed in 14.566957394s

• [SLOW TEST:28.583 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:08:01.738: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-6923
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-6923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6923 to expose endpoints map[]
Jun 22 07:08:03.407: INFO: successfully validated that service endpoint-test2 in namespace services-6923 exposes endpoints map[] (240.038518ms elapsed)
STEP: Creating pod pod1 in namespace services-6923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6923 to expose endpoints map[pod1:[80]]
Jun 22 07:08:08.347: INFO: successfully validated that service endpoint-test2 in namespace services-6923 exposes endpoints map[pod1:[80]] (4.831230936s elapsed)
STEP: Creating pod pod2 in namespace services-6923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6923 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 22 07:08:13.269: INFO: Unexpected endpoints: found map[7a233b72-94bc-11e9-a9b0-fa163e4f9fd7:[80]], expected map[pod1:[80] pod2:[80]] (4.816101866s elapsed, will retry)
Jun 22 07:08:14.504: INFO: successfully validated that service endpoint-test2 in namespace services-6923 exposes endpoints map[pod1:[80] pod2:[80]] (6.050271482s elapsed)
STEP: Deleting pod pod1 in namespace services-6923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6923 to expose endpoints map[pod2:[80]]
Jun 22 07:08:15.193: INFO: successfully validated that service endpoint-test2 in namespace services-6923 exposes endpoints map[pod2:[80]] (460.6397ms elapsed)
STEP: Deleting pod pod2 in namespace services-6923
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6923 to expose endpoints map[]
Jun 22 07:08:16.329: INFO: successfully validated that service endpoint-test2 in namespace services-6923 exposes endpoints map[] (1.073343553s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:08:17.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6923" for this suite.
Jun 22 07:08:53.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:08:57.718: INFO: namespace services-6923 deletion completed in 40.603234564s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:55.980 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:08:57.719: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-3625/secret-test-9ba94f08-94bc-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 07:09:00.470: INFO: Waiting up to 5m0s for pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e" in namespace "secrets-3625" to be "success or failure"
Jun 22 07:09:00.932: INFO: Pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 461.824503ms
Jun 22 07:09:02.941: INFO: Pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.470168687s
Jun 22 07:09:04.946: INFO: Pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475970207s
Jun 22 07:09:06.996: INFO: Pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.525450956s
STEP: Saw pod success
Jun 22 07:09:06.996: INFO: Pod "pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:09:07.001: INFO: Trying to get logs from node slave8 pod pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e container env-test: <nil>
STEP: delete the pod
Jun 22 07:09:07.405: INFO: Waiting for pod pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e to disappear
Jun 22 07:09:07.453: INFO: Pod pod-configmaps-9bc82714-94bc-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:09:07.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3625" for this suite.
Jun 22 07:09:23.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:09:25.736: INFO: namespace secrets-3625 deletion completed in 18.262691712s

• [SLOW TEST:28.017 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:09:25.739: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1265
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-f9xm
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 07:09:27.391: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-f9xm" in namespace "subpath-1265" to be "success or failure"
Jun 22 07:09:27.395: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017295ms
Jun 22 07:09:29.405: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013482381s
Jun 22 07:09:31.456: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064916415s
Jun 22 07:09:34.064: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Pending", Reason="", readiness=false. Elapsed: 6.672270894s
Jun 22 07:09:36.417: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Pending", Reason="", readiness=false. Elapsed: 9.025450377s
Jun 22 07:09:38.470: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 11.078324902s
Jun 22 07:09:40.528: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 13.136849s
Jun 22 07:09:42.757: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 15.365334649s
Jun 22 07:09:45.144: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 17.752112933s
Jun 22 07:09:47.297: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 19.905963533s
Jun 22 07:09:49.541: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 22.149027563s
Jun 22 07:09:51.621: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 24.229399027s
Jun 22 07:09:54.125: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Running", Reason="", readiness=true. Elapsed: 26.733590313s
Jun 22 07:09:56.313: INFO: Pod "pod-subpath-test-configmap-f9xm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.921617875s
STEP: Saw pod success
Jun 22 07:09:56.313: INFO: Pod "pod-subpath-test-configmap-f9xm" satisfied condition "success or failure"
Jun 22 07:09:56.328: INFO: Trying to get logs from node slave6 pod pod-subpath-test-configmap-f9xm container test-container-subpath-configmap-f9xm: <nil>
STEP: delete the pod
Jun 22 07:09:56.832: INFO: Waiting for pod pod-subpath-test-configmap-f9xm to disappear
Jun 22 07:09:57.226: INFO: Pod pod-subpath-test-configmap-f9xm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-f9xm
Jun 22 07:09:57.226: INFO: Deleting pod "pod-subpath-test-configmap-f9xm" in namespace "subpath-1265"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:09:57.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1265" for this suite.
Jun 22 07:10:16.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:10:17.863: INFO: namespace subpath-1265 deletion completed in 20.401507541s

• [SLOW TEST:52.124 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:10:17.864: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-335
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-335
Jun 22 07:10:26.487: INFO: Started pod liveness-http in namespace container-probe-335
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 07:10:26.616: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:14:27.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-335" for this suite.
Jun 22 07:14:40.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:14:42.841: INFO: namespace container-probe-335 deletion completed in 14.464278545s

• [SLOW TEST:264.977 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:14:42.841: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8874
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-69289e5c-94bd-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:14:45.051: INFO: Waiting up to 5m0s for pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e" in namespace "configmap-8874" to be "success or failure"
Jun 22 07:14:45.421: INFO: Pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 369.536051ms
Jun 22 07:14:47.511: INFO: Pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.459595322s
Jun 22 07:14:49.592: INFO: Pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.540662359s
Jun 22 07:14:51.705: INFO: Pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.653428719s
STEP: Saw pod success
Jun 22 07:14:51.705: INFO: Pod "pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:14:51.711: INFO: Trying to get logs from node slave5 pod pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:14:51.885: INFO: Waiting for pod pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e to disappear
Jun 22 07:14:51.901: INFO: Pod pod-configmaps-694cd9ee-94bd-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:14:51.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8874" for this suite.
Jun 22 07:15:02.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:15:03.840: INFO: namespace configmap-8874 deletion completed in 11.925993169s

• [SLOW TEST:20.999 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:15:03.841: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1960
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:15:13.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1960" for this suite.
Jun 22 07:15:28.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:15:29.996: INFO: namespace kubelet-test-1960 deletion completed in 16.035600563s

• [SLOW TEST:26.156 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:15:29.997: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-666
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:15:31.542: INFO: Waiting up to 5m0s for pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e" in namespace "projected-666" to be "success or failure"
Jun 22 07:15:31.549: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.762533ms
Jun 22 07:15:33.755: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.213239501s
Jun 22 07:15:35.765: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.222723841s
Jun 22 07:15:37.770: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.228079586s
Jun 22 07:15:40.004: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.462216276s
STEP: Saw pod success
Jun 22 07:15:40.005: INFO: Pod "downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:15:40.019: INFO: Trying to get logs from node slave6 pod downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:15:40.603: INFO: Waiting for pod downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e to disappear
Jun 22 07:15:40.850: INFO: Pod downwardapi-volume-85026259-94bd-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:15:40.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-666" for this suite.
Jun 22 07:15:53.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:15:56.113: INFO: namespace projected-666 deletion completed in 15.25715741s

• [SLOW TEST:26.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:15:56.114: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3532
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-945459d5-94bd-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:15:57.618: INFO: Waiting up to 5m0s for pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e" in namespace "configmap-3532" to be "success or failure"
Jun 22 07:15:57.874: INFO: Pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 256.007962ms
Jun 22 07:15:59.928: INFO: Pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309443612s
Jun 22 07:16:01.934: INFO: Pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.31555283s
Jun 22 07:16:03.959: INFO: Pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.341004043s
STEP: Saw pod success
Jun 22 07:16:03.959: INFO: Pod "pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:16:04.233: INFO: Trying to get logs from node slave7 pod pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:16:05.556: INFO: Waiting for pod pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e to disappear
Jun 22 07:16:05.763: INFO: Pod pod-configmaps-948b359d-94bd-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:16:05.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3532" for this suite.
Jun 22 07:16:16.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:16:18.810: INFO: namespace configmap-3532 deletion completed in 13.018103539s

• [SLOW TEST:22.696 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:16:18.811: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-367
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 07:16:46.920: INFO: Container started at 2019-06-22 07:16:25 +0000 UTC, pod became ready at 2019-06-22 07:16:45 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:16:46.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-367" for this suite.
Jun 22 07:17:19.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:17:20.724: INFO: namespace container-probe-367 deletion completed in 33.790832954s

• [SLOW TEST:61.913 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:17:20.725: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7724
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jun 22 07:17:22.953: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 07:17:23.090: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 07:17:23.124: INFO: 
Logging pods the kubelet thinks is on node slave5 before test
Jun 22 07:17:23.168: INFO: kube-proxy-slave5 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:17:23.168: INFO: node-exporter-ghrg9 from monitoring started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:17:23.168: INFO: istio-cleanup-secrets-lnzkk from istio-system started at 2019-06-21 15:02:51 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container hyperkube ready: false, restart count 0
Jun 22 07:17:23.168: INFO: sonobuoy-e2e-job-c1f815093e224ac6 from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container e2e ready: true, restart count 0
Jun 22 07:17:23.168: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 07:17:23.168: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-crs8x from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 07:17:23.168: INFO: 	Container systemd-logs ready: true, restart count 2
Jun 22 07:17:23.168: INFO: calico-node-lsmzz from kube-system started at 2019-06-21 14:46:41 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:17:23.168: INFO: tiller-deploy-cdd5d96ff-lqrtt from kube-system started at 2019-06-21 15:02:13 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.168: INFO: 	Container tiller ready: true, restart count 0
Jun 22 07:17:23.168: INFO: 
Logging pods the kubelet thinks is on node slave6 before test
Jun 22 07:17:23.216: INFO: node-exporter-dxldj from monitoring started at 2019-06-21 14:46:43 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.216: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:17:23.216: INFO: calico-node-5954w from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.216: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:17:23.216: INFO: db-daemon-server-7859f8794b-hkc8d from kube-system started at 2019-06-21 15:02:18 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.216: INFO: 	Container db-daemon-server ready: true, restart count 0
Jun 22 07:17:23.216: INFO: kube-proxy-slave6 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:17:23.216: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-vc4bj from heptio-sonobuoy started at 2019-06-22 05:09:20 +0000 UTC (2 container statuses recorded)
Jun 22 07:17:23.216: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 07:17:23.216: INFO: 	Container systemd-logs ready: true, restart count 2
Jun 22 07:17:23.216: INFO: 
Logging pods the kubelet thinks is on node slave7 before test
Jun 22 07:17:23.253: INFO: node-exporter-fr8lm from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.253: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:17:23.253: INFO: calico-node-lm5w9 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.253: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:17:23.253: INFO: kube-proxy-slave7 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:17:23.253: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-scz7v from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:17:23.253: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 07:17:23.253: INFO: 	Container systemd-logs ready: true, restart count 2
Jun 22 07:17:23.253: INFO: 
Logging pods the kubelet thinks is on node slave8 before test
Jun 22 07:17:23.275: INFO: kube-proxy-slave8 from kube-system started at <nil> (0 container statuses recorded)
Jun 22 07:17:23.275: INFO: calico-kube-controllers-594b85bfc9-ngjs5 from kube-system started at 2019-06-21 15:02:32 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 22 07:17:23.275: INFO: trigger-d85ff74f-kswkh from kube-system started at 2019-06-21 15:02:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container trigger ready: true, restart count 0
Jun 22 07:17:23.275: INFO: calico-node-j5pm7 from kube-system started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container calico-node ready: true, restart count 3
Jun 22 07:17:23.275: INFO: node-exporter-n2mhc from monitoring started at 2019-06-21 14:46:42 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container node-exporter ready: true, restart count 0
Jun 22 07:17:23.275: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 05:09:13 +0000 UTC (1 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 07:17:23.275: INFO: sonobuoy-systemd-logs-daemon-set-8a6d4b641c254f66-cckqp from heptio-sonobuoy started at 2019-06-22 05:09:21 +0000 UTC (2 container statuses recorded)
Jun 22 07:17:23.275: INFO: 	Container sonobuoy-worker ready: true, restart count 2
Jun 22 07:17:23.275: INFO: 	Container systemd-logs ready: true, restart count 2
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-cba24443-94bd-11e9-8f59-1e22372c056e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-cba24443-94bd-11e9-8f59-1e22372c056e off the node slave8
STEP: verifying the node doesn't have the label kubernetes.io/e2e-cba24443-94bd-11e9-8f59-1e22372c056e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:17:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7724" for this suite.
Jun 22 07:18:02.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:18:05.690: INFO: namespace sched-pred-7724 deletion completed in 28.984593016s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:44.964 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:18:05.691: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1078
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 22 07:18:24.415: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:24.415: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:24.694: INFO: Exec stderr: ""
Jun 22 07:18:24.694: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:24.694: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:24.979: INFO: Exec stderr: ""
Jun 22 07:18:24.979: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:24.979: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:25.406: INFO: Exec stderr: ""
Jun 22 07:18:25.406: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:25.406: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:25.736: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 22 07:18:25.736: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:25.736: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:26.016: INFO: Exec stderr: ""
Jun 22 07:18:26.016: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:26.016: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:26.444: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 22 07:18:26.444: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:26.444: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:26.596: INFO: Exec stderr: ""
Jun 22 07:18:26.596: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:26.596: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:26.732: INFO: Exec stderr: ""
Jun 22 07:18:26.732: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:26.732: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:27.555: INFO: Exec stderr: ""
Jun 22 07:18:27.555: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1078 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:18:27.555: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:18:27.923: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:18:27.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1078" for this suite.
Jun 22 07:19:18.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:19:19.817: INFO: namespace e2e-kubelet-etc-hosts-1078 deletion completed in 51.790072027s

• [SLOW TEST:74.126 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:19:19.817: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-8132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8132
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8132
STEP: Deleting pre-stop pod
Jun 22 07:19:45.738: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:19:45.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8132" for this suite.
Jun 22 07:20:34.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:20:35.877: INFO: namespace prestop-8132 deletion completed in 49.935703937s

• [SLOW TEST:76.060 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:20:35.878: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9096
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 22 07:20:37.566: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9096,SelfLink:/api/v1/namespaces/watch-9096/configmaps/e2e-watch-test-watch-closed,UID:3b553f77-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:674893,Generation:0,CreationTimestamp:2019-06-22 07:20:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 07:20:37.567: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9096,SelfLink:/api/v1/namespaces/watch-9096/configmaps/e2e-watch-test-watch-closed,UID:3b553f77-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:674897,Generation:0,CreationTimestamp:2019-06-22 07:20:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 22 07:20:38.157: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9096,SelfLink:/api/v1/namespaces/watch-9096/configmaps/e2e-watch-test-watch-closed,UID:3b553f77-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:674898,Generation:0,CreationTimestamp:2019-06-22 07:20:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 07:20:38.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9096,SelfLink:/api/v1/namespaces/watch-9096/configmaps/e2e-watch-test-watch-closed,UID:3b553f77-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:674899,Generation:0,CreationTimestamp:2019-06-22 07:20:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:20:38.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9096" for this suite.
Jun 22 07:20:53.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:20:55.812: INFO: namespace watch-9096 deletion completed in 17.612118899s

• [SLOW TEST:19.934 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:20:55.815: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-444
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-472a8859-94be-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:20:57.168: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e" in namespace "projected-444" to be "success or failure"
Jun 22 07:20:57.228: INFO: Pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 59.606308ms
Jun 22 07:20:59.235: INFO: Pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066394307s
Jun 22 07:21:01.434: INFO: Pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.26607834s
Jun 22 07:21:03.638: INFO: Pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.46936199s
STEP: Saw pod success
Jun 22 07:21:03.638: INFO: Pod "pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:21:03.644: INFO: Trying to get logs from node slave6 pod pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:21:03.699: INFO: Waiting for pod pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e to disappear
Jun 22 07:21:03.706: INFO: Pod pod-projected-configmaps-472c3bb4-94be-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:21:03.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-444" for this suite.
Jun 22 07:21:17.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:21:19.941: INFO: namespace projected-444 deletion completed in 16.218172954s

• [SLOW TEST:24.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:21:19.941: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-9495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 07:21:29.002: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:21:30.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9495" for this suite.
Jun 22 07:22:05.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:22:07.411: INFO: namespace replicaset-9495 deletion completed in 36.404462885s

• [SLOW TEST:47.470 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:22:07.411: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5097
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 07:22:09.353: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:22:25.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5097" for this suite.
Jun 22 07:22:38.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:22:39.853: INFO: namespace pods-5097 deletion completed in 14.241010605s

• [SLOW TEST:32.442 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:22:39.853: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2327
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jun 22 07:22:41.636: INFO: Waiting up to 5m0s for pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e" in namespace "containers-2327" to be "success or failure"
Jun 22 07:22:41.712: INFO: Pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 75.583214ms
Jun 22 07:22:43.787: INFO: Pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151230995s
Jun 22 07:22:45.939: INFO: Pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.302609334s
Jun 22 07:22:48.188: INFO: Pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.551799627s
STEP: Saw pod success
Jun 22 07:22:48.188: INFO: Pod "client-containers-8566a75b-94be-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:22:48.693: INFO: Trying to get logs from node slave6 pod client-containers-8566a75b-94be-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:22:50.176: INFO: Waiting for pod client-containers-8566a75b-94be-11e9-8f59-1e22372c056e to disappear
Jun 22 07:22:50.260: INFO: Pod client-containers-8566a75b-94be-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:22:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2327" for this suite.
Jun 22 07:23:02.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:23:04.103: INFO: namespace containers-2327 deletion completed in 13.836246805s

• [SLOW TEST:24.249 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:23:04.103: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5135
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5135
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 07:23:04.945: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 07:23:35.063: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.203.250:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:23:35.063: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:23:35.392: INFO: Found all expected endpoints: [netserver-0]
Jun 22 07:23:35.765: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.190.246:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:23:35.765: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:23:36.225: INFO: Found all expected endpoints: [netserver-1]
Jun 22 07:23:36.241: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.17.241:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:23:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:23:36.470: INFO: Found all expected endpoints: [netserver-2]
Jun 22 07:23:36.588: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.151.174.45:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5135 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 07:23:36.588: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
Jun 22 07:23:36.855: INFO: Found all expected endpoints: [netserver-3]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:23:36.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5135" for this suite.
Jun 22 07:24:22.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:24:25.598: INFO: namespace pod-network-test-5135 deletion completed in 48.729231223s

• [SLOW TEST:81.495 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:24:25.598: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1075
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-c4a7630a-94be-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:24:27.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e" in namespace "configmap-1075" to be "success or failure"
Jun 22 07:24:28.066: INFO: Pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 218.994937ms
Jun 22 07:24:30.132: INFO: Pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.284255538s
Jun 22 07:24:32.206: INFO: Pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.358942895s
Jun 22 07:24:34.218: INFO: Pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.37085337s
STEP: Saw pod success
Jun 22 07:24:34.218: INFO: Pod "pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:24:34.223: INFO: Trying to get logs from node slave8 pod pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:24:34.409: INFO: Waiting for pod pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e to disappear
Jun 22 07:24:34.416: INFO: Pod pod-configmaps-c4c87ec2-94be-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:24:34.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1075" for this suite.
Jun 22 07:24:48.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:24:50.276: INFO: namespace configmap-1075 deletion completed in 15.843198315s

• [SLOW TEST:24.678 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:24:50.276: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-44
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 07:24:51.427: INFO: Creating deployment "test-recreate-deployment"
Jun 22 07:24:51.520: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 07:24:51.575: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 22 07:24:54.002: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 07:24:54.008: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785091, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:24:56.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785092, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696785091, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:24:58.178: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 22 07:24:58.437: INFO: Updating deployment test-recreate-deployment
Jun 22 07:24:58.437: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 22 07:25:00.981: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-44,SelfLink:/apis/apps/v1/namespaces/deployment-44/deployments/test-recreate-deployment,UID:d2f92dcf-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:675977,Generation:2,CreationTimestamp:2019-06-22 07:24:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-22 07:25:00 +0000 UTC 2019-06-22 07:25:00 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-22 07:25:00 +0000 UTC 2019-06-22 07:24:51 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 07:25:00.989: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-44,SelfLink:/apis/apps/v1/namespaces/deployment-44/replicasets/test-recreate-deployment-c9cbd8684,UID:d7b1a504-94be-11e9-82ac-fa163e446741,ResourceVersion:675974,Generation:1,CreationTimestamp:2019-06-22 07:24:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d2f92dcf-94be-11e9-a9b0-fa163e4f9fd7 0xc0033a2b50 0xc0033a2b51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 07:25:00.989: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 07:25:00.989: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-44,SelfLink:/apis/apps/v1/namespaces/deployment-44/replicasets/test-recreate-deployment-7d57d5ff7c,UID:d2f01036-94be-11e9-82ac-fa163e446741,ResourceVersion:675962,Generation:2,CreationTimestamp:2019-06-22 07:24:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment d2f92dcf-94be-11e9-a9b0-fa163e4f9fd7 0xc0033a2a87 0xc0033a2a88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 07:25:01.512: INFO: Pod "test-recreate-deployment-c9cbd8684-66lz2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-66lz2,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-44,SelfLink:/api/v1/namespaces/deployment-44/pods/test-recreate-deployment-c9cbd8684-66lz2,UID:d7c7b948-94be-11e9-82ac-fa163e446741,ResourceVersion:675978,Generation:0,CreationTimestamp:2019-06-22 07:24:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 d7b1a504-94be-11e9-82ac-fa163e446741 0xc0033a3390 0xc0033a3391}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-76pgp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-76pgp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-76pgp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033a3400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033a3420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:25:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:25:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:25:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:24:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:,StartTime:2019-06-22 07:25:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:25:01.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-44" for this suite.
Jun 22 07:25:13.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:25:15.573: INFO: namespace deployment-44 deletion completed in 14.031347218s

• [SLOW TEST:25.297 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:25:15.574: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 07:25:16.680: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676061,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 07:25:16.681: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676061,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 07:25:27.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676087,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 07:25:27.062: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676087,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 07:25:37.262: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676113,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 07:25:37.262: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676113,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 07:25:47.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676138,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 07:25:47.333: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-a,UID:e1fa763e-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676138,Generation:0,CreationTimestamp:2019-06-22 07:25:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 07:25:57.417: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-b,UID:fa40c833-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676164,Generation:0,CreationTimestamp:2019-06-22 07:25:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 07:25:57.417: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-b,UID:fa40c833-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676164,Generation:0,CreationTimestamp:2019-06-22 07:25:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 07:26:07.576: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-b,UID:fa40c833-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676190,Generation:0,CreationTimestamp:2019-06-22 07:25:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 07:26:07.577: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-4459,SelfLink:/api/v1/namespaces/watch-4459/configmaps/e2e-watch-test-configmap-b,UID:fa40c833-94be-11e9-a9b0-fa163e4f9fd7,ResourceVersion:676190,Generation:0,CreationTimestamp:2019-06-22 07:25:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:26:17.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4459" for this suite.
Jun 22 07:26:30.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:26:32.268: INFO: namespace watch-4459 deletion completed in 14.679455547s

• [SLOW TEST:76.694 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:26:32.269: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7378
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-0fc55848-94bf-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:26:34.118: INFO: Waiting up to 5m0s for pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e" in namespace "configmap-7378" to be "success or failure"
Jun 22 07:26:34.655: INFO: Pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 536.954306ms
Jun 22 07:26:36.739: INFO: Pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.620815027s
Jun 22 07:26:38.966: INFO: Pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.848081299s
Jun 22 07:26:41.019: INFO: Pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.901356081s
STEP: Saw pod success
Jun 22 07:26:41.019: INFO: Pod "pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:26:41.082: INFO: Trying to get logs from node slave5 pod pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:26:41.411: INFO: Waiting for pod pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e to disappear
Jun 22 07:26:41.418: INFO: Pod pod-configmaps-0feea04c-94bf-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:26:41.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7378" for this suite.
Jun 22 07:26:53.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:26:55.395: INFO: namespace configmap-7378 deletion completed in 13.969960668s

• [SLOW TEST:23.126 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:26:55.396: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3190
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3190.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3190.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.122.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.122.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.122.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.122.167_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3190.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3190.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3190.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3190.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3190.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 167.122.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.122.167_udp@PTR;check="$$(dig +tcp +noall +answer +search 167.122.150.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.150.122.167_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 07:27:07.318: INFO: Unable to read wheezy_udp@dns-test-service.dns-3190.svc.cluster.local from pod dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e: the server could not find the requested resource (get pods dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e)
Jun 22 07:27:07.323: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3190.svc.cluster.local from pod dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e: the server could not find the requested resource (get pods dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e)
Jun 22 07:27:07.328: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local from pod dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e: the server could not find the requested resource (get pods dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e)
Jun 22 07:27:07.339: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local from pod dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e: the server could not find the requested resource (get pods dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e)
Jun 22 07:27:07.620: INFO: Lookups using dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e failed for: [wheezy_udp@dns-test-service.dns-3190.svc.cluster.local wheezy_tcp@dns-test-service.dns-3190.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3190.svc.cluster.local]

Jun 22 07:27:13.162: INFO: DNS probes using dns-3190/dns-test-1e6dbb20-94bf-11e9-8f59-1e22372c056e succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:27:16.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3190" for this suite.
Jun 22 07:27:34.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:27:36.538: INFO: namespace dns-3190 deletion completed in 19.958077893s

• [SLOW TEST:41.142 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:27:36.539: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6406
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-35bf13cf-94bf-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:27:37.595: INFO: Waiting up to 5m0s for pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e" in namespace "configmap-6406" to be "success or failure"
Jun 22 07:27:37.792: INFO: Pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 196.748602ms
Jun 22 07:27:39.797: INFO: Pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201781639s
Jun 22 07:27:42.084: INFO: Pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488458917s
Jun 22 07:27:44.462: INFO: Pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.866683185s
STEP: Saw pod success
Jun 22 07:27:44.462: INFO: Pod "pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:27:44.469: INFO: Trying to get logs from node slave6 pod pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:27:45.061: INFO: Waiting for pod pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e to disappear
Jun 22 07:27:45.284: INFO: Pod pod-configmaps-35c3ffaa-94bf-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:27:45.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6406" for this suite.
Jun 22 07:27:57.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:27:59.017: INFO: namespace configmap-6406 deletion completed in 13.657138553s

• [SLOW TEST:22.478 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:27:59.018: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1063
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 22 07:28:07.160: INFO: Successfully updated pod "labelsupdate43316ba9-94bf-11e9-8f59-1e22372c056e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:28:09.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1063" for this suite.
Jun 22 07:28:42.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:28:49.767: INFO: namespace projected-1063 deletion completed in 40.552337073s

• [SLOW TEST:50.750 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:28:49.767: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7184
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-619471c7-94bf-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 07:28:51.163: INFO: Waiting up to 5m0s for pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e" in namespace "secrets-7184" to be "success or failure"
Jun 22 07:28:51.168: INFO: Pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.207262ms
Jun 22 07:28:53.363: INFO: Pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.199925738s
Jun 22 07:28:55.426: INFO: Pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.262394974s
Jun 22 07:28:57.462: INFO: Pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.299028308s
STEP: Saw pod success
Jun 22 07:28:57.462: INFO: Pod "pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:28:57.471: INFO: Trying to get logs from node slave6 pod pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e container secret-env-test: <nil>
STEP: delete the pod
Jun 22 07:28:58.026: INFO: Waiting for pod pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e to disappear
Jun 22 07:28:58.057: INFO: Pod pod-secrets-61c55b29-94bf-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:28:58.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7184" for this suite.
Jun 22 07:29:10.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:29:12.176: INFO: namespace secrets-7184 deletion completed in 14.071164809s

• [SLOW TEST:22.409 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:29:12.177: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2136
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 07:29:13.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2136'
Jun 22 07:29:18.992: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 07:29:18.992: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 22 07:29:19.162: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dfc2n]
Jun 22 07:29:19.162: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dfc2n" in namespace "kubectl-2136" to be "running and ready"
Jun 22 07:29:19.166: INFO: Pod "e2e-test-nginx-rc-dfc2n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029603ms
Jun 22 07:29:21.209: INFO: Pod "e2e-test-nginx-rc-dfc2n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046741001s
Jun 22 07:29:23.215: INFO: Pod "e2e-test-nginx-rc-dfc2n": Phase="Pending", Reason="", readiness=false. Elapsed: 4.05238017s
Jun 22 07:29:25.408: INFO: Pod "e2e-test-nginx-rc-dfc2n": Phase="Running", Reason="", readiness=true. Elapsed: 6.245347799s
Jun 22 07:29:25.408: INFO: Pod "e2e-test-nginx-rc-dfc2n" satisfied condition "running and ready"
Jun 22 07:29:25.408: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dfc2n]
Jun 22 07:29:25.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 logs rc/e2e-test-nginx-rc --namespace=kubectl-2136'
Jun 22 07:29:26.127: INFO: stderr: ""
Jun 22 07:29:26.127: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jun 22 07:29:26.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete rc e2e-test-nginx-rc --namespace=kubectl-2136'
Jun 22 07:29:26.435: INFO: stderr: ""
Jun 22 07:29:26.435: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:29:26.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2136" for this suite.
Jun 22 07:30:03.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:30:05.142: INFO: namespace kubectl-2136 deletion completed in 38.644992597s

• [SLOW TEST:52.966 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:30:05.143: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4875
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 07:30:16.170: INFO: Successfully updated pod "pod-update-activedeadlineseconds-8f2c5c5c-94bf-11e9-8f59-1e22372c056e"
Jun 22 07:30:16.170: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-8f2c5c5c-94bf-11e9-8f59-1e22372c056e" in namespace "pods-4875" to be "terminated due to deadline exceeded"
Jun 22 07:30:16.174: INFO: Pod "pod-update-activedeadlineseconds-8f2c5c5c-94bf-11e9-8f59-1e22372c056e": Phase="Running", Reason="", readiness=true. Elapsed: 3.629049ms
Jun 22 07:30:18.263: INFO: Pod "pod-update-activedeadlineseconds-8f2c5c5c-94bf-11e9-8f59-1e22372c056e": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.09257368s
Jun 22 07:30:18.263: INFO: Pod "pod-update-activedeadlineseconds-8f2c5c5c-94bf-11e9-8f59-1e22372c056e" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:30:18.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4875" for this suite.
Jun 22 07:30:34.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:30:36.202: INFO: namespace pods-4875 deletion completed in 17.917193095s

• [SLOW TEST:31.059 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:30:36.204: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-4703
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:30:45.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4703" for this suite.
Jun 22 07:31:19.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:31:21.658: INFO: namespace replication-controller-4703 deletion completed in 36.580216577s

• [SLOW TEST:45.454 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:31:21.658: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8181
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jun 22 07:31:23.406: INFO: Waiting up to 5m0s for pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e" in namespace "downward-api-8181" to be "success or failure"
Jun 22 07:31:23.455: INFO: Pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 48.737501ms
Jun 22 07:31:25.610: INFO: Pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.204061285s
Jun 22 07:31:27.618: INFO: Pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.212666201s
Jun 22 07:31:29.635: INFO: Pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.22930788s
STEP: Saw pod success
Jun 22 07:31:29.635: INFO: Pod "downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:31:29.653: INFO: Trying to get logs from node slave5 pod downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 07:31:29.889: INFO: Waiting for pod downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e to disappear
Jun 22 07:31:29.906: INFO: Pod downward-api-bc4cb6c5-94bf-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:31:29.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8181" for this suite.
Jun 22 07:31:40.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:31:41.997: INFO: namespace downward-api-8181 deletion completed in 12.081449136s

• [SLOW TEST:20.339 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:31:41.997: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 07:31:42.664: INFO: Creating deployment "nginx-deployment"
Jun 22 07:31:42.906: INFO: Waiting for observed generation 1
Jun 22 07:31:45.700: INFO: Waiting for all required pods to come up
Jun 22 07:31:46.464: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 07:31:59.079: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 22 07:31:59.094: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 22 07:31:59.187: INFO: Updating deployment nginx-deployment
Jun 22 07:31:59.187: INFO: Waiting for observed generation 2
Jun 22 07:32:01.340: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 07:32:01.377: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 07:32:01.698: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 22 07:32:02.766: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 07:32:02.767: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 07:32:02.775: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 22 07:32:03.145: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 22 07:32:03.146: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 22 07:32:03.503: INFO: Updating deployment nginx-deployment
Jun 22 07:32:03.503: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 22 07:32:04.122: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 07:32:06.858: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 22 07:32:08.274: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8631,SelfLink:/apis/apps/v1/namespaces/deployment-8631/deployments/nginx-deployment,UID:c81acb56-94bf-11e9-a9b0-fa163e4f9fd7,ResourceVersion:677761,Generation:3,CreationTimestamp:2019-06-22 07:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:21,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-22 07:32:04 +0000 UTC 2019-06-22 07:32:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-22 07:32:07 +0000 UTC 2019-06-22 07:31:43 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 07:32:08.684: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-8631,SelfLink:/apis/apps/v1/namespaces/deployment-8631/replicasets/nginx-deployment-5f9595f595,UID:d1d9aa88-94bf-11e9-82ac-fa163e446741,ResourceVersion:677754,Generation:3,CreationTimestamp:2019-06-22 07:31:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment c81acb56-94bf-11e9-a9b0-fa163e4f9fd7 0xc0032757a7 0xc0032757a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 07:32:08.685: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 22 07:32:08.685: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-8631,SelfLink:/apis/apps/v1/namespaces/deployment-8631/replicasets/nginx-deployment-6f478d8d8,UID:c8255a47-94bf-11e9-82ac-fa163e446741,ResourceVersion:677774,Generation:3,CreationTimestamp:2019-06-22 07:31:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment c81acb56-94bf-11e9-a9b0-fa163e4f9fd7 0xc003275877 0xc003275878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 22 07:32:09.017: INFO: Pod "nginx-deployment-5f9595f595-2wj82" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2wj82,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-2wj82,UID:d56d98d3-94bf-11e9-82ac-fa163e446741,ResourceVersion:677747,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c307 0xc00222c308}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222c380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222c3a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.017: INFO: Pod "nginx-deployment-5f9595f595-52zsc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-52zsc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-52zsc,UID:d1dc3b9b-94bf-11e9-82ac-fa163e446741,ResourceVersion:677638,Generation:0,CreationTimestamp:2019-06-22 07:31:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c420 0xc00222c421}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222c4a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222c4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:,StartTime:2019-06-22 07:31:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.017: INFO: Pod "nginx-deployment-5f9595f595-6s4ls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-6s4ls,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-6s4ls,UID:d1ef65f3-94bf-11e9-82ac-fa163e446741,ResourceVersion:677643,Generation:0,CreationTimestamp:2019-06-22 07:31:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c590 0xc00222c591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222c610} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222c630}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-06-22 07:31:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.017: INFO: Pod "nginx-deployment-5f9595f595-8v2tt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-8v2tt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-8v2tt,UID:d56d83dd-94bf-11e9-82ac-fa163e446741,ResourceVersion:677738,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c700 0xc00222c701}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222c780} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222c7a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.017: INFO: Pod "nginx-deployment-5f9595f595-9696m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9696m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-9696m,UID:d2984559-94bf-11e9-82ac-fa163e446741,ResourceVersion:677677,Generation:0,CreationTimestamp:2019-06-22 07:32:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c820 0xc00222c821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222c8a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222c8c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:02 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-06-22 07:32:02 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-bd8zg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bd8zg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-bd8zg,UID:d4d1fd00-94bf-11e9-82ac-fa163e446741,ResourceVersion:677700,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222c9a0 0xc00222c9a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222ca20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222ca40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-jhfnj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jhfnj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-jhfnj,UID:d56da7ec-94bf-11e9-82ac-fa163e446741,ResourceVersion:677744,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222cac0 0xc00222cac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222cb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222cb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-klfkt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-klfkt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-klfkt,UID:d50f1ce6-94bf-11e9-82ac-fa163e446741,ResourceVersion:677752,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222cbe0 0xc00222cbe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222cc60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222cc80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-kmk8x" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kmk8x,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-kmk8x,UID:d1ef463f-94bf-11e9-82ac-fa163e446741,ResourceVersion:677652,Generation:0,CreationTimestamp:2019-06-22 07:31:59 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222cd50 0xc00222cd51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222cdd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222cdf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:59 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:,StartTime:2019-06-22 07:31:59 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-kqvsh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kqvsh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-kqvsh,UID:d56e36ba-94bf-11e9-82ac-fa163e446741,ResourceVersion:677745,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222cec0 0xc00222cec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222cf40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222cf60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-lcbzq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lcbzq,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-lcbzq,UID:d27419f5-94bf-11e9-82ac-fa163e446741,ResourceVersion:677674,Generation:0,CreationTimestamp:2019-06-22 07:32:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222cfe0 0xc00222cfe1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d060} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d080}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:00 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:00 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.64,PodIP:,StartTime:2019-06-22 07:32:00 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-nxvjf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nxvjf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-nxvjf,UID:d50f69f8-94bf-11e9-82ac-fa163e446741,ResourceVersion:677753,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222d150 0xc00222d151}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d1e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-5f9595f595-z5ffn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-z5ffn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-5f9595f595-z5ffn,UID:d6057331-94bf-11e9-82ac-fa163e446741,ResourceVersion:677751,Generation:0,CreationTimestamp:2019-06-22 07:32:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 d1d9aa88-94bf-11e9-82ac-fa163e446741 0xc00222d2d0 0xc00222d2d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d350} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d370}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-6f478d8d8-2hh8v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2hh8v,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-2hh8v,UID:d50ed82e-94bf-11e9-82ac-fa163e446741,ResourceVersion:677760,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222d3f0 0xc00222d3f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d460} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d480}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.018: INFO: Pod "nginx-deployment-6f478d8d8-2j8p9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2j8p9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-2j8p9,UID:d49ee9a3-94bf-11e9-82ac-fa163e446741,ResourceVersion:677732,Generation:0,CreationTimestamp:2019-06-22 07:32:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222d547 0xc00222d548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d5c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d5e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.64,PodIP:,StartTime:2019-06-22 07:32:04 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-2qp74" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2qp74,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-2qp74,UID:d50f27d5-94bf-11e9-82ac-fa163e446741,ResourceVersion:677715,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222d6a7 0xc00222d6a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-4fk6d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4fk6d,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-4fk6d,UID:d50f0446-94bf-11e9-82ac-fa163e446741,ResourceVersion:677781,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222d7c0 0xc00222d7c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-5t8dg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5t8dg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-5t8dg,UID:d56d1491-94bf-11e9-82ac-fa163e446741,ResourceVersion:677772,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222d917 0xc00222d918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222d990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222d9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-5tfb6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5tfb6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-5tfb6,UID:c8d75a5b-94bf-11e9-82ac-fa163e446741,ResourceVersion:677544,Generation:0,CreationTimestamp:2019-06-22 07:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222da77 0xc00222da78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222daf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222db10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:10.151.203.255,StartTime:2019-06-22 07:31:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5784dbaa8f1640ad0f6823e3dbca222b78b3a2aec843f7d317196caad59aabd7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-8zd6m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8zd6m,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-8zd6m,UID:d56d7165-94bf-11e9-82ac-fa163e446741,ResourceVersion:677741,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222dbe7 0xc00222dbe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222dc60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222dc80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-98jlb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-98jlb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-98jlb,UID:d4d21217-94bf-11e9-82ac-fa163e446741,ResourceVersion:677735,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222dd00 0xc00222dd01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222dd70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222dd90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:,StartTime:2019-06-22 07:32:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-9vxbs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9vxbs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-9vxbs,UID:d56d5386-94bf-11e9-82ac-fa163e446741,ResourceVersion:677777,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222de57 0xc00222de58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00222ded0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00222def0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-dkpzp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dkpzp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-dkpzp,UID:c8d6c931-94bf-11e9-82ac-fa163e446741,ResourceVersion:677594,Generation:0,CreationTimestamp:2019-06-22 07:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc00222dfb7 0xc00222dfb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.174.49,StartTime:2019-06-22 07:31:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2095f96527949019be42626c069e1a2ad13e5c628db9da87dcfda860c5bab519}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-fg2cr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fg2cr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-fg2cr,UID:c9327014-94bf-11e9-82ac-fa163e446741,ResourceVersion:677592,Generation:0,CreationTimestamp:2019-06-22 07:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738137 0xc002738138}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027381b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027381d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:10.151.203.2,StartTime:2019-06-22 07:31:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:54 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9ae34519aa7516f6eb209d4743b20460c6eb7d0b281fd7281c067c2d30f4edde}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.019: INFO: Pod "nginx-deployment-6f478d8d8-fxvpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fxvpm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-fxvpm,UID:d56db4a1-94bf-11e9-82ac-fa163e446741,ResourceVersion:677746,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc0027382a7 0xc0027382a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-gdj5x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gdj5x,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-gdj5x,UID:c89e32ac-94bf-11e9-82ac-fa163e446741,ResourceVersion:677543,Generation:0,CreationTimestamp:2019-06-22 07:31:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc0027383d0 0xc0027383d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:10.151.17.246,StartTime:2019-06-22 07:31:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2e24dc163606d0223382bc2e3840adb11d42f13435da55b4b6a09fcd940fa174}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-hh7vv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hh7vv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-hh7vv,UID:c8d73a4c-94bf-11e9-82ac-fa163e446741,ResourceVersion:677585,Generation:0,CreationTimestamp:2019-06-22 07:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738547 0xc002738548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027385d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027385f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.67,PodIP:10.151.174.50,StartTime:2019-06-22 07:31:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:52 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ca0864016d4f01d77c8b8bee51e75bd8a7c10247d165a2252621b2d78acf1c37}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-k7hjv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-k7hjv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-k7hjv,UID:c85fb4be-94bf-11e9-82ac-fa163e446741,ResourceVersion:677517,Generation:0,CreationTimestamp:2019-06-22 07:31:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc0027386c7 0xc0027386c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:43 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:10.151.203.254,StartTime:2019-06-22 07:31:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:48 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://04c7dda543b3991bd19587f0851b2af22657ad45485d27bc1c628a85555e1d76}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-l7vff" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l7vff,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-l7vff,UID:d56d6231-94bf-11e9-82ac-fa163e446741,ResourceVersion:677743,Generation:0,CreationTimestamp:2019-06-22 07:32:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738837 0xc002738838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027388e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-n6knx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-n6knx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-n6knx,UID:c89e59e2-94bf-11e9-82ac-fa163e446741,ResourceVersion:677563,Generation:0,CreationTimestamp:2019-06-22 07:31:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738980 0xc002738981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027389f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:52 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:52 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:44 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.64,PodIP:10.151.190.251,StartTime:2019-06-22 07:31:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:51 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://bf22e1bbf3f0bac1688117b52d501d7caeff0286ac0f91998615e193b5d0154b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-tslpd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tslpd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-tslpd,UID:d50f801c-94bf-11e9-82ac-fa163e446741,ResourceVersion:677765,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738af7 0xc002738af8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave8,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.48,PodIP:,StartTime:2019-06-22 07:32:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.020: INFO: Pod "nginx-deployment-6f478d8d8-vsdh4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vsdh4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-vsdh4,UID:c9325ba1-94bf-11e9-82ac-fa163e446741,ResourceVersion:677562,Generation:0,CreationTimestamp:2019-06-22 07:31:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738c87 0xc002738c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:51 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:31:45 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.68,PodIP:10.151.17.247,StartTime:2019-06-22 07:31:45 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 07:31:50 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2560b252dd4a3421e415564eeaba963f323f46e3c6a2de46d5cdb038de006281}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 07:32:09.021: INFO: Pod "nginx-deployment-6f478d8d8-wdbnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wdbnz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-8631,SelfLink:/api/v1/namespaces/deployment-8631/pods/nginx-deployment-6f478d8d8-wdbnz,UID:d4d1d7a3-94bf-11e9-82ac-fa163e446741,ResourceVersion:677771,Generation:0,CreationTimestamp:2019-06-22 07:32:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 c8255a47-94bf-11e9-82ac-fa163e446741 0xc002738e27 0xc002738e28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ht9d {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ht9d,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9ht9d true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002738ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002738ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:05 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:32:04 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.64,PodIP:,StartTime:2019-06-22 07:32:05 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:32:09.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8631" for this suite.
Jun 22 07:33:02.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:33:04.151: INFO: namespace deployment-8631 deletion completed in 54.826945276s

• [SLOW TEST:82.154 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:33:04.151: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-744
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f95de288-94bf-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 07:33:05.694: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e" in namespace "projected-744" to be "success or failure"
Jun 22 07:33:05.702: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.481633ms
Jun 22 07:33:07.709: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015445961s
Jun 22 07:33:09.765: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.07098736s
Jun 22 07:33:11.770: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.075747864s
Jun 22 07:33:13.890: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.196224126s
STEP: Saw pod success
Jun 22 07:33:13.890: INFO: Pod "pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:33:13.931: INFO: Trying to get logs from node slave5 pod pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 07:33:14.455: INFO: Waiting for pod pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e to disappear
Jun 22 07:33:14.477: INFO: Pod pod-projected-secrets-f9628608-94bf-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:33:14.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-744" for this suite.
Jun 22 07:33:24.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:33:27.860: INFO: namespace projected-744 deletion completed in 13.36693059s

• [SLOW TEST:23.709 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:33:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8946
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-073dc2f3-94c0-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-073dc2f3-94c0-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:33:37.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8946" for this suite.
Jun 22 07:34:13.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:34:16.379: INFO: namespace configmap-8946 deletion completed in 39.006855554s

• [SLOW TEST:48.519 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:34:16.379: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6021
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-zznz
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 07:34:17.970: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zznz" in namespace "subpath-6021" to be "success or failure"
Jun 22 07:34:18.075: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Pending", Reason="", readiness=false. Elapsed: 104.797945ms
Jun 22 07:34:20.107: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137058884s
Jun 22 07:34:22.112: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.141667086s
Jun 22 07:34:24.118: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 6.148059393s
Jun 22 07:34:26.124: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 8.153654439s
Jun 22 07:34:28.256: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 10.285626394s
Jun 22 07:34:30.264: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 12.294306095s
Jun 22 07:34:32.271: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 14.300629131s
Jun 22 07:34:34.307: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 16.336569719s
Jun 22 07:34:36.437: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 18.466684492s
Jun 22 07:34:38.548: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 20.577744087s
Jun 22 07:34:40.646: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Running", Reason="", readiness=true. Elapsed: 22.675970186s
Jun 22 07:34:42.878: INFO: Pod "pod-subpath-test-downwardapi-zznz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.908024614s
STEP: Saw pod success
Jun 22 07:34:42.878: INFO: Pod "pod-subpath-test-downwardapi-zznz" satisfied condition "success or failure"
Jun 22 07:34:42.882: INFO: Trying to get logs from node slave7 pod pod-subpath-test-downwardapi-zznz container test-container-subpath-downwardapi-zznz: <nil>
STEP: delete the pod
Jun 22 07:34:42.931: INFO: Waiting for pod pod-subpath-test-downwardapi-zznz to disappear
Jun 22 07:34:42.934: INFO: Pod pod-subpath-test-downwardapi-zznz no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zznz
Jun 22 07:34:42.934: INFO: Deleting pod "pod-subpath-test-downwardapi-zznz" in namespace "subpath-6021"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:34:42.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6021" for this suite.
Jun 22 07:34:57.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:34:59.757: INFO: namespace subpath-6021 deletion completed in 16.465267952s

• [SLOW TEST:43.378 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:34:59.758: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-1277
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-46zl8 in namespace proxy-1277
I0622 07:35:01.313215      18 runners.go:184] Created replication controller with name: proxy-service-46zl8, namespace: proxy-1277, replica count: 1
I0622 07:35:02.363904      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:03.364220      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:04.364447      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:05.364627      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:06.364887      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:07.365200      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 07:35:08.365391      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 07:35:09.365574      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 07:35:10.365824      18 runners.go:184] proxy-service-46zl8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 07:35:10.379: INFO: setup took 9.691178164s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 22 07:35:10.401: INFO: (0) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 22.441409ms)
Jun 22 07:35:10.414: INFO: (0) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 35.190052ms)
Jun 22 07:35:10.420: INFO: (0) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 41.26641ms)
Jun 22 07:35:10.421: INFO: (0) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 41.956152ms)
Jun 22 07:35:10.421: INFO: (0) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 41.907199ms)
Jun 22 07:35:10.421: INFO: (0) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 41.974104ms)
Jun 22 07:35:10.421: INFO: (0) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 41.938211ms)
Jun 22 07:35:10.421: INFO: (0) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 42.112821ms)
Jun 22 07:35:10.423: INFO: (0) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 44.42954ms)
Jun 22 07:35:10.423: INFO: (0) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 44.538491ms)
Jun 22 07:35:10.424: INFO: (0) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 45.312608ms)
Jun 22 07:35:10.426: INFO: (0) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 47.553336ms)
Jun 22 07:35:10.426: INFO: (0) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 47.364765ms)
Jun 22 07:35:10.426: INFO: (0) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 47.416394ms)
Jun 22 07:35:10.427: INFO: (0) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 47.973245ms)
Jun 22 07:35:10.464: INFO: (0) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 85.139517ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 249.581437ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 249.720437ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 249.886932ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 249.815905ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 249.797972ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 250.046895ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 249.968497ms)
Jun 22 07:35:10.714: INFO: (1) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 250.031063ms)
Jun 22 07:35:10.715: INFO: (1) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 250.919756ms)
Jun 22 07:35:10.715: INFO: (1) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 250.837837ms)
Jun 22 07:35:10.727: INFO: (1) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 262.443884ms)
Jun 22 07:35:10.730: INFO: (1) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 265.577561ms)
Jun 22 07:35:10.731: INFO: (1) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 267.133634ms)
Jun 22 07:35:10.731: INFO: (1) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 267.060199ms)
Jun 22 07:35:10.732: INFO: (1) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 267.503321ms)
Jun 22 07:35:10.732: INFO: (1) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 267.545308ms)
Jun 22 07:35:10.747: INFO: (2) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 14.926332ms)
Jun 22 07:35:10.748: INFO: (2) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 15.892745ms)
Jun 22 07:35:10.749: INFO: (2) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 17.121445ms)
Jun 22 07:35:10.754: INFO: (2) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 22.253682ms)
Jun 22 07:35:10.754: INFO: (2) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 22.357861ms)
Jun 22 07:35:10.755: INFO: (2) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 23.222283ms)
Jun 22 07:35:10.755: INFO: (2) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 23.225508ms)
Jun 22 07:35:10.755: INFO: (2) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 23.205283ms)
Jun 22 07:35:10.755: INFO: (2) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 23.164497ms)
Jun 22 07:35:10.755: INFO: (2) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 23.190955ms)
Jun 22 07:35:10.756: INFO: (2) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 23.984075ms)
Jun 22 07:35:10.761: INFO: (2) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 28.640374ms)
Jun 22 07:35:10.761: INFO: (2) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 28.828837ms)
Jun 22 07:35:10.761: INFO: (2) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 29.027776ms)
Jun 22 07:35:10.766: INFO: (2) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 33.940334ms)
Jun 22 07:35:10.766: INFO: (2) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 33.942774ms)
Jun 22 07:35:10.777: INFO: (3) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 11.38276ms)
Jun 22 07:35:10.783: INFO: (3) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 16.443963ms)
Jun 22 07:35:10.783: INFO: (3) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 16.464257ms)
Jun 22 07:35:10.784: INFO: (3) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 17.355835ms)
Jun 22 07:35:10.784: INFO: (3) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 17.827844ms)
Jun 22 07:35:10.784: INFO: (3) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 17.834051ms)
Jun 22 07:35:10.784: INFO: (3) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 17.462748ms)
Jun 22 07:35:10.785: INFO: (3) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 18.273471ms)
Jun 22 07:35:10.785: INFO: (3) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 18.498617ms)
Jun 22 07:35:10.789: INFO: (3) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 22.254708ms)
Jun 22 07:35:10.794: INFO: (3) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 27.742356ms)
Jun 22 07:35:10.794: INFO: (3) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 27.613323ms)
Jun 22 07:35:10.795: INFO: (3) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 28.605494ms)
Jun 22 07:35:10.795: INFO: (3) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 28.174235ms)
Jun 22 07:35:10.795: INFO: (3) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 28.202383ms)
Jun 22 07:35:10.799: INFO: (3) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 32.121507ms)
Jun 22 07:35:10.809: INFO: (4) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 10.410135ms)
Jun 22 07:35:10.814: INFO: (4) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 15.246507ms)
Jun 22 07:35:10.814: INFO: (4) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 15.368518ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 16.109708ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 16.153558ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 16.154297ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 16.190286ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 16.326837ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 16.265409ms)
Jun 22 07:35:10.815: INFO: (4) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 16.59808ms)
Jun 22 07:35:10.823: INFO: (4) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 24.463629ms)
Jun 22 07:35:10.826: INFO: (4) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 27.718171ms)
Jun 22 07:35:10.826: INFO: (4) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 27.797461ms)
Jun 22 07:35:10.827: INFO: (4) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 28.543458ms)
Jun 22 07:35:10.827: INFO: (4) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 28.788794ms)
Jun 22 07:35:10.829: INFO: (4) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 29.80939ms)
Jun 22 07:35:10.873: INFO: (5) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 44.917502ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 44.514326ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 44.666258ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 44.93885ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 45.436898ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 45.403968ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 45.401725ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 45.489118ms)
Jun 22 07:35:10.874: INFO: (5) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 45.620659ms)
Jun 22 07:35:10.875: INFO: (5) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 46.26466ms)
Jun 22 07:35:10.884: INFO: (5) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 54.581825ms)
Jun 22 07:35:10.970: INFO: (5) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 141.114379ms)
Jun 22 07:35:10.970: INFO: (5) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 140.857549ms)
Jun 22 07:35:10.970: INFO: (5) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 141.255769ms)
Jun 22 07:35:10.970: INFO: (5) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 141.174269ms)
Jun 22 07:35:10.970: INFO: (5) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 141.346224ms)
Jun 22 07:35:10.980: INFO: (6) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 9.708164ms)
Jun 22 07:35:10.980: INFO: (6) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 9.770941ms)
Jun 22 07:35:10.982: INFO: (6) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 11.187707ms)
Jun 22 07:35:10.982: INFO: (6) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 11.294165ms)
Jun 22 07:35:10.985: INFO: (6) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 13.889362ms)
Jun 22 07:35:10.985: INFO: (6) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 14.184425ms)
Jun 22 07:35:11.000: INFO: (6) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 29.133075ms)
Jun 22 07:35:11.000: INFO: (6) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 29.144421ms)
Jun 22 07:35:11.000: INFO: (6) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 29.142899ms)
Jun 22 07:35:11.006: INFO: (6) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 35.425567ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 49.449036ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 49.469864ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 49.501998ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 49.59775ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 49.494621ms)
Jun 22 07:35:11.020: INFO: (6) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 49.558029ms)
Jun 22 07:35:11.029: INFO: (7) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 8.317686ms)
Jun 22 07:35:11.030: INFO: (7) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 8.982286ms)
Jun 22 07:35:11.030: INFO: (7) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 9.479297ms)
Jun 22 07:35:11.030: INFO: (7) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 9.00556ms)
Jun 22 07:35:11.030: INFO: (7) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 9.182052ms)
Jun 22 07:35:11.030: INFO: (7) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 9.883893ms)
Jun 22 07:35:11.032: INFO: (7) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 10.370087ms)
Jun 22 07:35:11.032: INFO: (7) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 10.919784ms)
Jun 22 07:35:11.032: INFO: (7) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 10.636729ms)
Jun 22 07:35:11.033: INFO: (7) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 11.162933ms)
Jun 22 07:35:11.077: INFO: (7) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 55.683621ms)
Jun 22 07:35:11.077: INFO: (7) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 55.880089ms)
Jun 22 07:35:11.077: INFO: (7) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 55.980947ms)
Jun 22 07:35:11.077: INFO: (7) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 55.850874ms)
Jun 22 07:35:11.078: INFO: (7) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 56.608422ms)
Jun 22 07:35:11.124: INFO: (7) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 102.278713ms)
Jun 22 07:35:11.138: INFO: (8) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 14.029535ms)
Jun 22 07:35:11.150: INFO: (8) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 24.978533ms)
Jun 22 07:35:11.150: INFO: (8) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 25.330044ms)
Jun 22 07:35:11.150: INFO: (8) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 25.253066ms)
Jun 22 07:35:11.152: INFO: (8) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 28.068793ms)
Jun 22 07:35:11.152: INFO: (8) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 28.147453ms)
Jun 22 07:35:11.153: INFO: (8) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 28.476553ms)
Jun 22 07:35:11.153: INFO: (8) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 28.465274ms)
Jun 22 07:35:11.155: INFO: (8) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 31.254986ms)
Jun 22 07:35:11.163: INFO: (8) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 38.822304ms)
Jun 22 07:35:11.163: INFO: (8) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 38.362503ms)
Jun 22 07:35:11.163: INFO: (8) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 38.783567ms)
Jun 22 07:35:11.180: INFO: (8) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 55.413255ms)
Jun 22 07:35:11.180: INFO: (8) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 56.085507ms)
Jun 22 07:35:11.181: INFO: (8) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 55.975156ms)
Jun 22 07:35:11.181: INFO: (8) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 57.21201ms)
Jun 22 07:35:11.274: INFO: (9) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 92.308829ms)
Jun 22 07:35:11.275: INFO: (9) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 92.528154ms)
Jun 22 07:35:11.275: INFO: (9) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 93.139809ms)
Jun 22 07:35:11.275: INFO: (9) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 92.76078ms)
Jun 22 07:35:11.275: INFO: (9) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 92.465375ms)
Jun 22 07:35:11.275: INFO: (9) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 93.186634ms)
Jun 22 07:35:11.276: INFO: (9) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 93.739778ms)
Jun 22 07:35:11.276: INFO: (9) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 93.251415ms)
Jun 22 07:35:11.277: INFO: (9) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 94.364206ms)
Jun 22 07:35:11.277: INFO: (9) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 94.712385ms)
Jun 22 07:35:11.285: INFO: (9) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 102.76977ms)
Jun 22 07:35:11.288: INFO: (9) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 105.528288ms)
Jun 22 07:35:11.288: INFO: (9) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 106.129747ms)
Jun 22 07:35:11.288: INFO: (9) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 105.708176ms)
Jun 22 07:35:11.288: INFO: (9) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 105.882648ms)
Jun 22 07:35:11.288: INFO: (9) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 106.705176ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 24.062902ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 24.393567ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 24.310831ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 25.085598ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 24.460813ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 25.420014ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 25.035375ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 24.99961ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 25.180295ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 25.436211ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 25.191117ms)
Jun 22 07:35:11.314: INFO: (10) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 25.105523ms)
Jun 22 07:35:11.315: INFO: (10) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 25.849458ms)
Jun 22 07:35:11.315: INFO: (10) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 26.753504ms)
Jun 22 07:35:11.316: INFO: (10) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 26.262422ms)
Jun 22 07:35:11.316: INFO: (10) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 26.051633ms)
Jun 22 07:35:11.382: INFO: (11) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 66.492499ms)
Jun 22 07:35:11.395: INFO: (11) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 79.477295ms)
Jun 22 07:35:11.395: INFO: (11) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 79.159073ms)
Jun 22 07:35:11.396: INFO: (11) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 79.073756ms)
Jun 22 07:35:11.398: INFO: (11) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 81.033035ms)
Jun 22 07:35:11.398: INFO: (11) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 81.786204ms)
Jun 22 07:35:11.398: INFO: (11) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 81.500122ms)
Jun 22 07:35:11.398: INFO: (11) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 82.163949ms)
Jun 22 07:35:11.398: INFO: (11) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 81.986726ms)
Jun 22 07:35:11.402: INFO: (11) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 85.091277ms)
Jun 22 07:35:11.408: INFO: (11) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 91.869464ms)
Jun 22 07:35:11.418: INFO: (11) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 101.419041ms)
Jun 22 07:35:11.418: INFO: (11) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 101.622969ms)
Jun 22 07:35:11.418: INFO: (11) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 101.6002ms)
Jun 22 07:35:11.418: INFO: (11) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 101.254649ms)
Jun 22 07:35:11.418: INFO: (11) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 101.97658ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 41.715444ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 42.021941ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 42.101851ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 42.877907ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 42.778177ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 42.658128ms)
Jun 22 07:35:11.461: INFO: (12) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 42.174707ms)
Jun 22 07:35:11.462: INFO: (12) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 43.387722ms)
Jun 22 07:35:11.462: INFO: (12) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 43.882981ms)
Jun 22 07:35:11.462: INFO: (12) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 43.617929ms)
Jun 22 07:35:11.469: INFO: (12) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 50.553791ms)
Jun 22 07:35:11.478: INFO: (12) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 60.111016ms)
Jun 22 07:35:11.478: INFO: (12) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 59.05081ms)
Jun 22 07:35:11.478: INFO: (12) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 59.474106ms)
Jun 22 07:35:11.478: INFO: (12) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 59.718563ms)
Jun 22 07:35:11.482: INFO: (12) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 62.816333ms)
Jun 22 07:35:11.749: INFO: (13) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 267.081273ms)
Jun 22 07:35:11.786: INFO: (13) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 302.948353ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 303.617688ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 304.358881ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 304.184285ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 304.434137ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 304.053622ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 304.805975ms)
Jun 22 07:35:11.787: INFO: (13) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 304.478001ms)
Jun 22 07:35:11.788: INFO: (13) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 305.389388ms)
Jun 22 07:35:11.885: INFO: (13) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 403.135063ms)
Jun 22 07:35:11.885: INFO: (13) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 402.504521ms)
Jun 22 07:35:11.886: INFO: (13) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 403.038916ms)
Jun 22 07:35:11.886: INFO: (13) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 403.471238ms)
Jun 22 07:35:11.886: INFO: (13) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 403.423046ms)
Jun 22 07:35:11.887: INFO: (13) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 404.335982ms)
Jun 22 07:35:12.014: INFO: (14) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 126.645968ms)
Jun 22 07:35:12.014: INFO: (14) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 126.462517ms)
Jun 22 07:35:12.014: INFO: (14) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 127.611223ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 126.914729ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 126.756791ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 126.595845ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 127.626582ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 126.963293ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 127.163725ms)
Jun 22 07:35:12.015: INFO: (14) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 127.502236ms)
Jun 22 07:35:12.021: INFO: (14) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 133.493479ms)
Jun 22 07:35:12.021: INFO: (14) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 133.522064ms)
Jun 22 07:35:12.021: INFO: (14) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 133.385327ms)
Jun 22 07:35:12.022: INFO: (14) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 134.802153ms)
Jun 22 07:35:12.037: INFO: (14) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 148.966479ms)
Jun 22 07:35:12.037: INFO: (14) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 149.607837ms)
Jun 22 07:35:12.047: INFO: (15) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 9.645212ms)
Jun 22 07:35:12.285: INFO: (15) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 248.205094ms)
Jun 22 07:35:12.285: INFO: (15) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 248.182791ms)
Jun 22 07:35:12.285: INFO: (15) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 248.37459ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 248.528139ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 248.503784ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 248.784039ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 248.78534ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 248.941429ms)
Jun 22 07:35:12.286: INFO: (15) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 248.912767ms)
Jun 22 07:35:12.292: INFO: (15) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 255.091291ms)
Jun 22 07:35:12.302: INFO: (15) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 265.284514ms)
Jun 22 07:35:12.302: INFO: (15) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 265.264539ms)
Jun 22 07:35:12.302: INFO: (15) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 265.312746ms)
Jun 22 07:35:12.310: INFO: (15) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 272.880177ms)
Jun 22 07:35:12.310: INFO: (15) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 273.032851ms)
Jun 22 07:35:12.322: INFO: (16) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 10.856957ms)
Jun 22 07:35:12.334: INFO: (16) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 22.76034ms)
Jun 22 07:35:12.335: INFO: (16) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 23.783659ms)
Jun 22 07:35:12.335: INFO: (16) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 23.79833ms)
Jun 22 07:35:12.335: INFO: (16) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 23.912811ms)
Jun 22 07:35:12.335: INFO: (16) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 23.930563ms)
Jun 22 07:35:12.335: INFO: (16) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 23.940139ms)
Jun 22 07:35:12.336: INFO: (16) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 24.031236ms)
Jun 22 07:35:12.336: INFO: (16) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 24.014665ms)
Jun 22 07:35:12.336: INFO: (16) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 23.499019ms)
Jun 22 07:35:12.337: INFO: (16) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 25.712537ms)
Jun 22 07:35:12.342: INFO: (16) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 30.103096ms)
Jun 22 07:35:12.342: INFO: (16) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 30.162967ms)
Jun 22 07:35:12.342: INFO: (16) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 30.45572ms)
Jun 22 07:35:12.342: INFO: (16) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 30.629939ms)
Jun 22 07:35:12.343: INFO: (16) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 31.770033ms)
Jun 22 07:35:12.561: INFO: (17) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 216.835667ms)
Jun 22 07:35:12.568: INFO: (17) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 223.936336ms)
Jun 22 07:35:12.569: INFO: (17) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 225.291428ms)
Jun 22 07:35:12.569: INFO: (17) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 225.090912ms)
Jun 22 07:35:12.569: INFO: (17) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 225.443016ms)
Jun 22 07:35:12.569: INFO: (17) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 225.190734ms)
Jun 22 07:35:12.569: INFO: (17) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 225.536931ms)
Jun 22 07:35:12.571: INFO: (17) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 226.271201ms)
Jun 22 07:35:12.571: INFO: (17) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 227.006982ms)
Jun 22 07:35:12.571: INFO: (17) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 226.739376ms)
Jun 22 07:35:12.571: INFO: (17) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 226.776759ms)
Jun 22 07:35:12.571: INFO: (17) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 227.483377ms)
Jun 22 07:35:12.572: INFO: (17) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 228.344673ms)
Jun 22 07:35:12.575: INFO: (17) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 231.433293ms)
Jun 22 07:35:12.575: INFO: (17) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 232.048849ms)
Jun 22 07:35:12.654: INFO: (17) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 310.178868ms)
Jun 22 07:35:12.717: INFO: (18) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 62.414102ms)
Jun 22 07:35:12.718: INFO: (18) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 63.975018ms)
Jun 22 07:35:12.718: INFO: (18) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 64.216319ms)
Jun 22 07:35:12.718: INFO: (18) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 63.773271ms)
Jun 22 07:35:12.719: INFO: (18) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 63.944834ms)
Jun 22 07:35:12.721: INFO: (18) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 66.546102ms)
Jun 22 07:35:12.724: INFO: (18) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 69.467593ms)
Jun 22 07:35:12.725: INFO: (18) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 70.896046ms)
Jun 22 07:35:12.726: INFO: (18) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 71.093148ms)
Jun 22 07:35:12.726: INFO: (18) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 71.19776ms)
Jun 22 07:35:12.727: INFO: (18) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 72.07378ms)
Jun 22 07:35:12.729: INFO: (18) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 74.125419ms)
Jun 22 07:35:12.729: INFO: (18) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 74.460844ms)
Jun 22 07:35:12.729: INFO: (18) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 74.64371ms)
Jun 22 07:35:12.729: INFO: (18) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 74.67507ms)
Jun 22 07:35:12.729: INFO: (18) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 74.416195ms)
Jun 22 07:35:12.745: INFO: (19) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">... (200; 14.930678ms)
Jun 22 07:35:12.745: INFO: (19) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 14.938376ms)
Jun 22 07:35:12.754: INFO: (19) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 24.717046ms)
Jun 22 07:35:12.754: INFO: (19) /api/v1/namespaces/proxy-1277/pods/http:proxy-service-46zl8-5zq2s:162/proxy/: bar (200; 24.088455ms)
Jun 22 07:35:12.754: INFO: (19) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:462/proxy/: tls qux (200; 25.108862ms)
Jun 22 07:35:12.754: INFO: (19) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:443/proxy/tlsrewritem... (200; 24.409056ms)
Jun 22 07:35:12.754: INFO: (19) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:1080/proxy/rewriteme">test<... (200; 24.938767ms)
Jun 22 07:35:12.756: INFO: (19) /api/v1/namespaces/proxy-1277/pods/https:proxy-service-46zl8-5zq2s:460/proxy/: tls baz (200; 25.646784ms)
Jun 22 07:35:12.756: INFO: (19) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/: <a href="/api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s/proxy/rewriteme">test</a> (200; 25.464915ms)
Jun 22 07:35:12.756: INFO: (19) /api/v1/namespaces/proxy-1277/pods/proxy-service-46zl8-5zq2s:160/proxy/: foo (200; 25.552966ms)
Jun 22 07:35:12.771: INFO: (19) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname2/proxy/: tls qux (200; 41.181471ms)
Jun 22 07:35:12.771: INFO: (19) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname2/proxy/: bar (200; 41.038326ms)
Jun 22 07:35:12.772: INFO: (19) /api/v1/namespaces/proxy-1277/services/https:proxy-service-46zl8:tlsportname1/proxy/: tls baz (200; 41.931727ms)
Jun 22 07:35:12.772: INFO: (19) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname1/proxy/: foo (200; 42.188941ms)
Jun 22 07:35:12.772: INFO: (19) /api/v1/namespaces/proxy-1277/services/http:proxy-service-46zl8:portname2/proxy/: bar (200; 42.110467ms)
Jun 22 07:35:12.772: INFO: (19) /api/v1/namespaces/proxy-1277/services/proxy-service-46zl8:portname1/proxy/: foo (200; 41.63089ms)
STEP: deleting ReplicationController proxy-service-46zl8 in namespace proxy-1277, will wait for the garbage collector to delete the pods
Jun 22 07:35:12.927: INFO: Deleting ReplicationController proxy-service-46zl8 took: 90.866472ms
Jun 22 07:35:13.827: INFO: Terminating ReplicationController proxy-service-46zl8 pods took: 900.247781ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:35:25.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1277" for this suite.
Jun 22 07:35:42.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:35:42.804: INFO: namespace proxy-1277 deletion completed in 17.488834248s

• [SLOW TEST:43.047 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:35:42.805: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4331
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 07:35:45.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4331'
Jun 22 07:35:45.852: INFO: stderr: ""
Jun 22 07:35:45.852: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jun 22 07:35:45.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete pods e2e-test-nginx-pod --namespace=kubectl-4331'
Jun 22 07:35:48.878: INFO: stderr: ""
Jun 22 07:35:48.878: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:35:48.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4331" for this suite.
Jun 22 07:36:01.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:36:04.051: INFO: namespace kubectl-4331 deletion completed in 14.812407077s

• [SLOW TEST:21.246 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:36:04.051: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4566
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:36:13.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4566" for this suite.
Jun 22 07:37:05.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:37:09.271: INFO: namespace kubelet-test-4566 deletion completed in 56.242934613s

• [SLOW TEST:65.220 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:37:09.272: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9074
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7267
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-3871
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:37:59.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9074" for this suite.
Jun 22 07:38:11.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:38:14.549: INFO: namespace namespaces-9074 deletion completed in 15.195881238s
STEP: Destroying namespace "nsdeletetest-7267" for this suite.
Jun 22 07:38:14.558: INFO: Namespace nsdeletetest-7267 was already deleted
STEP: Destroying namespace "nsdeletetest-3871" for this suite.
Jun 22 07:38:29.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:38:32.251: INFO: namespace nsdeletetest-3871 deletion completed in 17.692491236s

• [SLOW TEST:82.979 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:38:32.252: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8969
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jun 22 07:38:33.805: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:38:41.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8969" for this suite.
Jun 22 07:38:58.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:38:59.889: INFO: namespace init-container-8969 deletion completed in 18.13063784s

• [SLOW TEST:27.638 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:38:59.890: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1724
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cd35b288-94c0-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume configMaps
Jun 22 07:39:01.224: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e" in namespace "projected-1724" to be "success or failure"
Jun 22 07:39:01.318: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 93.955462ms
Jun 22 07:39:03.477: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25370118s
Jun 22 07:39:05.577: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353402062s
Jun 22 07:39:07.585: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.361692255s
Jun 22 07:39:09.770: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.545972969s
STEP: Saw pod success
Jun 22 07:39:09.770: INFO: Pod "pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:39:09.810: INFO: Trying to get logs from node slave5 pod pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 07:39:10.629: INFO: Waiting for pod pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e to disappear
Jun 22 07:39:10.642: INFO: Pod pod-projected-configmaps-cd5ed727-94c0-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:39:10.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1724" for this suite.
Jun 22 07:39:22.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:39:25.165: INFO: namespace projected-1724 deletion completed in 14.515552479s

• [SLOW TEST:25.275 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:39:25.166: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5811
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-dc5a24c6-94c0-11e9-8f59-1e22372c056e
STEP: Creating secret with name s-test-opt-upd-dc5a2519-94c0-11e9-8f59-1e22372c056e
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-dc5a24c6-94c0-11e9-8f59-1e22372c056e
STEP: Updating secret s-test-opt-upd-dc5a2519-94c0-11e9-8f59-1e22372c056e
STEP: Creating secret with name s-test-opt-create-dc5a2535-94c0-11e9-8f59-1e22372c056e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:40:53.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5811" for this suite.
Jun 22 07:41:27.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:41:29.119: INFO: namespace secrets-5811 deletion completed in 36.106131834s

• [SLOW TEST:123.953 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:41:29.120: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:41:30.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1389" for this suite.
Jun 22 07:41:42.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:41:44.417: INFO: namespace kubelet-test-1389 deletion completed in 14.09343132s

• [SLOW TEST:15.297 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:41:44.418: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2774
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 07:41:46.565: INFO: Waiting up to 5m0s for pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e" in namespace "emptydir-2774" to be "success or failure"
Jun 22 07:41:46.978: INFO: Pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 412.298713ms
Jun 22 07:41:49.016: INFO: Pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.450357519s
Jun 22 07:41:51.084: INFO: Pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518746545s
Jun 22 07:41:53.094: INFO: Pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.528420838s
STEP: Saw pod success
Jun 22 07:41:53.094: INFO: Pod "pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:41:53.097: INFO: Trying to get logs from node slave7 pod pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:41:53.658: INFO: Waiting for pod pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e to disappear
Jun 22 07:41:53.666: INFO: Pod pod-2f8b9f59-94c1-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:41:53.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2774" for this suite.
Jun 22 07:42:03.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:42:06.110: INFO: namespace emptydir-2774 deletion completed in 12.434880537s

• [SLOW TEST:21.692 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:42:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 07:42:07.540: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 07:42:12.762: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 07:42:12.762: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 07:42:14.772: INFO: Creating deployment "test-rollover-deployment"
Jun 22 07:42:14.919: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 07:42:17.212: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 07:42:17.226: INFO: Ensure that both replica sets have 1 created replica
Jun 22 07:42:17.236: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 22 07:42:17.412: INFO: Updating deployment test-rollover-deployment
Jun 22 07:42:17.412: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 07:42:19.587: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 07:42:20.054: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 07:42:20.689: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:20.689: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786140, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:23.158: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:23.158: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786140, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:25.048: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:25.049: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786140, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:26.701: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:26.701: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786145, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:28.840: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:28.840: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786145, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:30.699: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:30.699: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786145, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:32.710: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:32.711: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786145, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:34.722: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 07:42:34.722: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786145, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:37.553: INFO: 
Jun 22 07:42:37.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:2, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786156, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696786135, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 07:42:38.843: INFO: 
Jun 22 07:42:38.843: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jun 22 07:42:38.964: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7758,SelfLink:/apis/apps/v1/namespaces/deployment-7758/deployments/test-rollover-deployment,UID:40dead73-94c1-11e9-a9b0-fa163e4f9fd7,ResourceVersion:680377,Generation:2,CreationTimestamp:2019-06-22 07:42:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-22 07:42:15 +0000 UTC 2019-06-22 07:42:15 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-22 07:42:37 +0000 UTC 2019-06-22 07:42:15 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 07:42:38.969: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-7758,SelfLink:/apis/apps/v1/namespaces/deployment-7758/replicasets/test-rollover-deployment-766b4d6c9d,UID:425870d3-94c1-11e9-82ac-fa163e446741,ResourceVersion:680362,Generation:2,CreationTimestamp:2019-06-22 07:42:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 40dead73-94c1-11e9-a9b0-fa163e4f9fd7 0xc0034a92d7 0xc0034a92d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 07:42:38.969: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 07:42:38.969: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7758,SelfLink:/apis/apps/v1/namespaces/deployment-7758/replicasets/test-rollover-controller,UID:3c70e15c-94c1-11e9-a9b0-fa163e4f9fd7,ResourceVersion:680375,Generation:2,CreationTimestamp:2019-06-22 07:42:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 40dead73-94c1-11e9-a9b0-fa163e4f9fd7 0xc0034a9127 0xc0034a9128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 07:42:38.969: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-7758,SelfLink:/apis/apps/v1/namespaces/deployment-7758/replicasets/test-rollover-deployment-6455657675,UID:40f36e18-94c1-11e9-82ac-fa163e446741,ResourceVersion:680310,Generation:2,CreationTimestamp:2019-06-22 07:42:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 40dead73-94c1-11e9-a9b0-fa163e4f9fd7 0xc0034a91f7 0xc0034a91f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 07:42:38.980: INFO: Pod "test-rollover-deployment-766b4d6c9d-v64dt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-v64dt,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-7758,SelfLink:/api/v1/namespaces/deployment-7758/pods/test-rollover-deployment-766b4d6c9d-v64dt,UID:42b84849-94c1-11e9-82ac-fa163e446741,ResourceVersion:680334,Generation:0,CreationTimestamp:2019-06-22 07:42:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 425870d3-94c1-11e9-82ac-fa163e446741 0xc0034a9df7 0xc0034a9df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7tqvm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7tqvm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7tqvm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:slave6,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0034a9e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0034a9e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:42:18 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:42:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:42:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 07:42:18 +0000 UTC  }],Message:,Reason:,HostIP:192.168.202.64,PodIP:10.151.190.16,StartTime:2019-06-22 07:42:18 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-22 07:42:24 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker://sha256:e4423e943a205fe1d81768e60603c8f2c5821576bad0801c1e91b8ba586124a0 docker://42aba604c417caa2cbe70ed195763b2765de3324a17a0d85bee410baea25e7f1}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:42:38.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7758" for this suite.
Jun 22 07:42:57.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:42:58.951: INFO: namespace deployment-7758 deletion completed in 19.95426446s

• [SLOW TEST:52.840 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:42:58.952: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-p2hh
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 07:43:00.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-p2hh" in namespace "subpath-8234" to be "success or failure"
Jun 22 07:43:00.530: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.579441ms
Jun 22 07:43:02.924: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.40271053s
Jun 22 07:43:04.955: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433277508s
Jun 22 07:43:06.960: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 6.439011708s
Jun 22 07:43:08.967: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 8.44587894s
Jun 22 07:43:10.973: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 10.45202953s
Jun 22 07:43:12.982: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 12.461069224s
Jun 22 07:43:14.988: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 14.466783089s
Jun 22 07:43:16.994: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 16.472306164s
Jun 22 07:43:19.008: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 18.486661689s
Jun 22 07:43:21.139: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 20.617471125s
Jun 22 07:43:23.160: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Running", Reason="", readiness=true. Elapsed: 22.638870468s
Jun 22 07:43:25.367: INFO: Pod "pod-subpath-test-secret-p2hh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.845310554s
STEP: Saw pod success
Jun 22 07:43:25.367: INFO: Pod "pod-subpath-test-secret-p2hh" satisfied condition "success or failure"
Jun 22 07:43:25.380: INFO: Trying to get logs from node slave7 pod pod-subpath-test-secret-p2hh container test-container-subpath-secret-p2hh: <nil>
STEP: delete the pod
Jun 22 07:43:25.909: INFO: Waiting for pod pod-subpath-test-secret-p2hh to disappear
Jun 22 07:43:26.089: INFO: Pod pod-subpath-test-secret-p2hh no longer exists
STEP: Deleting pod pod-subpath-test-secret-p2hh
Jun 22 07:43:26.090: INFO: Deleting pod "pod-subpath-test-secret-p2hh" in namespace "subpath-8234"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:43:26.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8234" for this suite.
Jun 22 07:43:38.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:43:39.824: INFO: namespace subpath-8234 deletion completed in 13.71509807s

• [SLOW TEST:40.872 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:43:39.825: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9153
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jun 22 07:43:41.334: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:43:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9153" for this suite.
Jun 22 07:44:42.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:44:44.813: INFO: namespace pods-9153 deletion completed in 54.350228118s

• [SLOW TEST:64.988 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:44:44.814: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1891
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 07:44:45.960: INFO: Waiting up to 5m0s for pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e" in namespace "emptydir-1891" to be "success or failure"
Jun 22 07:44:46.022: INFO: Pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 61.573583ms
Jun 22 07:44:48.109: INFO: Pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.148288983s
Jun 22 07:44:50.113: INFO: Pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.152682504s
Jun 22 07:44:52.281: INFO: Pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.32038985s
STEP: Saw pod success
Jun 22 07:44:52.281: INFO: Pod "pod-9ace93e7-94c1-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:44:52.782: INFO: Trying to get logs from node slave8 pod pod-9ace93e7-94c1-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:44:52.859: INFO: Waiting for pod pod-9ace93e7-94c1-11e9-8f59-1e22372c056e to disappear
Jun 22 07:44:52.866: INFO: Pod pod-9ace93e7-94c1-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:44:52.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1891" for this suite.
Jun 22 07:45:03.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:45:07.267: INFO: namespace emptydir-1891 deletion completed in 14.389954192s

• [SLOW TEST:22.454 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:45:07.268: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-589
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-589
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jun 22 07:45:09.015: INFO: Found 0 stateful pods, waiting for 3
Jun 22 07:45:19.048: INFO: Found 2 stateful pods, waiting for 3
Jun 22 07:45:29.027: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:45:29.027: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:45:29.027: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 22 07:45:29.577: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 22 07:45:39.936: INFO: Updating stateful set ss2
Jun 22 07:45:39.961: INFO: Waiting for Pod statefulset-589/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jun 22 07:45:53.814: INFO: Found 2 stateful pods, waiting for 3
Jun 22 07:46:03.924: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:46:03.924: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:46:03.924: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 22 07:46:13.828: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:46:13.828: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:46:13.828: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 22 07:46:14.480: INFO: Updating stateful set ss2
Jun 22 07:46:14.895: INFO: Waiting for Pod statefulset-589/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 07:46:25.536: INFO: Updating stateful set ss2
Jun 22 07:46:25.849: INFO: Waiting for StatefulSet statefulset-589/ss2 to complete update
Jun 22 07:46:25.849: INFO: Waiting for Pod statefulset-589/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jun 22 07:46:36.085: INFO: Waiting for StatefulSet statefulset-589/ss2 to complete update
Jun 22 07:46:36.085: INFO: Waiting for Pod statefulset-589/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 22 07:46:46.438: INFO: Deleting all statefulset in ns statefulset-589
Jun 22 07:46:46.445: INFO: Scaling statefulset ss2 to 0
Jun 22 07:47:16.988: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 07:47:17.001: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:47:17.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-589" for this suite.
Jun 22 07:47:39.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:47:41.034: INFO: namespace statefulset-589 deletion completed in 23.743390319s

• [SLOW TEST:153.766 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:47:41.034: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-29
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-29
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-29
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-29
Jun 22 07:47:41.982: INFO: Found 0 stateful pods, waiting for 1
Jun 22 07:47:51.991: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 22 07:47:51.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 07:47:52.312: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 07:47:52.312: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 07:47:52.312: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 07:47:52.318: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 07:48:02.381: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 07:48:02.382: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 07:48:02.958: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999522s
Jun 22 07:48:04.045: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.854681234s
Jun 22 07:48:05.057: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.7672205s
Jun 22 07:48:06.061: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.755466628s
Jun 22 07:48:07.165: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.75125407s
Jun 22 07:48:08.224: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.647381389s
Jun 22 07:48:09.366: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.588334987s
Jun 22 07:48:10.491: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.446049578s
Jun 22 07:48:11.913: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.321639186s
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-29
Jun 22 07:48:12.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 07:48:13.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 07:48:13.256: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 07:48:13.256: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 07:48:13.286: INFO: Found 1 stateful pods, waiting for 3
Jun 22 07:48:23.547: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:48:23.547: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:48:23.547: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Jun 22 07:48:33.301: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:48:33.302: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 07:48:33.304: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 22 07:48:33.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 07:48:33.644: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 07:48:33.644: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 07:48:33.644: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 07:48:33.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 07:48:34.030: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 07:48:34.030: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 07:48:34.030: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 07:48:34.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 07:48:35.922: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 07:48:35.922: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 07:48:35.922: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 07:48:35.922: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 07:48:36.188: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 22 07:48:46.349: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 07:48:46.349: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 07:48:46.349: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 07:48:47.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999571s
Jun 22 07:48:48.122: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.95952897s
Jun 22 07:48:49.145: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.937366642s
Jun 22 07:48:50.282: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.913373279s
Jun 22 07:48:51.320: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.776672727s
Jun 22 07:48:52.363: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.739202014s
Jun 22 07:48:53.369: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.695876128s
Jun 22 07:48:55.146: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.69001266s
Jun 22 07:48:56.214: INFO: Verifying statefulset ss doesn't scale past 3 for another 912.715675ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-29
Jun 22 07:48:57.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 07:48:57.718: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 07:48:57.718: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 07:48:57.718: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 07:48:57.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 07:48:58.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 07:48:58.266: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 07:48:58.266: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 07:48:58.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 exec --namespace=statefulset-29 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 07:48:58.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 07:48:58.857: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 07:48:58.857: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 07:48:58.857: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jun 22 07:49:29.507: INFO: Deleting all statefulset in ns statefulset-29
Jun 22 07:49:29.511: INFO: Scaling statefulset ss to 0
Jun 22 07:49:29.522: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 07:49:29.526: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:49:29.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-29" for this suite.
Jun 22 07:49:43.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:49:45.600: INFO: namespace statefulset-29 deletion completed in 15.971829408s

• [SLOW TEST:124.566 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:49:45.600: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3496
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-4e0c73de-94c2-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 07:49:46.949: INFO: Waiting up to 5m0s for pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e" in namespace "secrets-3496" to be "success or failure"
Jun 22 07:49:47.041: INFO: Pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 91.206073ms
Jun 22 07:49:49.135: INFO: Pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185382045s
Jun 22 07:49:51.152: INFO: Pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.202750867s
Jun 22 07:49:53.242: INFO: Pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.292203799s
STEP: Saw pod success
Jun 22 07:49:53.242: INFO: Pod "pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:49:53.246: INFO: Trying to get logs from node slave8 pod pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 07:49:53.600: INFO: Waiting for pod pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e to disappear
Jun 22 07:49:53.608: INFO: Pod pod-secrets-4e24c53d-94c2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:49:53.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3496" for this suite.
Jun 22 07:50:05.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:50:07.425: INFO: namespace secrets-3496 deletion completed in 13.810225562s

• [SLOW TEST:21.825 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:50:07.425: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 07:50:10.007: INFO: Waiting up to 5m0s for pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e" in namespace "emptydir-4484" to be "success or failure"
Jun 22 07:50:10.012: INFO: Pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.669787ms
Jun 22 07:50:12.046: INFO: Pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038529869s
Jun 22 07:50:14.121: INFO: Pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.113148501s
Jun 22 07:50:16.260: INFO: Pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.252918633s
STEP: Saw pod success
Jun 22 07:50:16.260: INFO: Pod "pod-5bebdf80-94c2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:50:16.352: INFO: Trying to get logs from node slave6 pod pod-5bebdf80-94c2-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:50:16.691: INFO: Waiting for pod pod-5bebdf80-94c2-11e9-8f59-1e22372c056e to disappear
Jun 22 07:50:16.698: INFO: Pod pod-5bebdf80-94c2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:50:16.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4484" for this suite.
Jun 22 07:50:28.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:50:30.572: INFO: namespace emptydir-4484 deletion completed in 13.868236535s

• [SLOW TEST:23.147 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:50:30.573: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2419
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 07:50:32.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2419'
Jun 22 07:50:37.726: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 07:50:37.726: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 22 07:50:37.927: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jun 22 07:50:38.279: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 22 07:50:38.763: INFO: scanned /root for discovery docs: <nil>
Jun 22 07:50:38.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-2419'
Jun 22 07:51:02.488: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 07:51:02.489: INFO: stdout: "Created e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86\nScaling up e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 22 07:51:02.489: INFO: stdout: "Created e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86\nScaling up e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 22 07:51:02.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-2419'
Jun 22 07:51:02.768: INFO: stderr: ""
Jun 22 07:51:02.768: INFO: stdout: "e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86-662bb "
Jun 22 07:51:02.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86-662bb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2419'
Jun 22 07:51:02.910: INFO: stderr: ""
Jun 22 07:51:02.910: INFO: stdout: "true"
Jun 22 07:51:02.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 get pods e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86-662bb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2419'
Jun 22 07:51:03.225: INFO: stderr: ""
Jun 22 07:51:03.225: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 22 07:51:03.225: INFO: e2e-test-nginx-rc-690d4d64d40f4624e0f4dbfd496d3a86-662bb is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jun 22 07:51:03.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete rc e2e-test-nginx-rc --namespace=kubectl-2419'
Jun 22 07:51:03.485: INFO: stderr: ""
Jun 22 07:51:03.485: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:51:03.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2419" for this suite.
Jun 22 07:51:18.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:51:19.722: INFO: namespace kubectl-2419 deletion completed in 16.120452469s

• [SLOW TEST:49.149 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:51:19.723: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3644
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 07:51:21.271: INFO: Waiting up to 5m0s for pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e" in namespace "emptydir-3644" to be "success or failure"
Jun 22 07:51:21.440: INFO: Pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 169.015001ms
Jun 22 07:51:23.445: INFO: Pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.173844592s
Jun 22 07:51:25.670: INFO: Pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.399158451s
Jun 22 07:51:27.708: INFO: Pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.436619558s
STEP: Saw pod success
Jun 22 07:51:27.708: INFO: Pod "pod-8666352c-94c2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:51:27.717: INFO: Trying to get logs from node slave8 pod pod-8666352c-94c2-11e9-8f59-1e22372c056e container test-container: <nil>
STEP: delete the pod
Jun 22 07:51:28.320: INFO: Waiting for pod pod-8666352c-94c2-11e9-8f59-1e22372c056e to disappear
Jun 22 07:51:28.330: INFO: Pod pod-8666352c-94c2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:51:28.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3644" for this suite.
Jun 22 07:51:40.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:51:42.330: INFO: namespace emptydir-3644 deletion completed in 13.935835847s

• [SLOW TEST:22.607 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:51:42.331: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-3213
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3213.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3213.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 07:51:52.007: INFO: DNS probes using dns-3213/dns-test-9391ec3a-94c2-11e9-8f59-1e22372c056e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:51:52.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3213" for this suite.
Jun 22 07:52:10.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:52:12.639: INFO: namespace dns-3213 deletion completed in 19.881351006s

• [SLOW TEST:30.308 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:52:12.640: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3109
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jun 22 07:52:19.542: INFO: Successfully updated pod "annotationupdatea5ca09d6-94c2-11e9-8f59-1e22372c056e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:52:23.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3109" for this suite.
Jun 22 07:53:00.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:53:02.294: INFO: namespace projected-3109 deletion completed in 38.605921252s

• [SLOW TEST:49.655 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:53:02.296: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-12
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:53:11.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-12" for this suite.
Jun 22 07:54:14.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:54:16.753: INFO: namespace kubelet-test-12 deletion completed in 1m4.856229588s

• [SLOW TEST:74.457 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:54:16.754: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jun 22 07:54:18.022: INFO: Waiting up to 5m0s for pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e" in namespace "var-expansion-9619" to be "success or failure"
Jun 22 07:54:18.446: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 424.02636ms
Jun 22 07:54:20.522: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.49909002s
Jun 22 07:54:22.643: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.62054177s
Jun 22 07:54:24.665: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.642941836s
Jun 22 07:54:27.556: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 9.533195861s
STEP: Saw pod success
Jun 22 07:54:27.556: INFO: Pod "var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:54:27.581: INFO: Trying to get logs from node slave8 pod var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e container dapi-container: <nil>
STEP: delete the pod
Jun 22 07:54:28.621: INFO: Waiting for pod var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e to disappear
Jun 22 07:54:28.669: INFO: Pod var-expansion-efc2d665-94c2-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:54:28.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9619" for this suite.
Jun 22 07:54:43.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:54:44.953: INFO: namespace var-expansion-9619 deletion completed in 16.263326788s

• [SLOW TEST:28.200 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:54:44.954: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7579
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7579
Jun 22 07:54:54.795: INFO: Started pod liveness-http in namespace container-probe-7579
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 07:54:54.926: INFO: Initial restart count of pod liveness-http is 0
Jun 22 07:55:13.613: INFO: Restart count of pod container-probe-7579/liveness-http is now 1 (18.687361164s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:55:14.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7579" for this suite.
Jun 22 07:55:28.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:55:30.760: INFO: namespace container-probe-7579 deletion completed in 16.387052549s

• [SLOW TEST:45.807 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:55:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1613
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-1c196195-94c3-11e9-8f59-1e22372c056e
STEP: Creating a pod to test consume secrets
Jun 22 07:55:32.728: INFO: Waiting up to 5m0s for pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e" in namespace "secrets-1613" to be "success or failure"
Jun 22 07:55:32.978: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 249.73265ms
Jun 22 07:55:35.287: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558793975s
Jun 22 07:55:37.503: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.775043445s
Jun 22 07:55:39.509: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.780872881s
Jun 22 07:55:41.549: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.82106623s
STEP: Saw pod success
Jun 22 07:55:41.549: INFO: Pod "pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:55:41.565: INFO: Trying to get logs from node slave7 pod pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 07:55:42.014: INFO: Waiting for pod pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e to disappear
Jun 22 07:55:42.021: INFO: Pod pod-secrets-1c415956-94c3-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:55:42.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1613" for this suite.
Jun 22 07:55:54.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:55:56.806: INFO: namespace secrets-1613 deletion completed in 14.762927376s

• [SLOW TEST:26.045 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:55:56.806: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0622 07:56:04.241162      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 07:56:04.241: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:56:04.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8700" for this suite.
Jun 22 07:56:17.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:56:19.053: INFO: namespace gc-8700 deletion completed in 14.758528808s

• [SLOW TEST:22.246 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:56:19.054: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jun 22 07:56:21.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e" in namespace "downward-api-2877" to be "success or failure"
Jun 22 07:56:21.030: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.72375ms
Jun 22 07:56:23.053: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030520731s
Jun 22 07:56:25.131: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.108405209s
Jun 22 07:56:27.757: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.734404861s
Jun 22 07:56:29.763: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.740328851s
STEP: Saw pod success
Jun 22 07:56:29.763: INFO: Pod "downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e" satisfied condition "success or failure"
Jun 22 07:56:29.937: INFO: Trying to get logs from node slave6 pod downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e container client-container: <nil>
STEP: delete the pod
Jun 22 07:56:30.721: INFO: Waiting for pod downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e to disappear
Jun 22 07:56:30.947: INFO: Pod downwardapi-volume-3914ff1a-94c3-11e9-8f59-1e22372c056e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:56:30.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2877" for this suite.
Jun 22 07:56:45.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:56:50.221: INFO: namespace downward-api-2877 deletion completed in 19.261479987s

• [SLOW TEST:31.167 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:56:50.224: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-1325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 07:57:09.373: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:09.726: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:11.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:11.736: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:13.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:13.760: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:15.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:15.810: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:17.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:17.732: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:19.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:19.734: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:21.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:22.019: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:23.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:23.785: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:25.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:25.743: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:27.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:28.337: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:29.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:29.854: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 07:57:31.726: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 07:57:32.034: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:57:32.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1325" for this suite.
Jun 22 07:58:12.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 07:58:14.091: INFO: namespace container-lifecycle-hook-1325 deletion completed in 41.934193359s

• [SLOW TEST:83.868 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 07:58:14.092: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7458
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jun 22 07:58:15.641: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 22 07:58:15.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:16.497: INFO: stderr: ""
Jun 22 07:58:16.497: INFO: stdout: "service/redis-slave created\n"
Jun 22 07:58:16.497: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 22 07:58:16.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:17.714: INFO: stderr: ""
Jun 22 07:58:17.714: INFO: stdout: "service/redis-master created\n"
Jun 22 07:58:17.714: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 07:58:17.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:18.921: INFO: stderr: ""
Jun 22 07:58:18.921: INFO: stdout: "service/frontend created\n"
Jun 22 07:58:18.921: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 22 07:58:18.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:19.561: INFO: stderr: ""
Jun 22 07:58:19.561: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 07:58:19.562: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 07:58:19.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:20.562: INFO: stderr: ""
Jun 22 07:58:20.562: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 22 07:58:20.563: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 22 07:58:20.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 create -f - --namespace=kubectl-7458'
Jun 22 07:58:22.237: INFO: stderr: ""
Jun 22 07:58:22.237: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 22 07:58:22.237: INFO: Waiting for all frontend pods to be Running.
Jun 22 07:58:32.288: INFO: Waiting for frontend to serve content.
Jun 22 07:58:33.232: INFO: Trying to add a new entry to the guestbook.
Jun 22 07:58:33.745: INFO: Verifying that added entry can be retrieved.
Jun 22 07:58:33.871: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:58:39.027: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:58:44.467: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:58:49.990: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:58:55.059: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:00.250: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:05.561: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:10.601: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:15.661: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:20.688: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jun 22 07:59:25.715: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jun 22 07:59:30.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:32.378: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:32.378: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 07:59:32.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:33.011: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:33.012: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 07:59:33.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:33.807: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:33.807: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 07:59:33.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:34.318: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:34.318: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 07:59:34.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:34.555: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:34.555: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 07:59:34.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-834690535 delete --grace-period=0 --force -f - --namespace=kubectl-7458'
Jun 22 07:59:34.770: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 07:59:34.771: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 07:59:34.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7458" for this suite.
Jun 22 08:00:15.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 08:00:17.399: INFO: namespace kubectl-7458 deletion completed in 42.61231427s

• [SLOW TEST:123.307 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jun 22 08:00:17.401: INFO: >>> kubeConfig: /tmp/kubeconfig-834690535
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-8447
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 08:00:20.197: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:20.197: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:20.197: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:20.280: INFO: Number of nodes with available pods: 0
Jun 22 08:00:20.280: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:21.857: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:21.857: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:21.857: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:21.939: INFO: Number of nodes with available pods: 0
Jun 22 08:00:21.939: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:22.302: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:22.303: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:22.303: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:22.307: INFO: Number of nodes with available pods: 0
Jun 22 08:00:22.307: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:23.305: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:23.305: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:23.305: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:23.415: INFO: Number of nodes with available pods: 0
Jun 22 08:00:23.415: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:24.747: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:24.747: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:24.747: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:24.990: INFO: Number of nodes with available pods: 0
Jun 22 08:00:24.991: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:25.852: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:25.852: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:25.852: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:26.093: INFO: Number of nodes with available pods: 0
Jun 22 08:00:26.093: INFO: Node slave5 is running more than one daemon pod
Jun 22 08:00:26.365: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:26.366: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:26.366: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:26.371: INFO: Number of nodes with available pods: 2
Jun 22 08:00:26.371: INFO: Node slave6 is running more than one daemon pod
Jun 22 08:00:27.415: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.415: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.415: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.421: INFO: Number of nodes with available pods: 4
Jun 22 08:00:27.421: INFO: Number of running nodes: 4, number of available pods: 4
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 08:00:27.521: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.521: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.521: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:27.530: INFO: Number of nodes with available pods: 3
Jun 22 08:00:27.530: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:28.535: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:28.535: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:28.535: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:28.545: INFO: Number of nodes with available pods: 3
Jun 22 08:00:28.545: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:29.822: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:29.822: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:29.823: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:29.833: INFO: Number of nodes with available pods: 3
Jun 22 08:00:29.833: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:30.845: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:30.845: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:30.845: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:30.852: INFO: Number of nodes with available pods: 3
Jun 22 08:00:30.852: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:31.580: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:31.580: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:31.580: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:31.945: INFO: Number of nodes with available pods: 3
Jun 22 08:00:31.945: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:32.986: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:32.986: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:32.986: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:33.193: INFO: Number of nodes with available pods: 3
Jun 22 08:00:33.193: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:33.743: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:33.743: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:33.743: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:34.121: INFO: Number of nodes with available pods: 3
Jun 22 08:00:34.121: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:34.570: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:34.570: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:34.570: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:34.581: INFO: Number of nodes with available pods: 3
Jun 22 08:00:34.581: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:35.552: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:35.552: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:35.552: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:35.567: INFO: Number of nodes with available pods: 3
Jun 22 08:00:35.567: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:36.540: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:36.540: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:36.540: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:36.550: INFO: Number of nodes with available pods: 3
Jun 22 08:00:36.550: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:37.539: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:37.539: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:37.539: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:37.548: INFO: Number of nodes with available pods: 3
Jun 22 08:00:37.548: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:38.816: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:38.816: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:38.816: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:39.175: INFO: Number of nodes with available pods: 3
Jun 22 08:00:39.175: INFO: Node slave8 is running more than one daemon pod
Jun 22 08:00:39.792: INFO: DaemonSet pods can't tolerate node master1 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:39.792: INFO: DaemonSet pods can't tolerate node master2 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:39.792: INFO: DaemonSet pods can't tolerate node master3 with taints [{Key:pro Value:master Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 08:00:40.314: INFO: Number of nodes with available pods: 4
Jun 22 08:00:40.314: INFO: Number of running nodes: 4, number of available pods: 4
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8447, will wait for the garbage collector to delete the pods
Jun 22 08:00:41.162: INFO: Deleting DaemonSet.extensions daemon-set took: 316.223589ms
Jun 22 08:00:42.563: INFO: Terminating DaemonSet.extensions daemon-set pods took: 1.400233875s
Jun 22 08:00:57.257: INFO: Number of nodes with available pods: 0
Jun 22 08:00:57.257: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 08:00:57.262: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8447/daemonsets","resourceVersion":"684804"},"items":null}

Jun 22 08:00:57.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8447/pods","resourceVersion":"684804"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jun 22 08:00:57.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8447" for this suite.
Jun 22 08:01:15.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 08:01:17.821: INFO: namespace daemonsets-8447 deletion completed in 20.498409168s

• [SLOW TEST:60.420 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
Jun 22 08:01:17.822: INFO: Running AfterSuite actions on all nodes
Jun 22 08:01:17.822: INFO: Running AfterSuite actions on node 1
Jun 22 08:01:17.822: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 10303.570 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 2h51m45.465819271s
Test Suite Passed
