I0703 09:48:28.620192      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-489780963
I0703 09:48:28.620438      15 e2e.go:240] Starting e2e run "b4e7e6a3-9d77-11e9-9a35-2ab8d9f547c5" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1562147307 - Will randomize all specs
Will run 204 of 3585 specs

Jul  3 09:48:28.891: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:48:28.893: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul  3 09:48:28.912: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul  3 09:48:28.961: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul  3 09:48:28.961: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jul  3 09:48:28.961: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul  3 09:48:28.971: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'canal' (0 seconds elapsed)
Jul  3 09:48:28.971: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul  3 09:48:28.971: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Jul  3 09:48:28.972: INFO: e2e test version: v1.14.3
Jul  3 09:48:28.973: INFO: kube-apiserver version: v1.14.3
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:48:28.973: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
Jul  3 09:48:29.013: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-b5e97b01-9d77-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 09:48:29.030: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5" in namespace "projected-8836" to be "success or failure"
Jul  3 09:48:29.036: INFO: Pod "pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.087154ms
Jul  3 09:48:31.040: INFO: Pod "pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010377416s
Jul  3 09:48:33.045: INFO: Pod "pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015583821s
STEP: Saw pod success
Jul  3 09:48:33.045: INFO: Pod "pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:48:33.049: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 09:48:33.072: INFO: Waiting for pod pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:48:33.075: INFO: Pod pod-projected-configmaps-b5ea77ca-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:48:33.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8836" for this suite.
Jul  3 09:48:39.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:48:39.260: INFO: namespace projected-8836 deletion completed in 6.180459619s

• [SLOW TEST:10.287 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:48:39.260: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Jul  3 09:48:39.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 cluster-info'
Jul  3 09:48:39.536: INFO: stderr: ""
Jul  3 09:48:39.536: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://10.10.10.1:443/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:48:39.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1393" for this suite.
Jul  3 09:48:45.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:48:45.696: INFO: namespace kubectl-1393 deletion completed in 6.156025384s

• [SLOW TEST:6.436 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:48:45.697: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 09:48:45.741: INFO: (0) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.58241ms)
Jul  3 09:48:45.787: INFO: (1) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 46.330655ms)
Jul  3 09:48:45.793: INFO: (2) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.51348ms)
Jul  3 09:48:45.798: INFO: (3) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.950956ms)
Jul  3 09:48:45.803: INFO: (4) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.980989ms)
Jul  3 09:48:45.808: INFO: (5) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.380198ms)
Jul  3 09:48:45.813: INFO: (6) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.292639ms)
Jul  3 09:48:45.818: INFO: (7) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.855349ms)
Jul  3 09:48:45.823: INFO: (8) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.849975ms)
Jul  3 09:48:45.828: INFO: (9) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.072293ms)
Jul  3 09:48:45.833: INFO: (10) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.862331ms)
Jul  3 09:48:45.838: INFO: (11) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.724583ms)
Jul  3 09:48:45.843: INFO: (12) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.035897ms)
Jul  3 09:48:45.848: INFO: (13) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.121395ms)
Jul  3 09:48:45.853: INFO: (14) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.95347ms)
Jul  3 09:48:45.858: INFO: (15) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.891559ms)
Jul  3 09:48:45.863: INFO: (16) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.803658ms)
Jul  3 09:48:45.868: INFO: (17) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.080841ms)
Jul  3 09:48:45.872: INFO: (18) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.717741ms)
Jul  3 09:48:45.880: INFO: (19) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.937044ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:48:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1703" for this suite.
Jul  3 09:48:51.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:48:52.045: INFO: namespace proxy-1703 deletion completed in 6.159688254s

• [SLOW TEST:6.348 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:48:52.045: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 09:48:52.086: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-4924" to be "success or failure"
Jul  3 09:48:52.091: INFO: Pod "downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475261ms
Jul  3 09:48:54.096: INFO: Pod "downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009052152s
Jul  3 09:48:56.100: INFO: Pod "downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013747941s
STEP: Saw pod success
Jul  3 09:48:56.100: INFO: Pod "downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:48:56.104: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 09:48:56.127: INFO: Waiting for pod downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:48:56.131: INFO: Pod downwardapi-volume-c3a85d74-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:48:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4924" for this suite.
Jul  3 09:49:02.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:02.339: INFO: namespace downward-api-4924 deletion completed in 6.20448044s

• [SLOW TEST:10.294 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:02.340: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  3 09:49:02.382: INFO: Waiting up to 5m0s for pod "pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-4526" to be "success or failure"
Jul  3 09:49:02.387: INFO: Pod "pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014336ms
Jul  3 09:49:04.391: INFO: Pod "pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009616416s
Jul  3 09:49:06.395: INFO: Pod "pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013533566s
STEP: Saw pod success
Jul  3 09:49:06.395: INFO: Pod "pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:49:06.398: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 09:49:06.421: INFO: Waiting for pod pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:49:06.423: INFO: Pod pod-c9cb478a-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:06.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4526" for this suite.
Jul  3 09:49:12.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:12.586: INFO: namespace emptydir-4526 deletion completed in 6.158682178s

• [SLOW TEST:10.247 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul  3 09:49:12.634: INFO: Waiting up to 5m0s for pod "pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-4504" to be "success or failure"
Jul  3 09:49:12.640: INFO: Pod "pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.313958ms
Jul  3 09:49:14.644: INFO: Pod "pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010336762s
Jul  3 09:49:16.648: INFO: Pod "pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014330439s
STEP: Saw pod success
Jul  3 09:49:16.648: INFO: Pod "pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:49:16.652: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 09:49:16.766: INFO: Waiting for pod pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:49:16.769: INFO: Pod pod-cfe768c1-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:16.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4504" for this suite.
Jul  3 09:49:22.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:22.931: INFO: namespace emptydir-4504 deletion completed in 6.157636989s

• [SLOW TEST:10.345 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:22.931: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 09:49:22.972: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5" in namespace "projected-4546" to be "success or failure"
Jul  3 09:49:22.977: INFO: Pod "downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.269545ms
Jul  3 09:49:24.983: INFO: Pod "downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011266026s
STEP: Saw pod success
Jul  3 09:49:24.983: INFO: Pod "downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:49:24.986: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 09:49:25.019: INFO: Waiting for pod downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:49:25.023: INFO: Pod downwardapi-volume-d6113d49-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:25.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4546" for this suite.
Jul  3 09:49:31.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:31.193: INFO: namespace projected-4546 deletion completed in 6.165945373s

• [SLOW TEST:8.262 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:31.194: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul  3 09:49:31.260: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1851,SelfLink:/api/v1/namespaces/watch-1851/configmaps/e2e-watch-test-resource-version,UID:daffef6c-9d77-11e9-9bcb-a6641e9563f8,ResourceVersion:280938,Generation:0,CreationTimestamp:2019-07-03 09:49:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  3 09:49:31.260: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-1851,SelfLink:/api/v1/namespaces/watch-1851/configmaps/e2e-watch-test-resource-version,UID:daffef6c-9d77-11e9-9bcb-a6641e9563f8,ResourceVersion:280939,Generation:0,CreationTimestamp:2019-07-03 09:49:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:31.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1851" for this suite.
Jul  3 09:49:37.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:37.425: INFO: namespace watch-1851 deletion completed in 6.161245271s

• [SLOW TEST:6.232 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:37.426: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:37.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7456" for this suite.
Jul  3 09:49:43.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:49:43.640: INFO: namespace kubelet-test-7456 deletion completed in 6.157166503s

• [SLOW TEST:6.215 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:49:43.641: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4432 to expose endpoints map[]
Jul  3 09:49:43.689: INFO: Get endpoints failed (4.597653ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul  3 09:49:44.692: INFO: successfully validated that service multi-endpoint-test in namespace services-4432 exposes endpoints map[] (1.007989577s elapsed)
STEP: Creating pod pod1 in namespace services-4432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4432 to expose endpoints map[pod1:[100]]
Jul  3 09:49:47.731: INFO: successfully validated that service multi-endpoint-test in namespace services-4432 exposes endpoints map[pod1:[100]] (3.030669072s elapsed)
STEP: Creating pod pod2 in namespace services-4432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4432 to expose endpoints map[pod1:[100] pod2:[101]]
Jul  3 09:49:50.782: INFO: successfully validated that service multi-endpoint-test in namespace services-4432 exposes endpoints map[pod1:[100] pod2:[101]] (3.045544982s elapsed)
STEP: Deleting pod pod1 in namespace services-4432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4432 to expose endpoints map[pod2:[101]]
Jul  3 09:49:51.804: INFO: successfully validated that service multi-endpoint-test in namespace services-4432 exposes endpoints map[pod2:[101]] (1.015090924s elapsed)
STEP: Deleting pod pod2 in namespace services-4432
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4432 to expose endpoints map[]
Jul  3 09:49:51.817: INFO: successfully validated that service multi-endpoint-test in namespace services-4432 exposes endpoints map[] (5.154968ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:49:51.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4432" for this suite.
Jul  3 09:50:13.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:50:14.006: INFO: namespace services-4432 deletion completed in 22.168921344s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:30.366 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:50:14.006: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 09:50:14.050: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5" in namespace "projected-8849" to be "success or failure"
Jul  3 09:50:14.056: INFO: Pod "downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738185ms
Jul  3 09:50:16.060: INFO: Pod "downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009989706s
STEP: Saw pod success
Jul  3 09:50:16.060: INFO: Pod "downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:50:16.063: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 09:50:16.086: INFO: Waiting for pod downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:50:16.089: INFO: Pod downwardapi-volume-f482ecc1-9d77-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:50:16.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8849" for this suite.
Jul  3 09:50:22.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:50:22.264: INFO: namespace projected-8849 deletion completed in 6.170192329s

• [SLOW TEST:8.257 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:50:22.264: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-f96f80e6-9d77-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:50:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8002" for this suite.
Jul  3 09:50:50.487: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:50:50.637: INFO: namespace configmap-8002 deletion completed in 22.163902932s

• [SLOW TEST:28.374 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:50:50.639: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul  3 09:51:00.722: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:01.317: INFO: Exec stderr: ""
Jul  3 09:51:01.317: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:01.317: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:02.012: INFO: Exec stderr: ""
Jul  3 09:51:02.012: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:02.012: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:02.625: INFO: Exec stderr: ""
Jul  3 09:51:02.625: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:02.625: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:03.269: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul  3 09:51:03.270: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:03.270: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:03.889: INFO: Exec stderr: ""
Jul  3 09:51:03.889: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:03.889: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:04.472: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul  3 09:51:04.472: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:04.979: INFO: Exec stderr: ""
Jul  3 09:51:04.979: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:04.979: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:05.534: INFO: Exec stderr: ""
Jul  3 09:51:05.534: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:05.534: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:06.157: INFO: Exec stderr: ""
Jul  3 09:51:06.157: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-7853 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 09:51:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 09:51:06.697: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:51:06.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-7853" for this suite.
Jul  3 09:51:48.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:51:48.869: INFO: namespace e2e-kubelet-etc-hosts-7853 deletion completed in 42.167517562s

• [SLOW TEST:58.230 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:51:48.870: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Jul  3 09:51:48.913: INFO: Waiting up to 5m0s for pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5" in namespace "containers-9776" to be "success or failure"
Jul  3 09:51:48.917: INFO: Pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.227213ms
Jul  3 09:51:50.922: INFO: Pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008914372s
Jul  3 09:51:52.926: INFO: Pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013329911s
Jul  3 09:51:54.931: INFO: Pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018006376s
STEP: Saw pod success
Jul  3 09:51:54.931: INFO: Pod "client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:51:54.935: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 09:51:54.969: INFO: Waiting for pod client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:51:54.973: INFO: Pod client-containers-2d0dee38-9d78-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:51:54.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9776" for this suite.
Jul  3 09:52:00.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:52:01.140: INFO: namespace containers-9776 deletion completed in 6.162187489s

• [SLOW TEST:12.270 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:52:01.140: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 09:52:01.173: INFO: Creating deployment "nginx-deployment"
Jul  3 09:52:01.180: INFO: Waiting for observed generation 1
Jul  3 09:52:03.205: INFO: Waiting for all required pods to come up
Jul  3 09:52:03.214: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul  3 09:52:13.288: INFO: Waiting for deployment "nginx-deployment" to complete
Jul  3 09:52:13.296: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul  3 09:52:13.304: INFO: Updating deployment nginx-deployment
Jul  3 09:52:13.304: INFO: Waiting for observed generation 2
Jul  3 09:52:15.313: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul  3 09:52:15.317: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul  3 09:52:15.320: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  3 09:52:15.330: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul  3 09:52:15.330: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul  3 09:52:15.333: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul  3 09:52:15.339: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul  3 09:52:15.339: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul  3 09:52:15.348: INFO: Updating deployment nginx-deployment
Jul  3 09:52:15.348: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul  3 09:52:15.367: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul  3 09:52:17.383: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  3 09:52:17.390: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1148,SelfLink:/apis/apps/v1/namespaces/deployment-1148/deployments/nginx-deployment,UID:345f2fb7-9d78-11e9-9bcb-a6641e9563f8,ResourceVersion:281971,Generation:3,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-07-03 09:52:15 +0000 UTC 2019-07-03 09:52:15 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-03 09:52:15 +0000 UTC 2019-07-03 09:52:01 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul  3 09:52:17.394: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-1148,SelfLink:/apis/apps/v1/namespaces/deployment-1148/replicasets/nginx-deployment-5f9595f595,UID:3b9a1d78-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281962,Generation:3,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 345f2fb7-9d78-11e9-9bcb-a6641e9563f8 0xc0025040d7 0xc0025040d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 09:52:17.394: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul  3 09:52:17.395: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-1148,SelfLink:/apis/apps/v1/namespaces/deployment-1148/replicasets/nginx-deployment-6f478d8d8,UID:3460430c-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281956,Generation:3,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 345f2fb7-9d78-11e9-9bcb-a6641e9563f8 0xc0025041a7 0xc0025041a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul  3 09:52:17.402: INFO: Pod "nginx-deployment-5f9595f595-87sbh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-87sbh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-87sbh,UID:3ba26ec9-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281897,Generation:0,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b96c7 0xc0023b96c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b9730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b9750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:172.25.0.16,StartTime:2019-07-03 09:52:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.402: INFO: Pod "nginx-deployment-5f9595f595-9rrtc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9rrtc,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-9rrtc,UID:3ba133af-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281895,Generation:0,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b9830 0xc0023b9831}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b98a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b98c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.11,StartTime:2019-07-03 09:52:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.402: INFO: Pod "nginx-deployment-5f9595f595-j62cj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j62cj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-j62cj,UID:3b9abc2f-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282028,Generation:0,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b99a0 0xc0023b99a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b9a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b9a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.10,StartTime:2019-07-03 09:52:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.402: INFO: Pod "nginx-deployment-5f9595f595-jw9km" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jw9km,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-jw9km,UID:3cd3e1d0-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281966,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b9b30 0xc0023b9b31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b9bb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b9bd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.403: INFO: Pod "nginx-deployment-5f9595f595-k5j2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k5j2r,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-k5j2r,UID:3cda8770-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282027,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b9ca0 0xc0023b9ca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b9d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b9d40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.403: INFO: Pod "nginx-deployment-5f9595f595-kkl9m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kkl9m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-kkl9m,UID:3b9bedeb-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281893,Generation:0,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b9e20 0xc0023b9e21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023b9ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023b9ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:172.25.0.15,StartTime:2019-07-03 09:52:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.403: INFO: Pod "nginx-deployment-5f9595f595-lp5xn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lp5xn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-lp5xn,UID:3cd74877-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281982,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc0023b9fa0 0xc0023b9fa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c010} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c030}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.403: INFO: Pod "nginx-deployment-5f9595f595-lqf56" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lqf56,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-lqf56,UID:3cd75731-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281990,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c110 0xc002b2c111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c1a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.403: INFO: Pod "nginx-deployment-5f9595f595-sd55r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sd55r,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-sd55r,UID:3cd7177c-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282013,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c270 0xc002b2c271}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c2e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c300}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.404: INFO: Pod "nginx-deployment-5f9595f595-t89bf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-t89bf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-t89bf,UID:3cd5be22-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281980,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c3d0 0xc002b2c3d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.404: INFO: Pod "nginx-deployment-5f9595f595-v8ps7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-v8ps7,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-v8ps7,UID:3cd7380e-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281985,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c530 0xc002b2c531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.404: INFO: Pod "nginx-deployment-5f9595f595-wm7wp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wm7wp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-wm7wp,UID:3b9bde21-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281899,Generation:0,CreationTimestamp:2019-07-03 09:52:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c690 0xc002b2c691}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c700} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c720}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:13 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.18,StartTime:2019-07-03 09:52:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.404: INFO: Pod "nginx-deployment-5f9595f595-wv6nn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wv6nn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-5f9595f595-wv6nn,UID:3cd59fab-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281975,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 3b9a1d78-9d78-11e9-9413-46c858ac61fd 0xc002b2c800 0xc002b2c801}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.404: INFO: Pod "nginx-deployment-6f478d8d8-66sxt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-66sxt,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-66sxt,UID:3cd66428-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282029,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2c960 0xc002b2c961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2c9c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2c9e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.12,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-6klrj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6klrj,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-6klrj,UID:3463517a-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281792,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2cab0 0xc002b2cab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2cb10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2cb30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.8,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://34bb0ee4d63baa40b287e32942009842ac43cd786f42f7350a9314c610be5469}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-6x9nb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6x9nb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-6x9nb,UID:3cd81fe8-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282015,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2cc40 0xc002b2cc41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2ccd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2cd30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-8z8hp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-8z8hp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-8z8hp,UID:3cd7c805-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281986,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2cf77 0xc002b2cf78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2d0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2d0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-bs8vw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bs8vw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-bs8vw,UID:3cd7f551-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282014,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2d1b7 0xc002b2d1b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2d220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2d240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-d4wjg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-d4wjg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-d4wjg,UID:3cd6724a-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281979,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2d377 0xc002b2d378}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2d3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2d400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.405: INFO: Pod "nginx-deployment-6f478d8d8-ghbkg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ghbkg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-ghbkg,UID:34646a77-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281807,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2d6a7 0xc002b2d6a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2d7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2d7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.16,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d0f0b6d81d6a425c4be2ef44e51143fbe94820e373356b5e5dc55463545a419e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.406: INFO: Pod "nginx-deployment-6f478d8d8-gl67h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gl67h,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-gl67h,UID:34667725-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281784,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2d9b0 0xc002b2d9b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2db40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2db80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:172.25.0.12,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://af5682d6db4087d697aabadbb856341ce68585117c77524d4f6cc88b66fa7bac}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.406: INFO: Pod "nginx-deployment-6f478d8d8-h5vwm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h5vwm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-h5vwm,UID:3cd7de8f-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282026,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2dca0 0xc002b2dca1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b2dde0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b2de00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.406: INFO: Pod "nginx-deployment-6f478d8d8-jx2bc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-jx2bc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-jx2bc,UID:346273e0-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281756,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002b2df97 0xc002b2df98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002378080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023780a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.15,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0f14805e6632d5e936cdca3f73aa852b698e2adfcb8e86e18dfe55d228120312}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.406: INFO: Pod "nginx-deployment-6f478d8d8-l4klv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l4klv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-l4klv,UID:3cd3dae1-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281969,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc0023782c0 0xc0023782c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002378390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002378400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.406: INFO: Pod "nginx-deployment-6f478d8d8-p7jvg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-p7jvg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-p7jvg,UID:34666e98-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281813,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002378587 0xc002378588}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002378670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002378690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.9,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:09 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f77c583e7045966136e338c4950eb9e2d9d0e4574a0b75dcdf51c6c8c2510160}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.407: INFO: Pod "nginx-deployment-6f478d8d8-rzdjg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rzdjg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-rzdjg,UID:3cd62c75-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281989,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc0023788f0 0xc0023788f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023789e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002378a10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.411: INFO: Pod "nginx-deployment-6f478d8d8-t6css" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t6css,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-t6css,UID:3cd3a8c0-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282036,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002378b27 0xc002378b28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002378cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002378cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.19,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.411: INFO: Pod "nginx-deployment-6f478d8d8-tqfs6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tqfs6,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-tqfs6,UID:34647ea0-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281760,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002378e10 0xc002378e11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002378ee0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002378f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:172.25.0.11,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4de5e6d6b4c2d140d062bb1f45b7c384d0b17067a2e6cc7f5eeec40fc8acc40e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.411: INFO: Pod "nginx-deployment-6f478d8d8-vksh5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vksh5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-vksh5,UID:34666fc1-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281803,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002378fd0 0xc002378fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002379030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002379050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:09 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.17,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:08 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b03d1126b8f5a8958d77f39d4f81f71e2bc2339e73a00e71a3c1f511b3123a99}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.411: INFO: Pod "nginx-deployment-6f478d8d8-wdw2f" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wdw2f,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-wdw2f,UID:34647c43-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281765,Generation:0,CreationTimestamp:2019-07-03 09:52:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002379120 0xc002379121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002379180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023791a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:01 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.7,StartTime:2019-07-03 09:52:01 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 09:52:06 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3ffcc30956b0f8deccb688d85063838f258364ab2060cc29bcdadc14e0db4618}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.412: INFO: Pod "nginx-deployment-6f478d8d8-xjc29" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xjc29,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-xjc29,UID:3cd669bc-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281993,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002379270 0xc002379271}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0023792d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0023792f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.412: INFO: Pod "nginx-deployment-6f478d8d8-z48pn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z48pn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-z48pn,UID:3cd2dcda-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282035,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc0023793b7 0xc0023793b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-23-251.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002379420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002379440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.23.251,PodIP:172.25.0.17,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 09:52:17.412: INFO: Pod "nginx-deployment-6f478d8d8-zz7rr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zz7rr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-1148,SelfLink:/api/v1/namespaces/deployment-1148/pods/nginx-deployment-6f478d8d8-zz7rr,UID:3cd81fad-9d78-11e9-9413-46c858ac61fd,ResourceVersion:281995,Generation:0,CreationTimestamp:2019-07-03 09:52:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 3460430c-9d78-11e9-9413-46c858ac61fd 0xc002379510 0xc002379511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nknmn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nknmn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nknmn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002379570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002379590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:52:15 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:,StartTime:2019-07-03 09:52:15 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:52:17.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1148" for this suite.
Jul  3 09:52:25.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:52:25.591: INFO: namespace deployment-1148 deletion completed in 8.173222128s

• [SLOW TEST:24.451 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:52:25.592: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  3 09:52:30.176: INFO: Successfully updated pod "annotationupdate42f143fe-9d78-11e9-9a35-2ab8d9f547c5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:52:32.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2962" for this suite.
Jul  3 09:52:54.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:52:54.362: INFO: namespace projected-2962 deletion completed in 22.157780163s

• [SLOW TEST:28.770 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:52:54.363: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  3 09:52:54.404: INFO: Waiting up to 5m0s for pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-6161" to be "success or failure"
Jul  3 09:52:54.410: INFO: Pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.411166ms
Jul  3 09:52:56.414: INFO: Pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010009704s
Jul  3 09:52:58.419: INFO: Pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014498078s
Jul  3 09:53:00.423: INFO: Pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01880825s
STEP: Saw pod success
Jul  3 09:53:00.423: INFO: Pod "downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:53:00.426: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 09:53:00.450: INFO: Waiting for pod downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:53:00.453: INFO: Pod downward-api-541707aa-9d78-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:53:00.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6161" for this suite.
Jul  3 09:53:06.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:53:06.620: INFO: namespace downward-api-6161 deletion completed in 6.16312064s

• [SLOW TEST:12.258 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:53:06.621: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 09:53:06.654: INFO: Creating deployment "test-recreate-deployment"
Jul  3 09:53:06.662: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul  3 09:53:06.674: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul  3 09:53:08.682: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul  3 09:53:08.685: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697744386, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697744386, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697744386, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697744386, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 09:53:10.690: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul  3 09:53:10.698: INFO: Updating deployment test-recreate-deployment
Jul  3 09:53:10.698: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  3 09:53:10.759: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3753,SelfLink:/apis/apps/v1/namespaces/deployment-3753/deployments/test-recreate-deployment,UID:5b66b1e1-9d78-11e9-9bcb-a6641e9563f8,ResourceVersion:282588,Generation:2,CreationTimestamp:2019-07-03 09:53:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-03 09:53:10 +0000 UTC 2019-07-03 09:53:10 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-03 09:53:10 +0000 UTC 2019-07-03 09:53:06 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul  3 09:53:10.763: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3753,SelfLink:/apis/apps/v1/namespaces/deployment-3753/replicasets/test-recreate-deployment-c9cbd8684,UID:5dd3b4db-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282585,Generation:1,CreationTimestamp:2019-07-03 09:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5b66b1e1-9d78-11e9-9bcb-a6641e9563f8 0xc000d1f0e0 0xc000d1f0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 09:53:10.763: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul  3 09:53:10.763: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3753,SelfLink:/apis/apps/v1/namespaces/deployment-3753/replicasets/test-recreate-deployment-7d57d5ff7c,UID:5b67c11e-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282576,Generation:2,CreationTimestamp:2019-07-03 09:53:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5b66b1e1-9d78-11e9-9bcb-a6641e9563f8 0xc000d1f017 0xc000d1f018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 09:53:10.767: INFO: Pod "test-recreate-deployment-c9cbd8684-mz2ql" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-mz2ql,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3753,SelfLink:/api/v1/namespaces/deployment-3753/pods/test-recreate-deployment-c9cbd8684-mz2ql,UID:5dd46373-9d78-11e9-9413-46c858ac61fd,ResourceVersion:282589,Generation:0,CreationTimestamp:2019-07-03 09:53:10 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 5dd3b4db-9d78-11e9-9413-46c858ac61fd 0xc000d1f920 0xc000d1f921}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cg6vd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cg6vd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-cg6vd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d1f980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d1f9a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:53:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:53:10 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 09:53:10 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:,StartTime:2019-07-03 09:53:10 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:53:10.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3753" for this suite.
Jul  3 09:53:16.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:53:16.926: INFO: namespace deployment-3753 deletion completed in 6.154545814s

• [SLOW TEST:10.305 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:53:16.926: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-618ac429-9d78-11e9-9a35-2ab8d9f547c5
STEP: Creating configMap with name cm-test-opt-upd-618ac461-9d78-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-618ac429-9d78-11e9-9a35-2ab8d9f547c5
STEP: Updating configmap cm-test-opt-upd-618ac461-9d78-11e9-9a35-2ab8d9f547c5
STEP: Creating configMap with name cm-test-opt-create-618ac47c-9d78-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:54:48.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7422" for this suite.
Jul  3 09:55:10.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:55:10.530: INFO: namespace configmap-7422 deletion completed in 22.165102099s

• [SLOW TEST:113.604 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:55:10.531: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3688
Jul  3 09:55:14.585: INFO: Started pod liveness-exec in namespace container-probe-3688
STEP: checking the pod's current state and verifying that restartCount is present
Jul  3 09:55:14.588: INFO: Initial restart count of pod liveness-exec is 0
Jul  3 09:56:02.700: INFO: Restart count of pod container-probe-3688/liveness-exec is now 1 (48.111705878s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:56:02.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3688" for this suite.
Jul  3 09:56:08.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:56:08.915: INFO: namespace container-probe-3688 deletion completed in 6.200439273s

• [SLOW TEST:58.385 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:56:08.916: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Jul  3 09:56:08.958: INFO: Waiting up to 5m0s for pod "pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-9442" to be "success or failure"
Jul  3 09:56:08.962: INFO: Pod "pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.932494ms
Jul  3 09:56:10.967: INFO: Pod "pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008058273s
Jul  3 09:56:12.971: INFO: Pod "pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012305895s
STEP: Saw pod success
Jul  3 09:56:12.971: INFO: Pod "pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:56:12.974: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 09:56:13.001: INFO: Waiting for pod pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:56:13.004: INFO: Pod pod-c80dc7b1-9d78-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:56:13.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9442" for this suite.
Jul  3 09:56:19.022: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:56:19.173: INFO: namespace emptydir-9442 deletion completed in 6.16512452s

• [SLOW TEST:10.258 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:56:19.174: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  3 09:56:27.365: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:27.369: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:29.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:29.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:31.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:31.374: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:33.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:33.374: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:35.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:35.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:37.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:37.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:39.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:39.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:41.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:41.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:43.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:43.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:45.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:45.373: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:47.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:47.374: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:49.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:49.374: INFO: Pod pod-with-poststart-exec-hook still exists
Jul  3 09:56:51.369: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul  3 09:56:51.374: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:56:51.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3457" for this suite.
Jul  3 09:57:13.391: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:57:13.540: INFO: namespace container-lifecycle-hook-3457 deletion completed in 22.161669354s

• [SLOW TEST:54.366 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:57:13.540: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 09:57:13.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 version --client'
Jul  3 09:57:13.619: INFO: stderr: ""
Jul  3 09:57:13.619: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Jul  3 09:57:13.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-9992'
Jul  3 09:57:13.837: INFO: stderr: ""
Jul  3 09:57:13.837: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul  3 09:57:13.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-9992'
Jul  3 09:57:14.062: INFO: stderr: ""
Jul  3 09:57:14.062: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  3 09:57:15.066: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:57:15.066: INFO: Found 0 / 1
Jul  3 09:57:16.066: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:57:16.066: INFO: Found 0 / 1
Jul  3 09:57:17.066: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:57:17.066: INFO: Found 1 / 1
Jul  3 09:57:17.066: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  3 09:57:17.069: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:57:17.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  3 09:57:17.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 describe pod redis-master-4xvvg --namespace=kubectl-9992'
Jul  3 09:57:17.151: INFO: stderr: ""
Jul  3 09:57:17.152: INFO: stdout: "Name:               redis-master-4xvvg\nNamespace:          kubectl-9992\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-172-31-22-121.eu-central-1.compute.internal/172.31.22.121\nStart Time:         Wed, 03 Jul 2019 09:57:13 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 172.25.1.33\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://4de9c0e1cd90f9710aadb830fd4a5884462712e60093301ddec3c93f1abd95cf\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 03 Jul 2019 09:57:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-gwvc9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-gwvc9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-gwvc9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                     Message\n  ----    ------     ----  ----                                                     -------\n  Normal  Scheduled  4s    default-scheduler                                        Successfully assigned kubectl-9992/redis-master-4xvvg to ip-172-31-22-121.eu-central-1.compute.internal\n  Normal  Pulled     2s    kubelet, ip-172-31-22-121.eu-central-1.compute.internal  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, ip-172-31-22-121.eu-central-1.compute.internal  Created container redis-master\n  Normal  Started    2s    kubelet, ip-172-31-22-121.eu-central-1.compute.internal  Started container redis-master\n"
Jul  3 09:57:17.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 describe rc redis-master --namespace=kubectl-9992'
Jul  3 09:57:17.239: INFO: stderr: ""
Jul  3 09:57:17.239: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9992\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-4xvvg\n"
Jul  3 09:57:17.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 describe service redis-master --namespace=kubectl-9992'
Jul  3 09:57:17.317: INFO: stderr: ""
Jul  3 09:57:17.317: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9992\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.10.21\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.25.1.33:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul  3 09:57:17.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 describe node ip-172-31-22-121.eu-central-1.compute.internal'
Jul  3 09:57:17.415: INFO: stderr: ""
Jul  3 09:57:17.415: INFO: stdout: "Name:               ip-172-31-22-121.eu-central-1.compute.internal\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=t3.small\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-central-1\n                    failure-domain.beta.kubernetes.io/zone=eu-central-1a\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-22-121\n                    kubernetes.io/os=linux\n                    machine-controller/owned-by=6745cd13-9ca9-11e9-9a90-c2fd0ba78760\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"a6:8c:fb:64:3f:8e\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.31.22.121\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 02 Jul 2019 09:13:52 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 03 Jul 2019 09:56:34 +0000   Tue, 02 Jul 2019 09:13:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 03 Jul 2019 09:56:34 +0000   Tue, 02 Jul 2019 09:13:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 03 Jul 2019 09:56:34 +0000   Tue, 02 Jul 2019 09:13:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 03 Jul 2019 09:56:34 +0000   Tue, 02 Jul 2019 09:14:22 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:   172.31.22.121\n  ExternalIP:   3.120.176.228\n  InternalDNS:  ip-172-31-22-121.eu-central-1.compute.internal\n  Hostname:     ip-172-31-22-121.eu-central-1.compute.internal\n  ExternalDNS:  ec2-3-120-176-228.eu-central-1.compute.amazonaws.com\nCapacity:\n attachable-volumes-aws-ebs:  25\n cpu:                         2\n ephemeral-storage:           25346000Ki\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      2002696Ki\n pods:                        110\nAllocatable:\n attachable-volumes-aws-ebs:  25\n cpu:                         1800m\n ephemeral-storage:           21211389914\n hugepages-1Gi:               0\n hugepages-2Mi:               0\n memory:                      1695496Ki\n pods:                        110\nSystem Info:\n Machine ID:                 ec253b9a22c7ac6367a2fa5af1904d22\n System UUID:                EC253B9A-22C7-AC63-67A2-FA5AF1904D22\n Boot ID:                    38aa5408-a02a-4afe-a964-014a3f3449fe\n Kernel Version:             4.15.0-1043-aws\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.2\n Kubelet Version:            v1.14.3\n Kube-Proxy Version:         v1.14.3\nPodCIDR:                     172.25.1.0/24\nProviderID:                  aws:///eu-central-1a/i-04866f12a96aa83ae\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m15s\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-275sw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m8s\n  kube-system                canal-7p6kq                                                350m (19%)    100m (5%)   50Mi (3%)        50Mi (3%)      24h\n  kube-system                coredns-6bd858f7c-4g2hs                                    100m (5%)     0 (0%)      70Mi (4%)        170Mi (10%)    24h\n  kube-system                coredns-6bd858f7c-6f8lz                                    100m (5%)     0 (0%)      70Mi (4%)        170Mi (10%)    24h\n  kube-system                kube-proxy-6k2j2                                           75m (4%)      250m (13%)  50Mi (3%)        250Mi (15%)    24h\n  kube-system                node-exporter-bxmkq                                        3m (0%)       200m (11%)  16Mi (0%)        50Mi (3%)      24h\n  kubectl-9992               redis-master-4xvvg                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         4s\n  velero                     restic-tx9l2                                               5m (0%)       100m (5%)   32Mi (1%)        300Mi (18%)    24h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                    Requests     Limits\n  --------                    --------     ------\n  cpu                         633m (35%)   650m (36%)\n  memory                      288Mi (17%)  990Mi (59%)\n  ephemeral-storage           0 (0%)       0 (0%)\n  attachable-volumes-aws-ebs  0            0\nEvents:                       <none>\n"
Jul  3 09:57:17.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 describe namespace kubectl-9992'
Jul  3 09:57:17.490: INFO: stderr: ""
Jul  3 09:57:17.490: INFO: stdout: "Name:         kubectl-9992\nLabels:       e2e-framework=kubectl\n              e2e-run=b4e7e6a3-9d77-11e9-9a35-2ab8d9f547c5\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:57:17.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9992" for this suite.
Jul  3 09:57:39.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:57:39.653: INFO: namespace kubectl-9992 deletion completed in 22.158080913s

• [SLOW TEST:26.113 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:57:39.653: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul  3 09:57:39.696: INFO: Pod name pod-release: Found 0 pods out of 1
Jul  3 09:57:44.700: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:57:45.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8564" for this suite.
Jul  3 09:57:51.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:57:51.886: INFO: namespace replication-controller-8564 deletion completed in 6.161530945s

• [SLOW TEST:12.234 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:57:51.888: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 09:57:51.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5972'
Jul  3 09:57:52.019: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  3 09:57:52.019: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Jul  3 09:57:54.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5972'
Jul  3 09:57:54.104: INFO: stderr: ""
Jul  3 09:57:54.104: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:57:54.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5972" for this suite.
Jul  3 09:58:00.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:58:00.271: INFO: namespace kubectl-5972 deletion completed in 6.161908145s

• [SLOW TEST:8.383 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:58:00.271: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8007
Jul  3 09:58:04.323: INFO: Started pod liveness-http in namespace container-probe-8007
STEP: checking the pod's current state and verifying that restartCount is present
Jul  3 09:58:04.326: INFO: Initial restart count of pod liveness-http is 0
Jul  3 09:58:20.364: INFO: Restart count of pod container-probe-8007/liveness-http is now 1 (16.037482231s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:58:20.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8007" for this suite.
Jul  3 09:58:26.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:58:26.548: INFO: namespace container-probe-8007 deletion completed in 6.167940796s

• [SLOW TEST:26.277 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:58:26.548: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 09:58:26.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-7384" to be "success or failure"
Jul  3 09:58:26.595: INFO: Pod "downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.475089ms
Jul  3 09:58:28.600: INFO: Pod "downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009634022s
Jul  3 09:58:30.605: INFO: Pod "downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014596331s
STEP: Saw pod success
Jul  3 09:58:30.605: INFO: Pod "downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:58:30.609: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 09:58:30.631: INFO: Waiting for pod downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:58:30.635: INFO: Pod downwardapi-volume-1a16b5ec-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:58:30.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7384" for this suite.
Jul  3 09:58:36.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:58:36.798: INFO: namespace downward-api-7384 deletion completed in 6.159336146s

• [SLOW TEST:10.250 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:58:36.799: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul  3 09:58:36.831: INFO: namespace kubectl-783
Jul  3 09:58:36.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-783'
Jul  3 09:58:37.191: INFO: stderr: ""
Jul  3 09:58:37.191: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  3 09:58:38.196: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:58:38.196: INFO: Found 0 / 1
Jul  3 09:58:39.195: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:58:39.195: INFO: Found 1 / 1
Jul  3 09:58:39.195: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  3 09:58:39.199: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 09:58:39.199: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  3 09:58:39.199: INFO: wait on redis-master startup in kubectl-783 
Jul  3 09:58:39.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 logs redis-master-z4v5d redis-master --namespace=kubectl-783'
Jul  3 09:58:39.499: INFO: stderr: ""
Jul  3 09:58:39.499: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Jul 09:58:38.573 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Jul 09:58:38.573 # Server started, Redis version 3.2.12\n1:M 03 Jul 09:58:38.573 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Jul 09:58:38.573 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul  3 09:58:39.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-783'
Jul  3 09:58:39.758: INFO: stderr: ""
Jul  3 09:58:39.758: INFO: stdout: "service/rm2 exposed\n"
Jul  3 09:58:39.767: INFO: Service rm2 in namespace kubectl-783 found.
STEP: exposing service
Jul  3 09:58:41.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-783'
Jul  3 09:58:41.861: INFO: stderr: ""
Jul  3 09:58:41.861: INFO: stdout: "service/rm3 exposed\n"
Jul  3 09:58:41.864: INFO: Service rm3 in namespace kubectl-783 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:58:43.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-783" for this suite.
Jul  3 09:59:05.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:59:06.035: INFO: namespace kubectl-783 deletion completed in 22.159506689s

• [SLOW TEST:29.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:59:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 09:59:06.078: INFO: Waiting up to 5m0s for pod "downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5" in namespace "projected-8476" to be "success or failure"
Jul  3 09:59:06.083: INFO: Pod "downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.367489ms
Jul  3 09:59:08.087: INFO: Pod "downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008572531s
Jul  3 09:59:10.097: INFO: Pod "downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018594766s
STEP: Saw pod success
Jul  3 09:59:10.097: INFO: Pod "downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:59:10.101: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 09:59:10.125: INFO: Waiting for pod downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:59:10.128: INFO: Pod downwardapi-volume-319fe72c-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:59:10.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8476" for this suite.
Jul  3 09:59:16.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:59:16.289: INFO: namespace projected-8476 deletion completed in 6.156270468s

• [SLOW TEST:10.254 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:59:16.290: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  3 09:59:22.373: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  3 09:59:22.377: INFO: Pod pod-with-prestop-http-hook still exists
Jul  3 09:59:24.377: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  3 09:59:24.382: INFO: Pod pod-with-prestop-http-hook still exists
Jul  3 09:59:26.377: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul  3 09:59:26.382: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:59:26.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9726" for this suite.
Jul  3 09:59:48.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:59:48.562: INFO: namespace container-lifecycle-hook-9726 deletion completed in 22.165015781s

• [SLOW TEST:32.272 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:59:48.562: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-4af95c87-9d79-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 09:59:48.612: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5" in namespace "projected-6998" to be "success or failure"
Jul  3 09:59:48.617: INFO: Pod "pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.550287ms
Jul  3 09:59:50.621: INFO: Pod "pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009010411s
Jul  3 09:59:52.626: INFO: Pod "pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014243459s
STEP: Saw pod success
Jul  3 09:59:52.626: INFO: Pod "pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 09:59:52.630: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  3 09:59:52.657: INFO: Waiting for pod pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 09:59:52.661: INFO: Pod pod-projected-secrets-4afa67ef-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 09:59:52.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6998" for this suite.
Jul  3 09:59:58.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 09:59:58.887: INFO: namespace projected-6998 deletion completed in 6.222240562s

• [SLOW TEST:10.325 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 09:59:58.888: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  3 09:59:58.940: INFO: Waiting up to 5m0s for pod "pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-8221" to be "success or failure"
Jul  3 09:59:58.944: INFO: Pod "pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.269261ms
Jul  3 10:00:00.948: INFO: Pod "pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008410395s
STEP: Saw pod success
Jul  3 10:00:00.948: INFO: Pod "pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:00:00.952: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:00:00.974: INFO: Waiting for pod pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:00:00.978: INFO: Pod pod-5122115e-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:00:00.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8221" for this suite.
Jul  3 10:00:06.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:00:07.147: INFO: namespace emptydir-8221 deletion completed in 6.165268777s

• [SLOW TEST:8.259 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:00:07.148: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-c9bs
STEP: Creating a pod to test atomic-volume-subpath
Jul  3 10:00:07.198: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-c9bs" in namespace "subpath-7183" to be "success or failure"
Jul  3 10:00:07.204: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Pending", Reason="", readiness=false. Elapsed: 5.547672ms
Jul  3 10:00:09.208: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010093166s
Jul  3 10:00:11.213: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 4.014566611s
Jul  3 10:00:13.217: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 6.019099768s
Jul  3 10:00:15.222: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 8.023906371s
Jul  3 10:00:17.226: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 10.028239298s
Jul  3 10:00:19.231: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 12.032621846s
Jul  3 10:00:21.235: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 14.037055627s
Jul  3 10:00:23.240: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 16.04187074s
Jul  3 10:00:25.244: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 18.045953801s
Jul  3 10:00:27.248: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 20.050420662s
Jul  3 10:00:29.253: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Running", Reason="", readiness=true. Elapsed: 22.0546005s
Jul  3 10:00:31.257: INFO: Pod "pod-subpath-test-secret-c9bs": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.059219759s
STEP: Saw pod success
Jul  3 10:00:31.257: INFO: Pod "pod-subpath-test-secret-c9bs" satisfied condition "success or failure"
Jul  3 10:00:31.261: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-subpath-test-secret-c9bs container test-container-subpath-secret-c9bs: <nil>
STEP: delete the pod
Jul  3 10:00:31.283: INFO: Waiting for pod pod-subpath-test-secret-c9bs to disappear
Jul  3 10:00:31.286: INFO: Pod pod-subpath-test-secret-c9bs no longer exists
STEP: Deleting pod pod-subpath-test-secret-c9bs
Jul  3 10:00:31.286: INFO: Deleting pod "pod-subpath-test-secret-c9bs" in namespace "subpath-7183"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:00:31.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7183" for this suite.
Jul  3 10:00:37.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:00:37.716: INFO: namespace subpath-7183 deletion completed in 6.422499345s

• [SLOW TEST:30.569 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:00:37.717: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6849ef51-9d79-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:00:37.793: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5" in namespace "projected-6595" to be "success or failure"
Jul  3 10:00:37.802: INFO: Pod "pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 9.045098ms
Jul  3 10:00:39.807: INFO: Pod "pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013685154s
Jul  3 10:00:41.812: INFO: Pod "pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018343489s
STEP: Saw pod success
Jul  3 10:00:41.812: INFO: Pod "pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:00:41.815: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:00:41.837: INFO: Waiting for pod pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:00:41.840: INFO: Pod pod-projected-configmaps-684aeb93-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:00:41.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6595" for this suite.
Jul  3 10:00:47.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:00:48.005: INFO: namespace projected-6595 deletion completed in 6.159740257s

• [SLOW TEST:10.288 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:00:48.005: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-6e67874b-9d79-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:00:48.053: INFO: Waiting up to 5m0s for pod "pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5" in namespace "secrets-1605" to be "success or failure"
Jul  3 10:00:48.058: INFO: Pod "pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.106359ms
Jul  3 10:00:50.062: INFO: Pod "pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009577269s
Jul  3 10:00:52.067: INFO: Pod "pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013923841s
STEP: Saw pod success
Jul  3 10:00:52.067: INFO: Pod "pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:00:52.070: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:00:52.092: INFO: Waiting for pod pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:00:52.095: INFO: Pod pod-secrets-6e686640-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:00:52.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1605" for this suite.
Jul  3 10:00:58.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:00:58.259: INFO: namespace secrets-1605 deletion completed in 6.160391551s

• [SLOW TEST:10.254 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:00:58.259: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 10:00:58.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2454'
Jul  3 10:00:58.371: INFO: stderr: ""
Jul  3 10:00:58.371: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul  3 10:01:03.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pod e2e-test-nginx-pod --namespace=kubectl-2454 -o json'
Jul  3 10:01:03.488: INFO: stderr: ""
Jul  3 10:01:03.488: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-03T10:00:58Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2454\",\n        \"resourceVersion\": \"284716\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2454/pods/e2e-test-nginx-pod\",\n        \"uid\": \"748e19d9-9d79-11e9-9bcb-a6641e9563f8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-vxcw8\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-22-121.eu-central-1.compute.internal\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-vxcw8\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-vxcw8\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-03T10:00:58Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-03T10:01:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-03T10:01:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-03T10:00:58Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://f7927c19c17a82576016e939c8ce9c6c797dd5319d4ba7b83c4c6e87b41840d9\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-03T10:00:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.22.121\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.25.1.46\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-03T10:00:58Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul  3 10:01:03.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 replace -f - --namespace=kubectl-2454'
Jul  3 10:01:04.004: INFO: stderr: ""
Jul  3 10:01:04.004: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Jul  3 10:01:04.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete pods e2e-test-nginx-pod --namespace=kubectl-2454'
Jul  3 10:01:12.645: INFO: stderr: ""
Jul  3 10:01:12.645: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:01:12.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2454" for this suite.
Jul  3 10:01:18.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:01:18.885: INFO: namespace kubectl-2454 deletion completed in 6.234621388s

• [SLOW TEST:20.625 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:01:18.885: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul  3 10:01:18.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-925'
Jul  3 10:01:19.157: INFO: stderr: ""
Jul  3 10:01:19.157: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 10:01:19.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-925'
Jul  3 10:01:19.234: INFO: stderr: ""
Jul  3 10:01:19.234: INFO: stdout: "update-demo-nautilus-259ms update-demo-nautilus-87wbj "
Jul  3 10:01:19.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-259ms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-925'
Jul  3 10:01:19.303: INFO: stderr: ""
Jul  3 10:01:19.303: INFO: stdout: ""
Jul  3 10:01:19.304: INFO: update-demo-nautilus-259ms is created but not running
Jul  3 10:01:24.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-925'
Jul  3 10:01:24.373: INFO: stderr: ""
Jul  3 10:01:24.373: INFO: stdout: "update-demo-nautilus-259ms update-demo-nautilus-87wbj "
Jul  3 10:01:24.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-259ms -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-925'
Jul  3 10:01:24.438: INFO: stderr: ""
Jul  3 10:01:24.438: INFO: stdout: "true"
Jul  3 10:01:24.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-259ms -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-925'
Jul  3 10:01:24.502: INFO: stderr: ""
Jul  3 10:01:24.502: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 10:01:24.502: INFO: validating pod update-demo-nautilus-259ms
Jul  3 10:01:24.605: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 10:01:24.605: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 10:01:24.605: INFO: update-demo-nautilus-259ms is verified up and running
Jul  3 10:01:24.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-87wbj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-925'
Jul  3 10:01:24.670: INFO: stderr: ""
Jul  3 10:01:24.670: INFO: stdout: "true"
Jul  3 10:01:24.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-87wbj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-925'
Jul  3 10:01:24.736: INFO: stderr: ""
Jul  3 10:01:24.736: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 10:01:24.736: INFO: validating pod update-demo-nautilus-87wbj
Jul  3 10:01:24.828: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 10:01:24.828: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 10:01:24.828: INFO: update-demo-nautilus-87wbj is verified up and running
STEP: using delete to clean up resources
Jul  3 10:01:24.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-925'
Jul  3 10:01:24.900: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 10:01:24.900: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  3 10:01:24.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-925'
Jul  3 10:01:24.972: INFO: stderr: "No resources found.\n"
Jul  3 10:01:24.972: INFO: stdout: ""
Jul  3 10:01:24.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=update-demo --namespace=kubectl-925 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 10:01:25.042: INFO: stderr: ""
Jul  3 10:01:25.042: INFO: stdout: "update-demo-nautilus-259ms\nupdate-demo-nautilus-87wbj\n"
Jul  3 10:01:25.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-925'
Jul  3 10:01:25.644: INFO: stderr: "No resources found.\n"
Jul  3 10:01:25.645: INFO: stdout: ""
Jul  3 10:01:25.645: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=update-demo --namespace=kubectl-925 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 10:01:25.756: INFO: stderr: ""
Jul  3 10:01:25.756: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:01:25.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-925" for this suite.
Jul  3 10:01:31.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:01:31.929: INFO: namespace kubectl-925 deletion completed in 6.163944423s

• [SLOW TEST:13.044 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:01:31.930: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:02:31.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5788" for this suite.
Jul  3 10:02:54.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:02:54.162: INFO: namespace container-probe-5788 deletion completed in 22.17430284s

• [SLOW TEST:82.232 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:02:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:02:54.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-9737" to be "success or failure"
Jul  3 10:02:54.211: INFO: Pod "downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.852432ms
Jul  3 10:02:56.215: INFO: Pod "downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010113327s
Jul  3 10:02:58.220: INFO: Pod "downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01462389s
STEP: Saw pod success
Jul  3 10:02:58.220: INFO: Pod "downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:02:58.223: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:02:58.284: INFO: Waiting for pod downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:02:58.287: INFO: Pod downwardapi-volume-b9996a1e-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:02:58.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9737" for this suite.
Jul  3 10:03:04.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:03:04.458: INFO: namespace downward-api-9737 deletion completed in 6.167238508s

• [SLOW TEST:10.296 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:03:04.459: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Jul  3 10:03:04.502: INFO: Waiting up to 5m0s for pod "var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5" in namespace "var-expansion-9057" to be "success or failure"
Jul  3 10:03:04.508: INFO: Pod "var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.319462ms
Jul  3 10:03:06.512: INFO: Pod "var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009456315s
STEP: Saw pod success
Jul  3 10:03:06.512: INFO: Pod "var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:03:06.515: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 10:03:06.537: INFO: Waiting for pod var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:03:06.541: INFO: Pod var-expansion-bfbca3c9-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:03:06.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9057" for this suite.
Jul  3 10:03:12.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:03:12.707: INFO: namespace var-expansion-9057 deletion completed in 6.161901199s

• [SLOW TEST:8.249 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:03:12.708: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c4a76dc9-9d79-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:03:12.757: INFO: Waiting up to 5m0s for pod "pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5" in namespace "secrets-2709" to be "success or failure"
Jul  3 10:03:12.761: INFO: Pod "pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.174327ms
Jul  3 10:03:14.765: INFO: Pod "pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008547152s
STEP: Saw pod success
Jul  3 10:03:14.766: INFO: Pod "pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:03:14.769: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:03:14.791: INFO: Waiting for pod pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:03:14.795: INFO: Pod pod-secrets-c4a86681-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:03:14.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2709" for this suite.
Jul  3 10:03:20.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:03:20.957: INFO: namespace secrets-2709 deletion completed in 6.157664961s

• [SLOW TEST:8.249 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:03:20.957: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-c9920654-9d79-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:03:21.004: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5" in namespace "configmap-2388" to be "success or failure"
Jul  3 10:03:21.009: INFO: Pod "pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.903606ms
Jul  3 10:03:23.013: INFO: Pod "pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008940591s
STEP: Saw pod success
Jul  3 10:03:23.013: INFO: Pod "pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:03:23.016: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:03:23.080: INFO: Waiting for pod pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:03:23.083: INFO: Pod pod-configmaps-c9931e95-9d79-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:03:23.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2388" for this suite.
Jul  3 10:03:29.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:03:29.244: INFO: namespace configmap-2388 deletion completed in 6.155222126s

• [SLOW TEST:8.286 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:03:29.244: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:03:33.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5798" for this suite.
Jul  3 10:03:39.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:03:39.455: INFO: namespace kubelet-test-5798 deletion completed in 6.155035887s

• [SLOW TEST:10.211 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:03:39.455: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  3 10:03:39.489: INFO: PodSpec: initContainers in spec.initContainers
Jul  3 10:04:23.377: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-d498be2a-9d79-11e9-9a35-2ab8d9f547c5", GenerateName:"", Namespace:"init-container-9715", SelfLink:"/api/v1/namespaces/init-container-9715/pods/pod-init-d498be2a-9d79-11e9-9a35-2ab8d9f547c5", UID:"d49978ad-9d79-11e9-9bcb-a6641e9563f8", ResourceVersion:"285632", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697745019, loc:(*time.Location)(0x8a1a0e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"489441930"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-h7nwl", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00249f180), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h7nwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h7nwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-h7nwl", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002a5ad08), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-22-121.eu-central-1.compute.internal", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009cf560), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a5ad80)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a5ada0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002a5ada8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002a5adac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697745019, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697745019, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697745019, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697745019, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.22.121", PodIP:"172.25.1.54", StartTime:(*v1.Time)(0xc002907940), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002886e70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002886ee0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://178c982546d8846a09b7c377bc38cb5d507ce4f8923221478cd3309723ef8114"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002907980), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002907960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:04:23.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9715" for this suite.
Jul  3 10:04:45.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:04:45.546: INFO: namespace init-container-9715 deletion completed in 22.162768818s

• [SLOW TEST:66.091 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:04:45.546: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  3 10:04:45.581: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:04:49.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5823" for this suite.
Jul  3 10:05:11.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:05:12.074: INFO: namespace init-container-5823 deletion completed in 22.169811469s

• [SLOW TEST:26.528 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:05:12.075: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:05:12.132: INFO: Create a RollingUpdate DaemonSet
Jul  3 10:05:12.139: INFO: Check that daemon pods launch on every node of the cluster
Jul  3 10:05:12.148: INFO: Number of nodes with available pods: 0
Jul  3 10:05:12.148: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:05:13.155: INFO: Number of nodes with available pods: 0
Jul  3 10:05:13.155: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:05:14.156: INFO: Number of nodes with available pods: 2
Jul  3 10:05:14.156: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:05:15.157: INFO: Number of nodes with available pods: 3
Jul  3 10:05:15.157: INFO: Number of running nodes: 3, number of available pods: 3
Jul  3 10:05:15.157: INFO: Update the DaemonSet to trigger a rollout
Jul  3 10:05:15.169: INFO: Updating DaemonSet daemon-set
Jul  3 10:05:23.183: INFO: Roll back the DaemonSet before rollout is complete
Jul  3 10:05:23.191: INFO: Updating DaemonSet daemon-set
Jul  3 10:05:23.191: INFO: Make sure DaemonSet rollback is complete
Jul  3 10:05:23.195: INFO: Wrong image for pod: daemon-set-jv59q. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  3 10:05:23.195: INFO: Pod daemon-set-jv59q is not available
Jul  3 10:05:24.204: INFO: Wrong image for pod: daemon-set-jv59q. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  3 10:05:24.204: INFO: Pod daemon-set-jv59q is not available
Jul  3 10:05:25.203: INFO: Wrong image for pod: daemon-set-jv59q. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul  3 10:05:25.203: INFO: Pod daemon-set-jv59q is not available
Jul  3 10:05:26.203: INFO: Pod daemon-set-ktdgl is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7726, will wait for the garbage collector to delete the pods
Jul  3 10:05:26.276: INFO: Deleting DaemonSet.extensions daemon-set took: 8.164701ms
Jul  3 10:05:26.776: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.192108ms
Jul  3 10:06:52.680: INFO: Number of nodes with available pods: 0
Jul  3 10:06:52.680: INFO: Number of running nodes: 0, number of available pods: 0
Jul  3 10:06:52.685: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7726/daemonsets","resourceVersion":"286250"},"items":null}

Jul  3 10:06:52.688: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7726/pods","resourceVersion":"286250"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:06:52.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7726" for this suite.
Jul  3 10:06:58.719: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:06:58.870: INFO: namespace daemonsets-7726 deletion completed in 6.164113169s

• [SLOW TEST:106.795 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:06:58.870: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-4b758f49-9d7a-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:06:58.925: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "configmap-2032" to be "success or failure"
Jul  3 10:06:58.931: INFO: Pod "pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.924894ms
Jul  3 10:07:00.936: INFO: Pod "pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010791547s
STEP: Saw pod success
Jul  3 10:07:00.936: INFO: Pod "pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:07:00.939: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:07:00.969: INFO: Waiting for pod pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:07:00.972: INFO: Pod pod-configmaps-4b770696-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:07:00.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2032" for this suite.
Jul  3 10:07:06.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:07:07.134: INFO: namespace configmap-2032 deletion completed in 6.157307099s

• [SLOW TEST:8.264 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:07:07.135: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Jul  3 10:07:07.178: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4953" to be "success or failure"
Jul  3 10:07:07.182: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.848771ms
Jul  3 10:07:09.186: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960924s
Jul  3 10:07:11.191: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013026537s
STEP: Saw pod success
Jul  3 10:07:11.191: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul  3 10:07:11.194: INFO: Trying to get logs from node ip-172-31-26-241.eu-central-1.compute.internal pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul  3 10:07:11.223: INFO: Waiting for pod pod-host-path-test to disappear
Jul  3 10:07:11.231: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:07:11.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4953" for this suite.
Jul  3 10:07:17.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:07:17.395: INFO: namespace hostpath-4953 deletion completed in 6.159005394s

• [SLOW TEST:10.260 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:07:17.396: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Jul  3 10:07:17.437: INFO: Waiting up to 5m0s for pod "client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "containers-385" to be "success or failure"
Jul  3 10:07:17.442: INFO: Pod "client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.960169ms
Jul  3 10:07:19.446: INFO: Pod "client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009419726s
Jul  3 10:07:21.450: INFO: Pod "client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013471834s
STEP: Saw pod success
Jul  3 10:07:21.450: INFO: Pod "client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:07:21.454: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:07:21.480: INFO: Waiting for pod client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:07:21.483: INFO: Pod client-containers-567f633c-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:07:21.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-385" for this suite.
Jul  3 10:07:27.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:07:27.641: INFO: namespace containers-385 deletion completed in 6.153649392s

• [SLOW TEST:10.246 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:07:27.642: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  3 10:07:32.239: INFO: Successfully updated pod "labelsupdate5c9b00e4-9d7a-11e9-9a35-2ab8d9f547c5"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:07:34.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6510" for this suite.
Jul  3 10:07:56.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:07:56.529: INFO: namespace projected-6510 deletion completed in 22.16078229s

• [SLOW TEST:28.887 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:07:56.529: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul  3 10:08:00.589: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-6dd31825-9d7a-11e9-9a35-2ab8d9f547c5,GenerateName:,Namespace:events-9085,SelfLink:/api/v1/namespaces/events-9085/pods/send-events-6dd31825-9d7a-11e9-9a35-2ab8d9f547c5,UID:6dd3bb1c-9d7a-11e9-9bcb-a6641e9563f8,ResourceVersion:286617,Generation:0,CreationTimestamp:2019-07-03 10:07:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 563259279,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-67d2p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-67d2p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-67d2p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a5b0a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a5b0c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:07:56 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:07:59 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:07:59 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:07:56 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.62,StartTime:2019-07-03 10:07:56 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-03 10:07:59 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://b2d4f0503e437259181da6c5286d19db302ff90068eb04d9004a1bdeeff4471b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul  3 10:08:02.593: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul  3 10:08:04.598: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:08:04.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9085" for this suite.
Jul  3 10:08:42.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:08:42.773: INFO: namespace events-9085 deletion completed in 38.159739781s

• [SLOW TEST:46.244 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:08:42.773: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-896348e7-9d7a-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:08:42.860: INFO: Waiting up to 5m0s for pod "pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "secrets-8002" to be "success or failure"
Jul  3 10:08:42.865: INFO: Pod "pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.156013ms
Jul  3 10:08:44.869: INFO: Pod "pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009373418s
Jul  3 10:08:46.873: INFO: Pod "pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013525631s
STEP: Saw pod success
Jul  3 10:08:46.873: INFO: Pod "pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:08:46.877: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:08:46.900: INFO: Waiting for pod pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:08:46.903: INFO: Pod pod-secrets-896a1c6f-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:08:46.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8002" for this suite.
Jul  3 10:08:52.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:08:53.065: INFO: namespace secrets-8002 deletion completed in 6.157043036s
STEP: Destroying namespace "secret-namespace-5399" for this suite.
Jul  3 10:08:59.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:08:59.220: INFO: namespace secret-namespace-5399 deletion completed in 6.154902139s

• [SLOW TEST:16.446 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:08:59.220: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  3 10:08:59.265: INFO: Waiting up to 5m0s for pod "pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-420" to be "success or failure"
Jul  3 10:08:59.270: INFO: Pod "pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.705078ms
Jul  3 10:09:01.275: INFO: Pod "pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009326175s
Jul  3 10:09:03.279: INFO: Pod "pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013571317s
STEP: Saw pod success
Jul  3 10:09:03.279: INFO: Pod "pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:09:03.282: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:09:03.303: INFO: Waiting for pod pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:09:03.306: INFO: Pod pod-93312591-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:09:03.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-420" for this suite.
Jul  3 10:09:09.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:09:09.473: INFO: namespace emptydir-420 deletion completed in 6.16282289s

• [SLOW TEST:10.253 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:09:09.474: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-994dabbe-9d7a-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:09:09.522: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "projected-5389" to be "success or failure"
Jul  3 10:09:09.528: INFO: Pod "pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.152155ms
Jul  3 10:09:11.532: INFO: Pod "pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009977017s
Jul  3 10:09:13.540: INFO: Pod "pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01739695s
STEP: Saw pod success
Jul  3 10:09:13.540: INFO: Pod "pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:09:13.545: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:09:13.568: INFO: Waiting for pod pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:09:13.571: INFO: Pod pod-projected-secrets-994eaa24-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:09:13.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5389" for this suite.
Jul  3 10:09:19.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:09:19.732: INFO: namespace projected-5389 deletion completed in 6.157115468s

• [SLOW TEST:10.258 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:09:19.732: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:09:19.783: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-2173" to be "success or failure"
Jul  3 10:09:19.789: INFO: Pod "downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.682733ms
Jul  3 10:09:21.794: INFO: Pod "downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01081192s
Jul  3 10:09:23.799: INFO: Pod "downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015784633s
STEP: Saw pod success
Jul  3 10:09:23.799: INFO: Pod "downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:09:23.803: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:09:23.829: INFO: Waiting for pod downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:09:23.832: INFO: Pod downwardapi-volume-9f6bf979-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:09:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2173" for this suite.
Jul  3 10:09:29.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:09:30.019: INFO: namespace downward-api-2173 deletion completed in 6.182927033s

• [SLOW TEST:10.287 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:09:30.020: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4788
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul  3 10:09:30.089: INFO: Found 0 stateful pods, waiting for 3
Jul  3 10:09:40.095: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:09:40.095: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:09:40.095: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  3 10:09:40.128: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul  3 10:09:50.167: INFO: Updating stateful set ss2
Jul  3 10:09:50.176: INFO: Waiting for Pod statefulset-4788/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Jul  3 10:10:00.225: INFO: Found 2 stateful pods, waiting for 3
Jul  3 10:10:10.231: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:10:10.231: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:10:10.231: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul  3 10:10:10.257: INFO: Updating stateful set ss2
Jul  3 10:10:10.267: INFO: Waiting for Pod statefulset-4788/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul  3 10:10:20.297: INFO: Updating stateful set ss2
Jul  3 10:10:20.305: INFO: Waiting for StatefulSet statefulset-4788/ss2 to complete update
Jul  3 10:10:20.305: INFO: Waiting for Pod statefulset-4788/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Jul  3 10:10:30.314: INFO: Waiting for StatefulSet statefulset-4788/ss2 to complete update
Jul  3 10:10:30.314: INFO: Waiting for Pod statefulset-4788/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  3 10:10:40.315: INFO: Deleting all statefulset in ns statefulset-4788
Jul  3 10:10:40.318: INFO: Scaling statefulset ss2 to 0
Jul  3 10:11:00.335: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:11:00.338: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:11:00.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4788" for this suite.
Jul  3 10:11:06.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:11:06.561: INFO: namespace statefulset-4788 deletion completed in 6.20385979s

• [SLOW TEST:96.541 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:11:06.561: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:11:06.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-1676" to be "success or failure"
Jul  3 10:11:06.625: INFO: Pod "downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.844209ms
Jul  3 10:11:08.630: INFO: Pod "downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009299313s
Jul  3 10:11:10.634: INFO: Pod "downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014067646s
STEP: Saw pod success
Jul  3 10:11:10.634: INFO: Pod "downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:11:10.638: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:11:10.674: INFO: Waiting for pod downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:11:10.677: INFO: Pod downwardapi-volume-df19f355-9d7a-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:11:10.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1676" for this suite.
Jul  3 10:11:16.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:11:16.846: INFO: namespace downward-api-1676 deletion completed in 6.163988138s

• [SLOW TEST:10.285 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:11:16.847: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3614
Jul  3 10:11:20.898: INFO: Started pod liveness-http in namespace container-probe-3614
STEP: checking the pod's current state and verifying that restartCount is present
Jul  3 10:11:20.901: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:15:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3614" for this suite.
Jul  3 10:15:27.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:15:27.632: INFO: namespace container-probe-3614 deletion completed in 6.156760226s

• [SLOW TEST:250.785 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:15:27.632: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-7ab3d0af-9d7b-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:15:27.680: INFO: Waiting up to 5m0s for pod "pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5" in namespace "configmap-7340" to be "success or failure"
Jul  3 10:15:27.684: INFO: Pod "pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.408881ms
Jul  3 10:15:29.689: INFO: Pod "pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00931482s
Jul  3 10:15:31.693: INFO: Pod "pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013764194s
STEP: Saw pod success
Jul  3 10:15:31.693: INFO: Pod "pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:15:31.697: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:15:31.726: INFO: Waiting for pod pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:15:31.729: INFO: Pod pod-configmaps-7ab4d011-9d7b-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:15:31.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7340" for this suite.
Jul  3 10:15:37.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:15:37.902: INFO: namespace configmap-7340 deletion completed in 6.168697997s

• [SLOW TEST:10.270 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:15:37.902: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 10:15:37.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9644'
Jul  3 10:15:38.183: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  3 10:15:38.183: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Jul  3 10:15:40.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9644'
Jul  3 10:15:40.270: INFO: stderr: ""
Jul  3 10:15:40.270: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:15:40.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9644" for this suite.
Jul  3 10:16:02.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:16:02.439: INFO: namespace kubectl-9644 deletion completed in 22.161670088s

• [SLOW TEST:24.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:16:02.439: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-9098
Jul  3 10:16:06.491: INFO: Started pod liveness-http in namespace container-probe-9098
STEP: checking the pod's current state and verifying that restartCount is present
Jul  3 10:16:06.495: INFO: Initial restart count of pod liveness-http is 0
Jul  3 10:16:18.524: INFO: Restart count of pod container-probe-9098/liveness-http is now 1 (12.02964823s elapsed)
Jul  3 10:16:36.564: INFO: Restart count of pod container-probe-9098/liveness-http is now 2 (30.069043079s elapsed)
Jul  3 10:16:56.608: INFO: Restart count of pod container-probe-9098/liveness-http is now 3 (50.113497712s elapsed)
Jul  3 10:17:16.656: INFO: Restart count of pod container-probe-9098/liveness-http is now 4 (1m10.16129808s elapsed)
Jul  3 10:18:18.793: INFO: Restart count of pod container-probe-9098/liveness-http is now 5 (2m12.298354341s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:18:18.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9098" for this suite.
Jul  3 10:18:24.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:18:25.034: INFO: namespace container-probe-9098 deletion completed in 6.217486029s

• [SLOW TEST:142.595 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:18:25.035: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5525
Jul  3 10:18:29.089: INFO: Started pod liveness-exec in namespace container-probe-5525
STEP: checking the pod's current state and verifying that restartCount is present
Jul  3 10:18:29.092: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:22:29.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5525" for this suite.
Jul  3 10:22:35.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:22:35.857: INFO: namespace container-probe-5525 deletion completed in 6.174905822s

• [SLOW TEST:250.822 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:22:35.858: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 10:22:35.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9947'
Jul  3 10:22:35.973: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  3 10:22:35.973: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul  3 10:22:35.983: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-4wvzs]
Jul  3 10:22:35.983: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-4wvzs" in namespace "kubectl-9947" to be "running and ready"
Jul  3 10:22:35.988: INFO: Pod "e2e-test-nginx-rc-4wvzs": Phase="Pending", Reason="", readiness=false. Elapsed: 4.816759ms
Jul  3 10:22:37.992: INFO: Pod "e2e-test-nginx-rc-4wvzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009208833s
Jul  3 10:22:39.997: INFO: Pod "e2e-test-nginx-rc-4wvzs": Phase="Running", Reason="", readiness=true. Elapsed: 4.014354522s
Jul  3 10:22:39.997: INFO: Pod "e2e-test-nginx-rc-4wvzs" satisfied condition "running and ready"
Jul  3 10:22:39.997: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-4wvzs]
Jul  3 10:22:39.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 logs rc/e2e-test-nginx-rc --namespace=kubectl-9947'
Jul  3 10:22:40.099: INFO: stderr: ""
Jul  3 10:22:40.099: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Jul  3 10:22:40.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete rc e2e-test-nginx-rc --namespace=kubectl-9947'
Jul  3 10:22:40.173: INFO: stderr: ""
Jul  3 10:22:40.173: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:22:40.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9947" for this suite.
Jul  3 10:22:46.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:22:46.366: INFO: namespace kubectl-9947 deletion completed in 6.187843464s

• [SLOW TEST:10.509 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:22:46.367: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5277
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  3 10:22:46.401: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  3 10:23:10.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.0.29:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:23:10.511: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:23:11.118: INFO: Found all expected endpoints: [netserver-0]
Jul  3 10:23:11.121: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.1.77:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:23:11.122: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:23:11.830: INFO: Found all expected endpoints: [netserver-1]
Jul  3 10:23:11.834: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.25.2.26:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5277 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:23:11.834: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:23:12.416: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:23:12.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5277" for this suite.
Jul  3 10:23:34.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:23:34.607: INFO: namespace pod-network-test-5277 deletion completed in 22.180241651s

• [SLOW TEST:48.241 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:23:34.607: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9cf66762-9d7c-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:23:34.654: INFO: Waiting up to 5m0s for pod "pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5" in namespace "configmap-4186" to be "success or failure"
Jul  3 10:23:34.660: INFO: Pod "pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.930061ms
Jul  3 10:23:36.664: INFO: Pod "pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009835971s
STEP: Saw pod success
Jul  3 10:23:36.664: INFO: Pod "pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:23:36.667: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:23:36.731: INFO: Waiting for pod pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:23:36.734: INFO: Pod pod-configmaps-9cf753bf-9d7c-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:23:36.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4186" for this suite.
Jul  3 10:23:42.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:23:42.898: INFO: namespace configmap-4186 deletion completed in 6.159022035s

• [SLOW TEST:8.290 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:23:42.898: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  3 10:23:42.941: INFO: Waiting up to 5m0s for pod "pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-5224" to be "success or failure"
Jul  3 10:23:42.945: INFO: Pod "pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.098946ms
Jul  3 10:23:44.950: INFO: Pod "pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008590118s
STEP: Saw pod success
Jul  3 10:23:44.950: INFO: Pod "pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:23:44.953: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:23:44.984: INFO: Waiting for pod pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:23:44.988: INFO: Pod pod-a1e767c0-9d7c-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:23:44.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5224" for this suite.
Jul  3 10:23:51.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:23:51.150: INFO: namespace emptydir-5224 deletion completed in 6.157236687s

• [SLOW TEST:8.253 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:23:51.150: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0703 10:23:57.217703      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 10:23:57.217: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:23:57.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5381" for this suite.
Jul  3 10:24:03.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:24:03.385: INFO: namespace gc-5381 deletion completed in 6.163924975s

• [SLOW TEST:12.235 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:24:03.386: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-ae1df9cb-9d7c-11e9-9a35-2ab8d9f547c5
STEP: Creating secret with name secret-projected-all-test-volume-ae1df9b5-9d7c-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul  3 10:24:03.439: INFO: Waiting up to 5m0s for pod "projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5" in namespace "projected-7981" to be "success or failure"
Jul  3 10:24:03.447: INFO: Pod "projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.006116ms
Jul  3 10:24:05.452: INFO: Pod "projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012207678s
Jul  3 10:24:07.456: INFO: Pod "projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016676333s
STEP: Saw pod success
Jul  3 10:24:07.456: INFO: Pod "projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:24:07.460: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul  3 10:24:07.494: INFO: Waiting for pod projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:24:07.497: INFO: Pod projected-volume-ae1df988-9d7c-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:24:07.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7981" for this suite.
Jul  3 10:24:13.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:24:13.670: INFO: namespace projected-7981 deletion completed in 6.1677425s

• [SLOW TEST:10.284 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:24:13.670: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1444
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  3 10:24:13.704: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  3 10:24:39.815: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.86:8080/dial?request=hostName&protocol=http&host=172.25.2.31&port=8080&tries=1'] Namespace:pod-network-test-1444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:24:39.815: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:24:40.361: INFO: Waiting for endpoints: map[]
Jul  3 10:24:40.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.86:8080/dial?request=hostName&protocol=http&host=172.25.1.85&port=8080&tries=1'] Namespace:pod-network-test-1444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:24:40.365: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:24:40.929: INFO: Waiting for endpoints: map[]
Jul  3 10:24:40.935: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.86:8080/dial?request=hostName&protocol=http&host=172.25.0.33&port=8080&tries=1'] Namespace:pod-network-test-1444 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:24:40.935: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:24:41.519: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:24:41.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1444" for this suite.
Jul  3 10:25:03.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:03.683: INFO: namespace pod-network-test-1444 deletion completed in 22.159193069s

• [SLOW TEST:50.013 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:03.683: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  3 10:25:03.725: INFO: Waiting up to 5m0s for pod "downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-9601" to be "success or failure"
Jul  3 10:25:03.731: INFO: Pod "downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.402218ms
Jul  3 10:25:05.735: INFO: Pod "downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010117905s
Jul  3 10:25:07.740: INFO: Pod "downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014936498s
STEP: Saw pod success
Jul  3 10:25:07.740: INFO: Pod "downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:25:07.744: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 10:25:07.781: INFO: Waiting for pod downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:25:07.784: INFO: Pod downward-api-d20df8f4-9d7c-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:25:07.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9601" for this suite.
Jul  3 10:25:13.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:13.950: INFO: namespace downward-api-9601 deletion completed in 6.161706682s

• [SLOW TEST:10.267 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:13.951: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  3 10:25:13.985: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  3 10:25:13.994: INFO: Waiting for terminating namespaces to be deleted...
Jul  3 10:25:13.998: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-22-121.eu-central-1.compute.internal before test
Jul  3 10:25:14.105: INFO: node-exporter-bxmkq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:25:14.105: INFO: coredns-6bd858f7c-6f8lz from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:25:14.105: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-03 09:48:02 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  3 10:25:14.105: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-275sw from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:25:14.105: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:25:14.105: INFO: canal-7p6kq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (3 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:25:14.105: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:25:14.105: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:25:14.105: INFO: kube-proxy-6k2j2 from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:25:14.105: INFO: restic-tx9l2 from velero started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:25:14.105: INFO: coredns-6bd858f7c-4g2hs from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.105: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:25:14.105: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-23-251.eu-central-1.compute.internal before test
Jul  3 10:25:14.159: INFO: tiller-deploy-794d4bbf57-bzs9k from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container tiller ready: true, restart count 0
Jul  3 10:25:14.159: INFO: velero-6bff998df8-65rvl from velero started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:25:14.159: INFO: canal-qkzt7 from kube-system started at 2019-07-02 09:13:50 +0000 UTC (3 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:25:14.159: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:25:14.159: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:25:14.159: INFO: kube-proxy-clv5z from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:25:14.159: INFO: node-exporter-95rzz from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:25:14.159: INFO: cluster-autoscaler-c8c56b5d9-vmxpf from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Jul  3 10:25:14.159: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-gv8pj from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:25:14.159: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:25:14.159: INFO: kubernetes-dashboard-57b5ff8798-hk65d from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul  3 10:25:14.159: INFO: restic-qcb2x from velero started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:25:14.159: INFO: openvpn-client-5f9d55d8df-st4fs from kube-system started at 2019-07-02 09:14:20 +0000 UTC (2 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container dnat-controller ready: true, restart count 0
Jul  3 10:25:14.159: INFO: 	Container openvpn-client ready: true, restart count 0
Jul  3 10:25:14.159: INFO: webterminal-c87695794-xnzsz from webterminal started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.159: INFO: 	Container webterminal ready: true, restart count 0
Jul  3 10:25:14.159: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-26-241.eu-central-1.compute.internal before test
Jul  3 10:25:14.228: INFO: kube-proxy-w9fsl from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:25:14.228: INFO: node-exporter-qkmqw from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:25:14.228: INFO: canal-rgl2j from kube-system started at 2019-07-02 09:14:10 +0000 UTC (3 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:25:14.228: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:25:14.228: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:25:14.228: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-bhvm7 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:25:14.228: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:25:14.228: INFO: restic-tbbgk from velero started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:25:14.228: INFO: sonobuoy-e2e-job-9f2f9fa81a324834 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:25:14.228: INFO: 	Container e2e ready: true, restart count 0
Jul  3 10:25:14.228: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15addeb4e2ed3b0c], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:25:15.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3571" for this suite.
Jul  3 10:25:21.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:21.430: INFO: namespace sched-pred-3571 deletion completed in 6.169927828s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.479 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:21.430: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul  3 10:25:26.003: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7691 pod-service-account-dcf1d74e-9d7c-11e9-9a35-2ab8d9f547c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul  3 10:25:26.654: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7691 pod-service-account-dcf1d74e-9d7c-11e9-9a35-2ab8d9f547c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul  3 10:25:27.263: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7691 pod-service-account-dcf1d74e-9d7c-11e9-9a35-2ab8d9f547c5 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:25:27.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7691" for this suite.
Jul  3 10:25:33.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:34.144: INFO: namespace svcaccounts-7691 deletion completed in 6.163613686s

• [SLOW TEST:12.714 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:34.144: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e4364492-9d7c-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:25:34.191: INFO: Waiting up to 5m0s for pod "pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5" in namespace "secrets-2443" to be "success or failure"
Jul  3 10:25:34.196: INFO: Pod "pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.949514ms
Jul  3 10:25:36.200: INFO: Pod "pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008922908s
Jul  3 10:25:38.206: INFO: Pod "pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014611716s
STEP: Saw pod success
Jul  3 10:25:38.206: INFO: Pod "pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:25:38.211: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:25:38.286: INFO: Waiting for pod pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:25:38.289: INFO: Pod pod-secrets-e437312a-9d7c-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:25:38.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2443" for this suite.
Jul  3 10:25:44.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:44.456: INFO: namespace secrets-2443 deletion completed in 6.161111738s

• [SLOW TEST:10.312 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:44.457: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul  3 10:25:44.495: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul  3 10:25:53.547: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:25:53.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1529" for this suite.
Jul  3 10:25:59.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:25:59.716: INFO: namespace pods-1529 deletion completed in 6.161168884s

• [SLOW TEST:15.259 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:25:59.716: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul  3 10:26:05.829: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:05.833: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:07.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:07.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:09.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:09.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:11.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:11.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:13.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:13.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:15.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:15.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:17.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:17.837: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:19.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:19.838: INFO: Pod pod-with-poststart-http-hook still exists
Jul  3 10:26:21.833: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul  3 10:26:21.838: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:26:21.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7999" for this suite.
Jul  3 10:26:43.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:26:44.013: INFO: namespace container-lifecycle-hook-7999 deletion completed in 22.170611898s

• [SLOW TEST:44.297 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:26:44.013: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  3 10:26:44.055: INFO: Waiting up to 5m0s for pod "pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-6163" to be "success or failure"
Jul  3 10:26:44.060: INFO: Pod "pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.869041ms
Jul  3 10:26:46.064: INFO: Pod "pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008783257s
Jul  3 10:26:48.069: INFO: Pod "pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013330658s
STEP: Saw pod success
Jul  3 10:26:48.069: INFO: Pod "pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:26:48.072: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:26:48.095: INFO: Waiting for pod pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:26:48.098: INFO: Pod pod-0ddb5bd9-9d7d-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:26:48.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6163" for this suite.
Jul  3 10:26:54.116: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:26:54.272: INFO: namespace emptydir-6163 deletion completed in 6.16834751s

• [SLOW TEST:10.259 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:26:54.272: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Jul  3 10:26:54.834: INFO: created pod pod-service-account-defaultsa
Jul  3 10:26:54.834: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul  3 10:26:54.841: INFO: created pod pod-service-account-mountsa
Jul  3 10:26:54.841: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul  3 10:26:54.846: INFO: created pod pod-service-account-nomountsa
Jul  3 10:26:54.846: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul  3 10:26:54.853: INFO: created pod pod-service-account-defaultsa-mountspec
Jul  3 10:26:54.853: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul  3 10:26:54.861: INFO: created pod pod-service-account-mountsa-mountspec
Jul  3 10:26:54.861: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul  3 10:26:54.869: INFO: created pod pod-service-account-nomountsa-mountspec
Jul  3 10:26:54.869: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul  3 10:26:54.890: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul  3 10:26:54.890: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul  3 10:26:54.896: INFO: created pod pod-service-account-mountsa-nomountspec
Jul  3 10:26:54.896: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul  3 10:26:54.904: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul  3 10:26:54.904: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:26:54.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-414" for this suite.
Jul  3 10:27:00.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:27:01.077: INFO: namespace svcaccounts-414 deletion completed in 6.161805068s

• [SLOW TEST:6.805 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:27:01.077: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-1806e1a3-9d7d-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:27:01.122: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5" in namespace "projected-6531" to be "success or failure"
Jul  3 10:27:01.128: INFO: Pod "pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.840468ms
Jul  3 10:27:03.132: INFO: Pod "pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009763505s
Jul  3 10:27:05.136: INFO: Pod "pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01418678s
STEP: Saw pod success
Jul  3 10:27:05.136: INFO: Pod "pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:27:05.140: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:27:05.187: INFO: Waiting for pod pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:27:05.192: INFO: Pod pod-projected-configmaps-1807d29a-9d7d-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:27:05.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6531" for this suite.
Jul  3 10:27:11.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:27:11.358: INFO: namespace projected-6531 deletion completed in 6.16035709s

• [SLOW TEST:10.281 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:27:11.359: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:27:34.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6025" for this suite.
Jul  3 10:27:40.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:27:40.836: INFO: namespace container-runtime-6025 deletion completed in 6.204110263s

• [SLOW TEST:29.478 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:27:40.837: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul  3 10:27:40.878: INFO: Waiting up to 5m0s for pod "pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-1981" to be "success or failure"
Jul  3 10:27:40.883: INFO: Pod "pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.314049ms
Jul  3 10:27:42.887: INFO: Pod "pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009279281s
STEP: Saw pod success
Jul  3 10:27:42.887: INFO: Pod "pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:27:42.890: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:27:42.950: INFO: Waiting for pod pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:27:42.953: INFO: Pod pod-2fb9cb62-9d7d-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:27:42.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1981" for this suite.
Jul  3 10:27:48.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:27:49.112: INFO: namespace emptydir-1981 deletion completed in 6.155013227s

• [SLOW TEST:8.276 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:27:49.112: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Jul  3 10:27:49.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 api-versions'
Jul  3 10:27:49.214: INFO: stderr: ""
Jul  3 10:27:49.214: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncluster.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\nvelero.io/v1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:27:49.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3682" for this suite.
Jul  3 10:27:55.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:27:55.378: INFO: namespace kubectl-3682 deletion completed in 6.159546639s

• [SLOW TEST:6.266 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:27:55.378: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul  3 10:27:55.645: INFO: Pod name wrapped-volume-race-388617f5-9d7d-11e9-9a35-2ab8d9f547c5: Found 0 pods out of 5
Jul  3 10:28:00.652: INFO: Pod name wrapped-volume-race-388617f5-9d7d-11e9-9a35-2ab8d9f547c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-388617f5-9d7d-11e9-9a35-2ab8d9f547c5 in namespace emptydir-wrapper-9356, will wait for the garbage collector to delete the pods
Jul  3 10:28:12.749: INFO: Deleting ReplicationController wrapped-volume-race-388617f5-9d7d-11e9-9a35-2ab8d9f547c5 took: 9.260208ms
Jul  3 10:28:13.249: INFO: Terminating ReplicationController wrapped-volume-race-388617f5-9d7d-11e9-9a35-2ab8d9f547c5 pods took: 500.16597ms
STEP: Creating RC which spawns configmap-volume pods
Jul  3 10:28:53.668: INFO: Pod name wrapped-volume-race-5b1b4b5b-9d7d-11e9-9a35-2ab8d9f547c5: Found 0 pods out of 5
Jul  3 10:28:58.675: INFO: Pod name wrapped-volume-race-5b1b4b5b-9d7d-11e9-9a35-2ab8d9f547c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5b1b4b5b-9d7d-11e9-9a35-2ab8d9f547c5 in namespace emptydir-wrapper-9356, will wait for the garbage collector to delete the pods
Jul  3 10:29:10.779: INFO: Deleting ReplicationController wrapped-volume-race-5b1b4b5b-9d7d-11e9-9a35-2ab8d9f547c5 took: 9.324628ms
Jul  3 10:29:11.279: INFO: Terminating ReplicationController wrapped-volume-race-5b1b4b5b-9d7d-11e9-9a35-2ab8d9f547c5 pods took: 500.278026ms
STEP: Creating RC which spawns configmap-volume pods
Jul  3 10:29:53.097: INFO: Pod name wrapped-volume-race-7e878d11-9d7d-11e9-9a35-2ab8d9f547c5: Found 0 pods out of 5
Jul  3 10:29:58.105: INFO: Pod name wrapped-volume-race-7e878d11-9d7d-11e9-9a35-2ab8d9f547c5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-7e878d11-9d7d-11e9-9a35-2ab8d9f547c5 in namespace emptydir-wrapper-9356, will wait for the garbage collector to delete the pods
Jul  3 10:30:10.203: INFO: Deleting ReplicationController wrapped-volume-race-7e878d11-9d7d-11e9-9a35-2ab8d9f547c5 took: 8.766545ms
Jul  3 10:30:10.703: INFO: Terminating ReplicationController wrapped-volume-race-7e878d11-9d7d-11e9-9a35-2ab8d9f547c5 pods took: 500.221348ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:30:53.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9356" for this suite.
Jul  3 10:31:01.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:31:01.275: INFO: namespace emptydir-wrapper-9356 deletion completed in 8.157888544s

• [SLOW TEST:185.896 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:31:01.275: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:31:27.332: INFO: Container started at 2019-07-03 10:31:03 +0000 UTC, pod became ready at 2019-07-03 10:31:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:31:27.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8772" for this suite.
Jul  3 10:31:49.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:31:49.495: INFO: namespace container-probe-8772 deletion completed in 22.15919629s

• [SLOW TEST:48.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:31:49.496: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:31:51.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8717" for this suite.
Jul  3 10:32:29.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:32:29.727: INFO: namespace kubelet-test-8717 deletion completed in 38.154750688s

• [SLOW TEST:40.232 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:32:29.727: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:32:35.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1824" for this suite.
Jul  3 10:32:41.873: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:32:42.031: INFO: namespace namespaces-1824 deletion completed in 6.17171032s
STEP: Destroying namespace "nsdeletetest-1658" for this suite.
Jul  3 10:32:42.034: INFO: Namespace nsdeletetest-1658 was already deleted
STEP: Destroying namespace "nsdeletetest-5325" for this suite.
Jul  3 10:32:48.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:32:48.185: INFO: namespace nsdeletetest-5325 deletion completed in 6.150750978s

• [SLOW TEST:18.458 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:32:48.186: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e6ebb7ec-9d7d-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:32:48.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5" in namespace "configmap-3362" to be "success or failure"
Jul  3 10:32:48.238: INFO: Pod "pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.238796ms
Jul  3 10:32:50.242: INFO: Pod "pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009488643s
STEP: Saw pod success
Jul  3 10:32:50.242: INFO: Pod "pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:32:50.245: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:32:50.269: INFO: Waiting for pod pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:32:50.272: INFO: Pod pod-configmaps-e6ecaa68-9d7d-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:32:50.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3362" for this suite.
Jul  3 10:32:56.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:32:56.438: INFO: namespace configmap-3362 deletion completed in 6.161132988s

• [SLOW TEST:8.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:32:56.438: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-8554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8554 to expose endpoints map[]
Jul  3 10:32:56.487: INFO: Get endpoints failed (2.967857ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jul  3 10:32:57.491: INFO: successfully validated that service endpoint-test2 in namespace services-8554 exposes endpoints map[] (1.006883905s elapsed)
STEP: Creating pod pod1 in namespace services-8554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8554 to expose endpoints map[pod1:[80]]
Jul  3 10:32:59.524: INFO: successfully validated that service endpoint-test2 in namespace services-8554 exposes endpoints map[pod1:[80]] (2.024249382s elapsed)
STEP: Creating pod pod2 in namespace services-8554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8554 to expose endpoints map[pod1:[80] pod2:[80]]
Jul  3 10:33:01.567: INFO: successfully validated that service endpoint-test2 in namespace services-8554 exposes endpoints map[pod1:[80] pod2:[80]] (2.037652044s elapsed)
STEP: Deleting pod pod1 in namespace services-8554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8554 to expose endpoints map[pod2:[80]]
Jul  3 10:33:01.585: INFO: successfully validated that service endpoint-test2 in namespace services-8554 exposes endpoints map[pod2:[80]] (10.048214ms elapsed)
STEP: Deleting pod pod2 in namespace services-8554
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8554 to expose endpoints map[]
Jul  3 10:33:01.597: INFO: successfully validated that service endpoint-test2 in namespace services-8554 exposes endpoints map[] (3.476552ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:33:01.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8554" for this suite.
Jul  3 10:33:23.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:33:23.773: INFO: namespace services-8554 deletion completed in 22.154801993s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:27.335 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:33:23.774: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Jul  3 10:33:23.819: INFO: Waiting up to 5m0s for pod "client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5" in namespace "containers-275" to be "success or failure"
Jul  3 10:33:23.823: INFO: Pod "client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.746122ms
Jul  3 10:33:25.827: INFO: Pod "client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007731529s
Jul  3 10:33:27.832: INFO: Pod "client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012751737s
STEP: Saw pod success
Jul  3 10:33:27.832: INFO: Pod "client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:33:27.835: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:33:27.858: INFO: Waiting for pod client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:33:27.861: INFO: Pod client-containers-fc225f88-9d7d-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:33:27.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-275" for this suite.
Jul  3 10:33:33.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:33:34.023: INFO: namespace containers-275 deletion completed in 6.158016131s

• [SLOW TEST:10.250 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:33:34.023: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:33:36.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7863" for this suite.
Jul  3 10:33:42.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:33:42.277: INFO: namespace emptydir-wrapper-7863 deletion completed in 6.15781428s

• [SLOW TEST:8.254 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:33:42.278: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:33:42.320: INFO: Waiting up to 5m0s for pod "downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-4443" to be "success or failure"
Jul  3 10:33:42.326: INFO: Pod "downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.297318ms
Jul  3 10:33:44.332: INFO: Pod "downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012118995s
STEP: Saw pod success
Jul  3 10:33:44.332: INFO: Pod "downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:33:44.335: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:33:44.357: INFO: Waiting for pod downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:33:44.360: INFO: Pod downwardapi-volume-07297d22-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:33:44.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4443" for this suite.
Jul  3 10:33:50.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:33:50.522: INFO: namespace downward-api-4443 deletion completed in 6.156804716s

• [SLOW TEST:8.244 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:33:50.522: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-0c137800-9d7e-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:33:50.568: INFO: Waiting up to 5m0s for pod "pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "secrets-7009" to be "success or failure"
Jul  3 10:33:50.572: INFO: Pod "pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055262ms
Jul  3 10:33:52.577: INFO: Pod "pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008660707s
STEP: Saw pod success
Jul  3 10:33:52.577: INFO: Pod "pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:33:52.580: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5 container secret-env-test: <nil>
STEP: delete the pod
Jul  3 10:33:52.605: INFO: Waiting for pod pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:33:52.608: INFO: Pod pod-secrets-0c146750-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:33:52.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7009" for this suite.
Jul  3 10:33:58.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:33:58.767: INFO: namespace secrets-7009 deletion completed in 6.154929851s

• [SLOW TEST:8.245 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:33:58.767: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Jul  3 10:33:58.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 --namespace=kubectl-938 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul  3 10:34:01.977: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul  3 10:34:01.977: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:34:03.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-938" for this suite.
Jul  3 10:34:10.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:34:10.165: INFO: namespace kubectl-938 deletion completed in 6.177028347s

• [SLOW TEST:11.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:34:10.165: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  3 10:34:10.212: INFO: Waiting up to 5m0s for pod "pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-2060" to be "success or failure"
Jul  3 10:34:10.215: INFO: Pod "pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.940613ms
Jul  3 10:34:12.220: INFO: Pod "pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008238163s
STEP: Saw pod success
Jul  3 10:34:12.220: INFO: Pod "pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:34:12.223: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:34:12.278: INFO: Waiting for pod pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:34:12.281: INFO: Pod pod-17c90d12-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:34:12.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2060" for this suite.
Jul  3 10:34:18.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:34:18.448: INFO: namespace emptydir-2060 deletion completed in 6.162282728s

• [SLOW TEST:8.283 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:34:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-6034
I0703 10:34:18.488417      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-6034, replica count: 1
I0703 10:34:19.538687      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0703 10:34:20.538832      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  3 10:34:20.648: INFO: Created: latency-svc-bjhwd
Jul  3 10:34:20.654: INFO: Got endpoints: latency-svc-bjhwd [15.472414ms]
Jul  3 10:34:20.664: INFO: Created: latency-svc-dv5z2
Jul  3 10:34:20.668: INFO: Got endpoints: latency-svc-dv5z2 [13.90161ms]
Jul  3 10:34:20.669: INFO: Created: latency-svc-mrdwz
Jul  3 10:34:20.674: INFO: Got endpoints: latency-svc-mrdwz [19.833978ms]
Jul  3 10:34:20.677: INFO: Created: latency-svc-s9bc5
Jul  3 10:34:20.679: INFO: Got endpoints: latency-svc-s9bc5 [24.645521ms]
Jul  3 10:34:20.684: INFO: Created: latency-svc-6k4r5
Jul  3 10:34:20.687: INFO: Got endpoints: latency-svc-6k4r5 [32.275858ms]
Jul  3 10:34:20.689: INFO: Created: latency-svc-k4zr5
Jul  3 10:34:20.694: INFO: Got endpoints: latency-svc-k4zr5 [39.4589ms]
Jul  3 10:34:20.696: INFO: Created: latency-svc-5mkd9
Jul  3 10:34:20.701: INFO: Got endpoints: latency-svc-5mkd9 [46.606037ms]
Jul  3 10:34:20.701: INFO: Created: latency-svc-6tmjl
Jul  3 10:34:20.706: INFO: Got endpoints: latency-svc-6tmjl [51.545612ms]
Jul  3 10:34:20.710: INFO: Created: latency-svc-hhz29
Jul  3 10:34:20.715: INFO: Got endpoints: latency-svc-hhz29 [60.191686ms]
Jul  3 10:34:20.716: INFO: Created: latency-svc-ppdgs
Jul  3 10:34:20.720: INFO: Got endpoints: latency-svc-ppdgs [65.475045ms]
Jul  3 10:34:20.724: INFO: Created: latency-svc-q49xs
Jul  3 10:34:20.726: INFO: Got endpoints: latency-svc-q49xs [71.470607ms]
Jul  3 10:34:20.728: INFO: Created: latency-svc-xhgpz
Jul  3 10:34:20.732: INFO: Got endpoints: latency-svc-xhgpz [77.436047ms]
Jul  3 10:34:20.735: INFO: Created: latency-svc-zwh79
Jul  3 10:34:20.738: INFO: Got endpoints: latency-svc-zwh79 [83.416949ms]
Jul  3 10:34:20.742: INFO: Created: latency-svc-rkjwl
Jul  3 10:34:20.746: INFO: Got endpoints: latency-svc-rkjwl [91.016669ms]
Jul  3 10:34:20.748: INFO: Created: latency-svc-5p2jt
Jul  3 10:34:20.756: INFO: Created: latency-svc-8d5x2
Jul  3 10:34:20.756: INFO: Got endpoints: latency-svc-5p2jt [101.823891ms]
Jul  3 10:34:20.759: INFO: Got endpoints: latency-svc-8d5x2 [104.721925ms]
Jul  3 10:34:20.763: INFO: Created: latency-svc-tc626
Jul  3 10:34:20.767: INFO: Got endpoints: latency-svc-tc626 [98.884372ms]
Jul  3 10:34:20.769: INFO: Created: latency-svc-b59k5
Jul  3 10:34:20.773: INFO: Got endpoints: latency-svc-b59k5 [99.000652ms]
Jul  3 10:34:20.776: INFO: Created: latency-svc-w9rpm
Jul  3 10:34:20.779: INFO: Got endpoints: latency-svc-w9rpm [100.042716ms]
Jul  3 10:34:20.782: INFO: Created: latency-svc-s7vnn
Jul  3 10:34:20.786: INFO: Got endpoints: latency-svc-s7vnn [99.525134ms]
Jul  3 10:34:20.787: INFO: Created: latency-svc-sc8c9
Jul  3 10:34:20.794: INFO: Got endpoints: latency-svc-sc8c9 [100.226605ms]
Jul  3 10:34:20.794: INFO: Created: latency-svc-lq2v5
Jul  3 10:34:20.797: INFO: Got endpoints: latency-svc-lq2v5 [95.693007ms]
Jul  3 10:34:20.800: INFO: Created: latency-svc-9whxd
Jul  3 10:34:20.803: INFO: Got endpoints: latency-svc-9whxd [96.975171ms]
Jul  3 10:34:20.805: INFO: Created: latency-svc-w2r98
Jul  3 10:34:20.810: INFO: Got endpoints: latency-svc-w2r98 [94.656822ms]
Jul  3 10:34:20.812: INFO: Created: latency-svc-4sdvh
Jul  3 10:34:20.816: INFO: Got endpoints: latency-svc-4sdvh [95.567326ms]
Jul  3 10:34:20.818: INFO: Created: latency-svc-n9fl8
Jul  3 10:34:20.822: INFO: Got endpoints: latency-svc-n9fl8 [95.716731ms]
Jul  3 10:34:20.825: INFO: Created: latency-svc-qlkg7
Jul  3 10:34:20.830: INFO: Got endpoints: latency-svc-qlkg7 [98.373698ms]
Jul  3 10:34:20.832: INFO: Created: latency-svc-mpp8x
Jul  3 10:34:20.835: INFO: Got endpoints: latency-svc-mpp8x [97.109924ms]
Jul  3 10:34:20.838: INFO: Created: latency-svc-mfmn7
Jul  3 10:34:20.842: INFO: Got endpoints: latency-svc-mfmn7 [96.169922ms]
Jul  3 10:34:20.844: INFO: Created: latency-svc-w7t49
Jul  3 10:34:20.850: INFO: Got endpoints: latency-svc-w7t49 [93.504109ms]
Jul  3 10:34:20.851: INFO: Created: latency-svc-tg4sf
Jul  3 10:34:20.856: INFO: Got endpoints: latency-svc-tg4sf [96.49361ms]
Jul  3 10:34:20.857: INFO: Created: latency-svc-lz6bk
Jul  3 10:34:20.863: INFO: Got endpoints: latency-svc-lz6bk [96.344798ms]
Jul  3 10:34:20.863: INFO: Created: latency-svc-94244
Jul  3 10:34:20.869: INFO: Got endpoints: latency-svc-94244 [96.071565ms]
Jul  3 10:34:20.871: INFO: Created: latency-svc-rmwbk
Jul  3 10:34:20.875: INFO: Got endpoints: latency-svc-rmwbk [95.412934ms]
Jul  3 10:34:20.877: INFO: Created: latency-svc-h5pk4
Jul  3 10:34:20.883: INFO: Created: latency-svc-rssn2
Jul  3 10:34:20.888: INFO: Created: latency-svc-sbsgf
Jul  3 10:34:20.894: INFO: Created: latency-svc-hnpqm
Jul  3 10:34:20.899: INFO: Created: latency-svc-mv66k
Jul  3 10:34:20.902: INFO: Got endpoints: latency-svc-h5pk4 [115.328697ms]
Jul  3 10:34:20.904: INFO: Created: latency-svc-lmpdp
Jul  3 10:34:20.909: INFO: Created: latency-svc-j2j2r
Jul  3 10:34:20.915: INFO: Created: latency-svc-dg2sz
Jul  3 10:34:20.920: INFO: Created: latency-svc-4p5r5
Jul  3 10:34:20.925: INFO: Created: latency-svc-rxw9p
Jul  3 10:34:20.932: INFO: Created: latency-svc-kzcvv
Jul  3 10:34:20.938: INFO: Created: latency-svc-6cjn5
Jul  3 10:34:20.943: INFO: Created: latency-svc-lf7cv
Jul  3 10:34:20.949: INFO: Created: latency-svc-xkgkn
Jul  3 10:34:20.953: INFO: Got endpoints: latency-svc-rssn2 [159.031974ms]
Jul  3 10:34:20.955: INFO: Created: latency-svc-glpz7
Jul  3 10:34:20.961: INFO: Created: latency-svc-wsn5h
Jul  3 10:34:20.966: INFO: Created: latency-svc-2scst
Jul  3 10:34:21.003: INFO: Got endpoints: latency-svc-sbsgf [206.133395ms]
Jul  3 10:34:21.013: INFO: Created: latency-svc-5gw8q
Jul  3 10:34:21.053: INFO: Got endpoints: latency-svc-hnpqm [250.0122ms]
Jul  3 10:34:21.062: INFO: Created: latency-svc-v8htg
Jul  3 10:34:21.102: INFO: Got endpoints: latency-svc-mv66k [292.751409ms]
Jul  3 10:34:21.113: INFO: Created: latency-svc-z9rjd
Jul  3 10:34:21.152: INFO: Got endpoints: latency-svc-lmpdp [336.73953ms]
Jul  3 10:34:21.161: INFO: Created: latency-svc-7gnhq
Jul  3 10:34:21.202: INFO: Got endpoints: latency-svc-j2j2r [380.147554ms]
Jul  3 10:34:21.211: INFO: Created: latency-svc-jdjh9
Jul  3 10:34:21.252: INFO: Got endpoints: latency-svc-dg2sz [422.0937ms]
Jul  3 10:34:21.262: INFO: Created: latency-svc-5jhx4
Jul  3 10:34:21.302: INFO: Got endpoints: latency-svc-4p5r5 [467.220236ms]
Jul  3 10:34:21.310: INFO: Created: latency-svc-qwd5t
Jul  3 10:34:21.353: INFO: Got endpoints: latency-svc-rxw9p [510.88032ms]
Jul  3 10:34:21.362: INFO: Created: latency-svc-t67tm
Jul  3 10:34:21.403: INFO: Got endpoints: latency-svc-kzcvv [553.241227ms]
Jul  3 10:34:21.414: INFO: Created: latency-svc-h4qrs
Jul  3 10:34:21.453: INFO: Got endpoints: latency-svc-6cjn5 [596.825457ms]
Jul  3 10:34:21.462: INFO: Created: latency-svc-vxrg9
Jul  3 10:34:21.506: INFO: Got endpoints: latency-svc-lf7cv [642.780841ms]
Jul  3 10:34:21.519: INFO: Created: latency-svc-wdrh7
Jul  3 10:34:21.553: INFO: Got endpoints: latency-svc-xkgkn [683.295181ms]
Jul  3 10:34:21.562: INFO: Created: latency-svc-jb4s7
Jul  3 10:34:21.602: INFO: Got endpoints: latency-svc-glpz7 [727.745843ms]
Jul  3 10:34:21.611: INFO: Created: latency-svc-l2fch
Jul  3 10:34:21.652: INFO: Got endpoints: latency-svc-wsn5h [750.758336ms]
Jul  3 10:34:21.662: INFO: Created: latency-svc-lc2pc
Jul  3 10:34:21.702: INFO: Got endpoints: latency-svc-2scst [748.889878ms]
Jul  3 10:34:21.712: INFO: Created: latency-svc-mq9mx
Jul  3 10:34:21.754: INFO: Got endpoints: latency-svc-5gw8q [750.681484ms]
Jul  3 10:34:21.767: INFO: Created: latency-svc-7zt5k
Jul  3 10:34:21.802: INFO: Got endpoints: latency-svc-v8htg [748.865486ms]
Jul  3 10:34:21.811: INFO: Created: latency-svc-9x2sp
Jul  3 10:34:21.852: INFO: Got endpoints: latency-svc-z9rjd [749.900741ms]
Jul  3 10:34:21.861: INFO: Created: latency-svc-4pmkz
Jul  3 10:34:21.902: INFO: Got endpoints: latency-svc-7gnhq [749.665539ms]
Jul  3 10:34:21.913: INFO: Created: latency-svc-fr47b
Jul  3 10:34:21.953: INFO: Got endpoints: latency-svc-jdjh9 [750.465298ms]
Jul  3 10:34:21.962: INFO: Created: latency-svc-rn6q4
Jul  3 10:34:22.002: INFO: Got endpoints: latency-svc-5jhx4 [749.50161ms]
Jul  3 10:34:22.011: INFO: Created: latency-svc-5p26n
Jul  3 10:34:22.053: INFO: Got endpoints: latency-svc-qwd5t [750.392953ms]
Jul  3 10:34:22.062: INFO: Created: latency-svc-ktpmm
Jul  3 10:34:22.103: INFO: Got endpoints: latency-svc-t67tm [750.192626ms]
Jul  3 10:34:22.113: INFO: Created: latency-svc-xdbhw
Jul  3 10:34:22.152: INFO: Got endpoints: latency-svc-h4qrs [749.280351ms]
Jul  3 10:34:22.161: INFO: Created: latency-svc-fmj9w
Jul  3 10:34:22.202: INFO: Got endpoints: latency-svc-vxrg9 [749.816957ms]
Jul  3 10:34:22.212: INFO: Created: latency-svc-x8l6l
Jul  3 10:34:22.252: INFO: Got endpoints: latency-svc-wdrh7 [746.254511ms]
Jul  3 10:34:22.261: INFO: Created: latency-svc-wbjpt
Jul  3 10:34:22.301: INFO: Got endpoints: latency-svc-jb4s7 [748.81976ms]
Jul  3 10:34:22.311: INFO: Created: latency-svc-bllv7
Jul  3 10:34:22.352: INFO: Got endpoints: latency-svc-l2fch [749.830974ms]
Jul  3 10:34:22.361: INFO: Created: latency-svc-ptzdp
Jul  3 10:34:22.402: INFO: Got endpoints: latency-svc-lc2pc [749.737388ms]
Jul  3 10:34:22.412: INFO: Created: latency-svc-prr2k
Jul  3 10:34:22.453: INFO: Got endpoints: latency-svc-mq9mx [750.935917ms]
Jul  3 10:34:22.462: INFO: Created: latency-svc-cpwtq
Jul  3 10:34:22.502: INFO: Got endpoints: latency-svc-7zt5k [748.288557ms]
Jul  3 10:34:22.511: INFO: Created: latency-svc-5l4c5
Jul  3 10:34:22.554: INFO: Got endpoints: latency-svc-9x2sp [751.513292ms]
Jul  3 10:34:22.563: INFO: Created: latency-svc-sdzlx
Jul  3 10:34:22.603: INFO: Got endpoints: latency-svc-4pmkz [750.624084ms]
Jul  3 10:34:22.612: INFO: Created: latency-svc-692z7
Jul  3 10:34:22.652: INFO: Got endpoints: latency-svc-fr47b [750.244868ms]
Jul  3 10:34:22.663: INFO: Created: latency-svc-hmpcf
Jul  3 10:34:22.702: INFO: Got endpoints: latency-svc-rn6q4 [749.659664ms]
Jul  3 10:34:22.713: INFO: Created: latency-svc-cwrlc
Jul  3 10:34:22.753: INFO: Got endpoints: latency-svc-5p26n [751.078601ms]
Jul  3 10:34:22.762: INFO: Created: latency-svc-5hc2d
Jul  3 10:34:22.802: INFO: Got endpoints: latency-svc-ktpmm [749.590343ms]
Jul  3 10:34:22.811: INFO: Created: latency-svc-vsnv4
Jul  3 10:34:22.853: INFO: Got endpoints: latency-svc-xdbhw [750.009285ms]
Jul  3 10:34:22.869: INFO: Created: latency-svc-w9cwq
Jul  3 10:34:22.902: INFO: Got endpoints: latency-svc-fmj9w [749.69013ms]
Jul  3 10:34:22.911: INFO: Created: latency-svc-289g2
Jul  3 10:34:22.953: INFO: Got endpoints: latency-svc-x8l6l [750.196458ms]
Jul  3 10:34:22.961: INFO: Created: latency-svc-csrmc
Jul  3 10:34:23.002: INFO: Got endpoints: latency-svc-wbjpt [750.081428ms]
Jul  3 10:34:23.017: INFO: Created: latency-svc-7xgnx
Jul  3 10:34:23.052: INFO: Got endpoints: latency-svc-bllv7 [750.935787ms]
Jul  3 10:34:23.067: INFO: Created: latency-svc-xh2nq
Jul  3 10:34:23.102: INFO: Got endpoints: latency-svc-ptzdp [749.733592ms]
Jul  3 10:34:23.112: INFO: Created: latency-svc-jxc8q
Jul  3 10:34:23.153: INFO: Got endpoints: latency-svc-prr2k [750.680659ms]
Jul  3 10:34:23.161: INFO: Created: latency-svc-lgfz6
Jul  3 10:34:23.203: INFO: Got endpoints: latency-svc-cpwtq [749.766576ms]
Jul  3 10:34:23.212: INFO: Created: latency-svc-9kdt9
Jul  3 10:34:23.252: INFO: Got endpoints: latency-svc-5l4c5 [750.203529ms]
Jul  3 10:34:23.261: INFO: Created: latency-svc-vg6jx
Jul  3 10:34:23.303: INFO: Got endpoints: latency-svc-sdzlx [749.28606ms]
Jul  3 10:34:23.312: INFO: Created: latency-svc-l6sdn
Jul  3 10:34:23.352: INFO: Got endpoints: latency-svc-692z7 [749.347585ms]
Jul  3 10:34:23.362: INFO: Created: latency-svc-tdt4k
Jul  3 10:34:23.402: INFO: Got endpoints: latency-svc-hmpcf [749.831965ms]
Jul  3 10:34:23.412: INFO: Created: latency-svc-pdmpr
Jul  3 10:34:23.453: INFO: Got endpoints: latency-svc-cwrlc [750.762442ms]
Jul  3 10:34:23.462: INFO: Created: latency-svc-6qs6l
Jul  3 10:34:23.502: INFO: Got endpoints: latency-svc-5hc2d [749.298827ms]
Jul  3 10:34:23.511: INFO: Created: latency-svc-ctj4r
Jul  3 10:34:23.552: INFO: Got endpoints: latency-svc-vsnv4 [750.044768ms]
Jul  3 10:34:23.562: INFO: Created: latency-svc-jfdjz
Jul  3 10:34:23.603: INFO: Got endpoints: latency-svc-w9cwq [749.449148ms]
Jul  3 10:34:23.617: INFO: Created: latency-svc-djq9g
Jul  3 10:34:23.653: INFO: Got endpoints: latency-svc-289g2 [750.243413ms]
Jul  3 10:34:23.661: INFO: Created: latency-svc-m9b9v
Jul  3 10:34:23.702: INFO: Got endpoints: latency-svc-csrmc [749.338495ms]
Jul  3 10:34:23.712: INFO: Created: latency-svc-ssmxk
Jul  3 10:34:23.753: INFO: Got endpoints: latency-svc-7xgnx [750.066887ms]
Jul  3 10:34:23.762: INFO: Created: latency-svc-w8j2z
Jul  3 10:34:23.803: INFO: Got endpoints: latency-svc-xh2nq [750.166736ms]
Jul  3 10:34:23.811: INFO: Created: latency-svc-qd76d
Jul  3 10:34:23.852: INFO: Got endpoints: latency-svc-jxc8q [750.117739ms]
Jul  3 10:34:23.862: INFO: Created: latency-svc-8m82s
Jul  3 10:34:23.902: INFO: Got endpoints: latency-svc-lgfz6 [749.270052ms]
Jul  3 10:34:23.911: INFO: Created: latency-svc-2fptm
Jul  3 10:34:23.952: INFO: Got endpoints: latency-svc-9kdt9 [749.54502ms]
Jul  3 10:34:23.962: INFO: Created: latency-svc-d8qsn
Jul  3 10:34:24.003: INFO: Got endpoints: latency-svc-vg6jx [750.227022ms]
Jul  3 10:34:24.012: INFO: Created: latency-svc-qpwnr
Jul  3 10:34:24.053: INFO: Got endpoints: latency-svc-l6sdn [749.645002ms]
Jul  3 10:34:24.061: INFO: Created: latency-svc-88cdc
Jul  3 10:34:24.103: INFO: Got endpoints: latency-svc-tdt4k [750.162236ms]
Jul  3 10:34:24.114: INFO: Created: latency-svc-sjx4x
Jul  3 10:34:24.152: INFO: Got endpoints: latency-svc-pdmpr [749.847821ms]
Jul  3 10:34:24.160: INFO: Created: latency-svc-482qc
Jul  3 10:34:24.202: INFO: Got endpoints: latency-svc-6qs6l [748.936273ms]
Jul  3 10:34:24.214: INFO: Created: latency-svc-xjjsp
Jul  3 10:34:24.252: INFO: Got endpoints: latency-svc-ctj4r [749.695972ms]
Jul  3 10:34:24.261: INFO: Created: latency-svc-6dq59
Jul  3 10:34:24.302: INFO: Got endpoints: latency-svc-jfdjz [749.345121ms]
Jul  3 10:34:24.311: INFO: Created: latency-svc-vl26j
Jul  3 10:34:24.352: INFO: Got endpoints: latency-svc-djq9g [749.075076ms]
Jul  3 10:34:24.361: INFO: Created: latency-svc-qzpgs
Jul  3 10:34:24.402: INFO: Got endpoints: latency-svc-m9b9v [749.709032ms]
Jul  3 10:34:24.411: INFO: Created: latency-svc-79q5q
Jul  3 10:34:24.452: INFO: Got endpoints: latency-svc-ssmxk [750.040817ms]
Jul  3 10:34:24.463: INFO: Created: latency-svc-gr854
Jul  3 10:34:24.502: INFO: Got endpoints: latency-svc-w8j2z [749.712585ms]
Jul  3 10:34:24.511: INFO: Created: latency-svc-69jq4
Jul  3 10:34:24.552: INFO: Got endpoints: latency-svc-qd76d [749.621717ms]
Jul  3 10:34:24.561: INFO: Created: latency-svc-kbljm
Jul  3 10:34:24.603: INFO: Got endpoints: latency-svc-8m82s [750.105482ms]
Jul  3 10:34:24.618: INFO: Created: latency-svc-2mfx6
Jul  3 10:34:24.652: INFO: Got endpoints: latency-svc-2fptm [749.899348ms]
Jul  3 10:34:24.661: INFO: Created: latency-svc-n9spb
Jul  3 10:34:24.702: INFO: Got endpoints: latency-svc-d8qsn [749.44923ms]
Jul  3 10:34:24.711: INFO: Created: latency-svc-h8z6n
Jul  3 10:34:24.753: INFO: Got endpoints: latency-svc-qpwnr [750.23749ms]
Jul  3 10:34:24.762: INFO: Created: latency-svc-64mqh
Jul  3 10:34:24.802: INFO: Got endpoints: latency-svc-88cdc [749.522726ms]
Jul  3 10:34:24.811: INFO: Created: latency-svc-782mb
Jul  3 10:34:24.852: INFO: Got endpoints: latency-svc-sjx4x [749.856867ms]
Jul  3 10:34:24.862: INFO: Created: latency-svc-6zscd
Jul  3 10:34:24.903: INFO: Got endpoints: latency-svc-482qc [750.426128ms]
Jul  3 10:34:24.913: INFO: Created: latency-svc-whwxd
Jul  3 10:34:24.953: INFO: Got endpoints: latency-svc-xjjsp [750.342597ms]
Jul  3 10:34:24.963: INFO: Created: latency-svc-krpcp
Jul  3 10:34:25.002: INFO: Got endpoints: latency-svc-6dq59 [750.332613ms]
Jul  3 10:34:25.019: INFO: Created: latency-svc-6nnv4
Jul  3 10:34:25.062: INFO: Got endpoints: latency-svc-vl26j [759.632284ms]
Jul  3 10:34:25.090: INFO: Created: latency-svc-wfgtf
Jul  3 10:34:25.102: INFO: Got endpoints: latency-svc-qzpgs [750.637701ms]
Jul  3 10:34:25.113: INFO: Created: latency-svc-pddhb
Jul  3 10:34:25.153: INFO: Got endpoints: latency-svc-79q5q [750.387095ms]
Jul  3 10:34:25.163: INFO: Created: latency-svc-hx5wg
Jul  3 10:34:25.202: INFO: Got endpoints: latency-svc-gr854 [749.859697ms]
Jul  3 10:34:25.211: INFO: Created: latency-svc-rn725
Jul  3 10:34:25.252: INFO: Got endpoints: latency-svc-69jq4 [749.653581ms]
Jul  3 10:34:25.261: INFO: Created: latency-svc-ldwrc
Jul  3 10:34:25.302: INFO: Got endpoints: latency-svc-kbljm [749.882655ms]
Jul  3 10:34:25.316: INFO: Created: latency-svc-5k7w6
Jul  3 10:34:25.352: INFO: Got endpoints: latency-svc-2mfx6 [749.662048ms]
Jul  3 10:34:25.361: INFO: Created: latency-svc-smlrl
Jul  3 10:34:25.402: INFO: Got endpoints: latency-svc-n9spb [750.066145ms]
Jul  3 10:34:25.412: INFO: Created: latency-svc-f7vdt
Jul  3 10:34:25.453: INFO: Got endpoints: latency-svc-h8z6n [750.32298ms]
Jul  3 10:34:25.462: INFO: Created: latency-svc-7z65s
Jul  3 10:34:25.502: INFO: Got endpoints: latency-svc-64mqh [749.130988ms]
Jul  3 10:34:25.511: INFO: Created: latency-svc-85rb8
Jul  3 10:34:25.553: INFO: Got endpoints: latency-svc-782mb [750.489906ms]
Jul  3 10:34:25.562: INFO: Created: latency-svc-gz4dt
Jul  3 10:34:25.604: INFO: Got endpoints: latency-svc-6zscd [751.24288ms]
Jul  3 10:34:25.614: INFO: Created: latency-svc-5vvhl
Jul  3 10:34:25.652: INFO: Got endpoints: latency-svc-whwxd [749.507758ms]
Jul  3 10:34:25.661: INFO: Created: latency-svc-wgxwn
Jul  3 10:34:25.702: INFO: Got endpoints: latency-svc-krpcp [749.698308ms]
Jul  3 10:34:25.712: INFO: Created: latency-svc-58lhp
Jul  3 10:34:25.752: INFO: Got endpoints: latency-svc-6nnv4 [749.607301ms]
Jul  3 10:34:25.762: INFO: Created: latency-svc-l99gs
Jul  3 10:34:25.802: INFO: Got endpoints: latency-svc-wfgtf [740.450024ms]
Jul  3 10:34:25.811: INFO: Created: latency-svc-96d5g
Jul  3 10:34:25.853: INFO: Got endpoints: latency-svc-pddhb [750.275739ms]
Jul  3 10:34:25.863: INFO: Created: latency-svc-bt8v6
Jul  3 10:34:25.903: INFO: Got endpoints: latency-svc-hx5wg [750.542985ms]
Jul  3 10:34:25.914: INFO: Created: latency-svc-8prjl
Jul  3 10:34:25.955: INFO: Got endpoints: latency-svc-rn725 [752.923315ms]
Jul  3 10:34:25.981: INFO: Created: latency-svc-tr7xs
Jul  3 10:34:26.007: INFO: Got endpoints: latency-svc-ldwrc [754.575477ms]
Jul  3 10:34:26.027: INFO: Created: latency-svc-5nw4n
Jul  3 10:34:26.053: INFO: Got endpoints: latency-svc-5k7w6 [750.907893ms]
Jul  3 10:34:26.062: INFO: Created: latency-svc-mkwkm
Jul  3 10:34:26.102: INFO: Got endpoints: latency-svc-smlrl [749.655538ms]
Jul  3 10:34:26.111: INFO: Created: latency-svc-62tn2
Jul  3 10:34:26.153: INFO: Got endpoints: latency-svc-f7vdt [750.380207ms]
Jul  3 10:34:26.163: INFO: Created: latency-svc-k4j4l
Jul  3 10:34:26.202: INFO: Got endpoints: latency-svc-7z65s [749.678487ms]
Jul  3 10:34:26.212: INFO: Created: latency-svc-whckl
Jul  3 10:34:26.253: INFO: Got endpoints: latency-svc-85rb8 [750.563244ms]
Jul  3 10:34:26.262: INFO: Created: latency-svc-dqjqk
Jul  3 10:34:26.303: INFO: Got endpoints: latency-svc-gz4dt [750.602039ms]
Jul  3 10:34:26.313: INFO: Created: latency-svc-9lvvd
Jul  3 10:34:26.353: INFO: Got endpoints: latency-svc-5vvhl [749.637932ms]
Jul  3 10:34:26.363: INFO: Created: latency-svc-v8nj7
Jul  3 10:34:26.402: INFO: Got endpoints: latency-svc-wgxwn [750.078241ms]
Jul  3 10:34:26.411: INFO: Created: latency-svc-6cvt6
Jul  3 10:34:26.453: INFO: Got endpoints: latency-svc-58lhp [750.466266ms]
Jul  3 10:34:26.462: INFO: Created: latency-svc-q7dt8
Jul  3 10:34:26.503: INFO: Got endpoints: latency-svc-l99gs [751.360067ms]
Jul  3 10:34:26.515: INFO: Created: latency-svc-jvltg
Jul  3 10:34:26.552: INFO: Got endpoints: latency-svc-96d5g [750.013399ms]
Jul  3 10:34:26.561: INFO: Created: latency-svc-d4jvp
Jul  3 10:34:26.602: INFO: Got endpoints: latency-svc-bt8v6 [749.581713ms]
Jul  3 10:34:26.612: INFO: Created: latency-svc-xks7x
Jul  3 10:34:26.652: INFO: Got endpoints: latency-svc-8prjl [748.875786ms]
Jul  3 10:34:26.662: INFO: Created: latency-svc-djcxq
Jul  3 10:34:26.702: INFO: Got endpoints: latency-svc-tr7xs [747.256188ms]
Jul  3 10:34:26.712: INFO: Created: latency-svc-sn8cd
Jul  3 10:34:26.752: INFO: Got endpoints: latency-svc-5nw4n [745.657672ms]
Jul  3 10:34:26.762: INFO: Created: latency-svc-xf5z6
Jul  3 10:34:26.802: INFO: Got endpoints: latency-svc-mkwkm [749.170439ms]
Jul  3 10:34:26.815: INFO: Created: latency-svc-qqs2t
Jul  3 10:34:26.852: INFO: Got endpoints: latency-svc-62tn2 [750.13123ms]
Jul  3 10:34:26.861: INFO: Created: latency-svc-4qcdx
Jul  3 10:34:26.902: INFO: Got endpoints: latency-svc-k4j4l [749.159625ms]
Jul  3 10:34:26.912: INFO: Created: latency-svc-hv5ss
Jul  3 10:34:26.952: INFO: Got endpoints: latency-svc-whckl [749.760258ms]
Jul  3 10:34:26.962: INFO: Created: latency-svc-qb8mt
Jul  3 10:34:27.002: INFO: Got endpoints: latency-svc-dqjqk [749.251328ms]
Jul  3 10:34:27.011: INFO: Created: latency-svc-86h4t
Jul  3 10:34:27.053: INFO: Got endpoints: latency-svc-9lvvd [749.195545ms]
Jul  3 10:34:27.063: INFO: Created: latency-svc-rdg7w
Jul  3 10:34:27.103: INFO: Got endpoints: latency-svc-v8nj7 [749.386185ms]
Jul  3 10:34:27.112: INFO: Created: latency-svc-blnr8
Jul  3 10:34:27.152: INFO: Got endpoints: latency-svc-6cvt6 [749.660059ms]
Jul  3 10:34:27.161: INFO: Created: latency-svc-rhwzw
Jul  3 10:34:27.203: INFO: Got endpoints: latency-svc-q7dt8 [750.17768ms]
Jul  3 10:34:27.212: INFO: Created: latency-svc-g6hjr
Jul  3 10:34:27.252: INFO: Got endpoints: latency-svc-jvltg [748.669263ms]
Jul  3 10:34:27.263: INFO: Created: latency-svc-sqpvj
Jul  3 10:34:27.302: INFO: Got endpoints: latency-svc-d4jvp [749.962265ms]
Jul  3 10:34:27.311: INFO: Created: latency-svc-gv8dj
Jul  3 10:34:27.353: INFO: Got endpoints: latency-svc-xks7x [750.967552ms]
Jul  3 10:34:27.362: INFO: Created: latency-svc-ldc95
Jul  3 10:34:27.402: INFO: Got endpoints: latency-svc-djcxq [750.016684ms]
Jul  3 10:34:27.412: INFO: Created: latency-svc-z4zbt
Jul  3 10:34:27.452: INFO: Got endpoints: latency-svc-sn8cd [749.935093ms]
Jul  3 10:34:27.461: INFO: Created: latency-svc-5gplk
Jul  3 10:34:27.503: INFO: Got endpoints: latency-svc-xf5z6 [750.672404ms]
Jul  3 10:34:27.512: INFO: Created: latency-svc-p9hj4
Jul  3 10:34:27.552: INFO: Got endpoints: latency-svc-qqs2t [749.940872ms]
Jul  3 10:34:27.561: INFO: Created: latency-svc-vpd9f
Jul  3 10:34:27.602: INFO: Got endpoints: latency-svc-4qcdx [749.421829ms]
Jul  3 10:34:27.614: INFO: Created: latency-svc-rc24r
Jul  3 10:34:27.652: INFO: Got endpoints: latency-svc-hv5ss [749.932408ms]
Jul  3 10:34:27.661: INFO: Created: latency-svc-nrk5k
Jul  3 10:34:27.702: INFO: Got endpoints: latency-svc-qb8mt [749.895461ms]
Jul  3 10:34:27.710: INFO: Created: latency-svc-hl4jd
Jul  3 10:34:27.752: INFO: Got endpoints: latency-svc-86h4t [750.298224ms]
Jul  3 10:34:27.763: INFO: Created: latency-svc-vrmjx
Jul  3 10:34:27.802: INFO: Got endpoints: latency-svc-rdg7w [749.588093ms]
Jul  3 10:34:27.811: INFO: Created: latency-svc-8lh57
Jul  3 10:34:27.852: INFO: Got endpoints: latency-svc-blnr8 [748.965596ms]
Jul  3 10:34:27.861: INFO: Created: latency-svc-7fppj
Jul  3 10:34:27.903: INFO: Got endpoints: latency-svc-rhwzw [750.970356ms]
Jul  3 10:34:27.911: INFO: Created: latency-svc-q5c2t
Jul  3 10:34:27.952: INFO: Got endpoints: latency-svc-g6hjr [749.191162ms]
Jul  3 10:34:27.961: INFO: Created: latency-svc-n4x9c
Jul  3 10:34:28.002: INFO: Got endpoints: latency-svc-sqpvj [750.191017ms]
Jul  3 10:34:28.013: INFO: Created: latency-svc-zmvr7
Jul  3 10:34:28.052: INFO: Got endpoints: latency-svc-gv8dj [750.163563ms]
Jul  3 10:34:28.062: INFO: Created: latency-svc-6sfhr
Jul  3 10:34:28.103: INFO: Got endpoints: latency-svc-ldc95 [749.440526ms]
Jul  3 10:34:28.112: INFO: Created: latency-svc-hrpll
Jul  3 10:34:28.153: INFO: Got endpoints: latency-svc-z4zbt [750.662641ms]
Jul  3 10:34:28.162: INFO: Created: latency-svc-rz5hc
Jul  3 10:34:28.202: INFO: Got endpoints: latency-svc-5gplk [750.019568ms]
Jul  3 10:34:28.211: INFO: Created: latency-svc-b8gfc
Jul  3 10:34:28.252: INFO: Got endpoints: latency-svc-p9hj4 [749.08969ms]
Jul  3 10:34:28.261: INFO: Created: latency-svc-cn2jj
Jul  3 10:34:28.303: INFO: Got endpoints: latency-svc-vpd9f [751.049998ms]
Jul  3 10:34:28.312: INFO: Created: latency-svc-cgczq
Jul  3 10:34:28.352: INFO: Got endpoints: latency-svc-rc24r [750.02612ms]
Jul  3 10:34:28.362: INFO: Created: latency-svc-5nnfn
Jul  3 10:34:28.402: INFO: Got endpoints: latency-svc-nrk5k [749.745797ms]
Jul  3 10:34:28.411: INFO: Created: latency-svc-fbkfm
Jul  3 10:34:28.452: INFO: Got endpoints: latency-svc-hl4jd [750.357299ms]
Jul  3 10:34:28.461: INFO: Created: latency-svc-vrxzg
Jul  3 10:34:28.502: INFO: Got endpoints: latency-svc-vrmjx [749.774639ms]
Jul  3 10:34:28.553: INFO: Got endpoints: latency-svc-8lh57 [750.510611ms]
Jul  3 10:34:28.602: INFO: Got endpoints: latency-svc-7fppj [750.282512ms]
Jul  3 10:34:28.653: INFO: Got endpoints: latency-svc-q5c2t [749.455614ms]
Jul  3 10:34:28.702: INFO: Got endpoints: latency-svc-n4x9c [749.813078ms]
Jul  3 10:34:28.752: INFO: Got endpoints: latency-svc-zmvr7 [749.847061ms]
Jul  3 10:34:28.803: INFO: Got endpoints: latency-svc-6sfhr [749.884632ms]
Jul  3 10:34:28.853: INFO: Got endpoints: latency-svc-hrpll [750.540527ms]
Jul  3 10:34:28.903: INFO: Got endpoints: latency-svc-rz5hc [750.012964ms]
Jul  3 10:34:28.953: INFO: Got endpoints: latency-svc-b8gfc [750.537488ms]
Jul  3 10:34:29.004: INFO: Got endpoints: latency-svc-cn2jj [751.865533ms]
Jul  3 10:34:29.053: INFO: Got endpoints: latency-svc-cgczq [749.591075ms]
Jul  3 10:34:29.102: INFO: Got endpoints: latency-svc-5nnfn [750.206665ms]
Jul  3 10:34:29.153: INFO: Got endpoints: latency-svc-fbkfm [751.065462ms]
Jul  3 10:34:29.202: INFO: Got endpoints: latency-svc-vrxzg [749.762046ms]
Jul  3 10:34:29.202: INFO: Latencies: [13.90161ms 19.833978ms 24.645521ms 32.275858ms 39.4589ms 46.606037ms 51.545612ms 60.191686ms 65.475045ms 71.470607ms 77.436047ms 83.416949ms 91.016669ms 93.504109ms 94.656822ms 95.412934ms 95.567326ms 95.693007ms 95.716731ms 96.071565ms 96.169922ms 96.344798ms 96.49361ms 96.975171ms 97.109924ms 98.373698ms 98.884372ms 99.000652ms 99.525134ms 100.042716ms 100.226605ms 101.823891ms 104.721925ms 115.328697ms 159.031974ms 206.133395ms 250.0122ms 292.751409ms 336.73953ms 380.147554ms 422.0937ms 467.220236ms 510.88032ms 553.241227ms 596.825457ms 642.780841ms 683.295181ms 727.745843ms 740.450024ms 745.657672ms 746.254511ms 747.256188ms 748.288557ms 748.669263ms 748.81976ms 748.865486ms 748.875786ms 748.889878ms 748.936273ms 748.965596ms 749.075076ms 749.08969ms 749.130988ms 749.159625ms 749.170439ms 749.191162ms 749.195545ms 749.251328ms 749.270052ms 749.280351ms 749.28606ms 749.298827ms 749.338495ms 749.345121ms 749.347585ms 749.386185ms 749.421829ms 749.440526ms 749.449148ms 749.44923ms 749.455614ms 749.50161ms 749.507758ms 749.522726ms 749.54502ms 749.581713ms 749.588093ms 749.590343ms 749.591075ms 749.607301ms 749.621717ms 749.637932ms 749.645002ms 749.653581ms 749.655538ms 749.659664ms 749.660059ms 749.662048ms 749.665539ms 749.678487ms 749.69013ms 749.695972ms 749.698308ms 749.709032ms 749.712585ms 749.733592ms 749.737388ms 749.745797ms 749.760258ms 749.762046ms 749.766576ms 749.774639ms 749.813078ms 749.816957ms 749.830974ms 749.831965ms 749.847061ms 749.847821ms 749.856867ms 749.859697ms 749.882655ms 749.884632ms 749.895461ms 749.899348ms 749.900741ms 749.932408ms 749.935093ms 749.940872ms 749.962265ms 750.009285ms 750.012964ms 750.013399ms 750.016684ms 750.019568ms 750.02612ms 750.040817ms 750.044768ms 750.066145ms 750.066887ms 750.078241ms 750.081428ms 750.105482ms 750.117739ms 750.13123ms 750.162236ms 750.163563ms 750.166736ms 750.17768ms 750.191017ms 750.192626ms 750.196458ms 750.203529ms 750.206665ms 750.227022ms 750.23749ms 750.243413ms 750.244868ms 750.275739ms 750.282512ms 750.298224ms 750.32298ms 750.332613ms 750.342597ms 750.357299ms 750.380207ms 750.387095ms 750.392953ms 750.426128ms 750.465298ms 750.466266ms 750.489906ms 750.510611ms 750.537488ms 750.540527ms 750.542985ms 750.563244ms 750.602039ms 750.624084ms 750.637701ms 750.662641ms 750.672404ms 750.680659ms 750.681484ms 750.758336ms 750.762442ms 750.907893ms 750.935787ms 750.935917ms 750.967552ms 750.970356ms 751.049998ms 751.065462ms 751.078601ms 751.24288ms 751.360067ms 751.513292ms 751.865533ms 752.923315ms 754.575477ms 759.632284ms]
Jul  3 10:34:29.202: INFO: 50 %ile: 749.69013ms
Jul  3 10:34:29.202: INFO: 90 %ile: 750.672404ms
Jul  3 10:34:29.203: INFO: 99 %ile: 754.575477ms
Jul  3 10:34:29.203: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:34:29.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-6034" for this suite.
Jul  3 10:34:39.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:34:39.368: INFO: namespace svc-latency-6034 deletion completed in 10.160904334s

• [SLOW TEST:20.920 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:34:39.368: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-85pz
STEP: Creating a pod to test atomic-volume-subpath
Jul  3 10:34:39.420: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-85pz" in namespace "subpath-2321" to be "success or failure"
Jul  3 10:34:39.425: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766486ms
Jul  3 10:34:41.430: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009389309s
Jul  3 10:34:43.434: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 4.01385402s
Jul  3 10:34:45.438: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 6.017881602s
Jul  3 10:34:47.443: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 8.022403829s
Jul  3 10:34:49.447: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 10.026774681s
Jul  3 10:34:51.452: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 12.031794858s
Jul  3 10:34:53.456: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 14.036031169s
Jul  3 10:34:55.460: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 16.040138433s
Jul  3 10:34:57.465: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 18.044535751s
Jul  3 10:34:59.469: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 20.048769671s
Jul  3 10:35:01.478: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Running", Reason="", readiness=true. Elapsed: 22.058111889s
Jul  3 10:35:03.483: INFO: Pod "pod-subpath-test-projected-85pz": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.06273821s
STEP: Saw pod success
Jul  3 10:35:03.483: INFO: Pod "pod-subpath-test-projected-85pz" satisfied condition "success or failure"
Jul  3 10:35:03.487: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-subpath-test-projected-85pz container test-container-subpath-projected-85pz: <nil>
STEP: delete the pod
Jul  3 10:35:03.578: INFO: Waiting for pod pod-subpath-test-projected-85pz to disappear
Jul  3 10:35:03.582: INFO: Pod pod-subpath-test-projected-85pz no longer exists
STEP: Deleting pod pod-subpath-test-projected-85pz
Jul  3 10:35:03.582: INFO: Deleting pod "pod-subpath-test-projected-85pz" in namespace "subpath-2321"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:35:03.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2321" for this suite.
Jul  3 10:35:09.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:35:09.748: INFO: namespace subpath-2321 deletion completed in 6.158475349s

• [SLOW TEST:30.380 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:35:09.749: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul  3 10:35:15.841: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:15.845: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:17.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:17.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:19.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:19.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:21.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:21.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:23.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:23.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:25.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:25.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:27.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:27.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:29.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:29.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:31.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:31.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:33.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:33.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:35.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:35.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:37.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:37.850: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:39.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:39.849: INFO: Pod pod-with-prestop-exec-hook still exists
Jul  3 10:35:41.845: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul  3 10:35:41.849: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:35:41.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3410" for this suite.
Jul  3 10:36:03.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:36:04.047: INFO: namespace container-lifecycle-hook-3410 deletion completed in 22.173390433s

• [SLOW TEST:54.299 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:36:04.048: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:36:04.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "projected-1499" to be "success or failure"
Jul  3 10:36:04.095: INFO: Pod "downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.035295ms
Jul  3 10:36:06.100: INFO: Pod "downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009411293s
Jul  3 10:36:08.104: INFO: Pod "downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013853615s
STEP: Saw pod success
Jul  3 10:36:08.104: INFO: Pod "downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:36:08.108: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:36:08.130: INFO: Waiting for pod downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:36:08.133: INFO: Pod downwardapi-volume-5ba9d2f2-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:36:08.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1499" for this suite.
Jul  3 10:36:14.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:36:14.290: INFO: namespace projected-1499 deletion completed in 6.152443562s

• [SLOW TEST:10.242 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:36:14.290: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Jul  3 10:36:14.333: INFO: Waiting up to 5m0s for pod "var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "var-expansion-3900" to be "success or failure"
Jul  3 10:36:14.338: INFO: Pod "var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.218029ms
Jul  3 10:36:16.342: INFO: Pod "var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008303988s
Jul  3 10:36:18.346: INFO: Pod "var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012429948s
STEP: Saw pod success
Jul  3 10:36:18.346: INFO: Pod "var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:36:18.349: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 10:36:18.381: INFO: Waiting for pod var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:36:18.384: INFO: Pod var-expansion-61c4eeeb-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:36:18.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3900" for this suite.
Jul  3 10:36:24.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:36:24.543: INFO: namespace var-expansion-3900 deletion completed in 6.154653304s

• [SLOW TEST:10.254 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:36:24.544: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Jul  3 10:36:28.605: INFO: Pod pod-hostip-67e16813-9d7e-11e9-9a35-2ab8d9f547c5 has hostIP: 172.31.22.121
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:36:28.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2711" for this suite.
Jul  3 10:36:50.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:36:50.764: INFO: namespace pods-2711 deletion completed in 22.15425422s

• [SLOW TEST:26.220 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:36:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-778214a7-9d7e-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:36:50.810: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "projected-5112" to be "success or failure"
Jul  3 10:36:50.814: INFO: Pod "pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.433371ms
Jul  3 10:36:52.819: INFO: Pod "pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008745399s
Jul  3 10:36:54.822: INFO: Pod "pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012670888s
STEP: Saw pod success
Jul  3 10:36:54.823: INFO: Pod "pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:36:54.826: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:36:54.849: INFO: Waiting for pod pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:36:54.852: INFO: Pod pod-projected-secrets-778315d5-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:36:54.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5112" for this suite.
Jul  3 10:37:00.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:37:01.027: INFO: namespace projected-5112 deletion completed in 6.170432572s

• [SLOW TEST:10.263 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:37:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  3 10:37:01.060: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  3 10:37:01.069: INFO: Waiting for terminating namespaces to be deleted...
Jul  3 10:37:01.072: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-22-121.eu-central-1.compute.internal before test
Jul  3 10:37:01.085: INFO: kube-proxy-6k2j2 from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:37:01.085: INFO: canal-7p6kq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (3 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:37:01.085: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:37:01.085: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:37:01.085: INFO: restic-tx9l2 from velero started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:37:01.085: INFO: coredns-6bd858f7c-4g2hs from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:37:01.085: INFO: coredns-6bd858f7c-6f8lz from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:37:01.085: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-03 09:48:02 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  3 10:37:01.085: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-275sw from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:37:01.085: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:37:01.085: INFO: node-exporter-bxmkq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.085: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:37:01.085: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-23-251.eu-central-1.compute.internal before test
Jul  3 10:37:01.175: INFO: kubernetes-dashboard-57b5ff8798-hk65d from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul  3 10:37:01.175: INFO: restic-qcb2x from velero started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:37:01.175: INFO: openvpn-client-5f9d55d8df-st4fs from kube-system started at 2019-07-02 09:14:20 +0000 UTC (2 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container dnat-controller ready: true, restart count 0
Jul  3 10:37:01.175: INFO: 	Container openvpn-client ready: true, restart count 0
Jul  3 10:37:01.175: INFO: webterminal-c87695794-xnzsz from webterminal started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container webterminal ready: true, restart count 0
Jul  3 10:37:01.175: INFO: canal-qkzt7 from kube-system started at 2019-07-02 09:13:50 +0000 UTC (3 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:37:01.175: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:37:01.175: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:37:01.175: INFO: kube-proxy-clv5z from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:37:01.175: INFO: node-exporter-95rzz from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:37:01.175: INFO: tiller-deploy-794d4bbf57-bzs9k from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container tiller ready: true, restart count 0
Jul  3 10:37:01.175: INFO: velero-6bff998df8-65rvl from velero started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:37:01.175: INFO: cluster-autoscaler-c8c56b5d9-vmxpf from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Jul  3 10:37:01.175: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-gv8pj from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:37:01.175: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:37:01.175: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:37:01.175: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-26-241.eu-central-1.compute.internal before test
Jul  3 10:37:01.251: INFO: canal-rgl2j from kube-system started at 2019-07-02 09:14:10 +0000 UTC (3 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:37:01.251: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:37:01.251: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:37:01.251: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-bhvm7 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:37:01.251: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:37:01.251: INFO: restic-tbbgk from velero started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:37:01.251: INFO: sonobuoy-e2e-job-9f2f9fa81a324834 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container e2e ready: true, restart count 0
Jul  3 10:37:01.251: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:37:01.251: INFO: kube-proxy-w9fsl from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:37:01.251: INFO: node-exporter-qkmqw from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:37:01.251: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-8024b324-9d7e-11e9-9a35-2ab8d9f547c5 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-8024b324-9d7e-11e9-9a35-2ab8d9f547c5 off the node ip-172-31-22-121.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label kubernetes.io/e2e-8024b324-9d7e-11e9-9a35-2ab8d9f547c5
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:37:09.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-811" for this suite.
Jul  3 10:37:17.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:37:17.497: INFO: namespace sched-pred-811 deletion completed in 8.158846078s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.470 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:37:17.497: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7679
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  3 10:37:17.531: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  3 10:37:41.627: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.2.39 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7679 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:37:41.627: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:37:43.238: INFO: Found all expected endpoints: [netserver-0]
Jul  3 10:37:43.242: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.1.134 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7679 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:37:43.242: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:37:44.862: INFO: Found all expected endpoints: [netserver-1]
Jul  3 10:37:44.866: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.25.0.37 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7679 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:37:44.866: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:37:46.520: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:37:46.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7679" for this suite.
Jul  3 10:38:08.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:38:08.689: INFO: namespace pod-network-test-7679 deletion completed in 22.164130266s

• [SLOW TEST:51.192 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:38:08.690: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:38:08.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-3992" to be "success or failure"
Jul  3 10:38:08.740: INFO: Pod "downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.10096ms
Jul  3 10:38:10.746: INFO: Pod "downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012121956s
Jul  3 10:38:12.751: INFO: Pod "downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016920588s
STEP: Saw pod success
Jul  3 10:38:12.751: INFO: Pod "downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:38:12.754: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:38:12.780: INFO: Waiting for pod downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:38:12.783: INFO: Pod downwardapi-volume-a5f4f50c-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:38:12.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3992" for this suite.
Jul  3 10:38:18.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:38:18.952: INFO: namespace downward-api-3992 deletion completed in 6.164152854s

• [SLOW TEST:10.262 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:38:18.952: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Jul  3 10:38:19.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-5304'
Jul  3 10:38:19.267: INFO: stderr: ""
Jul  3 10:38:19.267: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul  3 10:38:20.271: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 10:38:20.271: INFO: Found 0 / 1
Jul  3 10:38:21.271: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 10:38:21.272: INFO: Found 0 / 1
Jul  3 10:38:22.272: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 10:38:22.272: INFO: Found 1 / 1
Jul  3 10:38:22.272: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul  3 10:38:22.275: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 10:38:22.275: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul  3 10:38:22.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 patch pod redis-master-gsbz9 --namespace=kubectl-5304 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul  3 10:38:22.358: INFO: stderr: ""
Jul  3 10:38:22.358: INFO: stdout: "pod/redis-master-gsbz9 patched\n"
STEP: checking annotations
Jul  3 10:38:22.362: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 10:38:22.362: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:38:22.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5304" for this suite.
Jul  3 10:38:44.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:38:44.535: INFO: namespace kubectl-5304 deletion completed in 22.167896374s

• [SLOW TEST:25.583 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:38:44.535: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:38:44.582: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul  3 10:38:49.587: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  3 10:38:49.587: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  3 10:38:49.607: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5627,SelfLink:/apis/apps/v1/namespaces/deployment-5627/deployments/test-cleanup-deployment,UID:be50d833-9d7e-11e9-9bcb-a6641e9563f8,ResourceVersion:296872,Generation:1,CreationTimestamp:2019-07-03 10:38:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jul  3 10:38:49.613: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-5627,SelfLink:/apis/apps/v1/namespaces/deployment-5627/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:be533b9a-9d7e-11e9-9413-46c858ac61fd,ResourceVersion:296874,Generation:1,CreationTimestamp:2019-07-03 10:38:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment be50d833-9d7e-11e9-9bcb-a6641e9563f8 0xc00245f017 0xc00245f018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 10:38:49.613: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jul  3 10:38:49.613: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-5627,SelfLink:/apis/apps/v1/namespaces/deployment-5627/replicasets/test-cleanup-controller,UID:bb52e02a-9d7e-11e9-9bcb-a6641e9563f8,ResourceVersion:296873,Generation:1,CreationTimestamp:2019-07-03 10:38:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment be50d833-9d7e-11e9-9bcb-a6641e9563f8 0xc00245ef47 0xc00245ef48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  3 10:38:49.618: INFO: Pod "test-cleanup-controller-85g8r" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-85g8r,GenerateName:test-cleanup-controller-,Namespace:deployment-5627,SelfLink:/api/v1/namespaces/deployment-5627/pods/test-cleanup-controller-85g8r,UID:bb54711b-9d7e-11e9-9413-46c858ac61fd,ResourceVersion:296858,Generation:0,CreationTimestamp:2019-07-03 10:38:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller bb52e02a-9d7e-11e9-9bcb-a6641e9563f8 0xc0032c1297 0xc0032c1298}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nvl2l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nvl2l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-nvl2l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-22-121.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032c1310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032c1330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:38:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:38:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:38:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:38:44 +0000 UTC  }],Message:,Reason:,HostIP:172.31.22.121,PodIP:172.25.1.137,StartTime:2019-07-03 10:38:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-03 10:38:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6a94e55950ada1d1c5baf6a03479401399652bf17483e5f5430b316e5faf9891}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul  3 10:38:49.619: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-lws25" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-lws25,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-5627,SelfLink:/api/v1/namespaces/deployment-5627/pods/test-cleanup-deployment-55cbfbc8f5-lws25,UID:be540335-9d7e-11e9-9413-46c858ac61fd,ResourceVersion:296875,Generation:0,CreationTimestamp:2019-07-03 10:38:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 be533b9a-9d7e-11e9-9413-46c858ac61fd 0xc0032c1417 0xc0032c1418}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nvl2l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nvl2l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-nvl2l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0032c1480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0032c14a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:38:49.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5627" for this suite.
Jul  3 10:38:55.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:38:55.785: INFO: namespace deployment-5627 deletion completed in 6.161727978s

• [SLOW TEST:11.250 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:38:55.786: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-c207b6e3-9d7e-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-c207b6e3-9d7e-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:40:06.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9846" for this suite.
Jul  3 10:40:28.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:40:28.673: INFO: namespace projected-9846 deletion completed in 22.158271408s

• [SLOW TEST:92.887 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:40:28.673: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:40:28.716: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-3526" to be "success or failure"
Jul  3 10:40:28.721: INFO: Pod "downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.564401ms
Jul  3 10:40:30.725: INFO: Pod "downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009806586s
STEP: Saw pod success
Jul  3 10:40:30.726: INFO: Pod "downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:40:30.729: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:40:30.752: INFO: Waiting for pod downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:40:30.755: INFO: Pod downwardapi-volume-f9648505-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:40:30.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3526" for this suite.
Jul  3 10:40:36.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:40:36.912: INFO: namespace downward-api-3526 deletion completed in 6.15288327s

• [SLOW TEST:8.239 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:40:36.912: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:40:38.982: INFO: Waiting up to 5m0s for pod "client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5" in namespace "pods-8620" to be "success or failure"
Jul  3 10:40:38.986: INFO: Pod "client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.293491ms
Jul  3 10:40:40.991: INFO: Pod "client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008763312s
Jul  3 10:40:42.998: INFO: Pod "client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016012129s
STEP: Saw pod success
Jul  3 10:40:42.998: INFO: Pod "client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:40:43.003: INFO: Trying to get logs from node ip-172-31-26-241.eu-central-1.compute.internal pod client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5 container env3cont: <nil>
STEP: delete the pod
Jul  3 10:40:43.025: INFO: Waiting for pod client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:40:43.029: INFO: Pod client-envvars-ff836d98-9d7e-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:40:43.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8620" for this suite.
Jul  3 10:41:25.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:41:25.206: INFO: namespace pods-8620 deletion completed in 42.170941219s

• [SLOW TEST:48.294 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:41:25.206: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 10:41:25.241: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3192'
Jul  3 10:41:25.315: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  3 10:41:25.315: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Jul  3 10:41:25.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete jobs e2e-test-nginx-job --namespace=kubectl-3192'
Jul  3 10:41:25.398: INFO: stderr: ""
Jul  3 10:41:25.398: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:41:25.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3192" for this suite.
Jul  3 10:41:31.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:41:31.562: INFO: namespace kubectl-3192 deletion completed in 6.159591355s

• [SLOW TEST:6.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:41:31.562: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  3 10:41:31.595: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:41:35.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7917" for this suite.
Jul  3 10:41:41.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:41:41.494: INFO: namespace init-container-7917 deletion completed in 6.159484814s

• [SLOW TEST:9.932 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:41:41.494: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:41:41.553: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul  3 10:41:41.566: INFO: Number of nodes with available pods: 0
Jul  3 10:41:41.566: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul  3 10:41:41.588: INFO: Number of nodes with available pods: 0
Jul  3 10:41:41.588: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:42.592: INFO: Number of nodes with available pods: 0
Jul  3 10:41:42.592: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:43.592: INFO: Number of nodes with available pods: 0
Jul  3 10:41:43.592: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:44.592: INFO: Number of nodes with available pods: 0
Jul  3 10:41:44.592: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:45.592: INFO: Number of nodes with available pods: 1
Jul  3 10:41:45.592: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul  3 10:41:45.615: INFO: Number of nodes with available pods: 1
Jul  3 10:41:45.615: INFO: Number of running nodes: 0, number of available pods: 1
Jul  3 10:41:46.620: INFO: Number of nodes with available pods: 0
Jul  3 10:41:46.620: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul  3 10:41:46.631: INFO: Number of nodes with available pods: 0
Jul  3 10:41:46.631: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:47.635: INFO: Number of nodes with available pods: 0
Jul  3 10:41:47.635: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:48.636: INFO: Number of nodes with available pods: 0
Jul  3 10:41:48.636: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:49.636: INFO: Number of nodes with available pods: 0
Jul  3 10:41:49.636: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:50.635: INFO: Number of nodes with available pods: 0
Jul  3 10:41:50.635: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:51.636: INFO: Number of nodes with available pods: 0
Jul  3 10:41:51.636: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:52.635: INFO: Number of nodes with available pods: 0
Jul  3 10:41:52.635: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:53.635: INFO: Number of nodes with available pods: 0
Jul  3 10:41:53.635: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:54.636: INFO: Number of nodes with available pods: 0
Jul  3 10:41:54.636: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:41:55.635: INFO: Number of nodes with available pods: 1
Jul  3 10:41:55.635: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2180, will wait for the garbage collector to delete the pods
Jul  3 10:41:55.704: INFO: Deleting DaemonSet.extensions daemon-set took: 9.399281ms
Jul  3 10:41:56.205: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.252138ms
Jul  3 10:42:02.709: INFO: Number of nodes with available pods: 0
Jul  3 10:42:02.709: INFO: Number of running nodes: 0, number of available pods: 0
Jul  3 10:42:02.712: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2180/daemonsets","resourceVersion":"297734"},"items":null}

Jul  3 10:42:02.715: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2180/pods","resourceVersion":"297734"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:42:02.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2180" for this suite.
Jul  3 10:42:08.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:42:08.907: INFO: namespace daemonsets-2180 deletion completed in 6.163253489s

• [SLOW TEST:27.412 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:42:08.908: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-1
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1
STEP: Deleting pre-stop pod
Jul  3 10:42:24.079: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:42:24.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1" for this suite.
Jul  3 10:43:02.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:43:02.312: INFO: namespace prestop-1 deletion completed in 38.219490236s

• [SLOW TEST:53.404 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:43:02.312: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:43:02.345: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:43:06.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-311" for this suite.
Jul  3 10:43:58.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:43:58.884: INFO: namespace pods-311 deletion completed in 52.165398424s

• [SLOW TEST:56.572 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:43:58.885: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul  3 10:43:58.933: INFO: Waiting up to 5m0s for pod "pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-793" to be "success or failure"
Jul  3 10:43:58.938: INFO: Pod "pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.022376ms
Jul  3 10:44:00.942: INFO: Pod "pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009390008s
Jul  3 10:44:02.947: INFO: Pod "pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014101395s
STEP: Saw pod success
Jul  3 10:44:02.947: INFO: Pod "pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:44:02.950: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:44:02.975: INFO: Waiting for pod pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:44:02.979: INFO: Pod pod-76b0daba-9d7f-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:44:02.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-793" for this suite.
Jul  3 10:44:08.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:44:09.148: INFO: namespace emptydir-793 deletion completed in 6.165254427s

• [SLOW TEST:10.263 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:44:09.149: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Jul  3 10:44:09.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-1204'
Jul  3 10:44:09.474: INFO: stderr: ""
Jul  3 10:44:09.474: INFO: stdout: "pod/pause created\n"
Jul  3 10:44:09.474: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul  3 10:44:09.474: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1204" to be "running and ready"
Jul  3 10:44:09.481: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.980878ms
Jul  3 10:44:11.485: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011429673s
Jul  3 10:44:13.490: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.015809855s
Jul  3 10:44:13.490: INFO: Pod "pause" satisfied condition "running and ready"
Jul  3 10:44:13.490: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Jul  3 10:44:13.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 label pods pause testing-label=testing-label-value --namespace=kubectl-1204'
Jul  3 10:44:13.568: INFO: stderr: ""
Jul  3 10:44:13.568: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul  3 10:44:13.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pod pause -L testing-label --namespace=kubectl-1204'
Jul  3 10:44:13.633: INFO: stderr: ""
Jul  3 10:44:13.633: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul  3 10:44:13.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 label pods pause testing-label- --namespace=kubectl-1204'
Jul  3 10:44:13.707: INFO: stderr: ""
Jul  3 10:44:13.707: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul  3 10:44:13.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pod pause -L testing-label --namespace=kubectl-1204'
Jul  3 10:44:13.772: INFO: stderr: ""
Jul  3 10:44:13.772: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Jul  3 10:44:13.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-1204'
Jul  3 10:44:13.843: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 10:44:13.843: INFO: stdout: "pod \"pause\" force deleted\n"
Jul  3 10:44:13.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=pause --no-headers --namespace=kubectl-1204'
Jul  3 10:44:13.919: INFO: stderr: "No resources found.\n"
Jul  3 10:44:13.919: INFO: stdout: ""
Jul  3 10:44:13.919: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=pause --namespace=kubectl-1204 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 10:44:14.026: INFO: stderr: ""
Jul  3 10:44:14.026: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:44:14.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1204" for this suite.
Jul  3 10:44:20.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:44:20.218: INFO: namespace kubectl-1204 deletion completed in 6.186671898s

• [SLOW TEST:11.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:44:20.218: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-836749bb-9d7f-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:44:20.264: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5" in namespace "projected-9887" to be "success or failure"
Jul  3 10:44:20.270: INFO: Pod "pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.821589ms
Jul  3 10:44:22.274: INFO: Pod "pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010893513s
Jul  3 10:44:24.279: INFO: Pod "pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015208482s
STEP: Saw pod success
Jul  3 10:44:24.279: INFO: Pod "pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:44:24.282: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:44:24.304: INFO: Waiting for pod pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:44:24.307: INFO: Pod pod-projected-configmaps-83684fef-9d7f-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:44:24.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9887" for this suite.
Jul  3 10:44:30.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:44:30.529: INFO: namespace projected-9887 deletion completed in 6.216972565s

• [SLOW TEST:10.310 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:44:30.529: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul  3 10:44:35.608: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:44:36.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9088" for this suite.
Jul  3 10:44:58.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:44:58.795: INFO: namespace replicaset-9088 deletion completed in 22.162670095s

• [SLOW TEST:28.266 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:44:58.795: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-8538
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-8538
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8538
Jul  3 10:44:58.852: INFO: Found 0 stateful pods, waiting for 1
Jul  3 10:45:08.856: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul  3 10:45:08.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:45:09.568: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:45:09.568: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:45:09.568: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:45:09.572: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  3 10:45:19.577: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:45:19.577: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:45:19.594: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:19.594: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:19.594: INFO: 
Jul  3 10:45:19.594: INFO: StatefulSet ss has not reached scale 3, at 1
Jul  3 10:45:20.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995618775s
Jul  3 10:45:21.603: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990977193s
Jul  3 10:45:22.607: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98651581s
Jul  3 10:45:23.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98198091s
Jul  3 10:45:24.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977646616s
Jul  3 10:45:25.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.972520069s
Jul  3 10:45:26.626: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96770519s
Jul  3 10:45:27.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.96300989s
Jul  3 10:45:28.636: INFO: Verifying statefulset ss doesn't scale past 3 for another 958.098071ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8538
Jul  3 10:45:29.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:45:30.385: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:45:30.385: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:45:30.385: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:45:30.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:45:31.072: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  3 10:45:31.073: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:45:31.073: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:45:31.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:45:31.881: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul  3 10:45:31.881: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:45:31.881: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:45:31.885: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:45:31.885: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:45:31.885: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul  3 10:45:31.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:45:32.525: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:45:32.525: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:45:32.525: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:45:32.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:45:33.137: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:45:33.137: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:45:33.137: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:45:33.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-8538 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:45:33.862: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:45:33.862: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:45:33.862: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:45:33.862: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:45:33.866: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jul  3 10:45:43.874: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:45:43.874: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:45:43.874: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:45:43.887: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:43.887: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:43.887: INFO: ss-1  ip-172-31-26-241.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  }]
Jul  3 10:45:43.887: INFO: ss-2  ip-172-31-23-251.eu-central-1.compute.internal  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  }]
Jul  3 10:45:43.888: INFO: 
Jul  3 10:45:43.888: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  3 10:45:44.892: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:44.892: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:44.892: INFO: ss-1  ip-172-31-26-241.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  }]
Jul  3 10:45:44.892: INFO: ss-2  ip-172-31-23-251.eu-central-1.compute.internal  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  }]
Jul  3 10:45:44.892: INFO: 
Jul  3 10:45:44.892: INFO: StatefulSet ss has not reached scale 0, at 3
Jul  3 10:45:45.897: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:45.897: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:45.897: INFO: ss-1  ip-172-31-26-241.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:33 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:19 +0000 UTC  }]
Jul  3 10:45:45.897: INFO: 
Jul  3 10:45:45.897: INFO: StatefulSet ss has not reached scale 0, at 2
Jul  3 10:45:46.901: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:46.901: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:46.901: INFO: 
Jul  3 10:45:46.901: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:47.905: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:47.905: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:47.906: INFO: 
Jul  3 10:45:47.906: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:48.910: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:48.910: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:48.910: INFO: 
Jul  3 10:45:48.910: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:49.915: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:49.915: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:49.915: INFO: 
Jul  3 10:45:49.915: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:50.919: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:50.919: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:50.919: INFO: 
Jul  3 10:45:50.919: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:51.923: INFO: POD   NODE                                            PHASE    GRACE  CONDITIONS
Jul  3 10:45:51.924: INFO: ss-0  ip-172-31-22-121.eu-central-1.compute.internal  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:45:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:44:58 +0000 UTC  }]
Jul  3 10:45:51.924: INFO: 
Jul  3 10:45:51.924: INFO: StatefulSet ss has not reached scale 0, at 1
Jul  3 10:45:52.928: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.625282ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8538
Jul  3 10:45:53.933: INFO: Scaling statefulset ss to 0
Jul  3 10:45:53.944: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  3 10:45:53.947: INFO: Deleting all statefulset in ns statefulset-8538
Jul  3 10:45:53.950: INFO: Scaling statefulset ss to 0
Jul  3 10:45:53.961: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:45:53.964: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:45:53.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8538" for this suite.
Jul  3 10:45:59.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:46:00.183: INFO: namespace statefulset-8538 deletion completed in 6.200246378s

• [SLOW TEST:61.388 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:46:00.184: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Jul  3 10:46:00.219: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul  3 10:46:00.227: INFO: Waiting for terminating namespaces to be deleted...
Jul  3 10:46:00.231: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-22-121.eu-central-1.compute.internal before test
Jul  3 10:46:00.271: INFO: restic-tx9l2 from velero started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:46:00.271: INFO: coredns-6bd858f7c-4g2hs from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:46:00.271: INFO: node-exporter-bxmkq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:46:00.271: INFO: coredns-6bd858f7c-6f8lz from kube-system started at 2019-07-02 09:14:24 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container coredns ready: true, restart count 0
Jul  3 10:46:00.271: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-03 09:48:02 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul  3 10:46:00.271: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-275sw from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:46:00.271: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:46:00.271: INFO: canal-7p6kq from kube-system started at 2019-07-02 09:13:52 +0000 UTC (3 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:46:00.271: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:46:00.271: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:46:00.271: INFO: kube-proxy-6k2j2 from kube-system started at 2019-07-02 09:13:52 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.271: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:46:00.271: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-23-251.eu-central-1.compute.internal before test
Jul  3 10:46:00.365: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-gv8pj from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:46:00.365: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:46:00.365: INFO: 	Container systemd-logs ready: true, restart count 0
Jul  3 10:46:00.365: INFO: cluster-autoscaler-c8c56b5d9-vmxpf from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.365: INFO: 	Container cluster-autoscaler ready: true, restart count 0
Jul  3 10:46:00.365: INFO: kubernetes-dashboard-57b5ff8798-hk65d from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.365: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Jul  3 10:46:00.365: INFO: openvpn-client-5f9d55d8df-st4fs from kube-system started at 2019-07-02 09:14:20 +0000 UTC (2 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container dnat-controller ready: true, restart count 0
Jul  3 10:46:00.366: INFO: 	Container openvpn-client ready: true, restart count 0
Jul  3 10:46:00.366: INFO: webterminal-c87695794-xnzsz from webterminal started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container webterminal ready: true, restart count 0
Jul  3 10:46:00.366: INFO: restic-qcb2x from velero started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:46:00.366: INFO: kube-proxy-clv5z from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:46:00.366: INFO: node-exporter-95rzz from kube-system started at 2019-07-02 09:13:50 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:46:00.366: INFO: tiller-deploy-794d4bbf57-bzs9k from kube-system started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container tiller ready: true, restart count 0
Jul  3 10:46:00.366: INFO: velero-6bff998df8-65rvl from velero started at 2019-07-02 09:14:20 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:46:00.366: INFO: canal-qkzt7 from kube-system started at 2019-07-02 09:13:50 +0000 UTC (3 container statuses recorded)
Jul  3 10:46:00.366: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:46:00.366: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:46:00.366: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:46:00.366: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-26-241.eu-central-1.compute.internal before test
Jul  3 10:46:00.418: INFO: restic-tbbgk from velero started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container velero ready: true, restart count 0
Jul  3 10:46:00.418: INFO: sonobuoy-e2e-job-9f2f9fa81a324834 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container e2e ready: true, restart count 0
Jul  3 10:46:00.418: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:46:00.418: INFO: kube-proxy-w9fsl from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container kube-proxy ready: true, restart count 0
Jul  3 10:46:00.418: INFO: node-exporter-qkmqw from kube-system started at 2019-07-02 09:14:10 +0000 UTC (1 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container node-exporter ready: true, restart count 0
Jul  3 10:46:00.418: INFO: canal-rgl2j from kube-system started at 2019-07-02 09:14:10 +0000 UTC (3 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container calico-node ready: true, restart count 0
Jul  3 10:46:00.418: INFO: 	Container install-cni ready: true, restart count 0
Jul  3 10:46:00.418: INFO: 	Container kube-flannel ready: true, restart count 0
Jul  3 10:46:00.418: INFO: sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-bhvm7 from heptio-sonobuoy started at 2019-07-03 09:48:09 +0000 UTC (2 container statuses recorded)
Jul  3 10:46:00.418: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul  3 10:46:00.418: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-172-31-22-121.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-23-251.eu-central-1.compute.internal
STEP: verifying the node has the label node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod sonobuoy-e2e-job-9f2f9fa81a324834 requesting resource cpu=0m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-275sw requesting resource cpu=0m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-bhvm7 requesting resource cpu=0m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod sonobuoy-systemd-logs-daemon-set-40923c1d2b7c4c39-gv8pj requesting resource cpu=0m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod canal-7p6kq requesting resource cpu=350m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod canal-qkzt7 requesting resource cpu=350m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod canal-rgl2j requesting resource cpu=350m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod cluster-autoscaler-c8c56b5d9-vmxpf requesting resource cpu=10m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod coredns-6bd858f7c-4g2hs requesting resource cpu=100m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod coredns-6bd858f7c-6f8lz requesting resource cpu=100m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod kube-proxy-6k2j2 requesting resource cpu=75m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod kube-proxy-clv5z requesting resource cpu=75m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod kube-proxy-w9fsl requesting resource cpu=75m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod kubernetes-dashboard-57b5ff8798-hk65d requesting resource cpu=50m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod node-exporter-95rzz requesting resource cpu=3m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod node-exporter-bxmkq requesting resource cpu=3m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod node-exporter-qkmqw requesting resource cpu=3m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod openvpn-client-5f9d55d8df-st4fs requesting resource cpu=30m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod tiller-deploy-794d4bbf57-bzs9k requesting resource cpu=0m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod restic-qcb2x requesting resource cpu=5m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod restic-tbbgk requesting resource cpu=5m on Node ip-172-31-26-241.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod restic-tx9l2 requesting resource cpu=5m on Node ip-172-31-22-121.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod velero-6bff998df8-65rvl requesting resource cpu=10m on Node ip-172-31-23-251.eu-central-1.compute.internal
Jul  3 10:46:00.477: INFO: Pod webterminal-c87695794-xnzsz requesting resource cpu=0m on Node ip-172-31-23-251.eu-central-1.compute.internal
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5.15addfd70cb0e4fe], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2801/filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5 to ip-172-31-22-121.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5.15addfd75087378b], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5.15addfd754bb7e53], Reason = [Created], Message = [Created container filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5.15addfd761fdb6c0], Reason = [Started], Message = [Started container filler-pod-bf24b4c9-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5.15addfd70d0d5813], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2801/filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5 to ip-172-31-23-251.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5.15addfd7584960cb], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5.15addfd75cade2cd], Reason = [Created], Message = [Created container filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5.15addfd76d3c7e26], Reason = [Started], Message = [Started container filler-pod-bf263973-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5.15addfd70d8051e2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2801/filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5 to ip-172-31-26-241.eu-central-1.compute.internal]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5.15addfd76ca1662a], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5.15addfd770e7d3e5], Reason = [Created], Message = [Created container filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5.15addfd7816f99de], Reason = [Started], Message = [Started container filler-pod-bf273278-9d7f-11e9-9a35-2ab8d9f547c5]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15addfd7fd47d189], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-22-121.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-23-251.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-172-31-26-241.eu-central-1.compute.internal
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:46:05.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2801" for this suite.
Jul  3 10:46:11.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:46:11.806: INFO: namespace sched-pred-2801 deletion completed in 6.207393511s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.622 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:46:11.806: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7604
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-7604
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7604
Jul  3 10:46:11.931: INFO: Found 0 stateful pods, waiting for 1
Jul  3 10:46:21.936: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul  3 10:46:21.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:46:22.629: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:46:22.629: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:46:22.629: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:46:22.633: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul  3 10:46:32.639: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:46:32.639: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:46:32.659: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999812s
Jul  3 10:46:33.663: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994424561s
Jul  3 10:46:34.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989743225s
Jul  3 10:46:35.675: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984968803s
Jul  3 10:46:36.680: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.977731804s
Jul  3 10:46:37.685: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.973144198s
Jul  3 10:46:38.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.968615037s
Jul  3 10:46:39.694: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964194346s
Jul  3 10:46:40.698: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.959608008s
Jul  3 10:46:41.702: INFO: Verifying statefulset ss doesn't scale past 1 for another 955.05264ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7604
Jul  3 10:46:42.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:46:43.347: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:46:43.347: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:46:43.347: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:46:43.351: INFO: Found 1 stateful pods, waiting for 3
Jul  3 10:46:53.356: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:46:53.356: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:46:53.356: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul  3 10:46:53.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:46:54.019: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:46:54.019: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:46:54.019: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:46:54.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:46:54.681: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:46:54.681: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:46:54.681: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:46:54.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:46:55.333: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:46:55.333: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:46:55.333: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:46:55.333: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:46:55.336: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul  3 10:47:05.345: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:47:05.345: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:47:05.345: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul  3 10:47:05.357: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999762s
Jul  3 10:47:06.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99594854s
Jul  3 10:47:07.366: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991744918s
Jul  3 10:47:08.370: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987370065s
Jul  3 10:47:09.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983035621s
Jul  3 10:47:10.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978655059s
Jul  3 10:47:11.384: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973856289s
Jul  3 10:47:12.389: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969110817s
Jul  3 10:47:13.393: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964699868s
Jul  3 10:47:14.401: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.8073ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7604
Jul  3 10:47:15.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:47:16.067: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:47:16.067: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:47:16.067: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:47:16.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:47:16.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:47:16.713: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:47:16.713: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:47:16.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-7604 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:47:17.371: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:47:17.371: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:47:17.371: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:47:17.372: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  3 10:47:27.388: INFO: Deleting all statefulset in ns statefulset-7604
Jul  3 10:47:27.391: INFO: Scaling statefulset ss to 0
Jul  3 10:47:27.401: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:47:27.405: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:47:27.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7604" for this suite.
Jul  3 10:47:33.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:47:33.587: INFO: namespace statefulset-7604 deletion completed in 6.164233783s

• [SLOW TEST:81.781 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:47:33.588: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:47:59.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6666" for this suite.
Jul  3 10:48:05.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:48:06.104: INFO: namespace namespaces-6666 deletion completed in 6.376203834s
STEP: Destroying namespace "nsdeletetest-6152" for this suite.
Jul  3 10:48:06.107: INFO: Namespace nsdeletetest-6152 was already deleted
STEP: Destroying namespace "nsdeletetest-7837" for this suite.
Jul  3 10:48:12.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:48:12.264: INFO: namespace nsdeletetest-7837 deletion completed in 6.156793561s

• [SLOW TEST:38.676 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:48:12.264: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-0db6e508-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:48:12.311: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5" in namespace "projected-5694" to be "success or failure"
Jul  3 10:48:12.316: INFO: Pod "pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.730789ms
Jul  3 10:48:14.321: INFO: Pod "pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00999331s
Jul  3 10:48:16.325: INFO: Pod "pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014051837s
STEP: Saw pod success
Jul  3 10:48:16.325: INFO: Pod "pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:48:16.328: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:48:16.379: INFO: Waiting for pod pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:48:16.382: INFO: Pod pod-projected-configmaps-0db7e4f0-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:48:16.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5694" for this suite.
Jul  3 10:48:22.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:48:22.543: INFO: namespace projected-5694 deletion completed in 6.157054992s

• [SLOW TEST:10.279 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:48:22.544: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 10:48:22.587: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5" in namespace "projected-397" to be "success or failure"
Jul  3 10:48:22.592: INFO: Pod "downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.956946ms
Jul  3 10:48:24.597: INFO: Pod "downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009286082s
Jul  3 10:48:26.602: INFO: Pod "downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014419764s
STEP: Saw pod success
Jul  3 10:48:26.602: INFO: Pod "downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:48:26.612: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 10:48:26.634: INFO: Waiting for pod downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:48:26.638: INFO: Pod downwardapi-volume-13d768c2-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:48:26.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-397" for this suite.
Jul  3 10:48:32.656: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:48:32.809: INFO: namespace projected-397 deletion completed in 6.167135582s

• [SLOW TEST:10.265 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:48:32.809: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:48:32.842: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:48:36.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8530" for this suite.
Jul  3 10:49:28.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:49:29.057: INFO: namespace pods-8530 deletion completed in 52.169300537s

• [SLOW TEST:56.248 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:49:29.058: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-3b7c97ae-9d80-11e9-9a35-2ab8d9f547c5
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:49:29.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6191" for this suite.
Jul  3 10:49:35.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:49:35.258: INFO: namespace configmap-6191 deletion completed in 6.161823373s

• [SLOW TEST:6.201 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:49:35.259: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul  3 10:49:35.301: INFO: Waiting up to 5m0s for pod "pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-5754" to be "success or failure"
Jul  3 10:49:35.307: INFO: Pod "pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.909344ms
Jul  3 10:49:37.311: INFO: Pod "pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010319437s
Jul  3 10:49:39.317: INFO: Pod "pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015835051s
STEP: Saw pod success
Jul  3 10:49:39.317: INFO: Pod "pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:49:39.321: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:49:39.384: INFO: Waiting for pod pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:49:39.388: INFO: Pod pod-3f2ef1b6-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:49:39.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5754" for this suite.
Jul  3 10:49:45.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:49:45.557: INFO: namespace emptydir-5754 deletion completed in 6.165250931s

• [SLOW TEST:10.298 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:49:45.558: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-455241e4-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:49:45.604: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5" in namespace "projected-9454" to be "success or failure"
Jul  3 10:49:45.608: INFO: Pod "pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.598805ms
Jul  3 10:49:47.613: INFO: Pod "pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108275s
Jul  3 10:49:49.618: INFO: Pod "pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013753177s
STEP: Saw pod success
Jul  3 10:49:49.618: INFO: Pod "pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:49:49.621: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:49:49.642: INFO: Waiting for pod pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:49:49.646: INFO: Pod pod-projected-secrets-455342d0-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:49:49.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9454" for this suite.
Jul  3 10:49:55.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:49:55.809: INFO: namespace projected-9454 deletion completed in 6.158578891s

• [SLOW TEST:10.251 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:49:55.809: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-4b6e7c06-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:49:55.856: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5" in namespace "projected-6488" to be "success or failure"
Jul  3 10:49:55.861: INFO: Pod "pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.719808ms
Jul  3 10:49:57.866: INFO: Pod "pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010039024s
Jul  3 10:49:59.871: INFO: Pod "pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015160948s
STEP: Saw pod success
Jul  3 10:49:59.871: INFO: Pod "pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:49:59.874: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:49:59.902: INFO: Waiting for pod pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:49:59.905: INFO: Pod pod-projected-secrets-4b6f679e-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:49:59.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6488" for this suite.
Jul  3 10:50:05.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:50:06.073: INFO: namespace projected-6488 deletion completed in 6.163826512s

• [SLOW TEST:10.264 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:50:06.073: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:50:06.109: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:50:07.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-678" for this suite.
Jul  3 10:50:13.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:50:13.323: INFO: namespace custom-resource-definition-678 deletion completed in 6.155539014s

• [SLOW TEST:7.250 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:50:13.324: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  3 10:50:13.393: INFO: Number of nodes with available pods: 0
Jul  3 10:50:13.393: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:14.402: INFO: Number of nodes with available pods: 0
Jul  3 10:50:14.402: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:15.402: INFO: Number of nodes with available pods: 1
Jul  3 10:50:15.402: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:16.402: INFO: Number of nodes with available pods: 2
Jul  3 10:50:16.402: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:17.402: INFO: Number of nodes with available pods: 2
Jul  3 10:50:17.402: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:18.496: INFO: Number of nodes with available pods: 3
Jul  3 10:50:18.496: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul  3 10:50:18.519: INFO: Number of nodes with available pods: 2
Jul  3 10:50:18.519: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:19.529: INFO: Number of nodes with available pods: 2
Jul  3 10:50:19.529: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:20.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:20.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:21.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:21.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:22.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:22.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:23.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:23.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:24.527: INFO: Number of nodes with available pods: 2
Jul  3 10:50:24.527: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:25.529: INFO: Number of nodes with available pods: 2
Jul  3 10:50:25.529: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:26.529: INFO: Number of nodes with available pods: 2
Jul  3 10:50:26.529: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:27.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:27.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:28.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:28.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:29.529: INFO: Number of nodes with available pods: 2
Jul  3 10:50:29.529: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:30.527: INFO: Number of nodes with available pods: 2
Jul  3 10:50:30.527: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:31.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:31.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:32.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:32.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:33.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:33.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:34.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:34.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:35.528: INFO: Number of nodes with available pods: 2
Jul  3 10:50:35.528: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 10:50:36.528: INFO: Number of nodes with available pods: 3
Jul  3 10:50:36.528: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1001, will wait for the garbage collector to delete the pods
Jul  3 10:50:36.601: INFO: Deleting DaemonSet.extensions daemon-set took: 16.476514ms
Jul  3 10:50:37.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.218511ms
Jul  3 10:50:50.006: INFO: Number of nodes with available pods: 0
Jul  3 10:50:50.006: INFO: Number of running nodes: 0, number of available pods: 0
Jul  3 10:50:50.009: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1001/daemonsets","resourceVersion":"300391"},"items":null}

Jul  3 10:50:50.012: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1001/pods","resourceVersion":"300391"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:50:50.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1001" for this suite.
Jul  3 10:50:56.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:50:56.207: INFO: namespace daemonsets-1001 deletion completed in 6.169522667s

• [SLOW TEST:42.883 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:50:56.207: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6f6f412b-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:50:56.258: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5" in namespace "projected-2993" to be "success or failure"
Jul  3 10:50:56.265: INFO: Pod "pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.082775ms
Jul  3 10:50:58.269: INFO: Pod "pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010591445s
Jul  3 10:51:00.274: INFO: Pod "pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015155006s
STEP: Saw pod success
Jul  3 10:51:00.274: INFO: Pod "pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:51:00.277: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 10:51:00.299: INFO: Waiting for pod pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:51:00.302: INFO: Pod pod-projected-configmaps-6f70456e-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:51:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2993" for this suite.
Jul  3 10:51:06.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:51:06.464: INFO: namespace projected-2993 deletion completed in 6.157618813s

• [SLOW TEST:10.257 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:51:06.465: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:51:06.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4874" for this suite.
Jul  3 10:51:12.521: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:51:12.687: INFO: namespace services-4874 deletion completed in 6.178442835s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.222 seconds]
[sig-network] Services
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:51:12.688: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7414/configmap-test-79418dc8-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 10:51:12.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5" in namespace "configmap-7414" to be "success or failure"
Jul  3 10:51:12.741: INFO: Pod "pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992137ms
Jul  3 10:51:14.747: INFO: Pod "pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010661471s
Jul  3 10:51:16.751: INFO: Pod "pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01519518s
STEP: Saw pod success
Jul  3 10:51:16.751: INFO: Pod "pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:51:16.755: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5 container env-test: <nil>
STEP: delete the pod
Jul  3 10:51:16.781: INFO: Waiting for pod pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:51:16.784: INFO: Pod pod-configmaps-79427cfe-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:51:16.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7414" for this suite.
Jul  3 10:51:22.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:51:22.969: INFO: namespace configmap-7414 deletion completed in 6.180145355s

• [SLOW TEST:10.282 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:51:22.969: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul  3 10:51:23.064: INFO: Waiting up to 5m0s for pod "pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-4383" to be "success or failure"
Jul  3 10:51:23.070: INFO: Pod "pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.268122ms
Jul  3 10:51:25.074: INFO: Pod "pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009167944s
Jul  3 10:51:27.078: INFO: Pod "pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013644072s
STEP: Saw pod success
Jul  3 10:51:27.078: INFO: Pod "pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:51:27.082: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:51:27.104: INFO: Waiting for pod pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:51:27.108: INFO: Pod pod-7f6a437f-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:51:27.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4383" for this suite.
Jul  3 10:51:33.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:51:33.272: INFO: namespace emptydir-4383 deletion completed in 6.159171339s

• [SLOW TEST:10.302 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:51:33.272: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8396.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8396.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8396.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8396.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 221.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.221_udp@PTR;check="$$(dig +tcp +noall +answer +search 221.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.221_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8396.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8396.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8396.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8396.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8396.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8396.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 221.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.221_udp@PTR;check="$$(dig +tcp +noall +answer +search 221.10.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.10.221_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  3 10:51:50.525: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local from pod dns-8396/dns-test-8589990a-9d80-11e9-9a35-2ab8d9f547c5: the server could not find the requested resource (get pods dns-test-8589990a-9d80-11e9-9a35-2ab8d9f547c5)
Jul  3 10:51:51.015: INFO: Lookups using dns-8396/dns-test-8589990a-9d80-11e9-9a35-2ab8d9f547c5 failed for: [jessie_tcp@_http._tcp.dns-test-service.dns-8396.svc.cluster.local]

Jul  3 10:51:57.751: INFO: DNS probes using dns-8396/dns-test-8589990a-9d80-11e9-9a35-2ab8d9f547c5 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:51:57.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8396" for this suite.
Jul  3 10:52:03.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:52:04.013: INFO: namespace dns-8396 deletion completed in 6.178262886s

• [SLOW TEST:30.741 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:52:04.014: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7295.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7295.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  3 10:52:16.795: INFO: DNS probes using dns-7295/dns-test-97d90e22-9d80-11e9-9a35-2ab8d9f547c5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:52:16.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7295" for this suite.
Jul  3 10:52:22.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:52:22.982: INFO: namespace dns-7295 deletion completed in 6.167586853s

• [SLOW TEST:18.969 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:52:22.983: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:52:27.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-768" for this suite.
Jul  3 10:53:07.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:53:07.218: INFO: namespace kubelet-test-768 deletion completed in 40.164975052s

• [SLOW TEST:44.236 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:53:07.219: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  3 10:53:11.790: INFO: Successfully updated pod "pod-update-activedeadlineseconds-bd855a30-9d80-11e9-9a35-2ab8d9f547c5"
Jul  3 10:53:11.790: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-bd855a30-9d80-11e9-9a35-2ab8d9f547c5" in namespace "pods-423" to be "terminated due to deadline exceeded"
Jul  3 10:53:11.793: INFO: Pod "pod-update-activedeadlineseconds-bd855a30-9d80-11e9-9a35-2ab8d9f547c5": Phase="Running", Reason="", readiness=true. Elapsed: 3.46667ms
Jul  3 10:53:13.798: INFO: Pod "pod-update-activedeadlineseconds-bd855a30-9d80-11e9-9a35-2ab8d9f547c5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007865259s
Jul  3 10:53:13.798: INFO: Pod "pod-update-activedeadlineseconds-bd855a30-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:53:13.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-423" for this suite.
Jul  3 10:53:19.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:53:19.978: INFO: namespace pods-423 deletion completed in 6.175908044s

• [SLOW TEST:12.760 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:53:19.979: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:53:20.027: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul  3 10:53:25.031: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  3 10:53:25.031: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul  3 10:53:27.035: INFO: Creating deployment "test-rollover-deployment"
Jul  3 10:53:27.044: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul  3 10:53:29.052: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul  3 10:53:29.059: INFO: Ensure that both replica sets have 1 created replica
Jul  3 10:53:29.066: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul  3 10:53:29.074: INFO: Updating deployment test-rollover-deployment
Jul  3 10:53:29.074: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul  3 10:53:31.087: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul  3 10:53:31.094: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul  3 10:53:31.101: INFO: all replica sets need to contain the pod-template-hash label
Jul  3 10:53:31.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748011, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 10:53:33.109: INFO: all replica sets need to contain the pod-template-hash label
Jul  3 10:53:33.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748011, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 10:53:35.109: INFO: all replica sets need to contain the pod-template-hash label
Jul  3 10:53:35.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748011, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 10:53:37.110: INFO: all replica sets need to contain the pod-template-hash label
Jul  3 10:53:37.110: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748011, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 10:53:39.109: INFO: all replica sets need to contain the pod-template-hash label
Jul  3 10:53:39.109: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748011, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748007, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 10:53:41.108: INFO: 
Jul  3 10:53:41.109: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  3 10:53:41.119: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-713,SelfLink:/apis/apps/v1/namespaces/deployment-713/deployments/test-rollover-deployment,UID:c9503d0f-9d80-11e9-9bcb-a6641e9563f8,ResourceVersion:301304,Generation:2,CreationTimestamp:2019-07-03 10:53:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-03 10:53:27 +0000 UTC 2019-07-03 10:53:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-03 10:53:41 +0000 UTC 2019-07-03 10:53:27 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  3 10:53:41.123: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-713,SelfLink:/apis/apps/v1/namespaces/deployment-713/replicasets/test-rollover-deployment-766b4d6c9d,UID:ca87547f-9d80-11e9-9413-46c858ac61fd,ResourceVersion:301294,Generation:2,CreationTimestamp:2019-07-03 10:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c9503d0f-9d80-11e9-9bcb-a6641e9563f8 0xc0015f2f27 0xc0015f2f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  3 10:53:41.123: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul  3 10:53:41.123: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-713,SelfLink:/apis/apps/v1/namespaces/deployment-713/replicasets/test-rollover-controller,UID:c52112e0-9d80-11e9-9bcb-a6641e9563f8,ResourceVersion:301303,Generation:2,CreationTimestamp:2019-07-03 10:53:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c9503d0f-9d80-11e9-9bcb-a6641e9563f8 0xc0015f2bb7 0xc0015f2bb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 10:53:41.123: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-713,SelfLink:/apis/apps/v1/namespaces/deployment-713/replicasets/test-rollover-deployment-6455657675,UID:c9522d71-9d80-11e9-9413-46c858ac61fd,ResourceVersion:301241,Generation:2,CreationTimestamp:2019-07-03 10:53:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment c9503d0f-9d80-11e9-9bcb-a6641e9563f8 0xc0015f2c87 0xc0015f2c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 10:53:41.129: INFO: Pod "test-rollover-deployment-766b4d6c9d-pw7qq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-pw7qq,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-713,SelfLink:/api/v1/namespaces/deployment-713/pods/test-rollover-deployment-766b4d6c9d-pw7qq,UID:ca8b4224-9d80-11e9-9413-46c858ac61fd,ResourceVersion:301259,Generation:0,CreationTimestamp:2019-07-03 10:53:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d ca87547f-9d80-11e9-9413-46c858ac61fd 0xc0015f3f37 0xc0015f3f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-x2xs8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-x2xs8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-x2xs8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029d6260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029d6280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:53:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:53:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:53:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 10:53:29 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.50,StartTime:2019-07-03 10:53:29 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-03 10:53:30 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://30b339ddeb436eeb760a40320f639bfc73b584dcdc22979662b1dc9cc500be95}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:53:41.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-713" for this suite.
Jul  3 10:53:47.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:53:47.295: INFO: namespace deployment-713 deletion completed in 6.161512886s

• [SLOW TEST:27.316 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:53:47.296: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Jul  3 10:53:47.336: INFO: Waiting up to 5m0s for pod "client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5" in namespace "containers-1641" to be "success or failure"
Jul  3 10:53:47.341: INFO: Pod "client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125673ms
Jul  3 10:53:49.345: INFO: Pod "client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00847052s
Jul  3 10:53:51.349: INFO: Pod "client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012838023s
STEP: Saw pod success
Jul  3 10:53:51.349: INFO: Pod "client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:53:51.353: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 10:53:51.376: INFO: Waiting for pod client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:53:51.379: INFO: Pod client-containers-d5687e2e-9d80-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:53:51.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1641" for this suite.
Jul  3 10:53:57.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:53:57.546: INFO: namespace containers-1641 deletion completed in 6.162822386s

• [SLOW TEST:10.250 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:53:57.546: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:53:57.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 version'
Jul  3 10:53:57.641: INFO: stderr: ""
Jul  3 10:53:57.641: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:44:30Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3\", GitCommit:\"5e53fd6bc17c0dec8434817e69b04a25d8ae0ff0\", GitTreeState:\"clean\", BuildDate:\"2019-06-06T01:36:19Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:53:57.641: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9853" for this suite.
Jul  3 10:54:03.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:54:03.814: INFO: namespace kubectl-9853 deletion completed in 6.167881857s

• [SLOW TEST:6.268 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:54:03.815: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 10:54:03.859: INFO: (0) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.704263ms)
Jul  3 10:54:03.905: INFO: (1) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 45.785404ms)
Jul  3 10:54:03.911: INFO: (2) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.294627ms)
Jul  3 10:54:03.916: INFO: (3) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.324935ms)
Jul  3 10:54:03.921: INFO: (4) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.958353ms)
Jul  3 10:54:03.926: INFO: (5) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.047488ms)
Jul  3 10:54:03.931: INFO: (6) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.906682ms)
Jul  3 10:54:03.945: INFO: (7) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 13.676291ms)
Jul  3 10:54:03.965: INFO: (8) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 20.163699ms)
Jul  3 10:54:03.971: INFO: (9) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.717794ms)
Jul  3 10:54:03.976: INFO: (10) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.129081ms)
Jul  3 10:54:03.981: INFO: (11) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.946904ms)
Jul  3 10:54:03.986: INFO: (12) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.964821ms)
Jul  3 10:54:03.992: INFO: (13) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.950451ms)
Jul  3 10:54:03.997: INFO: (14) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.08405ms)
Jul  3 10:54:04.005: INFO: (15) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 7.540955ms)
Jul  3 10:54:04.010: INFO: (16) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.59077ms)
Jul  3 10:54:04.017: INFO: (17) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 6.137787ms)
Jul  3 10:54:04.022: INFO: (18) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 5.225724ms)
Jul  3 10:54:04.027: INFO: (19) /api/v1/nodes/ip-172-31-22-121.eu-central-1.compute.internal/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="amazon/">amazon/</a>
<a href="apt/... (200; 4.824737ms)
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:54:04.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1566" for this suite.
Jul  3 10:54:10.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:54:10.209: INFO: namespace proxy-1566 deletion completed in 6.178155783s

• [SLOW TEST:6.395 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:54:10.209: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8492
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul  3 10:54:10.243: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul  3 10:54:28.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.171:8080/dial?request=hostName&protocol=udp&host=172.25.1.170&port=8081&tries=1'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:54:28.333: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:54:28.999: INFO: Waiting for endpoints: map[]
Jul  3 10:54:29.002: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.171:8080/dial?request=hostName&protocol=udp&host=172.25.2.51&port=8081&tries=1'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:54:29.002: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:54:29.631: INFO: Waiting for endpoints: map[]
Jul  3 10:54:29.634: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.25.1.171:8080/dial?request=hostName&protocol=udp&host=172.25.0.44&port=8081&tries=1'] Namespace:pod-network-test-8492 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul  3 10:54:29.634: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
Jul  3 10:54:30.316: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:54:30.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8492" for this suite.
Jul  3 10:54:52.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:54:52.482: INFO: namespace pod-network-test-8492 deletion completed in 22.160252881s

• [SLOW TEST:42.272 seconds]
[sig-network] Networking
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:54:52.482: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-fc4403aa-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating configMap with name cm-test-opt-upd-fc4403d4-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-fc4403aa-9d80-11e9-9a35-2ab8d9f547c5
STEP: Updating configmap cm-test-opt-upd-fc4403d4-9d80-11e9-9a35-2ab8d9f547c5
STEP: Creating configMap with name cm-test-opt-create-fc4403e4-9d80-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:54:59.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6649" for this suite.
Jul  3 10:55:21.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:55:21.179: INFO: namespace projected-6649 deletion completed in 22.154624189s

• [SLOW TEST:28.697 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:55:21.179: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-0d5e1ea2-9d81-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:55:21.226: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5" in namespace "projected-1626" to be "success or failure"
Jul  3 10:55:21.232: INFO: Pod "pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902406ms
Jul  3 10:55:23.242: INFO: Pod "pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015831816s
STEP: Saw pod success
Jul  3 10:55:23.242: INFO: Pod "pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:55:23.250: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:55:23.312: INFO: Waiting for pod pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:55:23.315: INFO: Pod pod-projected-secrets-0d5f1a87-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:55:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1626" for this suite.
Jul  3 10:55:29.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:55:29.475: INFO: namespace projected-1626 deletion completed in 6.155332673s

• [SLOW TEST:8.296 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:55:29.476: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul  3 10:55:29.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301890,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  3 10:55:29.521: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301890,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul  3 10:55:39.530: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301923,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  3 10:55:39.531: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301923,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul  3 10:55:49.538: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301956,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  3 10:55:49.538: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301956,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul  3 10:55:59.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301989,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  3 10:55:59.546: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-a,UID:12510564-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:301989,Generation:0,CreationTimestamp:2019-07-03 10:55:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul  3 10:56:09.554: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-b,UID:2a2d8b82-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:302021,Generation:0,CreationTimestamp:2019-07-03 10:56:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  3 10:56:09.554: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-b,UID:2a2d8b82-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:302021,Generation:0,CreationTimestamp:2019-07-03 10:56:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul  3 10:56:19.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-b,UID:2a2d8b82-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:302054,Generation:0,CreationTimestamp:2019-07-03 10:56:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  3 10:56:19.561: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-5244,SelfLink:/api/v1/namespaces/watch-5244/configmaps/e2e-watch-test-configmap-b,UID:2a2d8b82-9d81-11e9-9bcb-a6641e9563f8,ResourceVersion:302054,Generation:0,CreationTimestamp:2019-07-03 10:56:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:56:29.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5244" for this suite.
Jul  3 10:56:35.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:56:35.747: INFO: namespace watch-5244 deletion completed in 6.180484198s

• [SLOW TEST:66.271 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:56:35.748: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-39d08a23-9d81-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 10:56:35.797: INFO: Waiting up to 5m0s for pod "pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5" in namespace "secrets-8835" to be "success or failure"
Jul  3 10:56:35.802: INFO: Pod "pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.789248ms
Jul  3 10:56:37.806: INFO: Pod "pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008950382s
Jul  3 10:56:39.811: INFO: Pod "pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013492633s
STEP: Saw pod success
Jul  3 10:56:39.811: INFO: Pod "pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:56:39.814: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 10:56:39.838: INFO: Waiting for pod pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:56:39.841: INFO: Pod pod-secrets-39d1aa2f-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:56:39.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8835" for this suite.
Jul  3 10:56:45.858: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:56:46.009: INFO: namespace secrets-8835 deletion completed in 6.162970257s

• [SLOW TEST:10.261 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:56:46.009: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Jul  3 10:56:46.057: INFO: Waiting up to 5m0s for pod "var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5" in namespace "var-expansion-50" to be "success or failure"
Jul  3 10:56:46.062: INFO: Pod "var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.588863ms
Jul  3 10:56:48.067: INFO: Pod "var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009896793s
Jul  3 10:56:50.072: INFO: Pod "var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014953727s
STEP: Saw pod success
Jul  3 10:56:50.072: INFO: Pod "var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 10:56:50.075: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 10:56:50.102: INFO: Waiting for pod var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 10:56:50.106: INFO: Pod var-expansion-3feef39e-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:56:50.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-50" for this suite.
Jul  3 10:56:56.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:56:56.272: INFO: namespace var-expansion-50 deletion completed in 6.161090353s

• [SLOW TEST:10.263 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:56:56.272: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1347
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Jul  3 10:56:56.322: INFO: Found 0 stateful pods, waiting for 3
Jul  3 10:57:06.331: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:57:06.331: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:57:06.331: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul  3 10:57:06.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-1347 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:57:06.970: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:57:06.970: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:57:06.970: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul  3 10:57:17.010: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul  3 10:57:27.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-1347 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:57:27.634: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:57:27.634: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:57:27.634: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul  3 10:57:47.657: INFO: Waiting for StatefulSet statefulset-1347/ss2 to complete update
Jul  3 10:57:47.657: INFO: Waiting for Pod statefulset-1347/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Jul  3 10:57:57.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-1347 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul  3 10:57:58.338: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul  3 10:57:58.338: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul  3 10:57:58.338: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul  3 10:58:08.374: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul  3 10:58:18.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 exec --namespace=statefulset-1347 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul  3 10:58:19.007: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul  3 10:58:19.007: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul  3 10:58:19.007: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  3 10:58:39.032: INFO: Deleting all statefulset in ns statefulset-1347
Jul  3 10:58:39.035: INFO: Scaling statefulset ss2 to 0
Jul  3 10:58:49.052: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 10:58:49.056: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 10:58:49.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1347" for this suite.
Jul  3 10:58:55.088: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 10:58:55.246: INFO: namespace statefulset-1347 deletion completed in 6.171472689s

• [SLOW TEST:118.973 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 10:58:55.246: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-8cf6ee15-9d81-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-8cf6ee15-9d81-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:00:24.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7872" for this suite.
Jul  3 11:00:46.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:00:46.538: INFO: namespace configmap-7872 deletion completed in 22.167207175s

• [SLOW TEST:111.292 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:00:46.538: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  3 11:00:46.587: INFO: Waiting up to 5m0s for pod "downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-441" to be "success or failure"
Jul  3 11:00:46.592: INFO: Pod "downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.062476ms
Jul  3 11:00:48.597: INFO: Pod "downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009440225s
Jul  3 11:00:50.603: INFO: Pod "downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01566374s
STEP: Saw pod success
Jul  3 11:00:50.603: INFO: Pod "downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:00:50.608: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 11:00:50.631: INFO: Waiting for pod downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:00:50.635: INFO: Pod downward-api-cf4c321f-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:00:50.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-441" for this suite.
Jul  3 11:00:56.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:00:56.798: INFO: namespace downward-api-441 deletion completed in 6.158295025s

• [SLOW TEST:10.260 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:00:56.798: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  3 11:00:56.841: INFO: Waiting up to 5m0s for pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-3464" to be "success or failure"
Jul  3 11:00:56.846: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.345038ms
Jul  3 11:00:58.851: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01028129s
Jul  3 11:01:00.856: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015022905s
Jul  3 11:01:02.861: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019538969s
Jul  3 11:01:04.865: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.023778726s
STEP: Saw pod success
Jul  3 11:01:04.865: INFO: Pod "downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:01:04.868: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 11:01:04.981: INFO: Waiting for pod downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:01:04.984: INFO: Pod downward-api-d5699f02-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:01:04.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3464" for this suite.
Jul  3 11:01:11.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:01:11.158: INFO: namespace downward-api-3464 deletion completed in 6.166145772s

• [SLOW TEST:14.360 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:01:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ddf8de11-9d81-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 11:01:11.206: INFO: Waiting up to 5m0s for pod "pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5" in namespace "secrets-3771" to be "success or failure"
Jul  3 11:01:11.211: INFO: Pod "pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.023896ms
Jul  3 11:01:13.216: INFO: Pod "pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009840672s
Jul  3 11:01:15.220: INFO: Pod "pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013779823s
STEP: Saw pod success
Jul  3 11:01:15.220: INFO: Pod "pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:01:15.223: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 11:01:15.283: INFO: Waiting for pod pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:01:15.287: INFO: Pod pod-secrets-ddf9cf35-9d81-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:01:15.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3771" for this suite.
Jul  3 11:01:21.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:01:21.452: INFO: namespace secrets-3771 deletion completed in 6.160649081s

• [SLOW TEST:10.293 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:01:21.453: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0703 11:01:52.028480      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 11:01:52.028: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:01:52.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8405" for this suite.
Jul  3 11:01:58.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:01:58.185: INFO: namespace gc-8405 deletion completed in 6.152634512s

• [SLOW TEST:36.732 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:01:58.185: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Jul  3 11:01:58.223: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:02:04.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8634" for this suite.
Jul  3 11:02:10.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:02:10.354: INFO: namespace init-container-8634 deletion completed in 6.1524554s

• [SLOW TEST:12.169 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:02:10.354: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:02:15.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9797" for this suite.
Jul  3 11:02:37.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:02:37.583: INFO: namespace replication-controller-9797 deletion completed in 22.156875933s

• [SLOW TEST:27.229 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:02:37.583: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Jul  3 11:02:37.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-6762'
Jul  3 11:02:37.990: INFO: stderr: ""
Jul  3 11:02:37.990: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Jul  3 11:02:38.995: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 11:02:38.995: INFO: Found 0 / 1
Jul  3 11:02:39.995: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 11:02:39.995: INFO: Found 0 / 1
Jul  3 11:02:40.994: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 11:02:40.994: INFO: Found 1 / 1
Jul  3 11:02:40.994: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul  3 11:02:40.998: INFO: Selector matched 1 pods for map[app:redis]
Jul  3 11:02:40.998: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul  3 11:02:40.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 logs redis-master-xqks2 redis-master --namespace=kubectl-6762'
Jul  3 11:02:41.081: INFO: stderr: ""
Jul  3 11:02:41.081: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Jul 11:02:39.443 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Jul 11:02:39.443 # Server started, Redis version 3.2.12\n1:M 03 Jul 11:02:39.443 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Jul 11:02:39.443 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul  3 11:02:41.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 log redis-master-xqks2 redis-master --namespace=kubectl-6762 --tail=1'
Jul  3 11:02:41.161: INFO: stderr: ""
Jul  3 11:02:41.161: INFO: stdout: "1:M 03 Jul 11:02:39.443 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul  3 11:02:41.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 log redis-master-xqks2 redis-master --namespace=kubectl-6762 --limit-bytes=1'
Jul  3 11:02:41.246: INFO: stderr: ""
Jul  3 11:02:41.246: INFO: stdout: " "
STEP: exposing timestamps
Jul  3 11:02:41.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 log redis-master-xqks2 redis-master --namespace=kubectl-6762 --tail=1 --timestamps'
Jul  3 11:02:41.327: INFO: stderr: ""
Jul  3 11:02:41.328: INFO: stdout: "2019-07-03T11:02:39.444280118Z 1:M 03 Jul 11:02:39.443 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul  3 11:02:43.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 log redis-master-xqks2 redis-master --namespace=kubectl-6762 --since=1s'
Jul  3 11:02:43.907: INFO: stderr: ""
Jul  3 11:02:43.907: INFO: stdout: ""
Jul  3 11:02:43.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 log redis-master-xqks2 redis-master --namespace=kubectl-6762 --since=24h'
Jul  3 11:02:44.000: INFO: stderr: ""
Jul  3 11:02:44.000: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 03 Jul 11:02:39.443 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 03 Jul 11:02:39.443 # Server started, Redis version 3.2.12\n1:M 03 Jul 11:02:39.443 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 03 Jul 11:02:39.443 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Jul  3 11:02:44.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-6762'
Jul  3 11:02:44.075: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:02:44.075: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul  3 11:02:44.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6762'
Jul  3 11:02:44.151: INFO: stderr: "No resources found.\n"
Jul  3 11:02:44.151: INFO: stdout: ""
Jul  3 11:02:44.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=nginx --namespace=kubectl-6762 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 11:02:44.220: INFO: stderr: ""
Jul  3 11:02:44.220: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:02:44.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6762" for this suite.
Jul  3 11:02:50.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:02:50.382: INFO: namespace kubectl-6762 deletion completed in 6.157330497s

• [SLOW TEST:12.799 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:02:50.382: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Jul  3 11:02:50.427: INFO: Waiting up to 5m0s for pod "downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-3315" to be "success or failure"
Jul  3 11:02:50.432: INFO: Pod "downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449286ms
Jul  3 11:02:52.437: INFO: Pod "downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010143439s
Jul  3 11:02:54.441: INFO: Pod "downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014589653s
STEP: Saw pod success
Jul  3 11:02:54.441: INFO: Pod "downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:02:54.445: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5 container dapi-container: <nil>
STEP: delete the pod
Jul  3 11:02:54.477: INFO: Waiting for pod downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:02:54.480: INFO: Pod downward-api-191d4913-9d82-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:02:54.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3315" for this suite.
Jul  3 11:03:00.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:03:00.706: INFO: namespace downward-api-3315 deletion completed in 6.220805894s

• [SLOW TEST:10.323 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:03:00.706: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0703 11:03:40.783163      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 11:03:40.783: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:03:40.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-222" for this suite.
Jul  3 11:03:46.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:03:46.953: INFO: namespace gc-222 deletion completed in 6.166428258s

• [SLOW TEST:46.247 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:03:46.954: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5562.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5562.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5562.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5562.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5562.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5562.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul  3 11:03:51.747: INFO: DNS probes using dns-5562/dns-test-3ad514f7-9d82-11e9-9a35-2ab8d9f547c5 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:03:51.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5562" for this suite.
Jul  3 11:03:57.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:03:57.949: INFO: namespace dns-5562 deletion completed in 6.169332016s

• [SLOW TEST:10.995 seconds]
[sig-network] DNS
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:03:57.949: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Jul  3 11:03:57.985: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-489780963 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:03:58.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7579" for this suite.
Jul  3 11:04:04.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:04:04.220: INFO: namespace kubectl-7579 deletion completed in 6.172655531s

• [SLOW TEST:6.271 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:04:04.221: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Jul  3 11:04:04.253: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-489780963 proxy --unix-socket=/tmp/kubectl-proxy-unix367618790/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:04:04.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6596" for this suite.
Jul  3 11:04:10.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:04:10.468: INFO: namespace kubectl-6596 deletion completed in 6.156616497s

• [SLOW TEST:6.247 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:04:10.469: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5
Jul  3 11:04:10.512: INFO: Pod name my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5: Found 0 pods out of 1
Jul  3 11:04:15.516: INFO: Pod name my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5: Found 1 pods out of 1
Jul  3 11:04:15.516: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5" are running
Jul  3 11:04:15.520: INFO: Pod "my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5-4fvzh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:15 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:15 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:10 +0000 UTC Reason: Message:}])
Jul  3 11:04:15.520: INFO: Trying to dial the pod
Jul  3 11:04:20.623: INFO: Controller my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5: Got expected result from replica 1 [my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5-4fvzh]: "my-hostname-basic-48d93948-9d82-11e9-9a35-2ab8d9f547c5-4fvzh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:04:20.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7526" for this suite.
Jul  3 11:04:26.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:04:26.789: INFO: namespace replication-controller-7526 deletion completed in 6.161171166s

• [SLOW TEST:16.321 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:04:26.790: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Jul  3 11:04:27.362: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul  3 11:04:29.403: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 11:04:31.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 11:04:33.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 11:04:35.407: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697748667, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 11:04:38.537: INFO: Waited 1.122854442s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:04:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8704" for this suite.
Jul  3 11:04:45.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:04:45.671: INFO: namespace aggregator-8704 deletion completed in 6.162485172s

• [SLOW TEST:18.881 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:04:45.671: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul  3 11:04:45.722: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2601,SelfLink:/api/v1/namespaces/watch-2601/configmaps/e2e-watch-test-watch-closed,UID:5dd5a788-9d82-11e9-9bcb-a6641e9563f8,ResourceVersion:304744,Generation:0,CreationTimestamp:2019-07-03 11:04:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  3 11:04:45.722: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2601,SelfLink:/api/v1/namespaces/watch-2601/configmaps/e2e-watch-test-watch-closed,UID:5dd5a788-9d82-11e9-9bcb-a6641e9563f8,ResourceVersion:304745,Generation:0,CreationTimestamp:2019-07-03 11:04:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul  3 11:04:45.736: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2601,SelfLink:/api/v1/namespaces/watch-2601/configmaps/e2e-watch-test-watch-closed,UID:5dd5a788-9d82-11e9-9bcb-a6641e9563f8,ResourceVersion:304746,Generation:0,CreationTimestamp:2019-07-03 11:04:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  3 11:04:45.737: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-2601,SelfLink:/api/v1/namespaces/watch-2601/configmaps/e2e-watch-test-watch-closed,UID:5dd5a788-9d82-11e9-9bcb-a6641e9563f8,ResourceVersion:304747,Generation:0,CreationTimestamp:2019-07-03 11:04:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:04:45.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2601" for this suite.
Jul  3 11:04:51.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:04:51.901: INFO: namespace watch-2601 deletion completed in 6.159903877s

• [SLOW TEST:6.230 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:04:51.901: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 11:04:51.934: INFO: Creating ReplicaSet my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5
Jul  3 11:04:51.946: INFO: Pod name my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5: Found 0 pods out of 1
Jul  3 11:04:56.950: INFO: Pod name my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5: Found 1 pods out of 1
Jul  3 11:04:56.950: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5" is running
Jul  3 11:04:56.954: INFO: Pod "my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5-qzdvw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:53 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:53 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-03 11:04:51 +0000 UTC Reason: Message:}])
Jul  3 11:04:56.954: INFO: Trying to dial the pod
Jul  3 11:05:02.067: INFO: Controller my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5: Got expected result from replica 1 [my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5-qzdvw]: "my-hostname-basic-618b6890-9d82-11e9-9a35-2ab8d9f547c5-qzdvw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:05:02.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3513" for this suite.
Jul  3 11:05:08.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:05:08.238: INFO: namespace replicaset-3513 deletion completed in 6.162975601s

• [SLOW TEST:16.337 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:05:08.239: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 11:05:08.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7846'
Jul  3 11:05:08.348: INFO: stderr: ""
Jul  3 11:05:08.348: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Jul  3 11:05:08.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete pods e2e-test-nginx-pod --namespace=kubectl-7846'
Jul  3 11:05:12.651: INFO: stderr: ""
Jul  3 11:05:12.651: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:05:12.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7846" for this suite.
Jul  3 11:05:18.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:05:18.898: INFO: namespace kubectl-7846 deletion completed in 6.242342173s

• [SLOW TEST:10.660 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:05:18.899: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 11:05:18.978: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"71a6ecc1-9d82-11e9-9bcb-a6641e9563f8", Controller:(*bool)(0xc00247dd82), BlockOwnerDeletion:(*bool)(0xc00247dd83)}}
Jul  3 11:05:18.984: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"71a3730e-9d82-11e9-9bcb-a6641e9563f8", Controller:(*bool)(0xc002a8ca2a), BlockOwnerDeletion:(*bool)(0xc002a8ca2b)}}
Jul  3 11:05:18.991: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"71a49436-9d82-11e9-9bcb-a6641e9563f8", Controller:(*bool)(0xc00247df72), BlockOwnerDeletion:(*bool)(0xc00247df73)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:05:24.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8450" for this suite.
Jul  3 11:05:30.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:05:30.187: INFO: namespace gc-8450 deletion completed in 6.175570839s

• [SLOW TEST:11.288 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:05:30.188: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-785e25f9-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating secret with name s-test-opt-upd-785e2622-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-785e25f9-9d82-11e9-9a35-2ab8d9f547c5
STEP: Updating secret s-test-opt-upd-785e2622-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating secret with name s-test-opt-create-785e2631-9d82-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:05:38.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3330" for this suite.
Jul  3 11:06:00.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:06:00.899: INFO: namespace projected-3330 deletion completed in 22.166477811s

• [SLOW TEST:30.711 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:06:00.899: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-8aab8244-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 11:06:00.945: INFO: Waiting up to 5m0s for pod "pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5" in namespace "secrets-2349" to be "success or failure"
Jul  3 11:06:00.950: INFO: Pod "pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.982844ms
Jul  3 11:06:02.954: INFO: Pod "pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009132202s
STEP: Saw pod success
Jul  3 11:06:02.954: INFO: Pod "pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:06:02.957: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5 container secret-volume-test: <nil>
STEP: delete the pod
Jul  3 11:06:02.984: INFO: Waiting for pod pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:06:02.987: INFO: Pod pod-secrets-8aac6fd6-9d82-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:06:02.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2349" for this suite.
Jul  3 11:06:09.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:06:09.151: INFO: namespace secrets-2349 deletion completed in 6.159581229s

• [SLOW TEST:8.252 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:06:09.151: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0703 11:06:10.230590      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 11:06:10.230: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:06:10.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8887" for this suite.
Jul  3 11:06:16.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:06:16.389: INFO: namespace gc-8887 deletion completed in 6.15390553s

• [SLOW TEST:7.237 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:06:16.389: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0703 11:06:26.506405      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 11:06:26.506: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:06:26.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3383" for this suite.
Jul  3 11:06:32.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:06:32.674: INFO: namespace gc-3383 deletion completed in 6.163658415s

• [SLOW TEST:16.285 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:06:32.674: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:06:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4145" for this suite.
Jul  3 11:07:32.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:07:32.904: INFO: namespace kubelet-test-4145 deletion completed in 58.162137533s

• [SLOW TEST:60.229 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:07:32.904: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-c182bc27-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 11:07:32.953: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5" in namespace "projected-9868" to be "success or failure"
Jul  3 11:07:32.957: INFO: Pod "pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.306724ms
Jul  3 11:07:34.961: INFO: Pod "pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008728375s
Jul  3 11:07:36.965: INFO: Pod "pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012673428s
STEP: Saw pod success
Jul  3 11:07:36.965: INFO: Pod "pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:07:36.969: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 11:07:36.994: INFO: Waiting for pod pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:07:36.997: INFO: Pod pod-projected-configmaps-c183ad67-9d82-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:07:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9868" for this suite.
Jul  3 11:07:43.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:07:43.155: INFO: namespace projected-9868 deletion completed in 6.154441517s

• [SLOW TEST:10.252 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:07:43.156: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:07:43.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5250" for this suite.
Jul  3 11:08:05.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:08:05.370: INFO: namespace pods-5250 deletion completed in 22.161507534s

• [SLOW TEST:22.214 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:08:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7603
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7603
STEP: Creating statefulset with conflicting port in namespace statefulset-7603
STEP: Waiting until pod test-pod will start running in namespace statefulset-7603
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7603
Jul  3 11:08:09.446: INFO: Observed stateful pod in namespace: statefulset-7603, name: ss-0, uid: d6cbbd8f-9d82-11e9-9413-46c858ac61fd, status phase: Pending. Waiting for statefulset controller to delete.
Jul  3 11:08:09.635: INFO: Observed stateful pod in namespace: statefulset-7603, name: ss-0, uid: d6cbbd8f-9d82-11e9-9413-46c858ac61fd, status phase: Failed. Waiting for statefulset controller to delete.
Jul  3 11:08:09.641: INFO: Observed stateful pod in namespace: statefulset-7603, name: ss-0, uid: d6cbbd8f-9d82-11e9-9413-46c858ac61fd, status phase: Failed. Waiting for statefulset controller to delete.
Jul  3 11:08:09.644: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7603
STEP: Removing pod with conflicting port in namespace statefulset-7603
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7603 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Jul  3 11:08:13.669: INFO: Deleting all statefulset in ns statefulset-7603
Jul  3 11:08:13.672: INFO: Scaling statefulset ss to 0
Jul  3 11:08:23.689: INFO: Waiting for statefulset status.replicas updated to 0
Jul  3 11:08:23.693: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:08:23.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7603" for this suite.
Jul  3 11:08:29.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:08:29.897: INFO: namespace statefulset-7603 deletion completed in 6.185183429s

• [SLOW TEST:24.527 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:08:29.898: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-4857/secret-test-e37b832e-9d82-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume secrets
Jul  3 11:08:29.952: INFO: Waiting up to 5m0s for pod "pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5" in namespace "secrets-4857" to be "success or failure"
Jul  3 11:08:29.959: INFO: Pod "pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 7.150006ms
Jul  3 11:08:31.963: INFO: Pod "pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011197279s
STEP: Saw pod success
Jul  3 11:08:31.963: INFO: Pod "pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:08:31.966: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5 container env-test: <nil>
STEP: delete the pod
Jul  3 11:08:32.029: INFO: Waiting for pod pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:08:32.033: INFO: Pod pod-configmaps-e37cab12-9d82-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:08:32.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4857" for this suite.
Jul  3 11:08:38.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:08:38.195: INFO: namespace secrets-4857 deletion completed in 6.157538357s

• [SLOW TEST:8.297 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:08:38.196: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 11:08:38.249: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5" in namespace "projected-9857" to be "success or failure"
Jul  3 11:08:38.253: INFO: Pod "downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.609192ms
Jul  3 11:08:40.258: INFO: Pod "downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009224674s
Jul  3 11:08:42.263: INFO: Pod "downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01420779s
STEP: Saw pod success
Jul  3 11:08:42.263: INFO: Pod "downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:08:42.268: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 11:08:42.292: INFO: Waiting for pod downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:08:42.295: INFO: Pod downwardapi-volume-e86eb465-9d82-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:08:42.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9857" for this suite.
Jul  3 11:08:48.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:08:48.462: INFO: namespace projected-9857 deletion completed in 6.161778225s

• [SLOW TEST:10.266 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:08:48.462: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul  3 11:08:48.539: INFO: Number of nodes with available pods: 0
Jul  3 11:08:48.539: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:49.548: INFO: Number of nodes with available pods: 0
Jul  3 11:08:49.548: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:50.549: INFO: Number of nodes with available pods: 1
Jul  3 11:08:50.549: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:51.550: INFO: Number of nodes with available pods: 3
Jul  3 11:08:51.550: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul  3 11:08:51.578: INFO: Number of nodes with available pods: 2
Jul  3 11:08:51.578: INFO: Node ip-172-31-26-241.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:52.587: INFO: Number of nodes with available pods: 2
Jul  3 11:08:52.587: INFO: Node ip-172-31-26-241.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:53.587: INFO: Number of nodes with available pods: 2
Jul  3 11:08:53.587: INFO: Node ip-172-31-26-241.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:08:54.587: INFO: Number of nodes with available pods: 3
Jul  3 11:08:54.587: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1504, will wait for the garbage collector to delete the pods
Jul  3 11:08:54.656: INFO: Deleting DaemonSet.extensions daemon-set took: 9.082464ms
Jul  3 11:08:55.156: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.231317ms
Jul  3 11:09:10.061: INFO: Number of nodes with available pods: 0
Jul  3 11:09:10.061: INFO: Number of running nodes: 0, number of available pods: 0
Jul  3 11:09:10.065: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1504/daemonsets","resourceVersion":"306358"},"items":null}

Jul  3 11:09:10.068: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1504/pods","resourceVersion":"306358"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:09:10.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1504" for this suite.
Jul  3 11:09:16.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:09:16.253: INFO: namespace daemonsets-1504 deletion completed in 6.160827262s

• [SLOW TEST:27.791 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:09:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-l6v9
STEP: Creating a pod to test atomic-volume-subpath
Jul  3 11:09:16.303: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l6v9" in namespace "subpath-2990" to be "success or failure"
Jul  3 11:09:16.308: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.217497ms
Jul  3 11:09:18.313: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009773708s
Jul  3 11:09:20.317: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 4.014480196s
Jul  3 11:09:22.322: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 6.019030229s
Jul  3 11:09:24.327: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 8.02361672s
Jul  3 11:09:26.331: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.027676022s
Jul  3 11:09:28.335: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 12.032340911s
Jul  3 11:09:30.340: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 14.037270921s
Jul  3 11:09:32.345: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 16.04191885s
Jul  3 11:09:34.350: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 18.047231263s
Jul  3 11:09:36.354: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 20.05130673s
Jul  3 11:09:38.359: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Running", Reason="", readiness=true. Elapsed: 22.05574201s
Jul  3 11:09:40.363: INFO: Pod "pod-subpath-test-configmap-l6v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.060562206s
STEP: Saw pod success
Jul  3 11:09:40.364: INFO: Pod "pod-subpath-test-configmap-l6v9" satisfied condition "success or failure"
Jul  3 11:09:40.367: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-subpath-test-configmap-l6v9 container test-container-subpath-configmap-l6v9: <nil>
STEP: delete the pod
Jul  3 11:09:40.389: INFO: Waiting for pod pod-subpath-test-configmap-l6v9 to disappear
Jul  3 11:09:40.392: INFO: Pod pod-subpath-test-configmap-l6v9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l6v9
Jul  3 11:09:40.392: INFO: Deleting pod "pod-subpath-test-configmap-l6v9" in namespace "subpath-2990"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:09:40.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2990" for this suite.
Jul  3 11:09:46.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:09:46.558: INFO: namespace subpath-2990 deletion completed in 6.158879138s

• [SLOW TEST:30.305 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:09:46.558: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0703 11:09:56.622551      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul  3 11:09:56.622: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:09:56.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7118" for this suite.
Jul  3 11:10:02.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:10:02.802: INFO: namespace gc-7118 deletion completed in 6.176220692s

• [SLOW TEST:16.244 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:10:02.803: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-1add3b68-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating secret with name s-test-opt-upd-1add3bb4-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-1add3b68-9d83-11e9-9a35-2ab8d9f547c5
STEP: Updating secret s-test-opt-upd-1add3bb4-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating secret with name s-test-opt-create-1add3bcf-9d83-11e9-9a35-2ab8d9f547c5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:10:11.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7893" for this suite.
Jul  3 11:10:33.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:10:33.646: INFO: namespace secrets-7893 deletion completed in 22.232751572s

• [SLOW TEST:30.843 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:10:33.646: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Jul  3 11:10:33.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-985'
Jul  3 11:10:33.910: INFO: stderr: ""
Jul  3 11:10:33.910: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 11:10:33.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:34.001: INFO: stderr: ""
Jul  3 11:10:34.001: INFO: stdout: "update-demo-nautilus-pcp4b update-demo-nautilus-q24m2 "
Jul  3 11:10:34.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-pcp4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:34.069: INFO: stderr: ""
Jul  3 11:10:34.069: INFO: stdout: ""
Jul  3 11:10:34.069: INFO: update-demo-nautilus-pcp4b is created but not running
Jul  3 11:10:39.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:39.137: INFO: stderr: ""
Jul  3 11:10:39.137: INFO: stdout: "update-demo-nautilus-pcp4b update-demo-nautilus-q24m2 "
Jul  3 11:10:39.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-pcp4b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:39.202: INFO: stderr: ""
Jul  3 11:10:39.202: INFO: stdout: "true"
Jul  3 11:10:39.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-pcp4b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:39.266: INFO: stderr: ""
Jul  3 11:10:39.266: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:10:39.266: INFO: validating pod update-demo-nautilus-pcp4b
Jul  3 11:10:39.407: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:10:39.407: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:10:39.407: INFO: update-demo-nautilus-pcp4b is verified up and running
Jul  3 11:10:39.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:39.472: INFO: stderr: ""
Jul  3 11:10:39.472: INFO: stdout: "true"
Jul  3 11:10:39.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:39.537: INFO: stderr: ""
Jul  3 11:10:39.537: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:10:39.537: INFO: validating pod update-demo-nautilus-q24m2
Jul  3 11:10:39.631: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:10:39.631: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:10:39.631: INFO: update-demo-nautilus-q24m2 is verified up and running
STEP: scaling down the replication controller
Jul  3 11:10:39.632: INFO: scanned /root for discovery docs: <nil>
Jul  3 11:10:39.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-985'
Jul  3 11:10:40.731: INFO: stderr: ""
Jul  3 11:10:40.731: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 11:10:40.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:40.798: INFO: stderr: ""
Jul  3 11:10:40.798: INFO: stdout: "update-demo-nautilus-pcp4b update-demo-nautilus-q24m2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  3 11:10:45.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:45.866: INFO: stderr: ""
Jul  3 11:10:45.866: INFO: stdout: "update-demo-nautilus-pcp4b update-demo-nautilus-q24m2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  3 11:10:50.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:50.935: INFO: stderr: ""
Jul  3 11:10:50.935: INFO: stdout: "update-demo-nautilus-pcp4b update-demo-nautilus-q24m2 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul  3 11:10:55.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:56.007: INFO: stderr: ""
Jul  3 11:10:56.007: INFO: stdout: "update-demo-nautilus-q24m2 "
Jul  3 11:10:56.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:56.084: INFO: stderr: ""
Jul  3 11:10:56.084: INFO: stdout: "true"
Jul  3 11:10:56.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:56.151: INFO: stderr: ""
Jul  3 11:10:56.151: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:10:56.151: INFO: validating pod update-demo-nautilus-q24m2
Jul  3 11:10:56.157: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:10:56.157: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:10:56.157: INFO: update-demo-nautilus-q24m2 is verified up and running
STEP: scaling up the replication controller
Jul  3 11:10:56.159: INFO: scanned /root for discovery docs: <nil>
Jul  3 11:10:56.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-985'
Jul  3 11:10:57.266: INFO: stderr: ""
Jul  3 11:10:57.266: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 11:10:57.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:10:57.335: INFO: stderr: ""
Jul  3 11:10:57.335: INFO: stdout: "update-demo-nautilus-mptks update-demo-nautilus-q24m2 "
Jul  3 11:10:57.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-mptks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:10:57.399: INFO: stderr: ""
Jul  3 11:10:57.399: INFO: stdout: ""
Jul  3 11:10:57.399: INFO: update-demo-nautilus-mptks is created but not running
Jul  3 11:11:02.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-985'
Jul  3 11:11:02.474: INFO: stderr: ""
Jul  3 11:11:02.474: INFO: stdout: "update-demo-nautilus-mptks update-demo-nautilus-q24m2 "
Jul  3 11:11:02.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-mptks -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:11:02.561: INFO: stderr: ""
Jul  3 11:11:02.561: INFO: stdout: "true"
Jul  3 11:11:02.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-mptks -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:11:02.690: INFO: stderr: ""
Jul  3 11:11:02.690: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:11:02.690: INFO: validating pod update-demo-nautilus-mptks
Jul  3 11:11:02.783: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:11:02.783: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:11:02.783: INFO: update-demo-nautilus-mptks is verified up and running
Jul  3 11:11:02.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:11:02.851: INFO: stderr: ""
Jul  3 11:11:02.851: INFO: stdout: "true"
Jul  3 11:11:02.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-q24m2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-985'
Jul  3 11:11:02.922: INFO: stderr: ""
Jul  3 11:11:02.922: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:11:02.922: INFO: validating pod update-demo-nautilus-q24m2
Jul  3 11:11:02.966: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:11:02.966: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:11:02.966: INFO: update-demo-nautilus-q24m2 is verified up and running
STEP: using delete to clean up resources
Jul  3 11:11:02.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-985'
Jul  3 11:11:03.038: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:11:03.038: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul  3 11:11:03.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-985'
Jul  3 11:11:03.118: INFO: stderr: "No resources found.\n"
Jul  3 11:11:03.118: INFO: stdout: ""
Jul  3 11:11:03.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=update-demo --namespace=kubectl-985 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 11:11:03.184: INFO: stderr: ""
Jul  3 11:11:03.184: INFO: stdout: "update-demo-nautilus-mptks\nupdate-demo-nautilus-q24m2\n"
Jul  3 11:11:03.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-985'
Jul  3 11:11:03.763: INFO: stderr: "No resources found.\n"
Jul  3 11:11:03.763: INFO: stdout: ""
Jul  3 11:11:03.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -l name=update-demo --namespace=kubectl-985 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul  3 11:11:03.832: INFO: stderr: ""
Jul  3 11:11:03.832: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:11:03.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-985" for this suite.
Jul  3 11:11:09.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:11:10.013: INFO: namespace kubectl-985 deletion completed in 6.17589699s

• [SLOW TEST:36.367 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:11:10.013: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  3 11:11:12.599: INFO: Successfully updated pod "annotationupdate42eb28e5-9d83-11e9-9a35-2ab8d9f547c5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:11:14.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2890" for this suite.
Jul  3 11:11:36.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:11:36.830: INFO: namespace downward-api-2890 deletion completed in 22.204527077s

• [SLOW TEST:26.817 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:11:36.831: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-52f0e443-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 11:11:36.944: INFO: Waiting up to 5m0s for pod "pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5" in namespace "configmap-1524" to be "success or failure"
Jul  3 11:11:36.950: INFO: Pod "pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.197238ms
Jul  3 11:11:38.955: INFO: Pod "pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.0102821s
Jul  3 11:11:40.960: INFO: Pod "pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014995982s
STEP: Saw pod success
Jul  3 11:11:40.960: INFO: Pod "pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:11:40.963: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 11:11:40.987: INFO: Waiting for pod pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:11:40.990: INFO: Pod pod-configmaps-52f1d480-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:11:40.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1524" for this suite.
Jul  3 11:11:47.008: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:11:47.152: INFO: namespace configmap-1524 deletion completed in 6.157635503s

• [SLOW TEST:10.321 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:11:47.152: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-4gmsr in namespace proxy-9299
I0703 11:11:47.202517      15 runners.go:184] Created replication controller with name: proxy-service-4gmsr, namespace: proxy-9299, replica count: 1
I0703 11:11:48.252973      15 runners.go:184] proxy-service-4gmsr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0703 11:11:49.253161      15 runners.go:184] proxy-service-4gmsr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0703 11:11:50.253407      15 runners.go:184] proxy-service-4gmsr Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0703 11:11:51.253637      15 runners.go:184] proxy-service-4gmsr Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul  3 11:11:51.257: INFO: setup took 4.072151025s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul  3 11:11:51.351: INFO: (0) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 93.820534ms)
Jul  3 11:11:51.411: INFO: (0) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 153.455958ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 154.446026ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 154.748369ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 154.923309ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 154.336579ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 154.736171ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 154.660647ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 154.303229ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 154.930601ms)
Jul  3 11:11:51.412: INFO: (0) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 154.205002ms)
Jul  3 11:11:51.459: INFO: (0) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 201.209873ms)
Jul  3 11:11:51.503: INFO: (0) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 244.927272ms)
Jul  3 11:11:51.503: INFO: (0) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 245.518338ms)
Jul  3 11:11:51.503: INFO: (0) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 245.324427ms)
Jul  3 11:11:51.503: INFO: (0) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 245.040234ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 46.358089ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 46.662583ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 46.983439ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.883007ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 47.143518ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 47.114411ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 46.877277ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 47.007213ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 47.469711ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 46.878196ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 47.426621ms)
Jul  3 11:11:51.550: INFO: (1) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 47.013078ms)
Jul  3 11:11:51.565: INFO: (1) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 62.56996ms)
Jul  3 11:11:51.591: INFO: (1) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 87.729165ms)
Jul  3 11:11:51.591: INFO: (1) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 87.458307ms)
Jul  3 11:11:51.591: INFO: (1) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 87.371132ms)
Jul  3 11:11:51.679: INFO: (2) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 88.267408ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 88.156818ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 88.206536ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 88.861113ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 88.613895ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 88.521669ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 88.410913ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 88.516483ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 88.928536ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 88.600217ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 88.644146ms)
Jul  3 11:11:51.680: INFO: (2) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 88.50128ms)
Jul  3 11:11:51.768: INFO: (2) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 176.398038ms)
Jul  3 11:11:51.768: INFO: (2) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 176.46303ms)
Jul  3 11:11:51.768: INFO: (2) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 176.602013ms)
Jul  3 11:11:51.768: INFO: (2) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 176.472659ms)
Jul  3 11:11:51.813: INFO: (3) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 45.610002ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 100.007772ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 99.654763ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 99.857264ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 99.956831ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 99.72708ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 100.003343ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 99.951436ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 99.774309ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 100.12399ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 99.709454ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 99.832733ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 99.870939ms)
Jul  3 11:11:51.868: INFO: (3) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 99.705794ms)
Jul  3 11:11:51.869: INFO: (3) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 100.9843ms)
Jul  3 11:11:51.869: INFO: (3) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 101.102792ms)
Jul  3 11:11:51.919: INFO: (4) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 50.00384ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 52.527344ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 52.811516ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 52.921094ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 53.213311ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 53.166455ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 52.876055ms)
Jul  3 11:11:51.922: INFO: (4) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 52.976039ms)
Jul  3 11:11:51.923: INFO: (4) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 53.04229ms)
Jul  3 11:11:51.923: INFO: (4) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 53.39368ms)
Jul  3 11:11:51.923: INFO: (4) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 52.989474ms)
Jul  3 11:11:51.923: INFO: (4) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 53.414012ms)
Jul  3 11:11:52.011: INFO: (4) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 141.253302ms)
Jul  3 11:11:52.011: INFO: (4) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 141.234913ms)
Jul  3 11:11:52.011: INFO: (4) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 141.374213ms)
Jul  3 11:11:52.011: INFO: (4) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 141.314368ms)
Jul  3 11:11:52.061: INFO: (5) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 50.02396ms)
Jul  3 11:11:52.061: INFO: (5) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 49.836136ms)
Jul  3 11:11:52.061: INFO: (5) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 50.037668ms)
Jul  3 11:11:52.061: INFO: (5) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 50.127847ms)
Jul  3 11:11:52.061: INFO: (5) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 50.21314ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 50.148664ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 50.436957ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 50.72104ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 50.408352ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 50.534946ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 50.884858ms)
Jul  3 11:11:52.062: INFO: (5) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 50.652072ms)
Jul  3 11:11:52.102: INFO: (5) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 91.05979ms)
Jul  3 11:11:52.102: INFO: (5) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 90.999104ms)
Jul  3 11:11:52.103: INFO: (5) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 91.308896ms)
Jul  3 11:11:52.103: INFO: (5) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 91.412652ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 88.078515ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 88.476404ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 88.163656ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 88.423943ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 88.284905ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 88.380315ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 88.248949ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 88.393693ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 88.638412ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 88.731946ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 88.646017ms)
Jul  3 11:11:52.191: INFO: (6) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 88.35756ms)
Jul  3 11:11:52.279: INFO: (6) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 176.128095ms)
Jul  3 11:11:52.279: INFO: (6) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 175.902498ms)
Jul  3 11:11:52.279: INFO: (6) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 176.105372ms)
Jul  3 11:11:52.279: INFO: (6) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 175.965679ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 86.485317ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 86.833536ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 86.572668ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 86.910529ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 86.760403ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 87.159319ms)
Jul  3 11:11:52.366: INFO: (7) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 86.811128ms)
Jul  3 11:11:52.367: INFO: (7) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 87.260526ms)
Jul  3 11:11:52.367: INFO: (7) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 87.226084ms)
Jul  3 11:11:52.367: INFO: (7) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 87.126293ms)
Jul  3 11:11:52.367: INFO: (7) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 87.190992ms)
Jul  3 11:11:52.367: INFO: (7) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 87.312939ms)
Jul  3 11:11:52.451: INFO: (7) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 171.376725ms)
Jul  3 11:11:52.451: INFO: (7) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 171.572899ms)
Jul  3 11:11:52.451: INFO: (7) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 171.471185ms)
Jul  3 11:11:52.451: INFO: (7) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 171.3895ms)
Jul  3 11:11:52.497: INFO: (8) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 45.696359ms)
Jul  3 11:11:52.497: INFO: (8) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 45.660438ms)
Jul  3 11:11:52.497: INFO: (8) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 46.064511ms)
Jul  3 11:11:52.497: INFO: (8) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 45.704822ms)
Jul  3 11:11:52.497: INFO: (8) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.101071ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 114.595775ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 114.524093ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 114.525958ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 114.407937ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 114.905417ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 114.685113ms)
Jul  3 11:11:52.566: INFO: (8) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 114.90384ms)
Jul  3 11:11:52.567: INFO: (8) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 115.15726ms)
Jul  3 11:11:52.567: INFO: (8) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 114.986975ms)
Jul  3 11:11:52.567: INFO: (8) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 115.230392ms)
Jul  3 11:11:52.567: INFO: (8) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 115.3841ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 46.237904ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 46.178829ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 45.946128ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 45.900676ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.000087ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 46.420917ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 45.961937ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 46.650241ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 46.208709ms)
Jul  3 11:11:52.613: INFO: (9) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.173011ms)
Jul  3 11:11:52.614: INFO: (9) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 47.425967ms)
Jul  3 11:11:52.614: INFO: (9) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 47.013589ms)
Jul  3 11:11:52.699: INFO: (9) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 131.636696ms)
Jul  3 11:11:52.699: INFO: (9) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 131.859366ms)
Jul  3 11:11:52.699: INFO: (9) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 131.565852ms)
Jul  3 11:11:52.699: INFO: (9) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 131.908717ms)
Jul  3 11:11:52.752: INFO: (10) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 51.264634ms)
Jul  3 11:11:52.752: INFO: (10) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 51.804422ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 52.726294ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 53.970834ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 53.027791ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 52.808425ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 53.229467ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 52.845806ms)
Jul  3 11:11:52.753: INFO: (10) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 53.34066ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 54.328527ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 53.118908ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 53.772885ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 53.79649ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 53.739078ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 54.875717ms)
Jul  3 11:11:52.754: INFO: (10) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 53.622392ms)
Jul  3 11:11:52.797: INFO: (11) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 42.864075ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 48.260522ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 48.512782ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 48.263654ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 48.087294ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 48.258281ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 48.43346ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 48.722561ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 48.571393ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 48.360914ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 48.66729ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 48.570476ms)
Jul  3 11:11:52.803: INFO: (11) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 48.868576ms)
Jul  3 11:11:52.891: INFO: (11) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 136.250231ms)
Jul  3 11:11:52.891: INFO: (11) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 136.63664ms)
Jul  3 11:11:52.891: INFO: (11) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 136.240866ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 87.589522ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 87.508806ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 87.705255ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 87.749863ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 87.97354ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 88.157069ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 88.256969ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 87.784702ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 87.891663ms)
Jul  3 11:11:52.979: INFO: (12) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 88.041528ms)
Jul  3 11:11:52.980: INFO: (12) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 88.034791ms)
Jul  3 11:11:52.980: INFO: (12) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 88.278171ms)
Jul  3 11:11:53.071: INFO: (12) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 180.064134ms)
Jul  3 11:11:53.071: INFO: (12) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 179.909012ms)
Jul  3 11:11:53.071: INFO: (12) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 179.584499ms)
Jul  3 11:11:53.071: INFO: (12) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 179.862985ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 45.177978ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 44.983821ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 45.027997ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 45.396707ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 45.169268ms)
Jul  3 11:11:53.117: INFO: (13) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 45.4055ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 45.797525ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 46.111124ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 45.74399ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 45.883809ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 46.462478ms)
Jul  3 11:11:53.118: INFO: (13) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 46.289505ms)
Jul  3 11:11:53.203: INFO: (13) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 131.418045ms)
Jul  3 11:11:53.203: INFO: (13) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 131.631039ms)
Jul  3 11:11:53.203: INFO: (13) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 131.326317ms)
Jul  3 11:11:53.203: INFO: (13) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 131.54147ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 44.800442ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 45.659584ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 45.069728ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 45.772041ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 45.287967ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 45.658567ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 45.514573ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 45.211836ms)
Jul  3 11:11:53.249: INFO: (14) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 45.424423ms)
Jul  3 11:11:53.250: INFO: (14) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 45.91698ms)
Jul  3 11:11:53.250: INFO: (14) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 45.820344ms)
Jul  3 11:11:53.250: INFO: (14) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 46.042016ms)
Jul  3 11:11:53.295: INFO: (14) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 90.287161ms)
Jul  3 11:11:53.295: INFO: (14) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 90.332996ms)
Jul  3 11:11:53.295: INFO: (14) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 90.632259ms)
Jul  3 11:11:53.295: INFO: (14) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 90.718843ms)
Jul  3 11:11:53.341: INFO: (15) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 45.621883ms)
Jul  3 11:11:53.341: INFO: (15) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 45.557145ms)
Jul  3 11:11:53.341: INFO: (15) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 45.617705ms)
Jul  3 11:11:53.341: INFO: (15) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.339059ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 46.451122ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 46.311141ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 46.70114ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 46.510071ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 46.932237ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 46.988142ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 47.201674ms)
Jul  3 11:11:53.342: INFO: (15) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 47.28377ms)
Jul  3 11:11:53.343: INFO: (15) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 47.747027ms)
Jul  3 11:11:53.374: INFO: (15) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 78.735541ms)
Jul  3 11:11:53.374: INFO: (15) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 79.221944ms)
Jul  3 11:11:53.374: INFO: (15) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 78.858486ms)
Jul  3 11:11:53.426: INFO: (16) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 50.920802ms)
Jul  3 11:11:53.426: INFO: (16) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 50.836183ms)
Jul  3 11:11:53.426: INFO: (16) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 51.239147ms)
Jul  3 11:11:53.426: INFO: (16) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 51.075968ms)
Jul  3 11:11:53.426: INFO: (16) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 51.182392ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 51.959958ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 52.202238ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 52.273272ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 52.100225ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 52.182954ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 51.951845ms)
Jul  3 11:11:53.427: INFO: (16) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 52.386419ms)
Jul  3 11:11:53.515: INFO: (16) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 140.46856ms)
Jul  3 11:11:53.515: INFO: (16) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 140.824312ms)
Jul  3 11:11:53.515: INFO: (16) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 140.552758ms)
Jul  3 11:11:53.516: INFO: (16) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 140.76362ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 86.433569ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 87.134344ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 87.259461ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 87.227554ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 86.919299ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 86.848549ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 86.921762ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 87.235269ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 87.058159ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 87.503193ms)
Jul  3 11:11:53.603: INFO: (17) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 87.648178ms)
Jul  3 11:11:53.604: INFO: (17) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 87.689162ms)
Jul  3 11:11:53.692: INFO: (17) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 175.809329ms)
Jul  3 11:11:53.692: INFO: (17) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 175.668581ms)
Jul  3 11:11:53.692: INFO: (17) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 175.796404ms)
Jul  3 11:11:53.692: INFO: (17) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 176.121562ms)
Jul  3 11:11:53.737: INFO: (18) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 44.722516ms)
Jul  3 11:11:53.737: INFO: (18) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 45.344722ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 75.116303ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 75.044742ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 75.055492ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 75.124178ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 75.412973ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 75.159185ms)
Jul  3 11:11:53.767: INFO: (18) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 75.129643ms)
Jul  3 11:11:53.768: INFO: (18) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 75.455012ms)
Jul  3 11:11:53.768: INFO: (18) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 75.599536ms)
Jul  3 11:11:53.768: INFO: (18) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 75.313029ms)
Jul  3 11:11:53.768: INFO: (18) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 75.340095ms)
Jul  3 11:11:53.815: INFO: (18) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 122.287665ms)
Jul  3 11:11:53.815: INFO: (18) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 122.644184ms)
Jul  3 11:11:53.815: INFO: (18) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 122.753329ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname1/proxy/: foo (200; 87.846474ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname1/proxy/: foo (200; 87.84575ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/services/http:proxy-service-4gmsr:portname2/proxy/: bar (200; 88.123828ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname1/proxy/: tls baz (200; 87.708495ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:462/proxy/: tls qux (200; 87.801161ms)
Jul  3 11:11:53.903: INFO: (19) /api/v1/namespaces/proxy-9299/services/https:proxy-service-4gmsr:tlsportname2/proxy/: tls qux (200; 88.228027ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:443/proxy/tlsrewritem... (200; 88.017959ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj/proxy/rewriteme">test</a> (200; 88.159161ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 88.292559ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">... (200; 88.274548ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/https:proxy-service-4gmsr-mcbdj:460/proxy/: tls baz (200; 88.46031ms)
Jul  3 11:11:53.904: INFO: (19) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/: <a href="/api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:1080/proxy/rewriteme">test<... (200; 88.205148ms)
Jul  3 11:11:53.991: INFO: (19) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:162/proxy/: bar (200; 175.064357ms)
Jul  3 11:11:53.991: INFO: (19) /api/v1/namespaces/proxy-9299/pods/proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 175.546378ms)
Jul  3 11:11:53.991: INFO: (19) /api/v1/namespaces/proxy-9299/services/proxy-service-4gmsr:portname2/proxy/: bar (200; 175.659864ms)
Jul  3 11:11:53.991: INFO: (19) /api/v1/namespaces/proxy-9299/pods/http:proxy-service-4gmsr-mcbdj:160/proxy/: foo (200; 175.790445ms)
STEP: deleting ReplicationController proxy-service-4gmsr in namespace proxy-9299, will wait for the garbage collector to delete the pods
Jul  3 11:11:54.053: INFO: Deleting ReplicationController proxy-service-4gmsr took: 8.604197ms
Jul  3 11:11:54.554: INFO: Terminating ReplicationController proxy-service-4gmsr pods took: 500.163061ms
[AfterEach] version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:12:02.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9299" for this suite.
Jul  3 11:12:08.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:12:08.824: INFO: namespace proxy-9299 deletion completed in 6.161355342s

• [SLOW TEST:21.672 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:12:08.824: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul  3 11:12:08.878: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307274,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul  3 11:12:08.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307275,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul  3 11:12:08.878: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307276,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul  3 11:12:18.908: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307309,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul  3 11:12:18.908: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307310,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul  3 11:12:18.908: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3417,SelfLink:/api/v1/namespaces/watch-3417/configmaps/e2e-watch-test-label-changed,UID:65f8f564-9d83-11e9-9bcb-a6641e9563f8,ResourceVersion:307311,Generation:0,CreationTimestamp:2019-07-03 11:12:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:12:18.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3417" for this suite.
Jul  3 11:12:24.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:12:25.096: INFO: namespace watch-3417 deletion completed in 6.18360938s

• [SLOW TEST:16.272 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:12:25.097: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul  3 11:12:25.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5286'
Jul  3 11:12:25.207: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul  3 11:12:25.207: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jul  3 11:12:25.213: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
Jul  3 11:12:25.215: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jul  3 11:12:25.219: INFO: scanned /root for discovery docs: <nil>
Jul  3 11:12:25.219: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5286'
Jul  3 11:12:40.984: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  3 11:12:40.984: INFO: stdout: "Created e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9\nScaling up e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul  3 11:12:40.984: INFO: stdout: "Created e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9\nScaling up e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul  3 11:12:40.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5286'
Jul  3 11:12:41.216: INFO: stderr: ""
Jul  3 11:12:41.216: INFO: stdout: "e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9-rxh78 e2e-test-nginx-rc-qt77k "
STEP: Replicas for run=e2e-test-nginx-rc: expected=1 actual=2
Jul  3 11:12:46.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5286'
Jul  3 11:12:46.286: INFO: stderr: ""
Jul  3 11:12:46.286: INFO: stdout: "e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9-rxh78 "
Jul  3 11:12:46.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9-rxh78 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5286'
Jul  3 11:12:46.358: INFO: stderr: ""
Jul  3 11:12:46.358: INFO: stdout: "true"
Jul  3 11:12:46.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9-rxh78 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5286'
Jul  3 11:12:46.423: INFO: stderr: ""
Jul  3 11:12:46.423: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul  3 11:12:46.423: INFO: e2e-test-nginx-rc-be8d947a6c92c9a213bd30f55466c3c9-rxh78 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Jul  3 11:12:46.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete rc e2e-test-nginx-rc --namespace=kubectl-5286'
Jul  3 11:12:46.496: INFO: stderr: ""
Jul  3 11:12:46.496: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:12:46.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5286" for this suite.
Jul  3 11:13:08.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:08.671: INFO: namespace kubectl-5286 deletion completed in 22.162272656s

• [SLOW TEST:43.574 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:08.672: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul  3 11:13:08.714: INFO: Waiting up to 5m0s for pod "pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-603" to be "success or failure"
Jul  3 11:13:08.718: INFO: Pod "pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.245421ms
Jul  3 11:13:10.723: INFO: Pod "pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008588137s
STEP: Saw pod success
Jul  3 11:13:10.723: INFO: Pod "pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:10.726: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 11:13:10.757: INFO: Waiting for pod pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:10.760: INFO: Pod pod-89a472dc-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:10.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-603" for this suite.
Jul  3 11:13:16.777: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:16.988: INFO: namespace emptydir-603 deletion completed in 6.223283674s

• [SLOW TEST:8.316 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:16.988: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 11:13:17.032: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5" in namespace "projected-9574" to be "success or failure"
Jul  3 11:13:17.038: INFO: Pod "downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.342159ms
Jul  3 11:13:19.042: INFO: Pod "downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5": Phase="Running", Reason="", readiness=true. Elapsed: 2.009374146s
Jul  3 11:13:21.046: INFO: Pod "downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013935872s
STEP: Saw pod success
Jul  3 11:13:21.046: INFO: Pod "downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:21.050: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 11:13:21.077: INFO: Waiting for pod downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:21.080: INFO: Pod downwardapi-volume-8e99b9c6-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:21.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9574" for this suite.
Jul  3 11:13:27.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:27.249: INFO: namespace projected-9574 deletion completed in 6.164481531s

• [SLOW TEST:10.261 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:27.249: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 11:13:27.293: INFO: Waiting up to 5m0s for pod "downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5" in namespace "projected-9355" to be "success or failure"
Jul  3 11:13:27.297: INFO: Pod "downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.912964ms
Jul  3 11:13:29.302: INFO: Pod "downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00850909s
STEP: Saw pod success
Jul  3 11:13:29.302: INFO: Pod "downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:29.305: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 11:13:29.328: INFO: Waiting for pod downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:29.331: INFO: Pod downwardapi-volume-94b7638e-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:29.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9355" for this suite.
Jul  3 11:13:35.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:35.502: INFO: namespace projected-9355 deletion completed in 6.16610726s

• [SLOW TEST:8.253 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:35.502: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-99a2ba8d-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 11:13:35.550: INFO: Waiting up to 5m0s for pod "pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5" in namespace "configmap-688" to be "success or failure"
Jul  3 11:13:35.555: INFO: Pod "pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699753ms
Jul  3 11:13:37.559: INFO: Pod "pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008584103s
Jul  3 11:13:39.563: INFO: Pod "pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012882649s
STEP: Saw pod success
Jul  3 11:13:39.563: INFO: Pod "pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:39.567: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5 container configmap-volume-test: <nil>
STEP: delete the pod
Jul  3 11:13:39.627: INFO: Waiting for pod pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:39.631: INFO: Pod pod-configmaps-99a3c2ac-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:39.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-688" for this suite.
Jul  3 11:13:45.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:45.799: INFO: namespace configmap-688 deletion completed in 6.162411229s

• [SLOW TEST:10.297 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:45.801: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 11:13:45.843: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5" in namespace "downward-api-1640" to be "success or failure"
Jul  3 11:13:45.848: INFO: Pod "downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.055323ms
Jul  3 11:13:47.852: INFO: Pod "downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009326715s
STEP: Saw pod success
Jul  3 11:13:47.852: INFO: Pod "downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:47.856: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 11:13:47.879: INFO: Waiting for pod downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:47.882: INFO: Pod downwardapi-volume-9fc5fded-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:47.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1640" for this suite.
Jul  3 11:13:53.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:13:54.049: INFO: namespace downward-api-1640 deletion completed in 6.162803321s

• [SLOW TEST:8.248 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:13:54.049: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul  3 11:13:54.090: INFO: Waiting up to 5m0s for pod "pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5" in namespace "emptydir-9380" to be "success or failure"
Jul  3 11:13:54.095: INFO: Pod "pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.66135ms
Jul  3 11:13:56.099: INFO: Pod "pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008704303s
STEP: Saw pod success
Jul  3 11:13:56.099: INFO: Pod "pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:13:56.102: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5 container test-container: <nil>
STEP: delete the pod
Jul  3 11:13:56.125: INFO: Waiting for pod pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:13:56.129: INFO: Pod pod-a4b06dee-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:13:56.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9380" for this suite.
Jul  3 11:14:02.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:14:02.290: INFO: namespace emptydir-9380 deletion completed in 6.155824134s

• [SLOW TEST:8.241 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:14:02.291: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-vklb
STEP: Creating a pod to test atomic-volume-subpath
Jul  3 11:14:02.350: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-vklb" in namespace "subpath-6561" to be "success or failure"
Jul  3 11:14:02.355: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.531156ms
Jul  3 11:14:04.361: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 2.011488536s
Jul  3 11:14:06.365: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 4.015587923s
Jul  3 11:14:08.370: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 6.019947643s
Jul  3 11:14:10.374: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 8.024433641s
Jul  3 11:14:12.379: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 10.029248815s
Jul  3 11:14:14.385: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 12.035160595s
Jul  3 11:14:16.389: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 14.039395048s
Jul  3 11:14:18.394: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 16.043801869s
Jul  3 11:14:20.398: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 18.04845865s
Jul  3 11:14:22.403: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 20.053162868s
Jul  3 11:14:24.408: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Running", Reason="", readiness=true. Elapsed: 22.057885801s
Jul  3 11:14:26.413: INFO: Pod "pod-subpath-test-downwardapi-vklb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.063668944s
STEP: Saw pod success
Jul  3 11:14:26.414: INFO: Pod "pod-subpath-test-downwardapi-vklb" satisfied condition "success or failure"
Jul  3 11:14:26.417: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-subpath-test-downwardapi-vklb container test-container-subpath-downwardapi-vklb: <nil>
STEP: delete the pod
Jul  3 11:14:26.440: INFO: Waiting for pod pod-subpath-test-downwardapi-vklb to disappear
Jul  3 11:14:26.445: INFO: Pod pod-subpath-test-downwardapi-vklb no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-vklb
Jul  3 11:14:26.445: INFO: Deleting pod "pod-subpath-test-downwardapi-vklb" in namespace "subpath-6561"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:14:26.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6561" for this suite.
Jul  3 11:14:32.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:14:32.695: INFO: namespace subpath-6561 deletion completed in 6.241688297s

• [SLOW TEST:30.404 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:14:32.695: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Jul  3 11:14:37.282: INFO: Successfully updated pod "labelsupdatebbb99292-9d83-11e9-9a35-2ab8d9f547c5"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:14:39.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5326" for this suite.
Jul  3 11:15:01.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:15:01.485: INFO: namespace downward-api-5326 deletion completed in 22.174348132s

• [SLOW TEST:28.790 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:15:01.485: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 11:15:01.543: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul  3 11:15:01.560: INFO: Number of nodes with available pods: 0
Jul  3 11:15:01.560: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:15:02.569: INFO: Number of nodes with available pods: 0
Jul  3 11:15:02.569: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:15:03.570: INFO: Number of nodes with available pods: 0
Jul  3 11:15:03.570: INFO: Node ip-172-31-22-121.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:15:04.574: INFO: Number of nodes with available pods: 3
Jul  3 11:15:04.574: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul  3 11:15:04.603: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:04.603: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:04.603: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:05.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:05.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:05.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:06.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:06.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:06.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:07.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:07.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:07.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:07.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:08.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:08.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:08.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:08.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:09.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:09.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:09.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:09.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:10.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:10.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:10.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:10.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:11.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:11.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:11.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:11.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:12.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:12.613: INFO: Wrong image for pod: daemon-set-grvpq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:12.613: INFO: Pod daemon-set-grvpq is not available
Jul  3 11:15:12.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:13.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:13.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:13.613: INFO: Pod daemon-set-p7mwd is not available
Jul  3 11:15:14.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:14.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:14.613: INFO: Pod daemon-set-p7mwd is not available
Jul  3 11:15:15.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:15.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:16.617: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:16.617: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:16.617: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:17.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:17.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:17.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:18.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:18.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:18.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:19.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:19.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:19.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:20.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:20.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:20.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:21.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:21.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:21.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:22.614: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:22.614: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:22.614: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:23.614: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:23.614: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:23.614: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:24.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:24.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:24.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:25.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:25.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:25.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:26.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:26.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:26.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:27.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:27.613: INFO: Wrong image for pod: daemon-set-ls4fp. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:27.613: INFO: Pod daemon-set-ls4fp is not available
Jul  3 11:15:28.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:28.613: INFO: Pod daemon-set-pb9qc is not available
Jul  3 11:15:29.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:29.613: INFO: Pod daemon-set-pb9qc is not available
Jul  3 11:15:30.614: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:30.614: INFO: Pod daemon-set-pb9qc is not available
Jul  3 11:15:31.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:32.613: INFO: Wrong image for pod: daemon-set-dblfg. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul  3 11:15:32.613: INFO: Pod daemon-set-dblfg is not available
Jul  3 11:15:33.613: INFO: Pod daemon-set-xgbcb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Jul  3 11:15:33.625: INFO: Number of nodes with available pods: 2
Jul  3 11:15:33.625: INFO: Node ip-172-31-26-241.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:15:34.634: INFO: Number of nodes with available pods: 2
Jul  3 11:15:34.634: INFO: Node ip-172-31-26-241.eu-central-1.compute.internal is running more than one daemon pod
Jul  3 11:15:35.635: INFO: Number of nodes with available pods: 3
Jul  3 11:15:35.635: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8229, will wait for the garbage collector to delete the pods
Jul  3 11:15:35.717: INFO: Deleting DaemonSet.extensions daemon-set took: 9.577319ms
Jul  3 11:15:36.218: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.221004ms
Jul  3 11:15:50.023: INFO: Number of nodes with available pods: 0
Jul  3 11:15:50.023: INFO: Number of running nodes: 0, number of available pods: 0
Jul  3 11:15:50.030: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8229/daemonsets","resourceVersion":"308386"},"items":null}

Jul  3 11:15:50.033: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8229/pods","resourceVersion":"308386"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:15:50.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8229" for this suite.
Jul  3 11:15:56.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:15:56.211: INFO: namespace daemonsets-8229 deletion completed in 6.154684994s

• [SLOW TEST:54.726 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:15:56.211: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-531/configmap-test-ed81175b-9d83-11e9-9a35-2ab8d9f547c5
STEP: Creating a pod to test consume configMaps
Jul  3 11:15:56.259: INFO: Waiting up to 5m0s for pod "pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5" in namespace "configmap-531" to be "success or failure"
Jul  3 11:15:56.263: INFO: Pod "pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.365704ms
Jul  3 11:15:58.268: INFO: Pod "pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009803875s
STEP: Saw pod success
Jul  3 11:15:58.268: INFO: Pod "pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:15:58.272: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5 container env-test: <nil>
STEP: delete the pod
Jul  3 11:15:58.333: INFO: Waiting for pod pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:15:58.336: INFO: Pod pod-configmaps-ed82103a-9d83-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:15:58.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-531" for this suite.
Jul  3 11:16:04.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:16:04.510: INFO: namespace configmap-531 deletion completed in 6.168783177s

• [SLOW TEST:8.298 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:16:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Jul  3 11:16:04.543: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul  3 11:16:04.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:04.773: INFO: stderr: ""
Jul  3 11:16:04.773: INFO: stdout: "service/redis-slave created\n"
Jul  3 11:16:04.773: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul  3 11:16:04.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:05.013: INFO: stderr: ""
Jul  3 11:16:05.013: INFO: stdout: "service/redis-master created\n"
Jul  3 11:16:05.013: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul  3 11:16:05.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:05.265: INFO: stderr: ""
Jul  3 11:16:05.265: INFO: stdout: "service/frontend created\n"
Jul  3 11:16:05.265: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul  3 11:16:05.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:05.503: INFO: stderr: ""
Jul  3 11:16:05.503: INFO: stdout: "deployment.apps/frontend created\n"
Jul  3 11:16:05.503: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul  3 11:16:05.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:05.747: INFO: stderr: ""
Jul  3 11:16:05.747: INFO: stdout: "deployment.apps/redis-master created\n"
Jul  3 11:16:05.747: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul  3 11:16:05.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-3212'
Jul  3 11:16:06.030: INFO: stderr: ""
Jul  3 11:16:06.030: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul  3 11:16:06.030: INFO: Waiting for all frontend pods to be Running.
Jul  3 11:16:21.080: INFO: Waiting for frontend to serve content.
Jul  3 11:16:26.145: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jul  3 11:16:31.254: INFO: Trying to add a new entry to the guestbook.
Jul  3 11:16:31.387: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jul  3 11:16:31.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.523: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.523: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul  3 11:16:31.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.607: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.607: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  3 11:16:31.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.700: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.700: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  3 11:16:31.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.786: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.786: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul  3 11:16:31.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.866: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.866: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul  3 11:16:31.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 delete --grace-period=0 --force -f - --namespace=kubectl-3212'
Jul  3 11:16:31.950: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul  3 11:16:31.950: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:16:31.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3212" for this suite.
Jul  3 11:17:11.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:17:12.114: INFO: namespace kubectl-3212 deletion completed in 40.158621344s

• [SLOW TEST:67.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:17:12.114: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul  3 11:17:16.681: INFO: Successfully updated pod "pod-update-1abf0f5c-9d84-11e9-9a35-2ab8d9f547c5"
STEP: verifying the updated pod is in kubernetes
Jul  3 11:17:16.688: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:17:16.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5358" for this suite.
Jul  3 11:17:38.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:17:38.854: INFO: namespace pods-5358 deletion completed in 22.161971912s

• [SLOW TEST:26.740 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:17:38.855: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-kw9s
STEP: Creating a pod to test atomic-volume-subpath
Jul  3 11:17:38.907: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kw9s" in namespace "subpath-8669" to be "success or failure"
Jul  3 11:17:38.913: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.56554ms
Jul  3 11:17:40.917: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010001717s
Jul  3 11:17:42.922: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.014577246s
Jul  3 11:17:44.927: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.01959907s
Jul  3 11:17:46.932: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.024814432s
Jul  3 11:17:48.937: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.029431618s
Jul  3 11:17:50.941: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.03398451s
Jul  3 11:17:52.946: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.038449325s
Jul  3 11:17:54.950: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.043087228s
Jul  3 11:17:56.955: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.047962797s
Jul  3 11:17:58.963: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.055909948s
Jul  3 11:18:00.968: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Running", Reason="", readiness=true. Elapsed: 22.060658271s
Jul  3 11:18:02.972: INFO: Pod "pod-subpath-test-configmap-kw9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.065074696s
STEP: Saw pod success
Jul  3 11:18:02.972: INFO: Pod "pod-subpath-test-configmap-kw9s" satisfied condition "success or failure"
Jul  3 11:18:02.976: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod pod-subpath-test-configmap-kw9s container test-container-subpath-configmap-kw9s: <nil>
STEP: delete the pod
Jul  3 11:18:03.077: INFO: Waiting for pod pod-subpath-test-configmap-kw9s to disappear
Jul  3 11:18:03.081: INFO: Pod pod-subpath-test-configmap-kw9s no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kw9s
Jul  3 11:18:03.081: INFO: Deleting pod "pod-subpath-test-configmap-kw9s" in namespace "subpath-8669"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:18:03.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8669" for this suite.
Jul  3 11:18:09.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:18:09.246: INFO: namespace subpath-8669 deletion completed in 6.157573522s

• [SLOW TEST:30.391 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:18:09.246: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Jul  3 11:18:09.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 create -f - --namespace=kubectl-8090'
Jul  3 11:18:09.524: INFO: stderr: ""
Jul  3 11:18:09.524: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 11:18:09.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8090'
Jul  3 11:18:09.604: INFO: stderr: ""
Jul  3 11:18:09.604: INFO: stdout: "update-demo-nautilus-tlbrp update-demo-nautilus-vshph "
Jul  3 11:18:09.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-tlbrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:09.671: INFO: stderr: ""
Jul  3 11:18:09.671: INFO: stdout: ""
Jul  3 11:18:09.671: INFO: update-demo-nautilus-tlbrp is created but not running
Jul  3 11:18:14.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8090'
Jul  3 11:18:14.746: INFO: stderr: ""
Jul  3 11:18:14.746: INFO: stdout: "update-demo-nautilus-tlbrp update-demo-nautilus-vshph "
Jul  3 11:18:14.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-tlbrp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:14.811: INFO: stderr: ""
Jul  3 11:18:14.811: INFO: stdout: "true"
Jul  3 11:18:14.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-tlbrp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:14.877: INFO: stderr: ""
Jul  3 11:18:14.877: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:18:14.877: INFO: validating pod update-demo-nautilus-tlbrp
Jul  3 11:18:15.165: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:18:15.165: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:18:15.165: INFO: update-demo-nautilus-tlbrp is verified up and running
Jul  3 11:18:15.165: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-vshph -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:15.231: INFO: stderr: ""
Jul  3 11:18:15.231: INFO: stdout: "true"
Jul  3 11:18:15.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-nautilus-vshph -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:15.299: INFO: stderr: ""
Jul  3 11:18:15.299: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul  3 11:18:15.299: INFO: validating pod update-demo-nautilus-vshph
Jul  3 11:18:15.390: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul  3 11:18:15.390: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul  3 11:18:15.391: INFO: update-demo-nautilus-vshph is verified up and running
STEP: rolling-update to new replication controller
Jul  3 11:18:15.392: INFO: scanned /root for discovery docs: <nil>
Jul  3 11:18:15.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8090'
Jul  3 11:18:37.749: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul  3 11:18:37.749: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul  3 11:18:37.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8090'
Jul  3 11:18:37.822: INFO: stderr: ""
Jul  3 11:18:37.822: INFO: stdout: "update-demo-kitten-lpzsd update-demo-kitten-p4xz8 update-demo-nautilus-vshph "
STEP: Replicas for name=update-demo: expected=2 actual=3
Jul  3 11:18:42.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8090'
Jul  3 11:18:42.891: INFO: stderr: ""
Jul  3 11:18:42.891: INFO: stdout: "update-demo-kitten-lpzsd update-demo-kitten-p4xz8 "
Jul  3 11:18:42.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-kitten-lpzsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:42.956: INFO: stderr: ""
Jul  3 11:18:42.956: INFO: stdout: "true"
Jul  3 11:18:42.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-kitten-lpzsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:43.022: INFO: stderr: ""
Jul  3 11:18:43.022: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  3 11:18:43.022: INFO: validating pod update-demo-kitten-lpzsd
Jul  3 11:18:43.123: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  3 11:18:43.123: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  3 11:18:43.123: INFO: update-demo-kitten-lpzsd is verified up and running
Jul  3 11:18:43.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-kitten-p4xz8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:43.190: INFO: stderr: ""
Jul  3 11:18:43.190: INFO: stdout: "true"
Jul  3 11:18:43.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-489780963 get pods update-demo-kitten-p4xz8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8090'
Jul  3 11:18:43.255: INFO: stderr: ""
Jul  3 11:18:43.255: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul  3 11:18:43.255: INFO: validating pod update-demo-kitten-p4xz8
Jul  3 11:18:43.351: INFO: got data: {
  "image": "kitten.jpg"
}

Jul  3 11:18:43.351: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul  3 11:18:43.351: INFO: update-demo-kitten-p4xz8 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:18:43.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8090" for this suite.
Jul  3 11:19:05.369: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:19:05.518: INFO: namespace kubectl-8090 deletion completed in 22.162357936s

• [SLOW TEST:56.272 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:19:05.518: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Jul  3 11:19:05.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5" in namespace "projected-124" to be "success or failure"
Jul  3 11:19:05.577: INFO: Pod "downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.427809ms
Jul  3 11:19:07.582: INFO: Pod "downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009235237s
Jul  3 11:19:09.586: INFO: Pod "downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013929774s
STEP: Saw pod success
Jul  3 11:19:09.586: INFO: Pod "downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5" satisfied condition "success or failure"
Jul  3 11:19:09.590: INFO: Trying to get logs from node ip-172-31-22-121.eu-central-1.compute.internal pod downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5 container client-container: <nil>
STEP: delete the pod
Jul  3 11:19:09.610: INFO: Waiting for pod downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5 to disappear
Jul  3 11:19:09.614: INFO: Pod downwardapi-volume-5e57e9bb-9d84-11e9-9a35-2ab8d9f547c5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:19:09.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-124" for this suite.
Jul  3 11:19:15.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:19:15.788: INFO: namespace projected-124 deletion completed in 6.169787059s

• [SLOW TEST:10.270 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Jul  3 11:19:15.788: INFO: >>> kubeConfig: /tmp/kubeconfig-489780963
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Jul  3 11:19:15.821: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul  3 11:19:15.832: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul  3 11:19:20.837: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul  3 11:19:20.837: INFO: Creating deployment "test-rolling-update-deployment"
Jul  3 11:19:20.843: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul  3 11:19:20.850: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul  3 11:19:22.859: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul  3 11:19:22.862: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697749560, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697749560, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697749560, loc:(*time.Location)(0x8a1a0e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697749560, loc:(*time.Location)(0x8a1a0e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul  3 11:19:24.866: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Jul  3 11:19:24.878: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1361,SelfLink:/apis/apps/v1/namespaces/deployment-1361/deployments/test-rolling-update-deployment,UID:67738fcf-9d84-11e9-9bcb-a6641e9563f8,ResourceVersion:309617,Generation:1,CreationTimestamp:2019-07-03 11:19:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-03 11:19:20 +0000 UTC 2019-07-03 11:19:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-03 11:19:23 +0000 UTC 2019-07-03 11:19:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul  3 11:19:24.882: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-1361,SelfLink:/apis/apps/v1/namespaces/deployment-1361/replicasets/test-rolling-update-deployment-67599b4d9,UID:67757686-9d84-11e9-9413-46c858ac61fd,ResourceVersion:309606,Generation:1,CreationTimestamp:2019-07-03 11:19:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 67738fcf-9d84-11e9-9bcb-a6641e9563f8 0xc001a04d00 0xc001a04d01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul  3 11:19:24.882: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul  3 11:19:24.882: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1361,SelfLink:/apis/apps/v1/namespaces/deployment-1361/replicasets/test-rolling-update-controller,UID:647644a1-9d84-11e9-9bcb-a6641e9563f8,ResourceVersion:309615,Generation:2,CreationTimestamp:2019-07-03 11:19:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 67738fcf-9d84-11e9-9bcb-a6641e9563f8 0xc001a04c37 0xc001a04c38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul  3 11:19:24.886: INFO: Pod "test-rolling-update-deployment-67599b4d9-gzz27" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-gzz27,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-1361,SelfLink:/api/v1/namespaces/deployment-1361/pods/test-rolling-update-deployment-67599b4d9-gzz27,UID:67760cf3-9d84-11e9-9413-46c858ac61fd,ResourceVersion:309605,Generation:0,CreationTimestamp:2019-07-03 11:19:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 67757686-9d84-11e9-9413-46c858ac61fd 0xc001a05590 0xc001a05591}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-8stxr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-8stxr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-8stxr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-26-241.eu-central-1.compute.internal,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001a055f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001a05610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 11:19:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 11:19:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 11:19:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-03 11:19:20 +0000 UTC  }],Message:,Reason:,HostIP:172.31.26.241,PodIP:172.25.2.78,StartTime:2019-07-03 11:19:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-03 11:19:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://00c0661b8b17e3ebf9d8382fb79985b472c26350c8a584b6781060d79645d926}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Jul  3 11:19:24.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1361" for this suite.
Jul  3 11:19:30.906: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul  3 11:19:31.052: INFO: namespace deployment-1361 deletion completed in 6.162101421s

• [SLOW TEST:15.264 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.3-beta.0.37+5e53fd6bc17c0d/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSJul  3 11:19:31.053: INFO: Running AfterSuite actions on all nodes
Jul  3 11:19:31.056: INFO: Running AfterSuite actions on node 1
Jul  3 11:19:31.056: INFO: Skipping dumping logs from cluster

Ran 204 of 3585 Specs in 5462.166 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3381 Skipped PASS

Ginkgo ran 1 suite in 1h31m3.787779702s
Test Suite Passed
