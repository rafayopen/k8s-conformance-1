I0409 08:26:26.462414      15 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-393566007
I0409 08:26:26.462628      15 e2e.go:240] Starting e2e run "2a371b75-5aa1-11e9-b737-ea708a3858a3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1554798385 - Will randomize all specs
Will run 204 of 3584 specs

Apr  9 08:26:26.599: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 08:26:26.601: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Apr  9 08:26:26.610: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Apr  9 08:26:26.626: INFO: 1 / 1 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Apr  9 08:26:26.626: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Apr  9 08:26:26.626: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Apr  9 08:26:26.631: INFO: e2e test version: v1.14.0
Apr  9 08:26:26.631: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:26:26.632: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
Apr  9 08:26:26.655: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:26:26.659: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3" in namespace "projected-4330" to be "success or failure"
Apr  9 08:26:26.660: INFO: Pod "downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.248072ms
Apr  9 08:26:28.662: INFO: Pod "downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003369251s
Apr  9 08:26:30.664: INFO: Pod "downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005102845s
STEP: Saw pod success
Apr  9 08:26:30.664: INFO: Pod "downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:26:30.665: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:26:30.688: INFO: Waiting for pod downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:26:30.690: INFO: Pod downwardapi-volume-2ad846f8-5aa1-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:26:30.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4330" for this suite.
Apr  9 08:26:36.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:26:36.735: INFO: namespace projected-4330 deletion completed in 6.044186476s

• [SLOW TEST:10.103 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:26:36.735: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Apr  9 08:26:36.761: INFO: Waiting up to 5m0s for pod "client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3" in namespace "containers-5087" to be "success or failure"
Apr  9 08:26:36.763: INFO: Pod "client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.823502ms
Apr  9 08:26:38.765: INFO: Pod "client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003825226s
Apr  9 08:26:40.767: INFO: Pod "client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005742278s
STEP: Saw pod success
Apr  9 08:26:40.767: INFO: Pod "client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:26:40.768: INFO: Trying to get logs from node ip-172-31-13-147 pod client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 08:26:40.774: INFO: Waiting for pod client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:26:40.776: INFO: Pod client-containers-30dcdaac-5aa1-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:26:40.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5087" for this suite.
Apr  9 08:26:46.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:26:46.838: INFO: namespace containers-5087 deletion completed in 6.061111794s

• [SLOW TEST:10.103 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:26:46.838: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Apr  9 08:26:46.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9624,SelfLink:/api/v1/namespaces/watch-9624/configmaps/e2e-watch-test-resource-version,UID:36e2fa40-5aa1-11e9-bcbd-0af1119c727a,ResourceVersion:677,Generation:0,CreationTimestamp:2019-04-09 08:26:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 08:26:46.869: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-9624,SelfLink:/api/v1/namespaces/watch-9624/configmaps/e2e-watch-test-resource-version,UID:36e2fa40-5aa1-11e9-bcbd-0af1119c727a,ResourceVersion:678,Generation:0,CreationTimestamp:2019-04-09 08:26:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:26:46.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9624" for this suite.
Apr  9 08:26:52.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:26:52.918: INFO: namespace watch-9624 deletion completed in 6.041078805s

• [SLOW TEST:6.079 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:26:52.918: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 08:26:52.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3150'
Apr  9 08:26:53.722: INFO: stderr: ""
Apr  9 08:26:53.722: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Apr  9 08:26:53.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete pods e2e-test-nginx-pod --namespace=kubectl-3150'
Apr  9 08:26:59.250: INFO: stderr: ""
Apr  9 08:26:59.250: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:26:59.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3150" for this suite.
Apr  9 08:27:05.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:27:05.299: INFO: namespace kubectl-3150 deletion completed in 6.048142686s

• [SLOW TEST:12.381 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:27:05.300: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 08:27:05.332: INFO: Number of nodes with available pods: 0
Apr  9 08:27:05.332: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:06.335: INFO: Number of nodes with available pods: 0
Apr  9 08:27:06.335: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:07.335: INFO: Number of nodes with available pods: 0
Apr  9 08:27:07.335: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:08.335: INFO: Number of nodes with available pods: 1
Apr  9 08:27:08.335: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
Apr  9 08:27:08.343: INFO: Number of nodes with available pods: 0
Apr  9 08:27:08.343: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:09.347: INFO: Number of nodes with available pods: 0
Apr  9 08:27:09.347: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:10.347: INFO: Number of nodes with available pods: 0
Apr  9 08:27:10.347: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:11.347: INFO: Number of nodes with available pods: 0
Apr  9 08:27:11.347: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:12.347: INFO: Number of nodes with available pods: 0
Apr  9 08:27:12.347: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:13.347: INFO: Number of nodes with available pods: 0
Apr  9 08:27:13.347: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:27:14.347: INFO: Number of nodes with available pods: 1
Apr  9 08:27:14.347: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4705, will wait for the garbage collector to delete the pods
Apr  9 08:27:14.403: INFO: Deleting DaemonSet.extensions daemon-set took: 3.140059ms
Apr  9 08:27:14.703: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.289491ms
Apr  9 08:27:17.905: INFO: Number of nodes with available pods: 0
Apr  9 08:27:17.905: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 08:27:17.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4705/daemonsets","resourceVersion":"782"},"items":null}

Apr  9 08:27:17.908: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4705/pods","resourceVersion":"782"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:27:17.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4705" for this suite.
Apr  9 08:27:23.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:27:23.960: INFO: namespace daemonsets-4705 deletion completed in 6.047645145s

• [SLOW TEST:18.660 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:27:23.960: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3394
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr  9 08:27:23.990: INFO: Found 0 stateful pods, waiting for 3
Apr  9 08:27:33.992: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:27:33.992: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:27:33.992: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr  9 08:27:34.010: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Apr  9 08:27:44.043: INFO: Updating stateful set ss2
Apr  9 08:27:44.046: INFO: Waiting for Pod statefulset-3394/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Apr  9 08:27:54.096: INFO: Found 2 stateful pods, waiting for 3
Apr  9 08:28:04.099: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:28:04.099: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:28:04.099: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Apr  9 08:28:04.115: INFO: Updating stateful set ss2
Apr  9 08:28:04.119: INFO: Waiting for Pod statefulset-3394/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 08:28:14.136: INFO: Updating stateful set ss2
Apr  9 08:28:14.140: INFO: Waiting for StatefulSet statefulset-3394/ss2 to complete update
Apr  9 08:28:14.140: INFO: Waiting for Pod statefulset-3394/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 08:28:24.144: INFO: Deleting all statefulset in ns statefulset-3394
Apr  9 08:28:24.145: INFO: Scaling statefulset ss2 to 0
Apr  9 08:28:54.152: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 08:28:54.153: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:28:54.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3394" for this suite.
Apr  9 08:29:00.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:29:00.202: INFO: namespace statefulset-3394 deletion completed in 6.043246306s

• [SLOW TEST:96.242 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:29:00.202: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1308
Apr  9 08:29:04.234: INFO: Started pod liveness-http in namespace container-probe-1308
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 08:29:04.235: INFO: Initial restart count of pod liveness-http is 0
Apr  9 08:29:26.262: INFO: Restart count of pod container-probe-1308/liveness-http is now 1 (22.026626563s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:29:26.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1308" for this suite.
Apr  9 08:29:32.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:29:32.324: INFO: namespace container-probe-1308 deletion completed in 6.053544231s

• [SLOW TEST:32.122 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:29:32.325: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3950
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 08:29:32.341: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 08:29:58.379: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.1.105:8080/dial?request=hostName&protocol=udp&host=10.1.1.104&port=8081&tries=1'] Namespace:pod-network-test-3950 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:29:58.379: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 08:29:58.488: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:29:58.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3950" for this suite.
Apr  9 08:30:20.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:30:20.533: INFO: namespace pod-network-test-3950 deletion completed in 22.042758928s

• [SLOW TEST:48.208 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:30:20.533: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-ndxl8 in namespace proxy-1022
I0409 08:30:20.562416      15 runners.go:184] Created replication controller with name: proxy-service-ndxl8, namespace: proxy-1022, replica count: 1
I0409 08:30:21.612886      15 runners.go:184] proxy-service-ndxl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 08:30:22.613107      15 runners.go:184] proxy-service-ndxl8 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 08:30:23.613355      15 runners.go:184] proxy-service-ndxl8 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0409 08:30:24.613621      15 runners.go:184] proxy-service-ndxl8 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  9 08:30:24.615: INFO: setup took 4.066330388s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Apr  9 08:30:24.620: INFO: (0) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.666686ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.559386ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 5.509855ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.662485ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.447847ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 5.493877ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 5.593732ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.60705ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.740687ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.620292ms)
Apr  9 08:30:24.621: INFO: (0) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.624654ms)
Apr  9 08:30:24.628: INFO: (0) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 12.331245ms)
Apr  9 08:30:24.628: INFO: (0) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 12.81852ms)
Apr  9 08:30:24.628: INFO: (0) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 12.734299ms)
Apr  9 08:30:24.628: INFO: (0) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 13.206237ms)
Apr  9 08:30:24.629: INFO: (0) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 14.238191ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 5.022111ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.82423ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.873441ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.844244ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.951095ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.837351ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.85027ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.081923ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.019012ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.819843ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.847648ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.856388ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.019327ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.99202ms)
Apr  9 08:30:24.634: INFO: (1) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.007601ms)
Apr  9 08:30:24.635: INFO: (1) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 6.085933ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 3.888553ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 3.974498ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.068086ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.126115ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.167049ms)
Apr  9 08:30:24.640: INFO: (2) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.973945ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 22.023705ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 22.126814ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 22.138953ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 22.098524ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 22.264048ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 22.181773ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 22.324418ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 22.132078ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 22.295148ms)
Apr  9 08:30:24.658: INFO: (2) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 22.276199ms)
Apr  9 08:30:24.664: INFO: (3) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.600698ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 10.672538ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 10.752292ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 10.85276ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 10.773475ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 10.818358ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 10.764748ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 10.980936ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 11.045506ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 11.02548ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 11.186893ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 11.122055ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 11.319034ms)
Apr  9 08:30:24.669: INFO: (3) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 11.197853ms)
Apr  9 08:30:24.677: INFO: (3) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 19.093259ms)
Apr  9 08:30:24.677: INFO: (3) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 19.070168ms)
Apr  9 08:30:24.681: INFO: (4) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.513746ms)
Apr  9 08:30:24.681: INFO: (4) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 3.376635ms)
Apr  9 08:30:24.682: INFO: (4) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.366247ms)
Apr  9 08:30:24.682: INFO: (4) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.443079ms)
Apr  9 08:30:24.682: INFO: (4) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.489402ms)
Apr  9 08:30:24.682: INFO: (4) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.673278ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.857594ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.003061ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.163965ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.237011ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 5.470005ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 5.427024ms)
Apr  9 08:30:24.683: INFO: (4) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.561562ms)
Apr  9 08:30:24.684: INFO: (4) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.736676ms)
Apr  9 08:30:24.684: INFO: (4) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 5.987054ms)
Apr  9 08:30:24.684: INFO: (4) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 6.081985ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.07367ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.176352ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.332946ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.07762ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.197047ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.166869ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.075624ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.112071ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.204566ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.163213ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.271797ms)
Apr  9 08:30:24.688: INFO: (5) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.177124ms)
Apr  9 08:30:24.689: INFO: (5) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.252402ms)
Apr  9 08:30:24.689: INFO: (5) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.569336ms)
Apr  9 08:30:24.689: INFO: (5) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.802592ms)
Apr  9 08:30:24.689: INFO: (5) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.029728ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.99293ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 6.103998ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 5.78391ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.77772ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 5.902029ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.867426ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 5.903617ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 5.882865ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 6.103275ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 6.005568ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 6.094582ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 6.052854ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 6.245159ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 6.20177ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 6.246363ms)
Apr  9 08:30:24.695: INFO: (6) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 6.353384ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.166137ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.275007ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.241092ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.874609ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.824303ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.331843ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.510149ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.503001ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.433644ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.263088ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.588418ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.632893ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.051099ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.335575ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.546322ms)
Apr  9 08:30:24.701: INFO: (7) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.171263ms)
Apr  9 08:30:24.706: INFO: (8) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.348171ms)
Apr  9 08:30:24.706: INFO: (8) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.331938ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 6.475719ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 6.494894ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 6.586417ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 6.473976ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 6.71822ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 6.686221ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 6.866057ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 6.731464ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 6.833396ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 6.779489ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 6.88226ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 6.932612ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 7.080564ms)
Apr  9 08:30:24.708: INFO: (8) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 7.173474ms)
Apr  9 08:30:24.712: INFO: (9) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.898116ms)
Apr  9 08:30:24.712: INFO: (9) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.210596ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.234943ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.465395ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.217265ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.25833ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.481618ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.577366ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.415213ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.40695ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.39156ms)
Apr  9 08:30:24.713: INFO: (9) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.550323ms)
Apr  9 08:30:24.714: INFO: (9) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 6.091698ms)
Apr  9 08:30:24.714: INFO: (9) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 2.057084ms)
Apr  9 08:30:24.715: INFO: (9) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 6.281993ms)
Apr  9 08:30:24.715: INFO: (9) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 6.520319ms)
Apr  9 08:30:24.716: INFO: (10) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 1.488127ms)
Apr  9 08:30:24.717: INFO: (10) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 1.974063ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.277942ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.317351ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.296436ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.27297ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.260185ms)
Apr  9 08:30:24.719: INFO: (10) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.256601ms)
Apr  9 08:30:24.720: INFO: (10) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.009307ms)
Apr  9 08:30:24.721: INFO: (10) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.294567ms)
Apr  9 08:30:24.721: INFO: (10) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.757046ms)
Apr  9 08:30:24.722: INFO: (10) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 6.541965ms)
Apr  9 08:30:24.722: INFO: (10) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 6.661635ms)
Apr  9 08:30:24.722: INFO: (10) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 6.72801ms)
Apr  9 08:30:24.722: INFO: (10) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 6.735988ms)
Apr  9 08:30:24.722: INFO: (10) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 6.780294ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 3.526688ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 3.955326ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 3.706497ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 3.833827ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 3.902433ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.979465ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 3.731552ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 3.849053ms)
Apr  9 08:30:24.726: INFO: (11) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.261643ms)
Apr  9 08:30:24.727: INFO: (11) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.592874ms)
Apr  9 08:30:24.727: INFO: (11) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.554653ms)
Apr  9 08:30:24.727: INFO: (11) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.521228ms)
Apr  9 08:30:24.728: INFO: (11) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.053486ms)
Apr  9 08:30:24.728: INFO: (11) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.852539ms)
Apr  9 08:30:24.728: INFO: (11) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.020885ms)
Apr  9 08:30:24.728: INFO: (11) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.996421ms)
Apr  9 08:30:24.732: INFO: (12) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 3.936335ms)
Apr  9 08:30:24.732: INFO: (12) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.027682ms)
Apr  9 08:30:24.732: INFO: (12) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 3.629542ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.524739ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.682569ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.762512ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.618584ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.391204ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 5.223284ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.547271ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.89222ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.301964ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.515566ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.969502ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.034513ms)
Apr  9 08:30:24.733: INFO: (12) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.403053ms)
Apr  9 08:30:24.735: INFO: (13) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 1.769318ms)
Apr  9 08:30:24.735: INFO: (13) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 1.84284ms)
Apr  9 08:30:24.738: INFO: (13) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.073195ms)
Apr  9 08:30:24.738: INFO: (13) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.937947ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 5.256715ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.199727ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 5.105279ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 5.143284ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.090822ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 5.105382ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.123413ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 5.207132ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.283721ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.185027ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.649389ms)
Apr  9 08:30:24.739: INFO: (13) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 5.521878ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.911754ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 5.064092ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.968681ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.010113ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.150016ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.959578ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 5.136718ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.861356ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.932165ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.308626ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.089307ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 5.11824ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.228802ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 5.30188ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.997428ms)
Apr  9 08:30:24.744: INFO: (14) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.442154ms)
Apr  9 08:30:24.748: INFO: (15) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 3.341889ms)
Apr  9 08:30:24.748: INFO: (15) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 3.145478ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.490382ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.612014ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.447733ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.453588ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.634495ms)
Apr  9 08:30:24.749: INFO: (15) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.50721ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 5.077401ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 5.010541ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.079221ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.047117ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 5.173463ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.048053ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 5.139007ms)
Apr  9 08:30:24.750: INFO: (15) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.102502ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 3.239012ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 3.42995ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 3.549394ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.438698ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 3.631355ms)
Apr  9 08:30:24.753: INFO: (16) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 3.396089ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 5.036183ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 5.07657ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 5.165295ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 5.219074ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.996733ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 5.283204ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 5.267758ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 5.048088ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 5.077313ms)
Apr  9 08:30:24.755: INFO: (16) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 5.432285ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.259938ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 3.247726ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 3.473942ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 3.438227ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 3.297227ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 3.424643ms)
Apr  9 08:30:24.759: INFO: (17) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 3.442564ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.381447ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.543584ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.50867ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.586299ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.578205ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.541687ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.624535ms)
Apr  9 08:30:24.760: INFO: (17) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.66346ms)
Apr  9 08:30:24.761: INFO: (17) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 5.268427ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.411082ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.334513ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.328138ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.454283ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.262262ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.325217ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.575867ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.666927ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.705569ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.593678ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.776794ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.587693ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.67219ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.591482ms)
Apr  9 08:30:24.765: INFO: (18) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.61601ms)
Apr  9 08:30:24.766: INFO: (18) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.692915ms)
Apr  9 08:30:24.767: INFO: (19) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 1.229151ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">... (200; 4.437707ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.425119ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:462/proxy/: tls qux (200; 4.485755ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:1080/proxy/rewriteme">test<... (200; 4.316133ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:443/proxy/tlsrewritem... (200; 4.440985ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/: <a href="/api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj/proxy/rewriteme">test</a> (200; 4.582355ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname1/proxy/: tls baz (200; 4.410483ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/http:proxy-service-ndxl8-84mvj:160/proxy/: foo (200; 4.428156ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname2/proxy/: bar (200; 4.461605ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname1/proxy/: foo (200; 4.430822ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/proxy-service-ndxl8:portname2/proxy/: bar (200; 4.445019ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/pods/https:proxy-service-ndxl8-84mvj:460/proxy/: tls baz (200; 4.410655ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/http:proxy-service-ndxl8:portname1/proxy/: foo (200; 4.541511ms)
Apr  9 08:30:24.770: INFO: (19) /api/v1/namespaces/proxy-1022/services/https:proxy-service-ndxl8:tlsportname2/proxy/: tls qux (200; 4.778391ms)
Apr  9 08:30:24.771: INFO: (19) /api/v1/namespaces/proxy-1022/pods/proxy-service-ndxl8-84mvj:162/proxy/: bar (200; 4.999424ms)
STEP: deleting ReplicationController proxy-service-ndxl8 in namespace proxy-1022, will wait for the garbage collector to delete the pods
Apr  9 08:30:24.825: INFO: Deleting ReplicationController proxy-service-ndxl8 took: 2.688673ms
Apr  9 08:30:25.125: INFO: Terminating ReplicationController proxy-service-ndxl8 pods took: 300.283572ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:30:27.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1022" for this suite.
Apr  9 08:30:33.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:30:33.569: INFO: namespace proxy-1022 deletion completed in 6.042420126s

• [SLOW TEST:13.037 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:30:33.569: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 08:30:33.614: INFO: Number of nodes with available pods: 0
Apr  9 08:30:33.614: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:30:34.617: INFO: Number of nodes with available pods: 0
Apr  9 08:30:34.617: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:30:35.617: INFO: Number of nodes with available pods: 1
Apr  9 08:30:35.617: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Apr  9 08:30:35.628: INFO: Number of nodes with available pods: 0
Apr  9 08:30:35.628: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:30:36.632: INFO: Number of nodes with available pods: 0
Apr  9 08:30:36.632: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:30:37.631: INFO: Number of nodes with available pods: 1
Apr  9 08:30:37.631: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2513, will wait for the garbage collector to delete the pods
Apr  9 08:30:37.687: INFO: Deleting DaemonSet.extensions daemon-set took: 2.581517ms
Apr  9 08:30:37.788: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.232692ms
Apr  9 08:30:49.289: INFO: Number of nodes with available pods: 0
Apr  9 08:30:49.289: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 08:30:49.290: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2513/daemonsets","resourceVersion":"1506"},"items":null}

Apr  9 08:30:49.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2513/pods","resourceVersion":"1506"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:30:49.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2513" for this suite.
Apr  9 08:30:55.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:30:55.342: INFO: namespace daemonsets-2513 deletion completed in 6.046508261s

• [SLOW TEST:21.772 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:30:55.342: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cb01f4ce-5aa1-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:30:55.369: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3" in namespace "projected-3357" to be "success or failure"
Apr  9 08:30:55.371: INFO: Pod "pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.323769ms
Apr  9 08:30:57.373: INFO: Pod "pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004429803s
Apr  9 08:30:59.375: INFO: Pod "pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006284302s
STEP: Saw pod success
Apr  9 08:30:59.375: INFO: Pod "pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:30:59.376: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:30:59.385: INFO: Waiting for pod pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:30:59.393: INFO: Pod pod-projected-configmaps-cb022c56-5aa1-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:30:59.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3357" for this suite.
Apr  9 08:31:05.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:31:05.442: INFO: namespace projected-3357 deletion completed in 6.048061872s

• [SLOW TEST:10.100 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:31:05.442: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:31:05.458: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:31:09.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2477" for this suite.
Apr  9 08:31:51.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:31:51.522: INFO: namespace pods-2477 deletion completed in 42.041930541s

• [SLOW TEST:46.080 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:31:51.522: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:31:51.542: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:31:55.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3479" for this suite.
Apr  9 08:32:17.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:32:17.971: INFO: namespace init-container-3479 deletion completed in 22.058981494s

• [SLOW TEST:26.449 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:32:17.972: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Apr  9 08:32:17.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 api-versions'
Apr  9 08:32:18.077: INFO: stderr: ""
Apr  9 08:32:18.077: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:32:18.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1416" for this suite.
Apr  9 08:32:24.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:32:24.124: INFO: namespace kubectl-1416 deletion completed in 6.044736798s

• [SLOW TEST:6.152 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:32:24.124: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  9 08:32:24.149: INFO: Waiting up to 5m0s for pod "pod-ffec059c-5aa1-11e9-b737-ea708a3858a3" in namespace "emptydir-3615" to be "success or failure"
Apr  9 08:32:24.151: INFO: Pod "pod-ffec059c-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.934424ms
Apr  9 08:32:26.153: INFO: Pod "pod-ffec059c-5aa1-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004136297s
Apr  9 08:32:28.155: INFO: Pod "pod-ffec059c-5aa1-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006126149s
STEP: Saw pod success
Apr  9 08:32:28.155: INFO: Pod "pod-ffec059c-5aa1-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:32:28.157: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-ffec059c-5aa1-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 08:32:28.163: INFO: Waiting for pod pod-ffec059c-5aa1-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:32:28.164: INFO: Pod pod-ffec059c-5aa1-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:32:28.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3615" for this suite.
Apr  9 08:32:34.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:32:34.207: INFO: namespace emptydir-3615 deletion completed in 6.041765374s

• [SLOW TEST:10.083 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:32:34.208: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-05ef7021-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:32:34.240: INFO: Waiting up to 5m0s for pod "pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3" in namespace "configmap-4776" to be "success or failure"
Apr  9 08:32:34.243: INFO: Pod "pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519284ms
Apr  9 08:32:36.245: INFO: Pod "pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005556458s
STEP: Saw pod success
Apr  9 08:32:36.245: INFO: Pod "pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:32:36.246: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:32:36.259: INFO: Waiting for pod pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:32:36.260: INFO: Pod pod-configmaps-05f0a608-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:32:36.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4776" for this suite.
Apr  9 08:32:42.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:32:42.306: INFO: namespace configmap-4776 deletion completed in 6.044852615s

• [SLOW TEST:8.099 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:32:42.306: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:32:42.330: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Apr  9 08:32:42.334: INFO: Pod name sample-pod: Found 0 pods out of 1
Apr  9 08:32:47.336: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 08:32:47.336: INFO: Creating deployment "test-rolling-update-deployment"
Apr  9 08:32:47.338: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Apr  9 08:32:47.341: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Apr  9 08:32:49.344: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Apr  9 08:32:49.345: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690395567, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690395567, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690395567, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690395567, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:32:51.347: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 08:32:51.351: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1483,SelfLink:/apis/apps/v1/namespaces/deployment-1483/deployments/test-rolling-update-deployment,UID:0dbf8fc5-5aa2-11e9-bcbd-0af1119c727a,ResourceVersion:1875,Generation:1,CreationTimestamp:2019-04-09 08:32:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-09 08:32:47 +0000 UTC 2019-04-09 08:32:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-09 08:32:50 +0000 UTC 2019-04-09 08:32:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr  9 08:32:51.353: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-1483,SelfLink:/apis/apps/v1/namespaces/deployment-1483/replicasets/test-rolling-update-deployment-67599b4d9,UID:0dc0983a-5aa2-11e9-bcbd-0af1119c727a,ResourceVersion:1864,Generation:1,CreationTimestamp:2019-04-09 08:32:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0dbf8fc5-5aa2-11e9-bcbd-0af1119c727a 0xc0020f9e20 0xc0020f9e21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 08:32:51.353: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Apr  9 08:32:51.353: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1483,SelfLink:/apis/apps/v1/namespaces/deployment-1483/replicasets/test-rolling-update-controller,UID:0ac3a0f5-5aa2-11e9-bcbd-0af1119c727a,ResourceVersion:1874,Generation:2,CreationTimestamp:2019-04-09 08:32:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 0dbf8fc5-5aa2-11e9-bcbd-0af1119c727a 0xc0020f9d3f 0xc0020f9d50}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:32:51.354: INFO: Pod "test-rolling-update-deployment-67599b4d9-4x7z7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-4x7z7,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-1483,SelfLink:/api/v1/namespaces/deployment-1483/pods/test-rolling-update-deployment-67599b4d9-4x7z7,UID:0dc107ec-5aa2-11e9-bcbd-0af1119c727a,ResourceVersion:1863,Generation:0,CreationTimestamp:2019-04-09 08:32:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 0dc0983a-5aa2-11e9-bcbd-0af1119c727a 0xc0025f86b0 0xc0025f86b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vlpjc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vlpjc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vlpjc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f8720} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f8740}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:32:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:32:50 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:32:50 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:32:47 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.116,StartTime:2019-04-09 08:32:47 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-09 08:32:49 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://1b05462f500f76db1c7e875b23a3f163d78deba68c7aee76673618a72636f85d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:32:51.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1483" for this suite.
Apr  9 08:32:57.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:32:57.398: INFO: namespace deployment-1483 deletion completed in 6.042088457s

• [SLOW TEST:15.091 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:32:57.398: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-13c27eb8-5aa2-11e9-b737-ea708a3858a3
STEP: Creating secret with name s-test-opt-upd-13c27eff-5aa2-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-13c27eb8-5aa2-11e9-b737-ea708a3858a3
STEP: Updating secret s-test-opt-upd-13c27eff-5aa2-11e9-b737-ea708a3858a3
STEP: Creating secret with name s-test-opt-create-13c27f1e-5aa2-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:33:01.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-601" for this suite.
Apr  9 08:33:23.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:33:23.502: INFO: namespace secrets-601 deletion completed in 22.044958079s

• [SLOW TEST:26.104 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:33:23.502: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Apr  9 08:33:25.532: INFO: Pod pod-hostip-2350750d-5aa2-11e9-b737-ea708a3858a3 has hostIP: 172.31.13.147
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:33:25.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9967" for this suite.
Apr  9 08:33:47.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:33:47.577: INFO: namespace pods-9967 deletion completed in 22.042946603s

• [SLOW TEST:24.074 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:33:47.577: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 08:33:47.602: INFO: Waiting up to 5m0s for pod "downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3" in namespace "downward-api-4820" to be "success or failure"
Apr  9 08:33:47.603: INFO: Pod "downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.713179ms
Apr  9 08:33:49.606: INFO: Pod "downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003848586s
STEP: Saw pod success
Apr  9 08:33:49.606: INFO: Pod "downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:33:49.607: INFO: Trying to get logs from node ip-172-31-13-147 pod downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 08:33:49.617: INFO: Waiting for pod downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:33:49.618: INFO: Pod downward-api-31a9de28-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:33:49.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4820" for this suite.
Apr  9 08:33:55.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:33:55.662: INFO: namespace downward-api-4820 deletion completed in 6.042700115s

• [SLOW TEST:8.085 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:33:55.662: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Apr  9 08:33:55.688: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Apr  9 08:34:02.714: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:34:02.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9725" for this suite.
Apr  9 08:34:08.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:34:08.759: INFO: namespace pods-9725 deletion completed in 6.042270784s

• [SLOW TEST:13.096 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:34:08.759: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-3e4a960b-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:34:08.783: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-6787" to be "success or failure"
Apr  9 08:34:08.786: INFO: Pod "pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.643049ms
Apr  9 08:34:10.788: INFO: Pod "pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004678507s
STEP: Saw pod success
Apr  9 08:34:10.788: INFO: Pod "pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:34:10.789: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:34:10.799: INFO: Waiting for pod pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:34:10.819: INFO: Pod pod-projected-configmaps-3e4ad867-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:34:10.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6787" for this suite.
Apr  9 08:34:16.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:34:16.867: INFO: namespace projected-6787 deletion completed in 6.045036714s

• [SLOW TEST:8.109 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:34:16.868: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:34:16.893: INFO: Waiting up to 5m0s for pod "downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-3861" to be "success or failure"
Apr  9 08:34:16.895: INFO: Pod "downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.403622ms
Apr  9 08:34:18.897: INFO: Pod "downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003467622s
STEP: Saw pod success
Apr  9 08:34:18.897: INFO: Pod "downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:34:18.898: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:34:18.905: INFO: Waiting for pod downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:34:18.907: INFO: Pod downwardapi-volume-431f64ff-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:34:18.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3861" for this suite.
Apr  9 08:34:24.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:34:24.951: INFO: namespace projected-3861 deletion completed in 6.042283579s

• [SLOW TEST:8.083 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:34:24.951: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr  9 08:34:24.967: INFO: namespace kubectl-4846
Apr  9 08:34:24.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-4846'
Apr  9 08:34:25.161: INFO: stderr: ""
Apr  9 08:34:25.161: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 08:34:26.163: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:34:26.163: INFO: Found 0 / 1
Apr  9 08:34:27.163: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:34:27.163: INFO: Found 1 / 1
Apr  9 08:34:27.163: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 08:34:27.164: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:34:27.164: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 08:34:27.164: INFO: wait on redis-master startup in kubectl-4846 
Apr  9 08:34:27.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 logs redis-master-wf5qv redis-master --namespace=kubectl-4846'
Apr  9 08:34:27.231: INFO: stderr: ""
Apr  9 08:34:27.231: INFO: stdout: "1:M 09 Apr 08:34:26.346 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 09 Apr 08:34:26.346 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 09 Apr 08:34:26.346 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 08:34:26.346 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 08:34:26.346 # Server started, Redis version 3.2.12\n1:M 09 Apr 08:34:26.347 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 08:34:26.347 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Apr  9 08:34:27.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-4846'
Apr  9 08:34:27.307: INFO: stderr: ""
Apr  9 08:34:27.307: INFO: stdout: "service/rm2 exposed\n"
Apr  9 08:34:27.309: INFO: Service rm2 in namespace kubectl-4846 found.
STEP: exposing service
Apr  9 08:34:29.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-4846'
Apr  9 08:34:29.382: INFO: stderr: ""
Apr  9 08:34:29.383: INFO: stdout: "service/rm3 exposed\n"
Apr  9 08:34:29.384: INFO: Service rm3 in namespace kubectl-4846 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:34:31.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4846" for this suite.
Apr  9 08:34:53.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:34:53.434: INFO: namespace kubectl-4846 deletion completed in 22.045800084s

• [SLOW TEST:28.483 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:34:53.434: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 08:34:53.459: INFO: Waiting up to 5m0s for pod "downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3" in namespace "downward-api-412" to be "success or failure"
Apr  9 08:34:53.460: INFO: Pod "downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.489991ms
Apr  9 08:34:55.462: INFO: Pod "downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003308072s
STEP: Saw pod success
Apr  9 08:34:55.462: INFO: Pod "downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:34:55.464: INFO: Trying to get logs from node ip-172-31-13-147 pod downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 08:34:55.471: INFO: Waiting for pod downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:34:55.472: INFO: Pod downward-api-58eaed4d-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:34:55.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-412" for this suite.
Apr  9 08:35:01.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:35:01.514: INFO: namespace downward-api-412 deletion completed in 6.04150743s

• [SLOW TEST:8.080 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:35:01.515: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-5dbc1215-5aa2-11e9-b737-ea708a3858a3
STEP: Creating secret with name secret-projected-all-test-volume-5dbc1182-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test Check all projections for projected volume plugin
Apr  9 08:35:01.544: INFO: Waiting up to 5m0s for pod "projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-9827" to be "success or failure"
Apr  9 08:35:01.547: INFO: Pod "projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018696ms
Apr  9 08:35:03.549: INFO: Pod "projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005106122s
STEP: Saw pod success
Apr  9 08:35:03.549: INFO: Pod "projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:35:03.550: INFO: Trying to get logs from node ip-172-31-13-147 pod projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3 container projected-all-volume-test: <nil>
STEP: delete the pod
Apr  9 08:35:03.559: INFO: Waiting for pod projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:35:03.567: INFO: Pod projected-volume-5dbc1152-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:35:03.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9827" for this suite.
Apr  9 08:35:09.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:35:09.610: INFO: namespace projected-9827 deletion completed in 6.041611907s

• [SLOW TEST:8.095 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:35:09.611: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr  9 08:35:09.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-8523'
Apr  9 08:35:09.787: INFO: stderr: ""
Apr  9 08:35:09.787: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 08:35:09.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8523'
Apr  9 08:35:09.853: INFO: stderr: ""
Apr  9 08:35:09.853: INFO: stdout: "update-demo-nautilus-m6qqt update-demo-nautilus-z7k4h "
Apr  9 08:35:09.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-m6qqt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8523'
Apr  9 08:35:09.913: INFO: stderr: ""
Apr  9 08:35:09.913: INFO: stdout: ""
Apr  9 08:35:09.913: INFO: update-demo-nautilus-m6qqt is created but not running
Apr  9 08:35:14.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8523'
Apr  9 08:35:14.974: INFO: stderr: ""
Apr  9 08:35:14.974: INFO: stdout: "update-demo-nautilus-m6qqt update-demo-nautilus-z7k4h "
Apr  9 08:35:14.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-m6qqt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8523'
Apr  9 08:35:15.034: INFO: stderr: ""
Apr  9 08:35:15.034: INFO: stdout: "true"
Apr  9 08:35:15.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-m6qqt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8523'
Apr  9 08:35:15.095: INFO: stderr: ""
Apr  9 08:35:15.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 08:35:15.095: INFO: validating pod update-demo-nautilus-m6qqt
Apr  9 08:35:15.097: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 08:35:15.097: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 08:35:15.097: INFO: update-demo-nautilus-m6qqt is verified up and running
Apr  9 08:35:15.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-z7k4h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8523'
Apr  9 08:35:15.156: INFO: stderr: ""
Apr  9 08:35:15.156: INFO: stdout: "true"
Apr  9 08:35:15.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-z7k4h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8523'
Apr  9 08:35:15.216: INFO: stderr: ""
Apr  9 08:35:15.216: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 08:35:15.216: INFO: validating pod update-demo-nautilus-z7k4h
Apr  9 08:35:15.218: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 08:35:15.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 08:35:15.218: INFO: update-demo-nautilus-z7k4h is verified up and running
STEP: using delete to clean up resources
Apr  9 08:35:15.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-8523'
Apr  9 08:35:15.281: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:35:15.281: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  9 08:35:15.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8523'
Apr  9 08:35:15.345: INFO: stderr: "No resources found.\n"
Apr  9 08:35:15.345: INFO: stdout: ""
Apr  9 08:35:15.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=update-demo --namespace=kubectl-8523 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 08:35:15.406: INFO: stderr: ""
Apr  9 08:35:15.406: INFO: stdout: "update-demo-nautilus-m6qqt\nupdate-demo-nautilus-z7k4h\n"
Apr  9 08:35:15.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8523'
Apr  9 08:35:15.978: INFO: stderr: "No resources found.\n"
Apr  9 08:35:15.978: INFO: stdout: ""
Apr  9 08:35:15.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=update-demo --namespace=kubectl-8523 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 08:35:16.069: INFO: stderr: ""
Apr  9 08:35:16.069: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:35:16.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8523" for this suite.
Apr  9 08:35:22.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:35:22.113: INFO: namespace kubectl-8523 deletion completed in 6.042366074s

• [SLOW TEST:12.502 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:35:22.113: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:35:22.128: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:35:24.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4605" for this suite.
Apr  9 08:36:14.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:36:14.299: INFO: namespace pods-4605 deletion completed in 50.043318536s

• [SLOW TEST:52.186 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:36:14.299: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1460
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 08:36:14.314: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 08:36:34.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.1.1.130:8080/dial?request=hostName&protocol=http&host=10.1.1.129&port=8080&tries=1'] Namespace:pod-network-test-1460 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 08:36:34.349: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 08:36:34.470: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:36:34.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1460" for this suite.
Apr  9 08:36:56.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:36:56.514: INFO: namespace pod-network-test-1460 deletion completed in 22.04186846s

• [SLOW TEST:42.214 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:36:56.514: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Apr  9 08:36:56.529: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Apr  9 08:36:56.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:57.378: INFO: stderr: ""
Apr  9 08:36:57.378: INFO: stdout: "service/redis-slave created\n"
Apr  9 08:36:57.378: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Apr  9 08:36:57.378: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:57.524: INFO: stderr: ""
Apr  9 08:36:57.524: INFO: stdout: "service/redis-master created\n"
Apr  9 08:36:57.524: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Apr  9 08:36:57.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:57.690: INFO: stderr: ""
Apr  9 08:36:57.690: INFO: stdout: "service/frontend created\n"
Apr  9 08:36:57.691: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Apr  9 08:36:57.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:57.842: INFO: stderr: ""
Apr  9 08:36:57.842: INFO: stdout: "deployment.apps/frontend created\n"
Apr  9 08:36:57.842: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Apr  9 08:36:57.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:58.010: INFO: stderr: ""
Apr  9 08:36:58.010: INFO: stdout: "deployment.apps/redis-master created\n"
Apr  9 08:36:58.010: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Apr  9 08:36:58.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9377'
Apr  9 08:36:58.206: INFO: stderr: ""
Apr  9 08:36:58.206: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Apr  9 08:36:58.206: INFO: Waiting for all frontend pods to be Running.
Apr  9 08:37:13.257: INFO: Waiting for frontend to serve content.
Apr  9 08:37:13.267: INFO: Trying to add a new entry to the guestbook.
Apr  9 08:37:13.275: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Apr  9 08:37:13.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.363: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.363: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 08:37:13.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.428: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.428: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 08:37:13.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.495: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.495: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 08:37:13.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.558: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.558: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 08:37:13.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.619: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.619: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Apr  9 08:37:13.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9377'
Apr  9 08:37:13.681: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 08:37:13.681: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:37:13.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9377" for this suite.
Apr  9 08:37:51.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:37:51.728: INFO: namespace kubectl-9377 deletion completed in 38.045126824s

• [SLOW TEST:55.214 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:37:51.728: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:37:51.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-4570" to be "success or failure"
Apr  9 08:37:51.756: INFO: Pod "downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.761582ms
Apr  9 08:37:53.758: INFO: Pod "downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00384265s
STEP: Saw pod success
Apr  9 08:37:53.758: INFO: Pod "downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:37:53.760: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:37:53.768: INFO: Waiting for pod downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:37:53.769: INFO: Pod downwardapi-volume-c33087af-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:37:53.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4570" for this suite.
Apr  9 08:37:59.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:37:59.815: INFO: namespace projected-4570 deletion completed in 6.043493832s

• [SLOW TEST:8.086 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:37:59.815: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:37:59.835: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:38:03.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8483" for this suite.
Apr  9 08:38:09.751: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:38:09.789: INFO: namespace init-container-8483 deletion completed in 6.048673494s

• [SLOW TEST:9.975 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:38:09.790: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:38:09.815: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-2045" to be "success or failure"
Apr  9 08:38:09.819: INFO: Pod "downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054269ms
Apr  9 08:38:11.821: INFO: Pod "downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006082815s
STEP: Saw pod success
Apr  9 08:38:11.821: INFO: Pod "downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:38:11.822: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:38:11.832: INFO: Waiting for pod downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:38:11.833: INFO: Pod downwardapi-volume-cdf4670b-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:38:11.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2045" for this suite.
Apr  9 08:38:17.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:38:17.879: INFO: namespace projected-2045 deletion completed in 6.044617912s

• [SLOW TEST:8.089 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:38:17.879: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-28gr
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 08:38:17.909: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-28gr" in namespace "subpath-8265" to be "success or failure"
Apr  9 08:38:17.912: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.645345ms
Apr  9 08:38:19.914: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 2.004721713s
Apr  9 08:38:21.916: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 4.006772213s
Apr  9 08:38:23.918: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 6.008882635s
Apr  9 08:38:25.920: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 8.010884472s
Apr  9 08:38:27.922: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 10.012986885s
Apr  9 08:38:29.924: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 12.015076789s
Apr  9 08:38:31.926: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 14.017177048s
Apr  9 08:38:33.928: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 16.019232003s
Apr  9 08:38:35.930: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 18.021135685s
Apr  9 08:38:37.932: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Running", Reason="", readiness=true. Elapsed: 20.023163249s
Apr  9 08:38:39.934: INFO: Pod "pod-subpath-test-configmap-28gr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.025221779s
STEP: Saw pod success
Apr  9 08:38:39.934: INFO: Pod "pod-subpath-test-configmap-28gr" satisfied condition "success or failure"
Apr  9 08:38:39.936: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-subpath-test-configmap-28gr container test-container-subpath-configmap-28gr: <nil>
STEP: delete the pod
Apr  9 08:38:39.943: INFO: Waiting for pod pod-subpath-test-configmap-28gr to disappear
Apr  9 08:38:39.954: INFO: Pod pod-subpath-test-configmap-28gr no longer exists
STEP: Deleting pod pod-subpath-test-configmap-28gr
Apr  9 08:38:39.954: INFO: Deleting pod "pod-subpath-test-configmap-28gr" in namespace "subpath-8265"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:38:39.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8265" for this suite.
Apr  9 08:38:45.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:38:46.001: INFO: namespace subpath-8265 deletion completed in 6.044102294s

• [SLOW TEST:28.122 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:38:46.001: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-e389f3c9-5aa2-11e9-b737-ea708a3858a3
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:38:46.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3517" for this suite.
Apr  9 08:38:52.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:38:52.069: INFO: namespace configmap-3517 deletion completed in 6.043450879s

• [SLOW TEST:6.068 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:38:52.069: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e727be74-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:38:52.096: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3" in namespace "projected-7892" to be "success or failure"
Apr  9 08:38:52.099: INFO: Pod "pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.974894ms
Apr  9 08:38:54.103: INFO: Pod "pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006697606s
STEP: Saw pod success
Apr  9 08:38:54.103: INFO: Pod "pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:38:54.105: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:38:54.116: INFO: Waiting for pod pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:38:54.124: INFO: Pod pod-projected-configmaps-e728fff1-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:38:54.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7892" for this suite.
Apr  9 08:39:00.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:00.167: INFO: namespace projected-7892 deletion completed in 6.041847484s

• [SLOW TEST:8.098 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:00.167: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ebfb5771-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:39:00.195: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3" in namespace "configmap-5347" to be "success or failure"
Apr  9 08:39:00.198: INFO: Pod "pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.966934ms
Apr  9 08:39:02.200: INFO: Pod "pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005005835s
STEP: Saw pod success
Apr  9 08:39:02.200: INFO: Pod "pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:39:02.201: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:39:02.210: INFO: Waiting for pod pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:39:02.211: INFO: Pod pod-configmaps-ebfcb72a-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:02.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5347" for this suite.
Apr  9 08:39:08.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:08.269: INFO: namespace configmap-5347 deletion completed in 6.054708206s

• [SLOW TEST:8.102 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:08.269: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:39:08.286: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:09.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2555" for this suite.
Apr  9 08:39:15.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:15.357: INFO: namespace custom-resource-definition-2555 deletion completed in 6.041673957s

• [SLOW TEST:7.087 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:15.357: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:15.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8658" for this suite.
Apr  9 08:39:21.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:21.424: INFO: namespace services-8658 deletion completed in 6.042211844s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.067 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:21.425: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Apr  9 08:39:21.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 cluster-info'
Apr  9 08:39:21.506: INFO: stderr: ""
Apr  9 08:39:21.506: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.152.183.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:21.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5060" for this suite.
Apr  9 08:39:27.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:27.551: INFO: namespace kubectl-5060 deletion completed in 6.04316339s

• [SLOW TEST:6.126 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:27.551: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-fc4dbcd3-5aa2-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:39:27.578: INFO: Waiting up to 5m0s for pod "pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3" in namespace "secrets-2923" to be "success or failure"
Apr  9 08:39:27.580: INFO: Pod "pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.169031ms
Apr  9 08:39:29.582: INFO: Pod "pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003318178s
STEP: Saw pod success
Apr  9 08:39:29.582: INFO: Pod "pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:39:29.583: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:39:29.592: INFO: Waiting for pod pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:39:29.601: INFO: Pod pod-secrets-fc4f00b2-5aa2-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:29.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2923" for this suite.
Apr  9 08:39:35.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:35.646: INFO: namespace secrets-2923 deletion completed in 6.044006721s

• [SLOW TEST:8.095 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:35.646: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-0122518d-5aa3-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:39:37.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6216" for this suite.
Apr  9 08:39:59.692: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:39:59.729: INFO: namespace configmap-6216 deletion completed in 22.04182256s

• [SLOW TEST:24.082 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:39:59.729: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:39:59.744: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:40:03.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2424" for this suite.
Apr  9 08:40:09.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:40:09.226: INFO: namespace init-container-2424 deletion completed in 6.04888866s

• [SLOW TEST:9.497 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:40:09.227: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:40:09.241: INFO: Creating deployment "nginx-deployment"
Apr  9 08:40:09.244: INFO: Waiting for observed generation 1
Apr  9 08:40:11.264: INFO: Waiting for all required pods to come up
Apr  9 08:40:11.268: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Apr  9 08:40:17.273: INFO: Waiting for deployment "nginx-deployment" to complete
Apr  9 08:40:17.276: INFO: Updating deployment "nginx-deployment" with a non-existent image
Apr  9 08:40:17.279: INFO: Updating deployment nginx-deployment
Apr  9 08:40:17.279: INFO: Waiting for observed generation 2
Apr  9 08:40:19.282: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Apr  9 08:40:19.283: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Apr  9 08:40:19.285: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr  9 08:40:19.289: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Apr  9 08:40:19.289: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Apr  9 08:40:19.291: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Apr  9 08:40:19.293: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Apr  9 08:40:19.293: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Apr  9 08:40:19.296: INFO: Updating deployment nginx-deployment
Apr  9 08:40:19.296: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Apr  9 08:40:19.301: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Apr  9 08:40:19.310: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 08:40:21.326: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3088,SelfLink:/apis/apps/v1/namespaces/deployment-3088/deployments/nginx-deployment,UID:1524e361-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3618,Generation:3,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-04-09 08:40:19 +0000 UTC 2019-04-09 08:40:19 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-09 08:40:19 +0000 UTC 2019-04-09 08:40:09 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Apr  9 08:40:21.327: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-3088,SelfLink:/apis/apps/v1/namespaces/deployment-3088/replicasets/nginx-deployment-5f9595f595,UID:19ef792c-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3612,Generation:3,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1524e361-5aa3-11e9-bcbd-0af1119c727a 0xc002db2807 0xc002db2808}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:40:21.327: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Apr  9 08:40:21.327: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-3088,SelfLink:/apis/apps/v1/namespaces/deployment-3088/replicasets/nginx-deployment-6f478d8d8,UID:15255720-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3614,Generation:3,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 1524e361-5aa3-11e9-bcbd-0af1119c727a 0xc002db28d7 0xc002db28d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Apr  9 08:40:21.331: INFO: Pod "nginx-deployment-5f9595f595-2q6r8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2q6r8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-2q6r8,UID:1b2879c1-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3598,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc00275dd90 0xc00275dd91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275de10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275de30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.331: INFO: Pod "nginx-deployment-5f9595f595-5n2bs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5n2bs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-5n2bs,UID:1b242ec7-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3662,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc00275deb0 0xc00275deb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00275df30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00275df50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:40:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.331: INFO: Pod "nginx-deployment-5f9595f595-65lxh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-65lxh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-65lxh,UID:1b2cdaac-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3608,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46020 0xc002e46021}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e460a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e460c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.331: INFO: Pod "nginx-deployment-5f9595f595-9ntd4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9ntd4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-9ntd4,UID:19f06444-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3553,Generation:0,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46140 0xc002e46141}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e461c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e461e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.157,StartTime:2019-04-09 08:40:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.331: INFO: Pod "nginx-deployment-5f9595f595-hsqpt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-hsqpt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-hsqpt,UID:19f067b0-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3551,Generation:0,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e462e0 0xc002e462e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46380} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e463a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.158,StartTime:2019-04-09 08:40:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-jzfs2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jzfs2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-jzfs2,UID:1b289b80-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3596,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46490 0xc002e46491}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-rt9s8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rt9s8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-rt9s8,UID:1b285c20-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3595,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e465b0 0xc002e465b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-sqdfp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sqdfp,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-sqdfp,UID:19fdf251-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3548,Generation:0,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e466d0 0xc002e466d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.160,StartTime:2019-04-09 08:40:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-v7z6k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-v7z6k,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-v7z6k,UID:1b25d969-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3588,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46860 0xc002e46861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e468e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-w9cpg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w9cpg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-w9cpg,UID:19fc2159-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3532,Generation:0,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46980 0xc002e46981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46a00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:40:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-w9jfw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w9jfw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-w9jfw,UID:19efd336-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3504,Generation:0,CreationTimestamp:2019-04-09 08:40:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46af0 0xc002e46af1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:17 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:40:17 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-x8djt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-x8djt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-x8djt,UID:1b288981-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3600,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46c60 0xc002e46c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-5f9595f595-xg4pb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-xg4pb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-5f9595f595-xg4pb,UID:1b25da64-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3582,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 19ef792c-5aa3-11e9-bcbd-0af1119c727a 0xc002e46d80 0xc002e46d81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46e00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46e20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-6f478d8d8-26k9g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-26k9g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-26k9g,UID:1b286588-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3604,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e46ea0 0xc002e46ea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e46f10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e46f30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-6f478d8d8-6lwp5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6lwp5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-6lwp5,UID:1b287f30-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3605,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e46fb0 0xc002e46fb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-6f478d8d8-9mzls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9mzls,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-9mzls,UID:1b25c1fe-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3585,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e470c0 0xc002e470c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.332: INFO: Pod "nginx-deployment-6f478d8d8-bzlq8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bzlq8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-bzlq8,UID:1b25f31c-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3580,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e471d0 0xc002e471d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47250} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47270}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-dsbcs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dsbcs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-dsbcs,UID:1529afae-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3469,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e472f0 0xc002e472f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47360} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47380}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.153,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://2ef616d17d11125bad7db13166fd189ca109f9e1a2050907e7d44cdedeae78a6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-fgnm7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fgnm7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-fgnm7,UID:152b821e-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3460,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47460 0xc002e47461}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e474d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e474f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.152,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://87210be030678fa7cd38e00714f74287a2d7070ad2bb9a6314170a9987619fe6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-ftzs4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ftzs4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-ftzs4,UID:152c1177-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3444,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e475c0 0xc002e475c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.155,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://8ef4819a3bc38fcfe24d977688504bb4fb7c0c17b3642984d435625355d3ff2a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-g6gcs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-g6gcs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-g6gcs,UID:15296f29-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3454,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47720 0xc002e47721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e477b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.151,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://f5b017ad6c31f6226ec8690d9a19d6e7a13367a966060c3dc1fd43f21a374645}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-gpzss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-gpzss,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-gpzss,UID:1b236593-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3656,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47880 0xc002e47881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e478f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:40:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-ksf8d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ksf8d,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-ksf8d,UID:1b289e46-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3603,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e479d0 0xc002e479d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-l6x2j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l6x2j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-l6x2j,UID:15298fad-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3472,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47ae0 0xc002e47ae1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.150,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://e966dc0983ad2d9af5d6495a56f8222faade9154a18180aaba6e905406f088f8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-lqjw9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lqjw9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-lqjw9,UID:1529a58d-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3439,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47c40 0xc002e47c41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47cd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.149,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:10 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://a41db7dfe13c955b8f7a5dcf4d6e79a64a6f1b4144264c6b2492034c99d79858}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.333: INFO: Pod "nginx-deployment-6f478d8d8-nnrqn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nnrqn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-nnrqn,UID:1b25e48d-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3591,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47da0 0xc002e47da1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-nq9rk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nq9rk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-nq9rk,UID:1b23de31-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3659,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc002e47eb0 0xc002e47eb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e47f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e47f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:40:19 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-nvmkp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nvmkp,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-nvmkp,UID:1526df7b-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3432,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f0000 0xc0028f0001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f0070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f0090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:10 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:10 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.146,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:10 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://b940c4ba44af0b41546cabd34273c69c2ad871f84909d310a97f53780a65c68c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-tqt4c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-tqt4c,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-tqt4c,UID:152746d2-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3449,Generation:0,CreationTimestamp:2019-04-09 08:40:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f0160 0xc0028f0161}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f01d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f01f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:09 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.148,StartTime:2019-04-09 08:40:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 08:40:10 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://bdca8925efd34a45b5adb7a32c36621313c2bdf1429b90fb41b4189c94ccf865}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-vnbgz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vnbgz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-vnbgz,UID:1b25cb8d-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3576,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f02c0 0xc0028f02c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f0330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f0350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-z9xr2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z9xr2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-z9xr2,UID:1b286f57-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3601,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f03d0 0xc0028f03d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f0440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f0460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-zhl7b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zhl7b,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-zhl7b,UID:1b288e09-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3602,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f04e0 0xc0028f04e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f0550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f0570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 08:40:21.334: INFO: Pod "nginx-deployment-6f478d8d8-zktmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zktmq,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-3088,SelfLink:/api/v1/namespaces/deployment-3088/pods/nginx-deployment-6f478d8d8-zktmq,UID:1b241f73-5aa3-11e9-bcbd-0af1119c727a,ResourceVersion:3567,Generation:0,CreationTimestamp:2019-04-09 08:40:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 15255720-5aa3-11e9-bcbd-0af1119c727a 0xc0028f05f0 0xc0028f05f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-h45hk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-h45hk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-h45hk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028f0660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0028f0680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:40:19 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:40:21.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3088" for this suite.
Apr  9 08:40:27.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:40:27.391: INFO: namespace deployment-3088 deletion completed in 6.055719622s

• [SLOW TEST:18.165 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:40:27.391: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-1ffa935c-5aa3-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-1ffa935c-5aa3-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:41:43.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-778" for this suite.
Apr  9 08:42:05.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:42:05.604: INFO: namespace projected-778 deletion completed in 22.04513279s

• [SLOW TEST:98.212 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:42:05.604: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 08:42:05.624: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 08:42:05.626: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 08:42:05.627: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-13-147 before test
Apr  9 08:42:05.630: INFO: kube-dns-6bfbdd666c-966qk from kube-system started at 2019-04-09 08:25:19 +0000 UTC (3 container statuses recorded)
Apr  9 08:42:05.630: INFO: 	Container dnsmasq ready: true, restart count 0
Apr  9 08:42:05.630: INFO: 	Container kubedns ready: true, restart count 0
Apr  9 08:42:05.630: INFO: 	Container sidecar ready: true, restart count 0
Apr  9 08:42:05.630: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-09 08:26:02 +0000 UTC (1 container statuses recorded)
Apr  9 08:42:05.630: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  9 08:42:05.630: INFO: sonobuoy-systemd-logs-daemon-set-82c391b0bd954094-h8czw from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 08:42:05.630: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr  9 08:42:05.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  9 08:42:05.630: INFO: sonobuoy-e2e-job-e27e4b08c656417e from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 08:42:05.630: INFO: 	Container e2e ready: true, restart count 0
Apr  9 08:42:05.630: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-5bb76dbd-5aa3-11e9-b737-ea708a3858a3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-5bb76dbd-5aa3-11e9-b737-ea708a3858a3 off the node ip-172-31-13-147
STEP: verifying the node doesn't have the label kubernetes.io/e2e-5bb76dbd-5aa3-11e9-b737-ea708a3858a3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:42:09.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2321" for this suite.
Apr  9 08:42:21.689: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:42:21.727: INFO: namespace sched-pred-2321 deletion completed in 12.047173826s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:16.123 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:42:21.727: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  9 08:42:25.767: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:25.768: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:27.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:27.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:29.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:29.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:31.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:31.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:33.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:33.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:35.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:35.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:37.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:37.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:39.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:39.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:41.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:41.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:43.768: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:43.770: INFO: Pod pod-with-prestop-exec-hook still exists
Apr  9 08:42:45.769: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Apr  9 08:42:45.770: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:42:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5547" for this suite.
Apr  9 08:43:07.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:43:07.819: INFO: namespace container-lifecycle-hook-5547 deletion completed in 22.044330048s

• [SLOW TEST:46.091 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:43:07.819: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 08:43:07.835: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 08:43:07.845: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 08:43:07.846: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-13-147 before test
Apr  9 08:43:07.849: INFO: kube-dns-6bfbdd666c-966qk from kube-system started at 2019-04-09 08:25:19 +0000 UTC (3 container statuses recorded)
Apr  9 08:43:07.849: INFO: 	Container dnsmasq ready: true, restart count 0
Apr  9 08:43:07.849: INFO: 	Container kubedns ready: true, restart count 0
Apr  9 08:43:07.849: INFO: 	Container sidecar ready: true, restart count 0
Apr  9 08:43:07.849: INFO: sonobuoy-systemd-logs-daemon-set-82c391b0bd954094-h8czw from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 08:43:07.849: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr  9 08:43:07.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  9 08:43:07.849: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-09 08:26:02 +0000 UTC (1 container statuses recorded)
Apr  9 08:43:07.849: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  9 08:43:07.849: INFO: sonobuoy-e2e-job-e27e4b08c656417e from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 08:43:07.849: INFO: 	Container e2e ready: true, restart count 0
Apr  9 08:43:07.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.1593c1ce3a22bf5a], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:43:08.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1922" for this suite.
Apr  9 08:43:14.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:43:14.907: INFO: namespace sched-pred-1922 deletion completed in 6.048134961s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.088 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:43:14.908: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 08:43:14.934: INFO: Waiting up to 5m0s for pod "downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3" in namespace "downward-api-8804" to be "success or failure"
Apr  9 08:43:14.935: INFO: Pod "downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.220103ms
Apr  9 08:43:16.937: INFO: Pod "downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003367771s
Apr  9 08:43:18.939: INFO: Pod "downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005313214s
STEP: Saw pod success
Apr  9 08:43:18.939: INFO: Pod "downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:43:18.941: INFO: Trying to get logs from node ip-172-31-13-147 pod downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 08:43:18.948: INFO: Waiting for pod downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:43:18.949: INFO: Pod downward-api-83d1d58e-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:43:18.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8804" for this suite.
Apr  9 08:43:24.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:43:24.993: INFO: namespace downward-api-8804 deletion completed in 6.042322943s

• [SLOW TEST:10.086 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:43:24.993: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0409 08:43:31.033708      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 08:43:31.033: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:43:31.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6546" for this suite.
Apr  9 08:43:37.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:43:37.080: INFO: namespace gc-6546 deletion completed in 6.045009162s

• [SLOW TEST:12.086 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:43:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3377/configmap-test-910902cd-5aa3-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:43:37.108: INFO: Waiting up to 5m0s for pod "pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3" in namespace "configmap-3377" to be "success or failure"
Apr  9 08:43:37.113: INFO: Pod "pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.110163ms
Apr  9 08:43:39.115: INFO: Pod "pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007238298s
STEP: Saw pod success
Apr  9 08:43:39.115: INFO: Pod "pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:43:39.116: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3 container env-test: <nil>
STEP: delete the pod
Apr  9 08:43:39.123: INFO: Waiting for pod pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:43:39.124: INFO: Pod pod-configmaps-910a3c7e-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:43:39.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3377" for this suite.
Apr  9 08:43:45.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:43:45.167: INFO: namespace configmap-3377 deletion completed in 6.041637595s

• [SLOW TEST:8.087 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:43:45.167: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-8c64
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 08:43:45.239: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-8c64" in namespace "subpath-5879" to be "success or failure"
Apr  9 08:43:45.246: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.460743ms
Apr  9 08:43:47.249: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 2.009784563s
Apr  9 08:43:49.251: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 4.011856669s
Apr  9 08:43:51.253: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 6.013903304s
Apr  9 08:43:53.255: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 8.016110196s
Apr  9 08:43:55.257: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 10.018199461s
Apr  9 08:43:57.259: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 12.020321841s
Apr  9 08:43:59.261: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 14.02249061s
Apr  9 08:44:01.264: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 16.024639113s
Apr  9 08:44:03.266: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 18.026765471s
Apr  9 08:44:05.268: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Running", Reason="", readiness=true. Elapsed: 20.028896685s
Apr  9 08:44:07.270: INFO: Pod "pod-subpath-test-projected-8c64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.031039984s
STEP: Saw pod success
Apr  9 08:44:07.270: INFO: Pod "pod-subpath-test-projected-8c64" satisfied condition "success or failure"
Apr  9 08:44:07.271: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-subpath-test-projected-8c64 container test-container-subpath-projected-8c64: <nil>
STEP: delete the pod
Apr  9 08:44:07.281: INFO: Waiting for pod pod-subpath-test-projected-8c64 to disappear
Apr  9 08:44:07.288: INFO: Pod pod-subpath-test-projected-8c64 no longer exists
STEP: Deleting pod pod-subpath-test-projected-8c64
Apr  9 08:44:07.288: INFO: Deleting pod "pod-subpath-test-projected-8c64" in namespace "subpath-5879"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:44:07.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5879" for this suite.
Apr  9 08:44:13.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:44:13.336: INFO: namespace subpath-5879 deletion completed in 6.045272212s

• [SLOW TEST:28.169 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:44:13.336: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  9 08:44:13.369: INFO: Waiting up to 5m0s for pod "pod-a6a67903-5aa3-11e9-b737-ea708a3858a3" in namespace "emptydir-2321" to be "success or failure"
Apr  9 08:44:13.371: INFO: Pod "pod-a6a67903-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.402961ms
Apr  9 08:44:15.373: INFO: Pod "pod-a6a67903-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003482537s
STEP: Saw pod success
Apr  9 08:44:15.373: INFO: Pod "pod-a6a67903-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:44:15.374: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-a6a67903-5aa3-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 08:44:15.383: INFO: Waiting for pod pod-a6a67903-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:44:15.391: INFO: Pod pod-a6a67903-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:44:15.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2321" for this suite.
Apr  9 08:44:21.397: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:44:21.435: INFO: namespace emptydir-2321 deletion completed in 6.042880377s

• [SLOW TEST:8.099 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:44:21.436: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0409 08:44:31.505875      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 08:44:31.505: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:44:31.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-29" for this suite.
Apr  9 08:44:37.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:44:37.554: INFO: namespace gc-29 deletion completed in 6.047247356s

• [SLOW TEST:16.118 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:44:37.554: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:44:58.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4170" for this suite.
Apr  9 08:45:04.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:45:04.721: INFO: namespace container-runtime-4170 deletion completed in 6.042134064s

• [SLOW TEST:27.167 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:45:04.721: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-c54610cb-5aa3-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:45:04.751: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3" in namespace "projected-5167" to be "success or failure"
Apr  9 08:45:04.752: INFO: Pod "pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.141386ms
Apr  9 08:45:06.754: INFO: Pod "pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003704372s
Apr  9 08:45:08.757: INFO: Pod "pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005885153s
STEP: Saw pod success
Apr  9 08:45:08.757: INFO: Pod "pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:45:08.758: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:45:08.773: INFO: Waiting for pod pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:45:08.774: INFO: Pod pod-projected-secrets-c54772a3-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:45:08.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5167" for this suite.
Apr  9 08:45:14.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:45:14.818: INFO: namespace projected-5167 deletion completed in 6.042257862s

• [SLOW TEST:10.096 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:45:14.818: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Apr  9 08:45:18.860: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 08:45:18.862: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 08:45:20.862: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 08:45:20.863: INFO: Pod pod-with-prestop-http-hook still exists
Apr  9 08:45:22.862: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Apr  9 08:45:22.863: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:45:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4092" for this suite.
Apr  9 08:45:44.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:45:44.913: INFO: namespace container-lifecycle-hook-4092 deletion completed in 22.045062907s

• [SLOW TEST:30.095 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:45:44.913: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-dd3b1de7-5aa3-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 08:45:44.943: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3" in namespace "projected-2211" to be "success or failure"
Apr  9 08:45:44.945: INFO: Pod "pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.676529ms
Apr  9 08:45:46.947: INFO: Pod "pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003723041s
Apr  9 08:45:48.949: INFO: Pod "pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005737469s
STEP: Saw pod success
Apr  9 08:45:48.949: INFO: Pod "pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:45:48.950: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 08:45:48.960: INFO: Waiting for pod pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:45:48.967: INFO: Pod pod-projected-configmaps-dd3c55e3-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:45:48.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2211" for this suite.
Apr  9 08:45:54.974: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:45:55.013: INFO: namespace projected-2211 deletion completed in 6.04369991s

• [SLOW TEST:10.100 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:45:55.013: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-e33fcc09-5aa3-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:45:55.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3" in namespace "projected-2036" to be "success or failure"
Apr  9 08:45:55.042: INFO: Pod "pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.544549ms
Apr  9 08:45:57.044: INFO: Pod "pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004646681s
Apr  9 08:45:59.046: INFO: Pod "pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006593228s
STEP: Saw pod success
Apr  9 08:45:59.046: INFO: Pod "pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:45:59.047: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:45:59.055: INFO: Waiting for pod pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:45:59.056: INFO: Pod pod-projected-secrets-e3410dcc-5aa3-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:45:59.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2036" for this suite.
Apr  9 08:46:05.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:46:05.098: INFO: namespace projected-2036 deletion completed in 6.041268177s

• [SLOW TEST:10.085 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:46:05.098: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  9 08:46:07.635: INFO: Successfully updated pod "pod-update-e943c750-5aa3-11e9-b737-ea708a3858a3"
STEP: verifying the updated pod is in kubernetes
Apr  9 08:46:07.637: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:46:07.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3651" for this suite.
Apr  9 08:46:29.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:46:29.682: INFO: namespace pods-3651 deletion completed in 22.042967397s

• [SLOW TEST:24.583 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:46:29.682: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:46:29.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 version --client'
Apr  9 08:46:29.746: INFO: stderr: ""
Apr  9 08:46:29.746: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Apr  9 08:46:29.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-3247'
Apr  9 08:46:29.917: INFO: stderr: ""
Apr  9 08:46:29.917: INFO: stdout: "replicationcontroller/redis-master created\n"
Apr  9 08:46:29.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-3247'
Apr  9 08:46:30.066: INFO: stderr: ""
Apr  9 08:46:30.066: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 08:46:31.068: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:46:31.069: INFO: Found 0 / 1
Apr  9 08:46:32.068: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:46:32.068: INFO: Found 1 / 1
Apr  9 08:46:32.068: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 08:46:32.070: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 08:46:32.070: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 08:46:32.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 describe pod redis-master-jp7wm --namespace=kubectl-3247'
Apr  9 08:46:32.138: INFO: stderr: ""
Apr  9 08:46:32.138: INFO: stdout: "Name:               redis-master-jp7wm\nNamespace:          kubectl-3247\nPriority:           0\nPriorityClassName:  <none>\nNode:               ip-172-31-13-147/172.31.13.147\nStart Time:         Tue, 09 Apr 2019 08:46:29 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.1.1.221\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://334205d96237895804bce72f537d0e2d40b95fcdbb82d7b0c7d2baef0eaf0a6e\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 09 Apr 2019 08:46:31 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-pkkx2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-pkkx2:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-pkkx2\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                       Message\n  ----    ------     ----  ----                       -------\n  Normal  Scheduled  3s    default-scheduler          Successfully assigned kubectl-3247/redis-master-jp7wm to ip-172-31-13-147\n  Normal  Pulled     2s    kubelet, ip-172-31-13-147  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, ip-172-31-13-147  Created container redis-master\n  Normal  Started    1s    kubelet, ip-172-31-13-147  Started container redis-master\n"
Apr  9 08:46:32.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 describe rc redis-master --namespace=kubectl-3247'
Apr  9 08:46:32.212: INFO: stderr: ""
Apr  9 08:46:32.212: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-3247\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-jp7wm\n"
Apr  9 08:46:32.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 describe service redis-master --namespace=kubectl-3247'
Apr  9 08:46:32.280: INFO: stderr: ""
Apr  9 08:46:32.280: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-3247\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.152.183.57\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.1.1.221:6379\nSession Affinity:  None\nEvents:            <none>\n"
Apr  9 08:46:32.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 describe node ip-172-31-13-147'
Apr  9 08:46:32.358: INFO: stderr: ""
Apr  9 08:46:32.358: INFO: stdout: "Name:               ip-172-31-13-147\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-172-31-13-147\n                    kubernetes.io/os=linux\n                    microk8s.io/cluster=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 09 Apr 2019 08:20:27 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 09 Apr 2019 08:45:40 +0000   Tue, 09 Apr 2019 08:20:27 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 09 Apr 2019 08:45:40 +0000   Tue, 09 Apr 2019 08:20:27 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 09 Apr 2019 08:45:40 +0000   Tue, 09 Apr 2019 08:20:27 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 09 Apr 2019 08:45:40 +0000   Tue, 09 Apr 2019 08:25:19 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.31.13.147\n  Hostname:    ip-172-31-13-147\nCapacity:\n cpu:                4\n ephemeral-storage:  81253764Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16424488Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  80205188Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16322088Ki\n pods:               110\nSystem Info:\n Machine ID:                 459d196d2d9340b4b579febd98c9799a\n System UUID:                EC2210B3-FBB3-064E-3D1F-EB28BDCEEBDA\n Boot ID:                    df03ce35-dcee-4aae-8bd4-8a71fa585be6\n Kernel Version:             4.15.0-1035-aws\n OS Image:                   Ubuntu 18.04.2 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.5\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nNon-terminated Pods:         (5 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  heptio-sonobuoy            sonobuoy-e2e-job-e27e4b08c656417e                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-82c391b0bd954094-h8czw    0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\n  kube-system                kube-dns-6bfbdd666c-966qk                                  260m (6%)     0 (0%)      110Mi (0%)       170Mi (1%)     21m\n  kubectl-3247               redis-master-jp7wm                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                260m (6%)   0 (0%)\n  memory             110Mi (0%)  170Mi (1%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From                          Message\n  ----     ------                   ----               ----                          -------\n  Normal   Starting                 21m                kube-proxy, ip-172-31-13-147  Starting kube-proxy.\n  Normal   Starting                 21m                kubelet, ip-172-31-13-147     Starting kubelet.\n  Warning  InvalidDiskCapacity      21m                kubelet, ip-172-31-13-147     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  21m (x2 over 21m)  kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    21m (x2 over 21m)  kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     21m (x2 over 21m)  kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  21m                kubelet, ip-172-31-13-147     Updated Node Allocatable limit across pods\n  Normal   Starting                 21m                kubelet, ip-172-31-13-147     Starting kubelet.\n  Warning  InvalidDiskCapacity      21m                kubelet, ip-172-31-13-147     invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  21m                kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    21m                kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     21m                kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeHasSufficientPID\n  Normal   NodeNotReady             21m                kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeNotReady\n  Normal   NodeAllocatableEnforced  21m                kubelet, ip-172-31-13-147     Updated Node Allocatable limit across pods\n  Normal   NodeReady                21m                kubelet, ip-172-31-13-147     Node ip-172-31-13-147 status is now: NodeReady\n"
Apr  9 08:46:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 describe namespace kubectl-3247'
Apr  9 08:46:32.424: INFO: stderr: ""
Apr  9 08:46:32.424: INFO: stdout: "Name:         kubectl-3247\nLabels:       e2e-framework=kubectl\n              e2e-run=2a371b75-5aa1-11e9-b737-ea708a3858a3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:46:32.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3247" for this suite.
Apr  9 08:46:54.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:46:54.476: INFO: namespace kubectl-3247 deletion completed in 22.049973082s

• [SLOW TEST:24.794 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:46:54.476: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  9 08:46:54.505: INFO: Waiting up to 5m0s for pod "pod-06b2beea-5aa4-11e9-b737-ea708a3858a3" in namespace "emptydir-6500" to be "success or failure"
Apr  9 08:46:54.507: INFO: Pod "pod-06b2beea-5aa4-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095832ms
Apr  9 08:46:56.509: INFO: Pod "pod-06b2beea-5aa4-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004260348s
STEP: Saw pod success
Apr  9 08:46:56.509: INFO: Pod "pod-06b2beea-5aa4-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:46:56.511: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-06b2beea-5aa4-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 08:46:56.517: INFO: Waiting for pod pod-06b2beea-5aa4-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:46:56.519: INFO: Pod pod-06b2beea-5aa4-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:46:56.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6500" for this suite.
Apr  9 08:47:02.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:47:02.562: INFO: namespace emptydir-6500 deletion completed in 6.041860035s

• [SLOW TEST:8.086 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:47:02.562: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Apr  9 08:47:02.904: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Apr  9 08:47:04.928: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:47:06.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396422, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:47:09.545: INFO: Waited 611.717499ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:47:09.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-202" for this suite.
Apr  9 08:47:16.093: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:47:16.131: INFO: namespace aggregator-202 deletion completed in 6.136608417s

• [SLOW TEST:13.569 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:47:16.131: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Apr  9 08:47:16.160: INFO: Waiting up to 5m0s for pod "pod-139b1595-5aa4-11e9-b737-ea708a3858a3" in namespace "emptydir-9131" to be "success or failure"
Apr  9 08:47:16.163: INFO: Pod "pod-139b1595-5aa4-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.519896ms
Apr  9 08:47:18.164: INFO: Pod "pod-139b1595-5aa4-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004110718s
Apr  9 08:47:20.166: INFO: Pod "pod-139b1595-5aa4-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006210365s
STEP: Saw pod success
Apr  9 08:47:20.166: INFO: Pod "pod-139b1595-5aa4-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:47:20.168: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-139b1595-5aa4-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 08:47:20.176: INFO: Waiting for pod pod-139b1595-5aa4-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:47:20.184: INFO: Pod pod-139b1595-5aa4-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:47:20.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9131" for this suite.
Apr  9 08:47:26.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:47:26.228: INFO: namespace emptydir-9131 deletion completed in 6.042916144s

• [SLOW TEST:10.097 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:47:26.229: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:47:26.248: INFO: Creating deployment "test-recreate-deployment"
Apr  9 08:47:26.256: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Apr  9 08:47:26.262: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Apr  9 08:47:28.265: INFO: Waiting deployment "test-recreate-deployment" to complete
Apr  9 08:47:28.266: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396446, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396446, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396446, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690396446, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 08:47:30.268: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Apr  9 08:47:30.272: INFO: Updating deployment test-recreate-deployment
Apr  9 08:47:30.272: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 08:47:30.347: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1774,SelfLink:/apis/apps/v1/namespaces/deployment-1774/deployments/test-recreate-deployment,UID:199ed30f-5aa4-11e9-bcbd-0af1119c727a,ResourceVersion:5480,Generation:2,CreationTimestamp:2019-04-09 08:47:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-04-09 08:47:30 +0000 UTC 2019-04-09 08:47:30 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-04-09 08:47:30 +0000 UTC 2019-04-09 08:47:26 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Apr  9 08:47:30.349: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-1774,SelfLink:/apis/apps/v1/namespaces/deployment-1774/replicasets/test-recreate-deployment-c9cbd8684,UID:1c07f58b-5aa4-11e9-bcbd-0af1119c727a,ResourceVersion:5479,Generation:1,CreationTimestamp:2019-04-09 08:47:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 199ed30f-5aa4-11e9-bcbd-0af1119c727a 0xc0024f47e0 0xc0024f47e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:47:30.349: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Apr  9 08:47:30.349: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-1774,SelfLink:/apis/apps/v1/namespaces/deployment-1774/replicasets/test-recreate-deployment-7d57d5ff7c,UID:19a013fe-5aa4-11e9-bcbd-0af1119c727a,ResourceVersion:5469,Generation:2,CreationTimestamp:2019-04-09 08:47:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 199ed30f-5aa4-11e9-bcbd-0af1119c727a 0xc0024f4727 0xc0024f4728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 08:47:30.351: INFO: Pod "test-recreate-deployment-c9cbd8684-vk5kt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-vk5kt,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-1774,SelfLink:/api/v1/namespaces/deployment-1774/pods/test-recreate-deployment-c9cbd8684-vk5kt,UID:1c094724-5aa4-11e9-bcbd-0af1119c727a,ResourceVersion:5481,Generation:0,CreationTimestamp:2019-04-09 08:47:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 1c07f58b-5aa4-11e9-bcbd-0af1119c727a 0xc0024f5120 0xc0024f5121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wdv5g {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wdv5g,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wdv5g true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024f5190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024f51b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:47:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:47:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:47:30 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 08:47:30 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:,StartTime:2019-04-09 08:47:30 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:47:30.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1774" for this suite.
Apr  9 08:47:36.356: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:47:36.397: INFO: namespace deployment-1774 deletion completed in 6.044764966s

• [SLOW TEST:10.168 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:47:36.397: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 08:47:40.937: INFO: Successfully updated pod "labelsupdate1faee022-5aa4-11e9-b737-ea708a3858a3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:47:42.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9115" for this suite.
Apr  9 08:48:04.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:48:04.986: INFO: namespace downward-api-9115 deletion completed in 22.041765465s

• [SLOW TEST:28.589 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:48:04.986: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:48:05.011: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3" in namespace "downward-api-699" to be "success or failure"
Apr  9 08:48:05.013: INFO: Pod "downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.26909ms
Apr  9 08:48:07.015: INFO: Pod "downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003370921s
STEP: Saw pod success
Apr  9 08:48:07.015: INFO: Pod "downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:48:07.016: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:48:07.023: INFO: Waiting for pod downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:48:07.024: INFO: Pod downwardapi-volume-30b83cee-5aa4-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:48:07.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-699" for this suite.
Apr  9 08:48:13.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:48:13.075: INFO: namespace downward-api-699 deletion completed in 6.049860452s

• [SLOW TEST:8.089 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:48:13.075: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-8586
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-8586
STEP: Deleting pre-stop pod
Apr  9 08:48:24.127: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:48:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-8586" for this suite.
Apr  9 08:49:02.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:49:02.186: INFO: namespace prestop-8586 deletion completed in 38.043958586s

• [SLOW TEST:49.111 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:49:02.186: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-52d05516-5aa4-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:49:02.214: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3" in namespace "projected-6287" to be "success or failure"
Apr  9 08:49:02.215: INFO: Pod "pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.078951ms
Apr  9 08:49:04.217: INFO: Pod "pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00311681s
STEP: Saw pod success
Apr  9 08:49:04.217: INFO: Pod "pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:49:04.218: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:49:04.225: INFO: Waiting for pod pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:49:04.231: INFO: Pod pod-projected-secrets-52d18e98-5aa4-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:49:04.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6287" for this suite.
Apr  9 08:49:10.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:49:10.288: INFO: namespace projected-6287 deletion completed in 6.054853218s

• [SLOW TEST:8.101 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:49:10.288: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6769.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6769.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6769.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6769.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 166.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.166_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6769.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6769.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6769.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6769.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6769.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6769.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6769.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 166.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.166_udp@PTR;check="$$(dig +tcp +noall +answer +search 166.183.152.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.152.183.166_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 08:49:20.370: INFO: DNS probes using dns-6769/dns-test-57a68b27-5aa4-11e9-b737-ea708a3858a3 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:49:20.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6769" for this suite.
Apr  9 08:49:26.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:49:26.455: INFO: namespace dns-6769 deletion completed in 6.042260809s

• [SLOW TEST:16.167 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:49:26.455: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-9282
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9282
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9282
Apr  9 08:49:26.486: INFO: Found 0 stateful pods, waiting for 1
Apr  9 08:49:36.488: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Apr  9 08:49:36.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 08:49:36.666: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 08:49:36.666: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 08:49:36.666: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 08:49:36.668: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  9 08:49:46.670: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 08:49:46.670: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 08:49:46.676: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999905s
Apr  9 08:49:47.678: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998423233s
Apr  9 08:49:48.680: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.996171976s
Apr  9 08:49:49.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.994211005s
Apr  9 08:49:50.684: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991771774s
Apr  9 08:49:51.687: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989572183s
Apr  9 08:49:52.689: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.987378836s
Apr  9 08:49:53.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.985253478s
Apr  9 08:49:54.693: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.983128413s
Apr  9 08:49:55.695: INFO: Verifying statefulset ss doesn't scale past 1 for another 980.92704ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9282
Apr  9 08:49:56.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 08:49:56.886: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 08:49:56.886: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 08:49:56.886: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 08:49:56.888: INFO: Found 1 stateful pods, waiting for 3
Apr  9 08:50:06.890: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:50:06.890: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 08:50:06.890: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Apr  9 08:50:06.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 08:50:07.058: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 08:50:07.058: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 08:50:07.058: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 08:50:07.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 08:50:07.223: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 08:50:07.223: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 08:50:07.223: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 08:50:07.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 08:50:07.405: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 08:50:07.405: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 08:50:07.405: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 08:50:07.405: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 08:50:07.407: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Apr  9 08:50:17.411: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 08:50:17.411: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 08:50:17.411: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 08:50:17.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999025s
Apr  9 08:50:18.420: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995833936s
Apr  9 08:50:19.422: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993492712s
Apr  9 08:50:20.425: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991172667s
Apr  9 08:50:21.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988908733s
Apr  9 08:50:22.429: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.986918776s
Apr  9 08:50:23.431: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984884234s
Apr  9 08:50:24.433: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982747267s
Apr  9 08:50:25.435: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.980910802s
Apr  9 08:50:26.437: INFO: Verifying statefulset ss doesn't scale past 3 for another 978.637207ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9282
Apr  9 08:50:27.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 08:50:27.601: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 08:50:27.601: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 08:50:27.601: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 08:50:27.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 08:50:27.782: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 08:50:27.782: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 08:50:27.782: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 08:50:27.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-9282 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 08:50:27.980: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 08:50:27.980: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 08:50:27.980: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 08:50:27.980: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 08:50:37.988: INFO: Deleting all statefulset in ns statefulset-9282
Apr  9 08:50:37.990: INFO: Scaling statefulset ss to 0
Apr  9 08:50:37.994: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 08:50:37.995: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:50:38.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9282" for this suite.
Apr  9 08:50:44.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:50:44.048: INFO: namespace statefulset-9282 deletion completed in 6.046300497s

• [SLOW TEST:77.593 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:50:44.048: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 08:50:44.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8992'
Apr  9 08:50:44.868: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr  9 08:50:44.868: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Apr  9 08:50:44.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete jobs e2e-test-nginx-job --namespace=kubectl-8992'
Apr  9 08:50:44.933: INFO: stderr: ""
Apr  9 08:50:44.933: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:50:44.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8992" for this suite.
Apr  9 08:50:50.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:50:50.989: INFO: namespace kubectl-8992 deletion completed in 6.053966334s

• [SLOW TEST:6.941 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:50:50.989: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 08:50:51.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8059'
Apr  9 08:50:51.073: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr  9 08:50:51.073: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Apr  9 08:50:53.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8059'
Apr  9 08:50:53.161: INFO: stderr: ""
Apr  9 08:50:53.161: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:50:53.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8059" for this suite.
Apr  9 08:51:15.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:51:15.205: INFO: namespace kubectl-8059 deletion completed in 22.042393691s

• [SLOW TEST:24.216 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:51:15.206: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:52:15.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7861" for this suite.
Apr  9 08:52:37.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:52:37.276: INFO: namespace container-probe-7861 deletion completed in 22.044164624s

• [SLOW TEST:82.071 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:52:37.276: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-7529
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7529 to expose endpoints map[]
Apr  9 08:52:37.304: INFO: Get endpoints failed (8.590725ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Apr  9 08:52:38.306: INFO: successfully validated that service multi-endpoint-test in namespace services-7529 exposes endpoints map[] (1.01037633s elapsed)
STEP: Creating pod pod1 in namespace services-7529
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7529 to expose endpoints map[pod1:[100]]
Apr  9 08:52:40.318: INFO: successfully validated that service multi-endpoint-test in namespace services-7529 exposes endpoints map[pod1:[100]] (2.008650268s elapsed)
STEP: Creating pod pod2 in namespace services-7529
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7529 to expose endpoints map[pod1:[100] pod2:[101]]
Apr  9 08:52:42.355: INFO: successfully validated that service multi-endpoint-test in namespace services-7529 exposes endpoints map[pod1:[100] pod2:[101]] (2.034825445s elapsed)
STEP: Deleting pod pod1 in namespace services-7529
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7529 to expose endpoints map[pod2:[101]]
Apr  9 08:52:43.364: INFO: successfully validated that service multi-endpoint-test in namespace services-7529 exposes endpoints map[pod2:[101]] (1.005924584s elapsed)
STEP: Deleting pod pod2 in namespace services-7529
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7529 to expose endpoints map[]
Apr  9 08:52:44.369: INFO: successfully validated that service multi-endpoint-test in namespace services-7529 exposes endpoints map[] (1.002623373s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:52:44.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7529" for this suite.
Apr  9 08:52:50.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:52:50.428: INFO: namespace services-7529 deletion completed in 6.046438213s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:13.152 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:52:50.428: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:52:50.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8112" for this suite.
Apr  9 08:53:12.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:53:12.509: INFO: namespace kubelet-test-8112 deletion completed in 22.042178771s

• [SLOW TEST:22.081 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:53:12.509: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Apr  9 08:53:13.041: INFO: created pod pod-service-account-defaultsa
Apr  9 08:53:13.041: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Apr  9 08:53:13.043: INFO: created pod pod-service-account-mountsa
Apr  9 08:53:13.043: INFO: pod pod-service-account-mountsa service account token volume mount: true
Apr  9 08:53:13.046: INFO: created pod pod-service-account-nomountsa
Apr  9 08:53:13.046: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Apr  9 08:53:13.050: INFO: created pod pod-service-account-defaultsa-mountspec
Apr  9 08:53:13.050: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Apr  9 08:53:13.059: INFO: created pod pod-service-account-mountsa-mountspec
Apr  9 08:53:13.059: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Apr  9 08:53:13.067: INFO: created pod pod-service-account-nomountsa-mountspec
Apr  9 08:53:13.067: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Apr  9 08:53:13.080: INFO: created pod pod-service-account-defaultsa-nomountspec
Apr  9 08:53:13.080: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Apr  9 08:53:13.086: INFO: created pod pod-service-account-mountsa-nomountspec
Apr  9 08:53:13.086: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Apr  9 08:53:13.094: INFO: created pod pod-service-account-nomountsa-nomountspec
Apr  9 08:53:13.094: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:53:13.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-79" for this suite.
Apr  9 08:54:59.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:54:59.156: INFO: namespace svcaccounts-79 deletion completed in 1m46.058383098s

• [SLOW TEST:106.647 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:54:59.157: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:54:59.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3" in namespace "projected-6992" to be "success or failure"
Apr  9 08:54:59.184: INFO: Pod "downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.275066ms
Apr  9 08:55:01.186: INFO: Pod "downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003423585s
STEP: Saw pod success
Apr  9 08:55:01.186: INFO: Pod "downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:55:01.187: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:55:01.194: INFO: Waiting for pod downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:55:01.195: INFO: Pod downwardapi-volume-279597c5-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:55:01.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6992" for this suite.
Apr  9 08:55:07.201: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:55:07.238: INFO: namespace projected-6992 deletion completed in 6.041524167s

• [SLOW TEST:8.081 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:55:07.238: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-2c66becb-5aa5-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:55:07.265: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3" in namespace "projected-7962" to be "success or failure"
Apr  9 08:55:07.269: INFO: Pod "pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017115ms
Apr  9 08:55:09.271: INFO: Pod "pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005978567s
Apr  9 08:55:11.273: INFO: Pod "pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007952013s
STEP: Saw pod success
Apr  9 08:55:11.273: INFO: Pod "pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:55:11.275: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:55:11.281: INFO: Waiting for pod pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:55:11.282: INFO: Pod pod-projected-secrets-2c67febd-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:55:11.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7962" for this suite.
Apr  9 08:55:17.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:55:17.327: INFO: namespace projected-7962 deletion completed in 6.04286867s

• [SLOW TEST:10.088 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:55:17.327: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Apr  9 08:55:17.360: INFO: Waiting up to 5m0s for pod "var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3" in namespace "var-expansion-2425" to be "success or failure"
Apr  9 08:55:17.363: INFO: Pod "var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.561556ms
Apr  9 08:55:19.365: INFO: Pod "var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005552857s
STEP: Saw pod success
Apr  9 08:55:19.365: INFO: Pod "var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:55:19.367: INFO: Trying to get logs from node ip-172-31-13-147 pod var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 08:55:19.373: INFO: Waiting for pod var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:55:19.374: INFO: Pod var-expansion-326b4b45-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:55:19.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2425" for this suite.
Apr  9 08:55:25.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:55:25.417: INFO: namespace var-expansion-2425 deletion completed in 6.041350668s

• [SLOW TEST:8.090 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:55:25.417: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:55:28.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7604" for this suite.
Apr  9 08:55:50.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:55:50.495: INFO: namespace replication-controller-7604 deletion completed in 22.042817432s

• [SLOW TEST:25.078 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:55:50.495: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:55:50.520: INFO: Waiting up to 5m0s for pod "downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3" in namespace "projected-5504" to be "success or failure"
Apr  9 08:55:50.524: INFO: Pod "downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032503ms
Apr  9 08:55:52.527: INFO: Pod "downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.006171501s
Apr  9 08:55:54.529: INFO: Pod "downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00823399s
STEP: Saw pod success
Apr  9 08:55:54.529: INFO: Pod "downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:55:54.530: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:55:54.538: INFO: Waiting for pod downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:55:54.546: INFO: Pod downwardapi-volume-462f3dd5-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:55:54.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5504" for this suite.
Apr  9 08:56:00.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:56:00.591: INFO: namespace projected-5504 deletion completed in 6.043787032s

• [SLOW TEST:10.096 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:56:00.591: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:56:18.621: INFO: Container started at 2019-04-09 08:56:01 +0000 UTC, pod became ready at 2019-04-09 08:56:18 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:56:18.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2624" for this suite.
Apr  9 08:56:40.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:56:40.665: INFO: namespace container-probe-2624 deletion completed in 22.042566038s

• [SLOW TEST:40.074 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:56:40.665: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 08:56:40.693: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Apr  9 08:56:40.697: INFO: Number of nodes with available pods: 0
Apr  9 08:56:40.697: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:56:41.700: INFO: Number of nodes with available pods: 0
Apr  9 08:56:41.700: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:56:42.700: INFO: Number of nodes with available pods: 0
Apr  9 08:56:42.701: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:56:43.702: INFO: Number of nodes with available pods: 1
Apr  9 08:56:43.702: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Apr  9 08:56:43.716: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:44.727: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:45.727: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:46.727: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:46.727: INFO: Pod daemon-set-ftv5d is not available
Apr  9 08:56:47.727: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:47.727: INFO: Pod daemon-set-ftv5d is not available
Apr  9 08:56:48.727: INFO: Wrong image for pod: daemon-set-ftv5d. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Apr  9 08:56:48.727: INFO: Pod daemon-set-ftv5d is not available
Apr  9 08:56:49.727: INFO: Pod daemon-set-s87sg is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Apr  9 08:56:49.731: INFO: Number of nodes with available pods: 0
Apr  9 08:56:49.731: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:56:50.734: INFO: Number of nodes with available pods: 0
Apr  9 08:56:50.734: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 08:56:51.734: INFO: Number of nodes with available pods: 1
Apr  9 08:56:51.734: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-180, will wait for the garbage collector to delete the pods
Apr  9 08:56:51.794: INFO: Deleting DaemonSet.extensions daemon-set took: 2.532119ms
Apr  9 08:56:52.094: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.343097ms
Apr  9 08:56:59.296: INFO: Number of nodes with available pods: 0
Apr  9 08:56:59.296: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 08:56:59.297: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-180/daemonsets","resourceVersion":"7058"},"items":null}

Apr  9 08:56:59.298: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-180/pods","resourceVersion":"7058"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:56:59.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-180" for this suite.
Apr  9 08:57:05.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:57:05.346: INFO: namespace daemonsets-180 deletion completed in 6.043275745s

• [SLOW TEST:24.680 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:57:05.346: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:57:11.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3604" for this suite.
Apr  9 08:57:17.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:57:17.522: INFO: namespace namespaces-3604 deletion completed in 6.04268238s
STEP: Destroying namespace "nsdeletetest-709" for this suite.
Apr  9 08:57:17.523: INFO: Namespace nsdeletetest-709 was already deleted
STEP: Destroying namespace "nsdeletetest-7878" for this suite.
Apr  9 08:57:23.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:57:23.565: INFO: namespace nsdeletetest-7878 deletion completed in 6.042052402s

• [SLOW TEST:18.220 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:57:23.565: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 08:57:23.590: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3" in namespace "projected-2480" to be "success or failure"
Apr  9 08:57:23.593: INFO: Pod "downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119363ms
Apr  9 08:57:25.596: INFO: Pod "downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.005199708s
Apr  9 08:57:27.598: INFO: Pod "downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007251335s
STEP: Saw pod success
Apr  9 08:57:27.598: INFO: Pod "downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:57:27.599: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 08:57:27.605: INFO: Waiting for pod downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:57:27.607: INFO: Pod downwardapi-volume-7da89d46-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:57:27.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2480" for this suite.
Apr  9 08:57:33.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:57:33.650: INFO: namespace projected-2480 deletion completed in 6.041940646s

• [SLOW TEST:10.085 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:57:33.650: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-808
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-808
STEP: Creating statefulset with conflicting port in namespace statefulset-808
STEP: Waiting until pod test-pod will start running in namespace statefulset-808
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-808
Apr  9 08:57:35.694: INFO: Observed stateful pod in namespace: statefulset-808, name: ss-0, uid: 83b90c8e-5aa5-11e9-bcbd-0af1119c727a, status phase: Pending. Waiting for statefulset controller to delete.
Apr  9 08:57:39.242: INFO: Observed stateful pod in namespace: statefulset-808, name: ss-0, uid: 83b90c8e-5aa5-11e9-bcbd-0af1119c727a, status phase: Failed. Waiting for statefulset controller to delete.
Apr  9 08:57:39.247: INFO: Observed stateful pod in namespace: statefulset-808, name: ss-0, uid: 83b90c8e-5aa5-11e9-bcbd-0af1119c727a, status phase: Failed. Waiting for statefulset controller to delete.
Apr  9 08:57:39.256: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-808
STEP: Removing pod with conflicting port in namespace statefulset-808
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-808 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 08:57:41.279: INFO: Deleting all statefulset in ns statefulset-808
Apr  9 08:57:41.280: INFO: Scaling statefulset ss to 0
Apr  9 08:57:51.288: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 08:57:51.289: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:57:51.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-808" for this suite.
Apr  9 08:57:57.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:57:57.338: INFO: namespace statefulset-808 deletion completed in 6.041898875s

• [SLOW TEST:23.688 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:57:57.338: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:58:01.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-590" for this suite.
Apr  9 08:58:39.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:58:39.416: INFO: namespace kubelet-test-590 deletion completed in 38.041828074s

• [SLOW TEST:42.078 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:58:39.416: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9656.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9656.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 08:58:43.452: INFO: DNS probes using dns-9656/dns-test-aade3fbb-5aa5-11e9-b737-ea708a3858a3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:58:43.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9656" for this suite.
Apr  9 08:58:49.473: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:58:49.511: INFO: namespace dns-9656 deletion completed in 6.042982011s

• [SLOW TEST:10.095 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:58:49.511: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Apr  9 08:58:49.527: INFO: PodSpec: initContainers in spec.initContainers
Apr  9 08:59:35.102: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b0e2e7da-5aa5-11e9-b737-ea708a3858a3", GenerateName:"", Namespace:"init-container-4325", SelfLink:"/api/v1/namespaces/init-container-4325/pods/pod-init-b0e2e7da-5aa5-11e9-b737-ea708a3858a3", UID:"b0e420f8-5aa5-11e9-bcbd-0af1119c727a", ResourceVersion:"7524", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63690397129, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"527501236"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xfmqv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002690000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfmqv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfmqv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xfmqv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0015ce158), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-172-31-13-147", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028e0060), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0015ce260)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0015ce2f0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0015ce2f8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0015ce2fc)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690397129, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690397129, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690397129, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690397129, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.31.13.147", PodIP:"10.1.1.17", StartTime:(*v1.Time)(0xc002c9c060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008a57a0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008a5810)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://f1c7ed19b85a76fe70d4a040fed77431afa9f498f05b1c17de9dd37ca02c96c7"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c9c0a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002c9c080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:59:35.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4325" for this suite.
Apr  9 08:59:57.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 08:59:57.162: INFO: namespace init-container-4325 deletion completed in 22.047966193s

• [SLOW TEST:67.651 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 08:59:57.163: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d9355f9c-5aa5-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 08:59:57.189: INFO: Waiting up to 5m0s for pod "pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3" in namespace "secrets-9964" to be "success or failure"
Apr  9 08:59:57.190: INFO: Pod "pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.122488ms
Apr  9 08:59:59.192: INFO: Pod "pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.002886893s
STEP: Saw pod success
Apr  9 08:59:59.192: INFO: Pod "pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 08:59:59.196: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 08:59:59.205: INFO: Waiting for pod pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3 to disappear
Apr  9 08:59:59.214: INFO: Pod pod-secrets-d935c569-5aa5-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 08:59:59.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9964" for this suite.
Apr  9 09:00:05.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:00:05.258: INFO: namespace secrets-9964 deletion completed in 6.042488148s

• [SLOW TEST:8.096 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:00:05.258: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Apr  9 09:00:09.295: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.295: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.405: INFO: Exec stderr: ""
Apr  9 09:00:09.405: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.405: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.546: INFO: Exec stderr: ""
Apr  9 09:00:09.546: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.546: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.662: INFO: Exec stderr: ""
Apr  9 09:00:09.662: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.662: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.773: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Apr  9 09:00:09.773: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.773: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.887: INFO: Exec stderr: ""
Apr  9 09:00:09.887: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.887: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:09.995: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Apr  9 09:00:09.995: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:09.995: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:10.101: INFO: Exec stderr: ""
Apr  9 09:00:10.101: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:10.101: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:10.219: INFO: Exec stderr: ""
Apr  9 09:00:10.219: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:10.219: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:10.328: INFO: Exec stderr: ""
Apr  9 09:00:10.328: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1662 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:00:10.328: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:00:10.438: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:00:10.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1662" for this suite.
Apr  9 09:00:52.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:00:52.482: INFO: namespace e2e-kubelet-etc-hosts-1662 deletion completed in 42.042281022s

• [SLOW TEST:47.223 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:00:52.482: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Apr  9 09:00:52.508: INFO: Pod name pod-release: Found 0 pods out of 1
Apr  9 09:00:57.510: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:00:57.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6753" for this suite.
Apr  9 09:01:03.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:01:03.574: INFO: namespace replication-controller-6753 deletion completed in 6.05332165s

• [SLOW TEST:11.092 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:01:03.574: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:01:03.602: INFO: Conformance test suite needs a cluster with at least 2 nodes.
Apr  9 09:01:03.602: INFO: Create a RollingUpdate DaemonSet
Apr  9 09:01:03.604: INFO: Check that daemon pods launch on every node of the cluster
Apr  9 09:01:03.609: INFO: Number of nodes with available pods: 0
Apr  9 09:01:03.609: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:01:04.612: INFO: Number of nodes with available pods: 0
Apr  9 09:01:04.612: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:01:05.612: INFO: Number of nodes with available pods: 1
Apr  9 09:01:05.612: INFO: Number of running nodes: 1, number of available pods: 1
Apr  9 09:01:05.612: INFO: Update the DaemonSet to trigger a rollout
Apr  9 09:01:05.615: INFO: Updating DaemonSet daemon-set
Apr  9 09:01:09.620: INFO: Roll back the DaemonSet before rollout is complete
Apr  9 09:01:09.623: INFO: Updating DaemonSet daemon-set
Apr  9 09:01:09.623: INFO: Make sure DaemonSet rollback is complete
Apr  9 09:01:09.624: INFO: Wrong image for pod: daemon-set-46nwr. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr  9 09:01:09.624: INFO: Pod daemon-set-46nwr is not available
Apr  9 09:01:10.630: INFO: Wrong image for pod: daemon-set-46nwr. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Apr  9 09:01:10.630: INFO: Pod daemon-set-46nwr is not available
Apr  9 09:01:11.630: INFO: Pod daemon-set-frvxg is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4661, will wait for the garbage collector to delete the pods
Apr  9 09:01:11.688: INFO: Deleting DaemonSet.extensions daemon-set took: 2.586883ms
Apr  9 09:01:11.788: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.228559ms
Apr  9 09:01:19.289: INFO: Number of nodes with available pods: 0
Apr  9 09:01:19.289: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 09:01:19.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4661/daemonsets","resourceVersion":"7855"},"items":null}

Apr  9 09:01:19.294: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4661/pods","resourceVersion":"7855"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:01:19.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4661" for this suite.
Apr  9 09:01:25.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:01:25.347: INFO: namespace daemonsets-4661 deletion completed in 6.046938535s

• [SLOW TEST:21.773 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:01:25.347: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Apr  9 09:01:25.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-8973'
Apr  9 09:01:26.311: INFO: stderr: ""
Apr  9 09:01:26.311: INFO: stdout: "pod/pause created\n"
Apr  9 09:01:26.311: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Apr  9 09:01:26.311: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-8973" to be "running and ready"
Apr  9 09:01:26.314: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.890262ms
Apr  9 09:01:28.316: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.004943987s
Apr  9 09:01:28.316: INFO: Pod "pause" satisfied condition "running and ready"
Apr  9 09:01:28.316: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Apr  9 09:01:28.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 label pods pause testing-label=testing-label-value --namespace=kubectl-8973'
Apr  9 09:01:28.379: INFO: stderr: ""
Apr  9 09:01:28.379: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Apr  9 09:01:28.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pod pause -L testing-label --namespace=kubectl-8973'
Apr  9 09:01:28.439: INFO: stderr: ""
Apr  9 09:01:28.439: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Apr  9 09:01:28.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 label pods pause testing-label- --namespace=kubectl-8973'
Apr  9 09:01:28.500: INFO: stderr: ""
Apr  9 09:01:28.500: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Apr  9 09:01:28.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pod pause -L testing-label --namespace=kubectl-8973'
Apr  9 09:01:28.559: INFO: stderr: ""
Apr  9 09:01:28.559: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Apr  9 09:01:28.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-8973'
Apr  9 09:01:28.620: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 09:01:28.620: INFO: stdout: "pod \"pause\" force deleted\n"
Apr  9 09:01:28.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=pause --no-headers --namespace=kubectl-8973'
Apr  9 09:01:28.695: INFO: stderr: "No resources found.\n"
Apr  9 09:01:28.695: INFO: stdout: ""
Apr  9 09:01:28.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=pause --namespace=kubectl-8973 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 09:01:28.759: INFO: stderr: ""
Apr  9 09:01:28.759: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:01:28.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8973" for this suite.
Apr  9 09:01:34.772: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:01:34.809: INFO: namespace kubectl-8973 deletion completed in 6.048484147s

• [SLOW TEST:9.463 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:01:34.810: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Apr  9 09:01:34.834: INFO: Waiting up to 5m0s for pod "pod-13695ee9-5aa6-11e9-b737-ea708a3858a3" in namespace "emptydir-9255" to be "success or failure"
Apr  9 09:01:34.835: INFO: Pod "pod-13695ee9-5aa6-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.087369ms
Apr  9 09:01:36.837: INFO: Pod "pod-13695ee9-5aa6-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003132534s
STEP: Saw pod success
Apr  9 09:01:36.837: INFO: Pod "pod-13695ee9-5aa6-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:01:36.839: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-13695ee9-5aa6-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:01:36.848: INFO: Waiting for pod pod-13695ee9-5aa6-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:01:36.856: INFO: Pod pod-13695ee9-5aa6-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:01:36.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9255" for this suite.
Apr  9 09:01:42.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:01:42.899: INFO: namespace emptydir-9255 deletion completed in 6.04158817s

• [SLOW TEST:8.089 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:01:42.899: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Apr  9 09:01:42.914: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Apr  9 09:01:42.925: INFO: Waiting for terminating namespaces to be deleted...
Apr  9 09:01:42.926: INFO: 
Logging pods the kubelet thinks is on node ip-172-31-13-147 before test
Apr  9 09:01:42.929: INFO: sonobuoy-e2e-job-e27e4b08c656417e from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 09:01:42.929: INFO: 	Container e2e ready: true, restart count 0
Apr  9 09:01:42.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Apr  9 09:01:42.929: INFO: kube-dns-6bfbdd666c-966qk from kube-system started at 2019-04-09 08:25:19 +0000 UTC (3 container statuses recorded)
Apr  9 09:01:42.929: INFO: 	Container dnsmasq ready: true, restart count 0
Apr  9 09:01:42.929: INFO: 	Container kubedns ready: true, restart count 0
Apr  9 09:01:42.929: INFO: 	Container sidecar ready: true, restart count 0
Apr  9 09:01:42.929: INFO: sonobuoy from heptio-sonobuoy started at 2019-04-09 08:26:02 +0000 UTC (1 container statuses recorded)
Apr  9 09:01:42.929: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Apr  9 09:01:42.929: INFO: sonobuoy-systemd-logs-daemon-set-82c391b0bd954094-h8czw from heptio-sonobuoy started at 2019-04-09 08:26:06 +0000 UTC (2 container statuses recorded)
Apr  9 09:01:42.929: INFO: 	Container sonobuoy-systemd-logs-config ready: true, restart count 0
Apr  9 09:01:42.929: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node ip-172-31-13-147
Apr  9 09:01:42.937: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-172-31-13-147
Apr  9 09:01:42.937: INFO: Pod sonobuoy-e2e-job-e27e4b08c656417e requesting resource cpu=0m on Node ip-172-31-13-147
Apr  9 09:01:42.937: INFO: Pod sonobuoy-systemd-logs-daemon-set-82c391b0bd954094-h8czw requesting resource cpu=0m on Node ip-172-31-13-147
Apr  9 09:01:42.937: INFO: Pod kube-dns-6bfbdd666c-966qk requesting resource cpu=260m on Node ip-172-31-13-147
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3.1593c2d1da9e2255], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4312/filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3 to ip-172-31-13-147]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3.1593c2d21007f1a1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3.1593c2d21739f107], Reason = [Created], Message = [Created container filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3.1593c2d22406116b], Reason = [Started], Message = [Started container filler-pod-183f2d63-5aa6-11e9-b737-ea708a3858a3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.1593c2d2524354b4], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node ip-172-31-13-147
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:01:45.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4312" for this suite.
Apr  9 09:01:51.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:01:52.014: INFO: namespace sched-pred-4312 deletion completed in 6.043979052s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:9.115 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:01:52.014: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-1dabb923-5aa6-11e9-b737-ea708a3858a3
STEP: Creating configMap with name cm-test-opt-upd-1dabb95b-5aa6-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1dabb923-5aa6-11e9-b737-ea708a3858a3
STEP: Updating configmap cm-test-opt-upd-1dabb95b-5aa6-11e9-b737-ea708a3858a3
STEP: Creating configMap with name cm-test-opt-create-1dabb976-5aa6-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:01:56.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7699" for this suite.
Apr  9 09:02:18.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:18.119: INFO: namespace configmap-7699 deletion completed in 22.04212883s

• [SLOW TEST:26.105 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:18.119: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Apr  9 09:02:18.144: INFO: Waiting up to 5m0s for pod "pod-2d39da03-5aa6-11e9-b737-ea708a3858a3" in namespace "emptydir-5504" to be "success or failure"
Apr  9 09:02:18.145: INFO: Pod "pod-2d39da03-5aa6-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.155804ms
Apr  9 09:02:20.152: INFO: Pod "pod-2d39da03-5aa6-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007928975s
STEP: Saw pod success
Apr  9 09:02:20.152: INFO: Pod "pod-2d39da03-5aa6-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:02:20.154: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-2d39da03-5aa6-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:02:20.160: INFO: Waiting for pod pod-2d39da03-5aa6-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:02:20.161: INFO: Pod pod-2d39da03-5aa6-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:20.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5504" for this suite.
Apr  9 09:02:26.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:26.205: INFO: namespace emptydir-5504 deletion completed in 6.04196629s

• [SLOW TEST:8.085 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:26.205: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-320cb390-5aa6-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:02:26.231: INFO: Waiting up to 5m0s for pod "pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3" in namespace "secrets-3849" to be "success or failure"
Apr  9 09:02:26.234: INFO: Pod "pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404164ms
Apr  9 09:02:28.236: INFO: Pod "pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004229733s
STEP: Saw pod success
Apr  9 09:02:28.236: INFO: Pod "pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:02:28.237: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3 container secret-env-test: <nil>
STEP: delete the pod
Apr  9 09:02:28.243: INFO: Waiting for pod pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:02:28.244: INFO: Pod pod-secrets-320cf073-5aa6-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:28.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3849" for this suite.
Apr  9 09:02:34.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:34.288: INFO: namespace secrets-3849 deletion completed in 6.041927166s

• [SLOW TEST:8.083 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:34.288: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Apr  9 09:02:34.304: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-393566007 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:34.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4897" for this suite.
Apr  9 09:02:40.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:40.401: INFO: namespace kubectl-4897 deletion completed in 6.0418023s

• [SLOW TEST:6.113 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:40.401: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:02:40.427: INFO: (0) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.92825ms)
Apr  9 09:02:40.428: INFO: (1) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.690201ms)
Apr  9 09:02:40.430: INFO: (2) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.686076ms)
Apr  9 09:02:40.432: INFO: (3) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.643816ms)
Apr  9 09:02:40.433: INFO: (4) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.771873ms)
Apr  9 09:02:40.435: INFO: (5) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.650022ms)
Apr  9 09:02:40.437: INFO: (6) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.649702ms)
Apr  9 09:02:40.438: INFO: (7) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.656335ms)
Apr  9 09:02:40.440: INFO: (8) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.702292ms)
Apr  9 09:02:40.442: INFO: (9) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.770999ms)
Apr  9 09:02:40.444: INFO: (10) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.648245ms)
Apr  9 09:02:40.445: INFO: (11) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.660497ms)
Apr  9 09:02:40.447: INFO: (12) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.871536ms)
Apr  9 09:02:40.449: INFO: (13) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.755057ms)
Apr  9 09:02:40.451: INFO: (14) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.92524ms)
Apr  9 09:02:40.453: INFO: (15) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.688401ms)
Apr  9 09:02:40.454: INFO: (16) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.710178ms)
Apr  9 09:02:40.456: INFO: (17) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.639685ms)
Apr  9 09:02:40.460: INFO: (18) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.418665ms)
Apr  9 09:02:40.462: INFO: (19) /api/v1/nodes/ip-172-31-13-147:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.676911ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:40.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2521" for this suite.
Apr  9 09:02:46.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:46.508: INFO: namespace proxy-2521 deletion completed in 6.044306219s

• [SLOW TEST:6.106 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:46.508: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Apr  9 09:02:46.537: INFO: Waiting up to 5m0s for pod "client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3" in namespace "containers-2058" to be "success or failure"
Apr  9 09:02:46.546: INFO: Pod "client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.097448ms
Apr  9 09:02:48.548: INFO: Pod "client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010775741s
STEP: Saw pod success
Apr  9 09:02:48.548: INFO: Pod "client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:02:48.549: INFO: Trying to get logs from node ip-172-31-13-147 pod client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:02:48.555: INFO: Waiting for pod client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:02:48.556: INFO: Pod client-containers-3e270e95-5aa6-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:48.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2058" for this suite.
Apr  9 09:02:54.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:02:54.601: INFO: namespace containers-2058 deletion completed in 6.04350197s

• [SLOW TEST:8.093 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:02:54.601: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-42f88fc1-5aa6-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-42f88fc1-5aa6-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:02:58.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1020" for this suite.
Apr  9 09:03:20.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:03:20.688: INFO: namespace configmap-1020 deletion completed in 22.044327509s

• [SLOW TEST:26.087 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:03:20.688: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:03:24.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1109" for this suite.
Apr  9 09:03:30.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:03:30.761: INFO: namespace kubelet-test-1109 deletion completed in 6.043803196s

• [SLOW TEST:10.073 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:03:30.761: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7721
Apr  9 09:03:32.787: INFO: Started pod liveness-http in namespace container-probe-7721
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 09:03:32.788: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:07:33.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7721" for this suite.
Apr  9 09:07:39.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:07:39.083: INFO: namespace container-probe-7721 deletion completed in 6.047546599s

• [SLOW TEST:248.322 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:07:39.083: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-ec8937d4-5aa6-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:07:39.111: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3" in namespace "projected-6846" to be "success or failure"
Apr  9 09:07:39.114: INFO: Pod "pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.839848ms
Apr  9 09:07:41.117: INFO: Pod "pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005902671s
STEP: Saw pod success
Apr  9 09:07:41.117: INFO: Pod "pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:07:41.118: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:07:41.128: INFO: Waiting for pod pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:07:41.136: INFO: Pod pod-projected-secrets-ec8a7955-5aa6-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:07:41.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6846" for this suite.
Apr  9 09:07:47.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:07:47.179: INFO: namespace projected-6846 deletion completed in 6.042010039s

• [SLOW TEST:8.096 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:07:47.179: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 09:07:49.716: INFO: Successfully updated pod "annotationupdatef15c6ad4-5aa6-11e9-b737-ea708a3858a3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:07:53.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4426" for this suite.
Apr  9 09:08:15.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:08:15.769: INFO: namespace projected-4426 deletion completed in 22.042068925s

• [SLOW TEST:28.591 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:08:15.770: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 09:08:15.792: INFO: Waiting up to 5m0s for pod "downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3" in namespace "downward-api-5100" to be "success or failure"
Apr  9 09:08:15.800: INFO: Pod "downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.620553ms
Apr  9 09:08:17.802: INFO: Pod "downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009683485s
STEP: Saw pod success
Apr  9 09:08:17.802: INFO: Pod "downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:08:17.803: INFO: Trying to get logs from node ip-172-31-13-147 pod downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 09:08:17.810: INFO: Waiting for pod downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:08:17.811: INFO: Pod downward-api-02679e82-5aa7-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:08:17.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5100" for this suite.
Apr  9 09:08:23.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:08:23.857: INFO: namespace downward-api-5100 deletion completed in 6.044032123s

• [SLOW TEST:8.087 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:08:23.857: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-159
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 09:08:23.871: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 09:08:41.906: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.1.1.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-159 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:08:41.906: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:08:42.018: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:08:42.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-159" for this suite.
Apr  9 09:09:04.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:09:04.063: INFO: namespace pod-network-test-159 deletion completed in 22.043467212s

• [SLOW TEST:40.206 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:09:04.064: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Apr  9 09:09:04.092: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8879,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 09:09:04.092: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8880,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr  9 09:09:04.092: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8881,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Apr  9 09:09:14.103: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8895,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 09:09:14.103: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8896,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Apr  9 09:09:14.103: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2716,SelfLink:/api/v1/namespaces/watch-2716/configmaps/e2e-watch-test-label-changed,UID:1f3057c2-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:8897,Generation:0,CreationTimestamp:2019-04-09 09:09:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:09:14.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2716" for this suite.
Apr  9 09:09:20.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:09:20.150: INFO: namespace watch-2716 deletion completed in 6.045532909s

• [SLOW TEST:16.086 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:09:20.150: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:09:22.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-742" for this suite.
Apr  9 09:10:12.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:10:12.227: INFO: namespace kubelet-test-742 deletion completed in 50.042449779s

• [SLOW TEST:52.077 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:10:12.228: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:10:12.250: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Apr  9 09:10:17.252: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 09:10:17.252: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 09:10:17.265: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-236,SelfLink:/apis/apps/v1/namespaces/deployment-236/deployments/test-cleanup-deployment,UID:4acdd50d-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:9023,Generation:1,CreationTimestamp:2019-04-09 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Apr  9 09:10:17.279: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-236,SelfLink:/apis/apps/v1/namespaces/deployment-236/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:4acf4fc4-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:9025,Generation:1,CreationTimestamp:2019-04-09 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 4acdd50d-5aa7-11e9-bcbd-0af1119c727a 0xc0018abec7 0xc0018abec8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 09:10:17.279: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Apr  9 09:10:17.279: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-236,SelfLink:/apis/apps/v1/namespaces/deployment-236/replicasets/test-cleanup-controller,UID:47d1d714-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:9024,Generation:1,CreationTimestamp:2019-04-09 09:10:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 4acdd50d-5aa7-11e9-bcbd-0af1119c727a 0xc0018abdef 0xc0018abe00}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 09:10:17.282: INFO: Pod "test-cleanup-controller-srnbv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-srnbv,GenerateName:test-cleanup-controller-,Namespace:deployment-236,SelfLink:/api/v1/namespaces/deployment-236/pods/test-cleanup-controller-srnbv,UID:47d23d02-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:9018,Generation:0,CreationTimestamp:2019-04-09 09:10:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 47d1d714-5aa7-11e9-bcbd-0af1119c727a 0xc0021fb30f 0xc0021fb340}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z8rfd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z8rfd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z8rfd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021fb3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021fb4c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:10:12 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:10:14 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:10:14 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:10:12 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.69,StartTime:2019-04-09 09:10:12 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-04-09 09:10:13 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d containerd://5f9daa1daec086fe335aac41897d40e02d9bdfa8db0016702e4a90f1bff349cb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Apr  9 09:10:17.282: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-qfkcp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-qfkcp,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-236,SelfLink:/api/v1/namespaces/deployment-236/pods/test-cleanup-deployment-55cbfbc8f5-qfkcp,UID:4acf8823-5aa7-11e9-bcbd-0af1119c727a,ResourceVersion:9026,Generation:0,CreationTimestamp:2019-04-09 09:10:17 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 4acf4fc4-5aa7-11e9-bcbd-0af1119c727a 0xc0021fb5c7 0xc0021fb5c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z8rfd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z8rfd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-z8rfd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021fb6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021fb6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:10:17.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-236" for this suite.
Apr  9 09:10:23.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:10:23.346: INFO: namespace deployment-236 deletion completed in 6.061251689s

• [SLOW TEST:11.119 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:10:23.346: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-4024/secret-test-4e71cc27-5aa7-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:10:23.373: INFO: Waiting up to 5m0s for pod "pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3" in namespace "secrets-4024" to be "success or failure"
Apr  9 09:10:23.374: INFO: Pod "pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.096622ms
Apr  9 09:10:25.376: INFO: Pod "pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003014361s
STEP: Saw pod success
Apr  9 09:10:25.376: INFO: Pod "pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:10:25.378: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3 container env-test: <nil>
STEP: delete the pod
Apr  9 09:10:25.387: INFO: Waiting for pod pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:10:25.395: INFO: Pod pod-configmaps-4e730794-5aa7-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:10:25.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4024" for this suite.
Apr  9 09:10:31.402: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:10:31.439: INFO: namespace secrets-4024 deletion completed in 6.042766534s

• [SLOW TEST:8.093 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:10:31.440: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Apr  9 09:10:31.704: INFO: Pod name wrapped-volume-race-5357fcd8-5aa7-11e9-b737-ea708a3858a3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-5357fcd8-5aa7-11e9-b737-ea708a3858a3 in namespace emptydir-wrapper-5054, will wait for the garbage collector to delete the pods
Apr  9 09:10:47.809: INFO: Deleting ReplicationController wrapped-volume-race-5357fcd8-5aa7-11e9-b737-ea708a3858a3 took: 2.814706ms
Apr  9 09:10:48.110: INFO: Terminating ReplicationController wrapped-volume-race-5357fcd8-5aa7-11e9-b737-ea708a3858a3 pods took: 300.252467ms
STEP: Creating RC which spawns configmap-volume pods
Apr  9 09:11:22.418: INFO: Pod name wrapped-volume-race-71a3bddd-5aa7-11e9-b737-ea708a3858a3: Found 0 pods out of 5
Apr  9 09:11:27.421: INFO: Pod name wrapped-volume-race-71a3bddd-5aa7-11e9-b737-ea708a3858a3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-71a3bddd-5aa7-11e9-b737-ea708a3858a3 in namespace emptydir-wrapper-5054, will wait for the garbage collector to delete the pods
Apr  9 09:11:39.492: INFO: Deleting ReplicationController wrapped-volume-race-71a3bddd-5aa7-11e9-b737-ea708a3858a3 took: 3.044353ms
Apr  9 09:11:39.793: INFO: Terminating ReplicationController wrapped-volume-race-71a3bddd-5aa7-11e9-b737-ea708a3858a3 pods took: 300.242601ms
STEP: Creating RC which spawns configmap-volume pods
Apr  9 09:12:19.300: INFO: Pod name wrapped-volume-race-938b617b-5aa7-11e9-b737-ea708a3858a3: Found 0 pods out of 5
Apr  9 09:12:24.303: INFO: Pod name wrapped-volume-race-938b617b-5aa7-11e9-b737-ea708a3858a3: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-938b617b-5aa7-11e9-b737-ea708a3858a3 in namespace emptydir-wrapper-5054, will wait for the garbage collector to delete the pods
Apr  9 09:12:34.371: INFO: Deleting ReplicationController wrapped-volume-race-938b617b-5aa7-11e9-b737-ea708a3858a3 took: 2.640498ms
Apr  9 09:12:34.672: INFO: Terminating ReplicationController wrapped-volume-race-938b617b-5aa7-11e9-b737-ea708a3858a3 pods took: 300.271374ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:13:10.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5054" for this suite.
Apr  9 09:13:16.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:13:16.665: INFO: namespace emptydir-wrapper-5054 deletion completed in 6.057609953s

• [SLOW TEST:165.225 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:13:16.665: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Apr  9 09:13:16.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-2231'
Apr  9 09:13:17.565: INFO: stderr: ""
Apr  9 09:13:17.565: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 09:13:17.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2231'
Apr  9 09:13:17.640: INFO: stderr: ""
Apr  9 09:13:17.640: INFO: stdout: "update-demo-nautilus-dkqfn update-demo-nautilus-wlx8x "
Apr  9 09:13:17.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-dkqfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:17.710: INFO: stderr: ""
Apr  9 09:13:17.710: INFO: stdout: ""
Apr  9 09:13:17.710: INFO: update-demo-nautilus-dkqfn is created but not running
Apr  9 09:13:22.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2231'
Apr  9 09:13:22.772: INFO: stderr: ""
Apr  9 09:13:22.772: INFO: stdout: "update-demo-nautilus-dkqfn update-demo-nautilus-wlx8x "
Apr  9 09:13:22.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-dkqfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:22.832: INFO: stderr: ""
Apr  9 09:13:22.832: INFO: stdout: "true"
Apr  9 09:13:22.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-dkqfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:22.891: INFO: stderr: ""
Apr  9 09:13:22.891: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:13:22.891: INFO: validating pod update-demo-nautilus-dkqfn
Apr  9 09:13:22.893: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:13:22.893: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:13:22.893: INFO: update-demo-nautilus-dkqfn is verified up and running
Apr  9 09:13:22.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-wlx8x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:22.953: INFO: stderr: ""
Apr  9 09:13:22.953: INFO: stdout: "true"
Apr  9 09:13:22.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-wlx8x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:23.014: INFO: stderr: ""
Apr  9 09:13:23.014: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:13:23.014: INFO: validating pod update-demo-nautilus-wlx8x
Apr  9 09:13:23.016: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:13:23.016: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:13:23.016: INFO: update-demo-nautilus-wlx8x is verified up and running
STEP: rolling-update to new replication controller
Apr  9 09:13:23.018: INFO: scanned /root for discovery docs: <nil>
Apr  9 09:13:23.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2231'
Apr  9 09:13:45.275: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr  9 09:13:45.275: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 09:13:45.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2231'
Apr  9 09:13:45.338: INFO: stderr: ""
Apr  9 09:13:45.338: INFO: stdout: "update-demo-kitten-m72dq update-demo-kitten-tbt6c "
Apr  9 09:13:45.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-kitten-m72dq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:45.397: INFO: stderr: ""
Apr  9 09:13:45.397: INFO: stdout: "true"
Apr  9 09:13:45.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-kitten-m72dq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:45.465: INFO: stderr: ""
Apr  9 09:13:45.465: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr  9 09:13:45.465: INFO: validating pod update-demo-kitten-m72dq
Apr  9 09:13:45.468: INFO: got data: {
  "image": "kitten.jpg"
}

Apr  9 09:13:45.468: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr  9 09:13:45.468: INFO: update-demo-kitten-m72dq is verified up and running
Apr  9 09:13:45.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-kitten-tbt6c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:45.543: INFO: stderr: ""
Apr  9 09:13:45.543: INFO: stdout: "true"
Apr  9 09:13:45.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-kitten-tbt6c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2231'
Apr  9 09:13:45.601: INFO: stderr: ""
Apr  9 09:13:45.601: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Apr  9 09:13:45.601: INFO: validating pod update-demo-kitten-tbt6c
Apr  9 09:13:45.603: INFO: got data: {
  "image": "kitten.jpg"
}

Apr  9 09:13:45.603: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Apr  9 09:13:45.603: INFO: update-demo-kitten-tbt6c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:13:45.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2231" for this suite.
Apr  9 09:14:07.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:14:07.649: INFO: namespace kubectl-2231 deletion completed in 22.043910123s

• [SLOW TEST:50.984 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:14:07.649: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-d4252efc-5aa7-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:14:07.680: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3" in namespace "projected-4268" to be "success or failure"
Apr  9 09:14:07.684: INFO: Pod "pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.616298ms
Apr  9 09:14:09.686: INFO: Pod "pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006334879s
Apr  9 09:14:11.688: INFO: Pod "pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008050104s
STEP: Saw pod success
Apr  9 09:14:11.688: INFO: Pod "pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:14:11.689: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:14:11.698: INFO: Waiting for pod pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:14:11.705: INFO: Pod pod-projected-secrets-d42571c7-5aa7-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:14:11.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4268" for this suite.
Apr  9 09:14:17.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:14:17.758: INFO: namespace projected-4268 deletion completed in 6.051060126s

• [SLOW TEST:10.109 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:14:17.758: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3
Apr  9 09:14:17.784: INFO: Pod name my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3: Found 0 pods out of 1
Apr  9 09:14:22.786: INFO: Pod name my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3: Found 1 pods out of 1
Apr  9 09:14:22.786: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3" are running
Apr  9 09:14:22.787: INFO: Pod "my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3-8mpqm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:14:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:14:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:14:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:14:17 +0000 UTC Reason: Message:}])
Apr  9 09:14:22.787: INFO: Trying to dial the pod
Apr  9 09:14:27.793: INFO: Controller my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3: Got expected result from replica 1 [my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3-8mpqm]: "my-hostname-basic-da2a19be-5aa7-11e9-b737-ea708a3858a3-8mpqm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:14:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9738" for this suite.
Apr  9 09:14:33.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:14:33.836: INFO: namespace replication-controller-9738 deletion completed in 6.041757786s

• [SLOW TEST:16.078 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:14:33.836: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 09:14:33.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5476'
Apr  9 09:14:33.923: INFO: stderr: ""
Apr  9 09:14:33.923: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Apr  9 09:14:38.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pod e2e-test-nginx-pod --namespace=kubectl-5476 -o json'
Apr  9 09:14:39.034: INFO: stderr: ""
Apr  9 09:14:39.034: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-04-09T09:14:33Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5476\",\n        \"resourceVersion\": \"10448\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5476/pods/e2e-test-nginx-pod\",\n        \"uid\": \"e3c84805-5aa7-11e9-bcbd-0af1119c727a\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-5vv4v\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-172-31-13-147\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-5vv4v\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-5vv4v\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T09:14:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T09:14:35Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T09:14:35Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-04-09T09:14:33Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://38cb9be8814c7c321118b389073b7f9334c5166525ce9e8b44327f92ea92ca1b\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-04-09T09:14:35Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.31.13.147\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.1.1.109\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-04-09T09:14:33Z\"\n    }\n}\n"
STEP: replace the image in the pod
Apr  9 09:14:39.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 replace -f - --namespace=kubectl-5476'
Apr  9 09:14:39.187: INFO: stderr: ""
Apr  9 09:14:39.187: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Apr  9 09:14:39.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete pods e2e-test-nginx-pod --namespace=kubectl-5476'
Apr  9 09:14:49.248: INFO: stderr: ""
Apr  9 09:14:49.248: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:14:49.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5476" for this suite.
Apr  9 09:14:55.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:14:55.294: INFO: namespace kubectl-5476 deletion completed in 6.044623786s

• [SLOW TEST:21.458 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:14:55.294: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Apr  9 09:14:55.312: INFO: Waiting up to 5m0s for pod "var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3" in namespace "var-expansion-2893" to be "success or failure"
Apr  9 09:14:55.315: INFO: Pod "var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.999274ms
Apr  9 09:14:57.317: INFO: Pod "var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005093299s
STEP: Saw pod success
Apr  9 09:14:57.317: INFO: Pod "var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:14:57.318: INFO: Trying to get logs from node ip-172-31-13-147 pod var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 09:14:57.325: INFO: Waiting for pod var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:14:57.326: INFO: Pod var-expansion-f0897f74-5aa7-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:14:57.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2893" for this suite.
Apr  9 09:15:03.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:15:03.384: INFO: namespace var-expansion-2893 deletion completed in 6.04411134s

• [SLOW TEST:8.090 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:15:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-1524/configmap-test-f55bf7ed-5aa7-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:15:03.404: INFO: Waiting up to 5m0s for pod "pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3" in namespace "configmap-1524" to be "success or failure"
Apr  9 09:15:03.412: INFO: Pod "pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.862038ms
Apr  9 09:15:05.414: INFO: Pod "pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009949467s
STEP: Saw pod success
Apr  9 09:15:05.414: INFO: Pod "pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:15:05.416: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3 container env-test: <nil>
STEP: delete the pod
Apr  9 09:15:05.422: INFO: Waiting for pod pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:15:05.423: INFO: Pod pod-configmaps-f55c3157-5aa7-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:15:05.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1524" for this suite.
Apr  9 09:15:11.431: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:15:11.469: INFO: namespace configmap-1524 deletion completed in 6.044460323s

• [SLOW TEST:8.085 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:15:11.469: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:15:11.524: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fa302663-5aa7-11e9-bcbd-0af1119c727a", Controller:(*bool)(0xc001743daa), BlockOwnerDeletion:(*bool)(0xc001743dab)}}
Apr  9 09:15:11.532: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"fa2f6879-5aa7-11e9-bcbd-0af1119c727a", Controller:(*bool)(0xc001743f66), BlockOwnerDeletion:(*bool)(0xc001743f67)}}
Apr  9 09:15:11.536: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"fa2fbcf2-5aa7-11e9-bcbd-0af1119c727a", Controller:(*bool)(0xc0026de9d6), BlockOwnerDeletion:(*bool)(0xc0026de9d7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:15:16.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-622" for this suite.
Apr  9 09:15:22.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:15:22.591: INFO: namespace gc-622 deletion completed in 6.041177299s

• [SLOW TEST:11.122 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:15:22.591: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:15:24.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5573" for this suite.
Apr  9 09:15:30.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:15:30.677: INFO: namespace emptydir-wrapper-5573 deletion completed in 6.04443316s

• [SLOW TEST:8.086 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:15:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-05a0ba14-5aa8-11e9-b737-ea708a3858a3
STEP: Creating secret with name s-test-opt-upd-05a0ba57-5aa8-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-05a0ba14-5aa8-11e9-b737-ea708a3858a3
STEP: Updating secret s-test-opt-upd-05a0ba57-5aa8-11e9-b737-ea708a3858a3
STEP: Creating secret with name s-test-opt-create-05a0ba76-5aa8-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:16:42.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-108" for this suite.
Apr  9 09:17:04.868: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:17:04.906: INFO: namespace projected-108 deletion completed in 22.042478104s

• [SLOW TEST:94.228 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:17:04.906: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 09:17:04.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1755'
Apr  9 09:17:05.000: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr  9 09:17:05.000: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Apr  9 09:17:05.007: INFO: scanned /root for discovery docs: <nil>
Apr  9 09:17:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1755'
Apr  9 09:17:20.742: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Apr  9 09:17:20.742: INFO: stdout: "Created e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d\nScaling up e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Apr  9 09:17:20.742: INFO: stdout: "Created e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d\nScaling up e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Apr  9 09:17:20.742: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1755'
Apr  9 09:17:20.803: INFO: stderr: ""
Apr  9 09:17:20.803: INFO: stdout: "e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d-s56h9 "
Apr  9 09:17:20.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d-s56h9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Apr  9 09:17:20.862: INFO: stderr: ""
Apr  9 09:17:20.862: INFO: stdout: "true"
Apr  9 09:17:20.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d-s56h9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1755'
Apr  9 09:17:20.923: INFO: stderr: ""
Apr  9 09:17:20.923: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Apr  9 09:17:20.923: INFO: e2e-test-nginx-rc-a8072fa5f4edf45ca40a0cd4f66d805d-s56h9 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Apr  9 09:17:20.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete rc e2e-test-nginx-rc --namespace=kubectl-1755'
Apr  9 09:17:20.989: INFO: stderr: ""
Apr  9 09:17:20.989: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:17:20.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1755" for this suite.
Apr  9 09:17:26.996: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:17:27.035: INFO: namespace kubectl-1755 deletion completed in 6.044676495s

• [SLOW TEST:22.129 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:17:27.035: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Apr  9 09:17:27.051: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-393566007 proxy --unix-socket=/tmp/kubectl-proxy-unix390738430/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:17:27.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5452" for this suite.
Apr  9 09:17:33.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:17:33.144: INFO: namespace kubectl-5452 deletion completed in 6.042204014s

• [SLOW TEST:6.109 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:17:33.144: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-4ea0b8b9-5aa8-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:17:33.171: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3" in namespace "configmap-5150" to be "success or failure"
Apr  9 09:17:33.174: INFO: Pod "pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664451ms
Apr  9 09:17:35.176: INFO: Pod "pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.004769551s
Apr  9 09:17:37.178: INFO: Pod "pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006519929s
STEP: Saw pod success
Apr  9 09:17:37.178: INFO: Pod "pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:17:37.179: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:17:37.193: INFO: Waiting for pod pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:17:37.196: INFO: Pod pod-configmaps-4ea0f4e8-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:17:37.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5150" for this suite.
Apr  9 09:17:43.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:17:43.246: INFO: namespace configmap-5150 deletion completed in 6.048061426s

• [SLOW TEST:10.102 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:17:43.246: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 09:17:47.783: INFO: Successfully updated pod "annotationupdate54a4ca0b-5aa8-11e9-b737-ea708a3858a3"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:17:49.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3376" for this suite.
Apr  9 09:18:11.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:18:11.838: INFO: namespace downward-api-3376 deletion completed in 22.042273299s

• [SLOW TEST:28.592 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:18:11.838: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Apr  9 09:18:11.861: INFO: Waiting up to 5m0s for pod "client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3" in namespace "containers-5811" to be "success or failure"
Apr  9 09:18:11.862: INFO: Pod "client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.184222ms
Apr  9 09:18:13.864: INFO: Pod "client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.002900638s
STEP: Saw pod success
Apr  9 09:18:13.864: INFO: Pod "client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:18:13.865: INFO: Trying to get logs from node ip-172-31-13-147 pod client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:18:13.877: INFO: Waiting for pod client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:18:13.878: INFO: Pod client-containers-65b08781-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:18:13.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5811" for this suite.
Apr  9 09:18:19.884: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:18:19.921: INFO: namespace containers-5811 deletion completed in 6.041167475s

• [SLOW TEST:8.083 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:18:19.921: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:18:19.964: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3" in namespace "downward-api-315" to be "success or failure"
Apr  9 09:18:19.966: INFO: Pod "downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346855ms
Apr  9 09:18:21.967: INFO: Pod "downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003318885s
STEP: Saw pod success
Apr  9 09:18:21.968: INFO: Pod "downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:18:21.969: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:18:21.975: INFO: Waiting for pod downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:18:21.983: INFO: Pod downwardapi-volume-6a822634-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:18:21.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-315" for this suite.
Apr  9 09:18:27.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:18:28.031: INFO: namespace downward-api-315 deletion completed in 6.046639853s

• [SLOW TEST:8.110 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:18:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:18:28.062: INFO: (0) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 2.06231ms)
Apr  9 09:18:28.064: INFO: (1) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.898707ms)
Apr  9 09:18:28.066: INFO: (2) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.731266ms)
Apr  9 09:18:28.068: INFO: (3) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.616175ms)
Apr  9 09:18:28.069: INFO: (4) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.683926ms)
Apr  9 09:18:28.071: INFO: (5) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.806767ms)
Apr  9 09:18:28.078: INFO: (6) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.19532ms)
Apr  9 09:18:28.082: INFO: (7) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.530634ms)
Apr  9 09:18:28.084: INFO: (8) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.671287ms)
Apr  9 09:18:28.085: INFO: (9) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.659048ms)
Apr  9 09:18:28.087: INFO: (10) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.670212ms)
Apr  9 09:18:28.089: INFO: (11) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.619241ms)
Apr  9 09:18:28.090: INFO: (12) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.660278ms)
Apr  9 09:18:28.092: INFO: (13) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.661049ms)
Apr  9 09:18:28.094: INFO: (14) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.715952ms)
Apr  9 09:18:28.096: INFO: (15) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.65616ms)
Apr  9 09:18:28.097: INFO: (16) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.638852ms)
Apr  9 09:18:28.099: INFO: (17) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.714184ms)
Apr  9 09:18:28.101: INFO: (18) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 1.877411ms)
Apr  9 09:18:28.103: INFO: (19) /api/v1/nodes/ip-172-31-13-147/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 2.088499ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:18:28.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4227" for this suite.
Apr  9 09:18:34.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:18:34.150: INFO: namespace proxy-4227 deletion completed in 6.04606269s

• [SLOW TEST:6.119 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:18:34.150: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-z9jk
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 09:18:34.179: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-z9jk" in namespace "subpath-9869" to be "success or failure"
Apr  9 09:18:34.180: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486044ms
Apr  9 09:18:36.182: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 2.003273122s
Apr  9 09:18:38.184: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 4.005275598s
Apr  9 09:18:40.186: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 6.007057881s
Apr  9 09:18:42.188: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 8.008924416s
Apr  9 09:18:44.190: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 10.010897681s
Apr  9 09:18:46.192: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 12.013070199s
Apr  9 09:18:48.194: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 14.015226449s
Apr  9 09:18:50.196: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 16.017167882s
Apr  9 09:18:52.198: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 18.019188569s
Apr  9 09:18:54.200: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 20.021014279s
Apr  9 09:18:56.202: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Running", Reason="", readiness=true. Elapsed: 22.022782136s
Apr  9 09:18:58.203: INFO: Pod "pod-subpath-test-configmap-z9jk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.024601036s
STEP: Saw pod success
Apr  9 09:18:58.203: INFO: Pod "pod-subpath-test-configmap-z9jk" satisfied condition "success or failure"
Apr  9 09:18:58.205: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-subpath-test-configmap-z9jk container test-container-subpath-configmap-z9jk: <nil>
STEP: delete the pod
Apr  9 09:18:58.215: INFO: Waiting for pod pod-subpath-test-configmap-z9jk to disappear
Apr  9 09:18:58.223: INFO: Pod pod-subpath-test-configmap-z9jk no longer exists
STEP: Deleting pod pod-subpath-test-configmap-z9jk
Apr  9 09:18:58.223: INFO: Deleting pod "pod-subpath-test-configmap-z9jk" in namespace "subpath-9869"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:18:58.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9869" for this suite.
Apr  9 09:19:04.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:19:04.266: INFO: namespace subpath-9869 deletion completed in 6.041559805s

• [SLOW TEST:30.116 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:19:04.267: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-84efdb1a-5aa8-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:19:04.294: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3" in namespace "projected-2966" to be "success or failure"
Apr  9 09:19:04.296: INFO: Pod "pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.688683ms
Apr  9 09:19:06.298: INFO: Pod "pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003840122s
Apr  9 09:19:08.300: INFO: Pod "pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005525163s
STEP: Saw pod success
Apr  9 09:19:08.300: INFO: Pod "pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:19:08.301: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:19:08.308: INFO: Waiting for pod pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:19:08.309: INFO: Pod pod-projected-configmaps-84f11fbb-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:19:08.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2966" for this suite.
Apr  9 09:19:14.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:19:14.354: INFO: namespace projected-2966 deletion completed in 6.043585295s

• [SLOW TEST:10.088 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:19:14.355: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  9 09:19:14.379: INFO: Waiting up to 5m0s for pod "pod-8af32036-5aa8-11e9-b737-ea708a3858a3" in namespace "emptydir-1302" to be "success or failure"
Apr  9 09:19:14.381: INFO: Pod "pod-8af32036-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.199325ms
Apr  9 09:19:16.383: INFO: Pod "pod-8af32036-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003202083s
Apr  9 09:19:18.385: INFO: Pod "pod-8af32036-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005296358s
STEP: Saw pod success
Apr  9 09:19:18.385: INFO: Pod "pod-8af32036-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:19:18.386: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-8af32036-5aa8-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:19:18.394: INFO: Waiting for pod pod-8af32036-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:19:18.395: INFO: Pod pod-8af32036-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:19:18.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1302" for this suite.
Apr  9 09:19:24.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:19:24.440: INFO: namespace emptydir-1302 deletion completed in 6.043642372s

• [SLOW TEST:10.085 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:19:24.440: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-8267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8267 to expose endpoints map[]
Apr  9 09:19:24.465: INFO: Get endpoints failed (2.278519ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Apr  9 09:19:25.467: INFO: successfully validated that service endpoint-test2 in namespace services-8267 exposes endpoints map[] (1.004099294s elapsed)
STEP: Creating pod pod1 in namespace services-8267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8267 to expose endpoints map[pod1:[80]]
Apr  9 09:19:27.478: INFO: successfully validated that service endpoint-test2 in namespace services-8267 exposes endpoints map[pod1:[80]] (2.008386415s elapsed)
STEP: Creating pod pod2 in namespace services-8267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8267 to expose endpoints map[pod1:[80] pod2:[80]]
Apr  9 09:19:30.497: INFO: successfully validated that service endpoint-test2 in namespace services-8267 exposes endpoints map[pod1:[80] pod2:[80]] (3.016510517s elapsed)
STEP: Deleting pod pod1 in namespace services-8267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8267 to expose endpoints map[pod2:[80]]
Apr  9 09:19:30.505: INFO: successfully validated that service endpoint-test2 in namespace services-8267 exposes endpoints map[pod2:[80]] (5.527458ms elapsed)
STEP: Deleting pod pod2 in namespace services-8267
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8267 to expose endpoints map[]
Apr  9 09:19:31.515: INFO: successfully validated that service endpoint-test2 in namespace services-8267 exposes endpoints map[] (1.00804854s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:19:31.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8267" for this suite.
Apr  9 09:19:53.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:19:53.565: INFO: namespace services-8267 deletion completed in 22.041493003s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.125 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:19:53.566: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  9 09:19:59.607: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 09:19:59.610: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 09:20:01.610: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 09:20:01.612: INFO: Pod pod-with-poststart-http-hook still exists
Apr  9 09:20:03.611: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Apr  9 09:20:03.612: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:20:03.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5330" for this suite.
Apr  9 09:20:25.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:20:25.656: INFO: namespace container-lifecycle-hook-5330 deletion completed in 22.042461672s

• [SLOW TEST:32.091 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:20:25.657: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6570.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6570.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6570.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6570.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6570.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6570.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Apr  9 09:20:29.700: INFO: DNS probes using dns-6570/dns-test-b572fe60-5aa8-11e9-b737-ea708a3858a3 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:20:29.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6570" for this suite.
Apr  9 09:20:35.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:20:35.758: INFO: namespace dns-6570 deletion completed in 6.042510157s

• [SLOW TEST:10.101 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:20:35.758: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1302
I0409 09:20:35.780370      15 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1302, replica count: 1
I0409 09:20:36.830817      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0409 09:20:37.831119      15 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Apr  9 09:20:37.934: INFO: Created: latency-svc-mb7ln
Apr  9 09:20:37.936: INFO: Got endpoints: latency-svc-mb7ln [5.126109ms]
Apr  9 09:20:37.949: INFO: Created: latency-svc-d5v4k
Apr  9 09:20:37.951: INFO: Got endpoints: latency-svc-d5v4k [15.162736ms]
Apr  9 09:20:37.952: INFO: Created: latency-svc-vjq9h
Apr  9 09:20:37.953: INFO: Got endpoints: latency-svc-vjq9h [16.667537ms]
Apr  9 09:20:37.953: INFO: Created: latency-svc-dhphj
Apr  9 09:20:37.955: INFO: Got endpoints: latency-svc-dhphj [18.309566ms]
Apr  9 09:20:37.959: INFO: Created: latency-svc-7s2zt
Apr  9 09:20:37.967: INFO: Got endpoints: latency-svc-7s2zt [30.658223ms]
Apr  9 09:20:37.969: INFO: Created: latency-svc-gqq84
Apr  9 09:20:37.971: INFO: Got endpoints: latency-svc-gqq84 [34.337215ms]
Apr  9 09:20:37.971: INFO: Created: latency-svc-p6926
Apr  9 09:20:37.974: INFO: Created: latency-svc-t6782
Apr  9 09:20:37.977: INFO: Got endpoints: latency-svc-p6926 [40.625698ms]
Apr  9 09:20:37.977: INFO: Got endpoints: latency-svc-t6782 [40.734905ms]
Apr  9 09:20:37.978: INFO: Created: latency-svc-t9tm9
Apr  9 09:20:37.986: INFO: Got endpoints: latency-svc-t9tm9 [49.580809ms]
Apr  9 09:20:37.987: INFO: Created: latency-svc-bns7v
Apr  9 09:20:37.990: INFO: Created: latency-svc-jwxb8
Apr  9 09:20:37.990: INFO: Got endpoints: latency-svc-bns7v [53.53229ms]
Apr  9 09:20:37.992: INFO: Created: latency-svc-zhzzh
Apr  9 09:20:37.993: INFO: Got endpoints: latency-svc-jwxb8 [56.176448ms]
Apr  9 09:20:38.002: INFO: Got endpoints: latency-svc-zhzzh [65.283025ms]
Apr  9 09:20:38.003: INFO: Created: latency-svc-swvql
Apr  9 09:20:38.006: INFO: Created: latency-svc-fr5bw
Apr  9 09:20:38.006: INFO: Got endpoints: latency-svc-swvql [69.770875ms]
Apr  9 09:20:38.009: INFO: Got endpoints: latency-svc-fr5bw [72.981225ms]
Apr  9 09:20:38.020: INFO: Created: latency-svc-bhrx2
Apr  9 09:20:38.021: INFO: Created: latency-svc-xbl9b
Apr  9 09:20:38.022: INFO: Got endpoints: latency-svc-bhrx2 [85.634018ms]
Apr  9 09:20:38.024: INFO: Got endpoints: latency-svc-xbl9b [87.157512ms]
Apr  9 09:20:38.029: INFO: Created: latency-svc-2f54g
Apr  9 09:20:38.038: INFO: Created: latency-svc-qldxw
Apr  9 09:20:38.040: INFO: Got endpoints: latency-svc-2f54g [89.000606ms]
Apr  9 09:20:38.043: INFO: Got endpoints: latency-svc-qldxw [90.376635ms]
Apr  9 09:20:38.045: INFO: Created: latency-svc-tgk4x
Apr  9 09:20:38.047: INFO: Got endpoints: latency-svc-tgk4x [92.058572ms]
Apr  9 09:20:38.048: INFO: Created: latency-svc-pxq92
Apr  9 09:20:38.050: INFO: Got endpoints: latency-svc-pxq92 [83.270153ms]
Apr  9 09:20:38.059: INFO: Created: latency-svc-4l5fx
Apr  9 09:20:38.061: INFO: Got endpoints: latency-svc-4l5fx [90.065474ms]
Apr  9 09:20:38.062: INFO: Created: latency-svc-jhjgv
Apr  9 09:20:38.064: INFO: Created: latency-svc-fbs7m
Apr  9 09:20:38.064: INFO: Got endpoints: latency-svc-jhjgv [87.500113ms]
Apr  9 09:20:38.069: INFO: Created: latency-svc-djbj8
Apr  9 09:20:38.069: INFO: Got endpoints: latency-svc-fbs7m [91.459489ms]
Apr  9 09:20:38.076: INFO: Got endpoints: latency-svc-djbj8 [89.960004ms]
Apr  9 09:20:38.078: INFO: Created: latency-svc-sthlg
Apr  9 09:20:38.079: INFO: Created: latency-svc-jj8b7
Apr  9 09:20:38.080: INFO: Got endpoints: latency-svc-sthlg [90.161489ms]
Apr  9 09:20:38.083: INFO: Got endpoints: latency-svc-jj8b7 [90.756557ms]
Apr  9 09:20:38.084: INFO: Created: latency-svc-rhjpw
Apr  9 09:20:38.092: INFO: Got endpoints: latency-svc-rhjpw [90.491896ms]
Apr  9 09:20:38.094: INFO: Created: latency-svc-8m4dv
Apr  9 09:20:38.096: INFO: Created: latency-svc-6wwqw
Apr  9 09:20:38.097: INFO: Got endpoints: latency-svc-6wwqw [87.639306ms]
Apr  9 09:20:38.097: INFO: Got endpoints: latency-svc-8m4dv [90.898001ms]
Apr  9 09:20:38.100: INFO: Created: latency-svc-qmj8q
Apr  9 09:20:38.108: INFO: Got endpoints: latency-svc-qmj8q [86.267686ms]
Apr  9 09:20:38.110: INFO: Created: latency-svc-57254
Apr  9 09:20:38.113: INFO: Created: latency-svc-666h5
Apr  9 09:20:38.113: INFO: Got endpoints: latency-svc-57254 [89.029211ms]
Apr  9 09:20:38.114: INFO: Got endpoints: latency-svc-666h5 [73.161502ms]
Apr  9 09:20:38.116: INFO: Created: latency-svc-5z5tg
Apr  9 09:20:38.126: INFO: Created: latency-svc-cx9ls
Apr  9 09:20:38.126: INFO: Got endpoints: latency-svc-5z5tg [82.826672ms]
Apr  9 09:20:38.128: INFO: Created: latency-svc-mgpmf
Apr  9 09:20:38.131: INFO: Created: latency-svc-qtl2z
Apr  9 09:20:38.133: INFO: Created: latency-svc-m6c64
Apr  9 09:20:38.146: INFO: Got endpoints: latency-svc-cx9ls [99.568376ms]
Apr  9 09:20:38.147: INFO: Created: latency-svc-7bqq2
Apr  9 09:20:38.149: INFO: Created: latency-svc-d6szf
Apr  9 09:20:38.151: INFO: Created: latency-svc-l44hk
Apr  9 09:20:38.153: INFO: Created: latency-svc-tggf7
Apr  9 09:20:38.155: INFO: Created: latency-svc-wstzw
Apr  9 09:20:38.166: INFO: Created: latency-svc-rq6xf
Apr  9 09:20:38.168: INFO: Created: latency-svc-27qbk
Apr  9 09:20:38.170: INFO: Created: latency-svc-mvkcz
Apr  9 09:20:38.172: INFO: Created: latency-svc-hm6h8
Apr  9 09:20:38.174: INFO: Created: latency-svc-5gnfl
Apr  9 09:20:38.188: INFO: Created: latency-svc-kl8lq
Apr  9 09:20:38.188: INFO: Got endpoints: latency-svc-mgpmf [137.974825ms]
Apr  9 09:20:38.190: INFO: Created: latency-svc-zj22t
Apr  9 09:20:38.192: INFO: Created: latency-svc-lf8sd
Apr  9 09:20:38.236: INFO: Got endpoints: latency-svc-qtl2z [175.615254ms]
Apr  9 09:20:38.240: INFO: Created: latency-svc-9nlm7
Apr  9 09:20:38.287: INFO: Got endpoints: latency-svc-m6c64 [222.463565ms]
Apr  9 09:20:38.291: INFO: Created: latency-svc-4qjph
Apr  9 09:20:38.336: INFO: Got endpoints: latency-svc-7bqq2 [267.612ms]
Apr  9 09:20:38.340: INFO: Created: latency-svc-f9chj
Apr  9 09:20:38.387: INFO: Got endpoints: latency-svc-d6szf [310.555765ms]
Apr  9 09:20:38.392: INFO: Created: latency-svc-7sp9l
Apr  9 09:20:38.436: INFO: Got endpoints: latency-svc-l44hk [356.106131ms]
Apr  9 09:20:38.441: INFO: Created: latency-svc-79lgl
Apr  9 09:20:38.487: INFO: Got endpoints: latency-svc-tggf7 [403.420493ms]
Apr  9 09:20:38.490: INFO: Created: latency-svc-8j65q
Apr  9 09:20:38.537: INFO: Got endpoints: latency-svc-wstzw [444.574778ms]
Apr  9 09:20:38.542: INFO: Created: latency-svc-4ljrr
Apr  9 09:20:38.586: INFO: Got endpoints: latency-svc-rq6xf [489.041079ms]
Apr  9 09:20:38.589: INFO: Created: latency-svc-vlgbn
Apr  9 09:20:38.637: INFO: Got endpoints: latency-svc-27qbk [539.451045ms]
Apr  9 09:20:38.639: INFO: Created: latency-svc-2ckqz
Apr  9 09:20:38.686: INFO: Got endpoints: latency-svc-mvkcz [577.849415ms]
Apr  9 09:20:38.691: INFO: Created: latency-svc-9wsxp
Apr  9 09:20:38.736: INFO: Got endpoints: latency-svc-hm6h8 [623.577809ms]
Apr  9 09:20:38.741: INFO: Created: latency-svc-47lcg
Apr  9 09:20:38.786: INFO: Got endpoints: latency-svc-5gnfl [672.511403ms]
Apr  9 09:20:38.789: INFO: Created: latency-svc-mkcfj
Apr  9 09:20:38.836: INFO: Got endpoints: latency-svc-kl8lq [710.257473ms]
Apr  9 09:20:38.840: INFO: Created: latency-svc-tc4mt
Apr  9 09:20:38.886: INFO: Got endpoints: latency-svc-zj22t [739.965938ms]
Apr  9 09:20:38.891: INFO: Created: latency-svc-xxzkf
Apr  9 09:20:38.937: INFO: Got endpoints: latency-svc-lf8sd [748.385442ms]
Apr  9 09:20:38.941: INFO: Created: latency-svc-l8qhs
Apr  9 09:20:38.986: INFO: Got endpoints: latency-svc-9nlm7 [749.809974ms]
Apr  9 09:20:38.989: INFO: Created: latency-svc-9lfxx
Apr  9 09:20:39.036: INFO: Got endpoints: latency-svc-4qjph [749.174497ms]
Apr  9 09:20:39.041: INFO: Created: latency-svc-gz8fl
Apr  9 09:20:39.086: INFO: Got endpoints: latency-svc-f9chj [749.998674ms]
Apr  9 09:20:39.089: INFO: Created: latency-svc-zbnld
Apr  9 09:20:39.136: INFO: Got endpoints: latency-svc-7sp9l [749.604968ms]
Apr  9 09:20:39.139: INFO: Created: latency-svc-q8d79
Apr  9 09:20:39.186: INFO: Got endpoints: latency-svc-79lgl [749.925492ms]
Apr  9 09:20:39.189: INFO: Created: latency-svc-fvx6p
Apr  9 09:20:39.236: INFO: Got endpoints: latency-svc-8j65q [749.610343ms]
Apr  9 09:20:39.240: INFO: Created: latency-svc-75l7z
Apr  9 09:20:39.287: INFO: Got endpoints: latency-svc-4ljrr [749.893308ms]
Apr  9 09:20:39.290: INFO: Created: latency-svc-tbgpq
Apr  9 09:20:39.337: INFO: Got endpoints: latency-svc-vlgbn [750.405794ms]
Apr  9 09:20:39.349: INFO: Created: latency-svc-xm7sg
Apr  9 09:20:39.387: INFO: Got endpoints: latency-svc-2ckqz [750.226791ms]
Apr  9 09:20:39.390: INFO: Created: latency-svc-8sc64
Apr  9 09:20:39.436: INFO: Got endpoints: latency-svc-9wsxp [750.18388ms]
Apr  9 09:20:39.440: INFO: Created: latency-svc-kn9zw
Apr  9 09:20:39.487: INFO: Got endpoints: latency-svc-47lcg [750.465707ms]
Apr  9 09:20:39.498: INFO: Created: latency-svc-hkd9j
Apr  9 09:20:39.537: INFO: Got endpoints: latency-svc-mkcfj [750.506825ms]
Apr  9 09:20:39.542: INFO: Created: latency-svc-75g57
Apr  9 09:20:39.586: INFO: Got endpoints: latency-svc-tc4mt [749.998532ms]
Apr  9 09:20:39.590: INFO: Created: latency-svc-wtnxc
Apr  9 09:20:39.636: INFO: Got endpoints: latency-svc-xxzkf [749.875753ms]
Apr  9 09:20:39.648: INFO: Created: latency-svc-6pdgl
Apr  9 09:20:39.686: INFO: Got endpoints: latency-svc-l8qhs [749.568154ms]
Apr  9 09:20:39.691: INFO: Created: latency-svc-g5mnh
Apr  9 09:20:39.737: INFO: Got endpoints: latency-svc-9lfxx [750.230656ms]
Apr  9 09:20:39.742: INFO: Created: latency-svc-5xcck
Apr  9 09:20:39.787: INFO: Got endpoints: latency-svc-gz8fl [750.411838ms]
Apr  9 09:20:39.798: INFO: Created: latency-svc-5bq42
Apr  9 09:20:39.836: INFO: Got endpoints: latency-svc-zbnld [749.806708ms]
Apr  9 09:20:39.841: INFO: Created: latency-svc-pfpsz
Apr  9 09:20:39.886: INFO: Got endpoints: latency-svc-q8d79 [750.117576ms]
Apr  9 09:20:39.890: INFO: Created: latency-svc-r9w5b
Apr  9 09:20:39.936: INFO: Got endpoints: latency-svc-fvx6p [750.20003ms]
Apr  9 09:20:39.948: INFO: Created: latency-svc-8m8bx
Apr  9 09:20:39.986: INFO: Got endpoints: latency-svc-75l7z [749.961214ms]
Apr  9 09:20:39.991: INFO: Created: latency-svc-jd72c
Apr  9 09:20:40.037: INFO: Got endpoints: latency-svc-tbgpq [750.122742ms]
Apr  9 09:20:40.040: INFO: Created: latency-svc-wgk8b
Apr  9 09:20:40.086: INFO: Got endpoints: latency-svc-xm7sg [749.433073ms]
Apr  9 09:20:40.089: INFO: Created: latency-svc-nf8dx
Apr  9 09:20:40.136: INFO: Got endpoints: latency-svc-8sc64 [749.276115ms]
Apr  9 09:20:40.139: INFO: Created: latency-svc-v26z2
Apr  9 09:20:40.186: INFO: Got endpoints: latency-svc-kn9zw [749.710667ms]
Apr  9 09:20:40.190: INFO: Created: latency-svc-bcc8w
Apr  9 09:20:40.236: INFO: Got endpoints: latency-svc-hkd9j [749.607818ms]
Apr  9 09:20:40.248: INFO: Created: latency-svc-jfgbh
Apr  9 09:20:40.286: INFO: Got endpoints: latency-svc-75g57 [749.498646ms]
Apr  9 09:20:40.289: INFO: Created: latency-svc-mfpxr
Apr  9 09:20:40.336: INFO: Got endpoints: latency-svc-wtnxc [749.49098ms]
Apr  9 09:20:40.348: INFO: Created: latency-svc-2lckp
Apr  9 09:20:40.386: INFO: Got endpoints: latency-svc-6pdgl [749.914968ms]
Apr  9 09:20:40.390: INFO: Created: latency-svc-f579w
Apr  9 09:20:40.436: INFO: Got endpoints: latency-svc-g5mnh [749.620486ms]
Apr  9 09:20:40.439: INFO: Created: latency-svc-x28b8
Apr  9 09:20:40.486: INFO: Got endpoints: latency-svc-5xcck [749.910251ms]
Apr  9 09:20:40.499: INFO: Created: latency-svc-74v5b
Apr  9 09:20:40.537: INFO: Got endpoints: latency-svc-5bq42 [749.897132ms]
Apr  9 09:20:40.543: INFO: Created: latency-svc-vwv94
Apr  9 09:20:40.586: INFO: Got endpoints: latency-svc-pfpsz [750.285684ms]
Apr  9 09:20:40.590: INFO: Created: latency-svc-tvzbr
Apr  9 09:20:40.636: INFO: Got endpoints: latency-svc-r9w5b [750.056848ms]
Apr  9 09:20:40.648: INFO: Created: latency-svc-4tf9r
Apr  9 09:20:40.686: INFO: Got endpoints: latency-svc-8m8bx [750.085649ms]
Apr  9 09:20:40.689: INFO: Created: latency-svc-9tmjd
Apr  9 09:20:40.736: INFO: Got endpoints: latency-svc-jd72c [749.803126ms]
Apr  9 09:20:40.740: INFO: Created: latency-svc-57hjp
Apr  9 09:20:40.786: INFO: Got endpoints: latency-svc-wgk8b [749.467329ms]
Apr  9 09:20:40.792: INFO: Created: latency-svc-wvjn5
Apr  9 09:20:40.836: INFO: Got endpoints: latency-svc-nf8dx [749.940676ms]
Apr  9 09:20:40.839: INFO: Created: latency-svc-gs8w6
Apr  9 09:20:40.886: INFO: Got endpoints: latency-svc-v26z2 [749.87256ms]
Apr  9 09:20:40.889: INFO: Created: latency-svc-sftvg
Apr  9 09:20:40.936: INFO: Got endpoints: latency-svc-bcc8w [749.936761ms]
Apr  9 09:20:40.941: INFO: Created: latency-svc-ndnjk
Apr  9 09:20:40.986: INFO: Got endpoints: latency-svc-jfgbh [749.903812ms]
Apr  9 09:20:40.991: INFO: Created: latency-svc-qjm4b
Apr  9 09:20:41.037: INFO: Got endpoints: latency-svc-mfpxr [750.640704ms]
Apr  9 09:20:41.040: INFO: Created: latency-svc-mgbc8
Apr  9 09:20:41.086: INFO: Got endpoints: latency-svc-2lckp [750.291256ms]
Apr  9 09:20:41.092: INFO: Created: latency-svc-lmqs6
Apr  9 09:20:41.136: INFO: Got endpoints: latency-svc-f579w [750.304652ms]
Apr  9 09:20:41.141: INFO: Created: latency-svc-q44zq
Apr  9 09:20:41.186: INFO: Got endpoints: latency-svc-x28b8 [750.058103ms]
Apr  9 09:20:41.189: INFO: Created: latency-svc-ctm97
Apr  9 09:20:41.237: INFO: Got endpoints: latency-svc-74v5b [750.161476ms]
Apr  9 09:20:41.240: INFO: Created: latency-svc-bbgzs
Apr  9 09:20:41.286: INFO: Got endpoints: latency-svc-vwv94 [749.455065ms]
Apr  9 09:20:41.289: INFO: Created: latency-svc-lmwkb
Apr  9 09:20:41.336: INFO: Got endpoints: latency-svc-tvzbr [749.736935ms]
Apr  9 09:20:41.341: INFO: Created: latency-svc-gkc7z
Apr  9 09:20:41.386: INFO: Got endpoints: latency-svc-4tf9r [749.937507ms]
Apr  9 09:20:41.391: INFO: Created: latency-svc-fg87q
Apr  9 09:20:41.437: INFO: Got endpoints: latency-svc-9tmjd [750.105073ms]
Apr  9 09:20:41.440: INFO: Created: latency-svc-8469w
Apr  9 09:20:41.486: INFO: Got endpoints: latency-svc-57hjp [750.26159ms]
Apr  9 09:20:41.491: INFO: Created: latency-svc-6v2fc
Apr  9 09:20:41.536: INFO: Got endpoints: latency-svc-wvjn5 [749.921476ms]
Apr  9 09:20:41.540: INFO: Created: latency-svc-dsb4f
Apr  9 09:20:41.587: INFO: Got endpoints: latency-svc-gs8w6 [751.117662ms]
Apr  9 09:20:41.592: INFO: Created: latency-svc-stsdn
Apr  9 09:20:41.636: INFO: Got endpoints: latency-svc-sftvg [749.994084ms]
Apr  9 09:20:41.640: INFO: Created: latency-svc-2x5w2
Apr  9 09:20:41.686: INFO: Got endpoints: latency-svc-ndnjk [750.158334ms]
Apr  9 09:20:41.689: INFO: Created: latency-svc-gpg76
Apr  9 09:20:41.737: INFO: Got endpoints: latency-svc-qjm4b [750.219182ms]
Apr  9 09:20:41.740: INFO: Created: latency-svc-zdwk4
Apr  9 09:20:41.786: INFO: Got endpoints: latency-svc-mgbc8 [749.278902ms]
Apr  9 09:20:41.789: INFO: Created: latency-svc-sg77q
Apr  9 09:20:41.836: INFO: Got endpoints: latency-svc-lmqs6 [750.042006ms]
Apr  9 09:20:41.841: INFO: Created: latency-svc-h76gd
Apr  9 09:20:41.886: INFO: Got endpoints: latency-svc-q44zq [750.001853ms]
Apr  9 09:20:41.899: INFO: Created: latency-svc-shfwb
Apr  9 09:20:41.937: INFO: Got endpoints: latency-svc-ctm97 [750.687258ms]
Apr  9 09:20:41.941: INFO: Created: latency-svc-zkx9j
Apr  9 09:20:41.986: INFO: Got endpoints: latency-svc-bbgzs [749.662715ms]
Apr  9 09:20:41.990: INFO: Created: latency-svc-z4d5g
Apr  9 09:20:42.036: INFO: Got endpoints: latency-svc-lmwkb [750.242866ms]
Apr  9 09:20:42.048: INFO: Created: latency-svc-qrcnd
Apr  9 09:20:42.086: INFO: Got endpoints: latency-svc-gkc7z [749.827959ms]
Apr  9 09:20:42.089: INFO: Created: latency-svc-ztzgd
Apr  9 09:20:42.137: INFO: Got endpoints: latency-svc-fg87q [750.205966ms]
Apr  9 09:20:42.140: INFO: Created: latency-svc-5wm7v
Apr  9 09:20:42.186: INFO: Got endpoints: latency-svc-8469w [749.351697ms]
Apr  9 09:20:42.190: INFO: Created: latency-svc-9pmdz
Apr  9 09:20:42.238: INFO: Got endpoints: latency-svc-6v2fc [751.53432ms]
Apr  9 09:20:42.249: INFO: Created: latency-svc-79xz2
Apr  9 09:20:42.287: INFO: Got endpoints: latency-svc-dsb4f [750.186331ms]
Apr  9 09:20:42.290: INFO: Created: latency-svc-mqc5k
Apr  9 09:20:42.336: INFO: Got endpoints: latency-svc-stsdn [748.498534ms]
Apr  9 09:20:42.341: INFO: Created: latency-svc-5vjd6
Apr  9 09:20:42.386: INFO: Got endpoints: latency-svc-2x5w2 [750.158154ms]
Apr  9 09:20:42.389: INFO: Created: latency-svc-7kdws
Apr  9 09:20:42.436: INFO: Got endpoints: latency-svc-gpg76 [749.730889ms]
Apr  9 09:20:42.439: INFO: Created: latency-svc-hvrfg
Apr  9 09:20:42.486: INFO: Got endpoints: latency-svc-zdwk4 [749.381844ms]
Apr  9 09:20:42.491: INFO: Created: latency-svc-ln68x
Apr  9 09:20:42.536: INFO: Got endpoints: latency-svc-sg77q [750.050845ms]
Apr  9 09:20:42.539: INFO: Created: latency-svc-4kgmt
Apr  9 09:20:42.587: INFO: Got endpoints: latency-svc-h76gd [750.357042ms]
Apr  9 09:20:42.590: INFO: Created: latency-svc-rgs5q
Apr  9 09:20:42.636: INFO: Got endpoints: latency-svc-shfwb [749.916905ms]
Apr  9 09:20:42.642: INFO: Created: latency-svc-klpc7
Apr  9 09:20:42.686: INFO: Got endpoints: latency-svc-zkx9j [749.222923ms]
Apr  9 09:20:42.689: INFO: Created: latency-svc-qghmz
Apr  9 09:20:42.736: INFO: Got endpoints: latency-svc-z4d5g [749.559993ms]
Apr  9 09:20:42.740: INFO: Created: latency-svc-s7kcp
Apr  9 09:20:42.786: INFO: Got endpoints: latency-svc-qrcnd [750.032355ms]
Apr  9 09:20:42.790: INFO: Created: latency-svc-7kw4s
Apr  9 09:20:42.836: INFO: Got endpoints: latency-svc-ztzgd [749.556167ms]
Apr  9 09:20:42.840: INFO: Created: latency-svc-ddhn8
Apr  9 09:20:42.886: INFO: Got endpoints: latency-svc-5wm7v [749.331674ms]
Apr  9 09:20:42.889: INFO: Created: latency-svc-j9b9x
Apr  9 09:20:42.936: INFO: Got endpoints: latency-svc-9pmdz [750.2874ms]
Apr  9 09:20:42.940: INFO: Created: latency-svc-7755b
Apr  9 09:20:42.986: INFO: Got endpoints: latency-svc-79xz2 [748.283284ms]
Apr  9 09:20:42.989: INFO: Created: latency-svc-sr6z4
Apr  9 09:20:43.036: INFO: Got endpoints: latency-svc-mqc5k [749.541372ms]
Apr  9 09:20:43.041: INFO: Created: latency-svc-7m72w
Apr  9 09:20:43.087: INFO: Got endpoints: latency-svc-5vjd6 [750.712178ms]
Apr  9 09:20:43.091: INFO: Created: latency-svc-dwgc6
Apr  9 09:20:43.136: INFO: Got endpoints: latency-svc-7kdws [750.051673ms]
Apr  9 09:20:43.141: INFO: Created: latency-svc-tx9zz
Apr  9 09:20:43.186: INFO: Got endpoints: latency-svc-hvrfg [750.239373ms]
Apr  9 09:20:43.189: INFO: Created: latency-svc-7nzbc
Apr  9 09:20:43.237: INFO: Got endpoints: latency-svc-ln68x [750.664064ms]
Apr  9 09:20:43.240: INFO: Created: latency-svc-d42wz
Apr  9 09:20:43.286: INFO: Got endpoints: latency-svc-4kgmt [750.02072ms]
Apr  9 09:20:43.289: INFO: Created: latency-svc-gmlsr
Apr  9 09:20:43.336: INFO: Got endpoints: latency-svc-rgs5q [749.434772ms]
Apr  9 09:20:43.339: INFO: Created: latency-svc-rfx7s
Apr  9 09:20:43.386: INFO: Got endpoints: latency-svc-klpc7 [749.720935ms]
Apr  9 09:20:43.390: INFO: Created: latency-svc-hcmq7
Apr  9 09:20:43.436: INFO: Got endpoints: latency-svc-qghmz [750.382525ms]
Apr  9 09:20:43.449: INFO: Created: latency-svc-w9fvs
Apr  9 09:20:43.486: INFO: Got endpoints: latency-svc-s7kcp [750.39162ms]
Apr  9 09:20:43.492: INFO: Created: latency-svc-4gr87
Apr  9 09:20:43.537: INFO: Got endpoints: latency-svc-7kw4s [750.129625ms]
Apr  9 09:20:43.540: INFO: Created: latency-svc-4v9lm
Apr  9 09:20:43.586: INFO: Got endpoints: latency-svc-ddhn8 [750.753825ms]
Apr  9 09:20:43.600: INFO: Created: latency-svc-g774w
Apr  9 09:20:43.637: INFO: Got endpoints: latency-svc-j9b9x [750.881332ms]
Apr  9 09:20:43.640: INFO: Created: latency-svc-7s2q5
Apr  9 09:20:43.687: INFO: Got endpoints: latency-svc-7755b [750.251409ms]
Apr  9 09:20:43.690: INFO: Created: latency-svc-mw7br
Apr  9 09:20:43.737: INFO: Got endpoints: latency-svc-sr6z4 [750.472621ms]
Apr  9 09:20:43.748: INFO: Created: latency-svc-pxb64
Apr  9 09:20:43.787: INFO: Got endpoints: latency-svc-7m72w [750.294369ms]
Apr  9 09:20:43.790: INFO: Created: latency-svc-w6jcw
Apr  9 09:20:43.836: INFO: Got endpoints: latency-svc-dwgc6 [749.668684ms]
Apr  9 09:20:43.839: INFO: Created: latency-svc-n4f28
Apr  9 09:20:43.886: INFO: Got endpoints: latency-svc-tx9zz [749.89946ms]
Apr  9 09:20:43.898: INFO: Created: latency-svc-tv5rg
Apr  9 09:20:43.936: INFO: Got endpoints: latency-svc-7nzbc [749.90334ms]
Apr  9 09:20:43.939: INFO: Created: latency-svc-sk5gd
Apr  9 09:20:43.986: INFO: Got endpoints: latency-svc-d42wz [749.683436ms]
Apr  9 09:20:43.990: INFO: Created: latency-svc-gcqzc
Apr  9 09:20:44.036: INFO: Got endpoints: latency-svc-gmlsr [750.126601ms]
Apr  9 09:20:44.049: INFO: Created: latency-svc-j55c2
Apr  9 09:20:44.086: INFO: Got endpoints: latency-svc-rfx7s [749.865644ms]
Apr  9 09:20:44.090: INFO: Created: latency-svc-hls52
Apr  9 09:20:44.136: INFO: Got endpoints: latency-svc-hcmq7 [750.067102ms]
Apr  9 09:20:44.140: INFO: Created: latency-svc-s95tz
Apr  9 09:20:44.187: INFO: Got endpoints: latency-svc-w9fvs [750.841152ms]
Apr  9 09:20:44.190: INFO: Created: latency-svc-6m9nt
Apr  9 09:20:44.236: INFO: Got endpoints: latency-svc-4gr87 [749.937828ms]
Apr  9 09:20:44.245: INFO: Created: latency-svc-kljr6
Apr  9 09:20:44.286: INFO: Got endpoints: latency-svc-4v9lm [749.378411ms]
Apr  9 09:20:44.292: INFO: Created: latency-svc-5qzg6
Apr  9 09:20:44.336: INFO: Got endpoints: latency-svc-g774w [749.305983ms]
Apr  9 09:20:44.340: INFO: Created: latency-svc-94pd2
Apr  9 09:20:44.386: INFO: Got endpoints: latency-svc-7s2q5 [749.488558ms]
Apr  9 09:20:44.392: INFO: Created: latency-svc-rck6p
Apr  9 09:20:44.436: INFO: Got endpoints: latency-svc-mw7br [749.505171ms]
Apr  9 09:20:44.442: INFO: Created: latency-svc-7c7nc
Apr  9 09:20:44.486: INFO: Got endpoints: latency-svc-pxb64 [749.534024ms]
Apr  9 09:20:44.497: INFO: Created: latency-svc-4p226
Apr  9 09:20:44.537: INFO: Got endpoints: latency-svc-w6jcw [750.249129ms]
Apr  9 09:20:44.540: INFO: Created: latency-svc-9nwrh
Apr  9 09:20:44.587: INFO: Got endpoints: latency-svc-n4f28 [750.937505ms]
Apr  9 09:20:44.590: INFO: Created: latency-svc-tqhtz
Apr  9 09:20:44.637: INFO: Got endpoints: latency-svc-tv5rg [750.34889ms]
Apr  9 09:20:44.640: INFO: Created: latency-svc-bjcm9
Apr  9 09:20:44.686: INFO: Got endpoints: latency-svc-sk5gd [750.129376ms]
Apr  9 09:20:44.690: INFO: Created: latency-svc-nmdpq
Apr  9 09:20:44.736: INFO: Got endpoints: latency-svc-gcqzc [749.960584ms]
Apr  9 09:20:44.740: INFO: Created: latency-svc-9g6md
Apr  9 09:20:44.786: INFO: Got endpoints: latency-svc-j55c2 [749.972442ms]
Apr  9 09:20:44.790: INFO: Created: latency-svc-nvvjx
Apr  9 09:20:44.836: INFO: Got endpoints: latency-svc-hls52 [750.018084ms]
Apr  9 09:20:44.839: INFO: Created: latency-svc-4n8tp
Apr  9 09:20:44.886: INFO: Got endpoints: latency-svc-s95tz [750.133365ms]
Apr  9 09:20:44.890: INFO: Created: latency-svc-c2zff
Apr  9 09:20:44.937: INFO: Got endpoints: latency-svc-6m9nt [749.356051ms]
Apr  9 09:20:44.948: INFO: Created: latency-svc-hnjtx
Apr  9 09:20:44.987: INFO: Got endpoints: latency-svc-kljr6 [750.238273ms]
Apr  9 09:20:44.991: INFO: Created: latency-svc-vwzcb
Apr  9 09:20:45.037: INFO: Got endpoints: latency-svc-5qzg6 [750.637728ms]
Apr  9 09:20:45.040: INFO: Created: latency-svc-sxvmw
Apr  9 09:20:45.087: INFO: Got endpoints: latency-svc-94pd2 [750.695848ms]
Apr  9 09:20:45.090: INFO: Created: latency-svc-d2wv8
Apr  9 09:20:45.137: INFO: Got endpoints: latency-svc-rck6p [750.07505ms]
Apr  9 09:20:45.141: INFO: Created: latency-svc-c5jmx
Apr  9 09:20:45.187: INFO: Got endpoints: latency-svc-7c7nc [750.598704ms]
Apr  9 09:20:45.190: INFO: Created: latency-svc-zrljx
Apr  9 09:20:45.237: INFO: Got endpoints: latency-svc-4p226 [750.204237ms]
Apr  9 09:20:45.248: INFO: Created: latency-svc-r69nt
Apr  9 09:20:45.287: INFO: Got endpoints: latency-svc-9nwrh [749.932773ms]
Apr  9 09:20:45.290: INFO: Created: latency-svc-pnncv
Apr  9 09:20:45.336: INFO: Got endpoints: latency-svc-tqhtz [749.056587ms]
Apr  9 09:20:45.340: INFO: Created: latency-svc-s2rv9
Apr  9 09:20:45.386: INFO: Got endpoints: latency-svc-bjcm9 [749.430399ms]
Apr  9 09:20:45.398: INFO: Created: latency-svc-hgz4c
Apr  9 09:20:45.436: INFO: Got endpoints: latency-svc-nmdpq [749.788644ms]
Apr  9 09:20:45.439: INFO: Created: latency-svc-f928r
Apr  9 09:20:45.486: INFO: Got endpoints: latency-svc-9g6md [749.659389ms]
Apr  9 09:20:45.490: INFO: Created: latency-svc-w765t
Apr  9 09:20:45.536: INFO: Got endpoints: latency-svc-nvvjx [749.847306ms]
Apr  9 09:20:45.549: INFO: Created: latency-svc-dtks8
Apr  9 09:20:45.586: INFO: Got endpoints: latency-svc-4n8tp [750.262436ms]
Apr  9 09:20:45.591: INFO: Created: latency-svc-jjz5n
Apr  9 09:20:45.636: INFO: Got endpoints: latency-svc-c2zff [749.690918ms]
Apr  9 09:20:45.644: INFO: Created: latency-svc-9gddl
Apr  9 09:20:45.686: INFO: Got endpoints: latency-svc-hnjtx [749.662062ms]
Apr  9 09:20:45.699: INFO: Created: latency-svc-kcj5j
Apr  9 09:20:45.737: INFO: Got endpoints: latency-svc-vwzcb [749.762958ms]
Apr  9 09:20:45.742: INFO: Created: latency-svc-zq72t
Apr  9 09:20:45.787: INFO: Got endpoints: latency-svc-sxvmw [750.291997ms]
Apr  9 09:20:45.836: INFO: Got endpoints: latency-svc-d2wv8 [749.484505ms]
Apr  9 09:20:45.886: INFO: Got endpoints: latency-svc-c5jmx [749.910412ms]
Apr  9 09:20:45.937: INFO: Got endpoints: latency-svc-zrljx [749.779116ms]
Apr  9 09:20:45.986: INFO: Got endpoints: latency-svc-r69nt [749.490691ms]
Apr  9 09:20:46.037: INFO: Got endpoints: latency-svc-pnncv [749.808205ms]
Apr  9 09:20:46.086: INFO: Got endpoints: latency-svc-s2rv9 [749.951244ms]
Apr  9 09:20:46.136: INFO: Got endpoints: latency-svc-hgz4c [750.219906ms]
Apr  9 09:20:46.186: INFO: Got endpoints: latency-svc-f928r [749.836038ms]
Apr  9 09:20:46.239: INFO: Got endpoints: latency-svc-w765t [752.382074ms]
Apr  9 09:20:46.286: INFO: Got endpoints: latency-svc-dtks8 [749.741452ms]
Apr  9 09:20:46.336: INFO: Got endpoints: latency-svc-jjz5n [749.613823ms]
Apr  9 09:20:46.386: INFO: Got endpoints: latency-svc-9gddl [749.531066ms]
Apr  9 09:20:46.436: INFO: Got endpoints: latency-svc-kcj5j [749.48011ms]
Apr  9 09:20:46.486: INFO: Got endpoints: latency-svc-zq72t [749.325249ms]
Apr  9 09:20:46.486: INFO: Latencies: [15.162736ms 16.667537ms 18.309566ms 30.658223ms 34.337215ms 40.625698ms 40.734905ms 49.580809ms 53.53229ms 56.176448ms 65.283025ms 69.770875ms 72.981225ms 73.161502ms 82.826672ms 83.270153ms 85.634018ms 86.267686ms 87.157512ms 87.500113ms 87.639306ms 89.000606ms 89.029211ms 89.960004ms 90.065474ms 90.161489ms 90.376635ms 90.491896ms 90.756557ms 90.898001ms 91.459489ms 92.058572ms 99.568376ms 137.974825ms 175.615254ms 222.463565ms 267.612ms 310.555765ms 356.106131ms 403.420493ms 444.574778ms 489.041079ms 539.451045ms 577.849415ms 623.577809ms 672.511403ms 710.257473ms 739.965938ms 748.283284ms 748.385442ms 748.498534ms 749.056587ms 749.174497ms 749.222923ms 749.276115ms 749.278902ms 749.305983ms 749.325249ms 749.331674ms 749.351697ms 749.356051ms 749.378411ms 749.381844ms 749.430399ms 749.433073ms 749.434772ms 749.455065ms 749.467329ms 749.48011ms 749.484505ms 749.488558ms 749.490691ms 749.49098ms 749.498646ms 749.505171ms 749.531066ms 749.534024ms 749.541372ms 749.556167ms 749.559993ms 749.568154ms 749.604968ms 749.607818ms 749.610343ms 749.613823ms 749.620486ms 749.659389ms 749.662062ms 749.662715ms 749.668684ms 749.683436ms 749.690918ms 749.710667ms 749.720935ms 749.730889ms 749.736935ms 749.741452ms 749.762958ms 749.779116ms 749.788644ms 749.803126ms 749.806708ms 749.808205ms 749.809974ms 749.827959ms 749.836038ms 749.847306ms 749.865644ms 749.87256ms 749.875753ms 749.893308ms 749.897132ms 749.89946ms 749.90334ms 749.903812ms 749.910251ms 749.910412ms 749.914968ms 749.916905ms 749.921476ms 749.925492ms 749.932773ms 749.936761ms 749.937507ms 749.937828ms 749.940676ms 749.951244ms 749.960584ms 749.961214ms 749.972442ms 749.994084ms 749.998532ms 749.998674ms 750.001853ms 750.018084ms 750.02072ms 750.032355ms 750.042006ms 750.050845ms 750.051673ms 750.056848ms 750.058103ms 750.067102ms 750.07505ms 750.085649ms 750.105073ms 750.117576ms 750.122742ms 750.126601ms 750.129376ms 750.129625ms 750.133365ms 750.158154ms 750.158334ms 750.161476ms 750.18388ms 750.186331ms 750.20003ms 750.204237ms 750.205966ms 750.219182ms 750.219906ms 750.226791ms 750.230656ms 750.238273ms 750.239373ms 750.242866ms 750.249129ms 750.251409ms 750.26159ms 750.262436ms 750.285684ms 750.2874ms 750.291256ms 750.291997ms 750.294369ms 750.304652ms 750.34889ms 750.357042ms 750.382525ms 750.39162ms 750.405794ms 750.411838ms 750.465707ms 750.472621ms 750.506825ms 750.598704ms 750.637728ms 750.640704ms 750.664064ms 750.687258ms 750.695848ms 750.712178ms 750.753825ms 750.841152ms 750.881332ms 750.937505ms 751.117662ms 751.53432ms 752.382074ms]
Apr  9 09:20:46.486: INFO: 50 %ile: 749.803126ms
Apr  9 09:20:46.486: INFO: 90 %ile: 750.39162ms
Apr  9 09:20:46.486: INFO: 99 %ile: 751.53432ms
Apr  9 09:20:46.486: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:20:46.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1302" for this suite.
Apr  9 09:20:56.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:20:56.535: INFO: namespace svc-latency-1302 deletion completed in 10.047966741s

• [SLOW TEST:20.777 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:20:56.536: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Apr  9 09:21:00.587: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:00.588: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:02.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:02.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:04.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:04.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:06.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:06.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:08.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:08.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:10.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:10.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:12.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:12.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:14.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:14.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:16.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:16.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:18.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:18.590: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:20.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:20.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:22.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:22.597: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:24.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:24.590: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:26.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:26.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:28.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:28.591: INFO: Pod pod-with-poststart-exec-hook still exists
Apr  9 09:21:30.589: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Apr  9 09:21:30.590: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:21:30.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3676" for this suite.
Apr  9 09:21:52.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:21:52.634: INFO: namespace container-lifecycle-hook-3676 deletion completed in 22.042539608s

• [SLOW TEST:56.099 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:21:52.635: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-e94ac9ba-5aa8-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:21:52.662: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3" in namespace "projected-875" to be "success or failure"
Apr  9 09:21:52.665: INFO: Pod "pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799529ms
Apr  9 09:21:54.667: INFO: Pod "pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004847868s
STEP: Saw pod success
Apr  9 09:21:54.667: INFO: Pod "pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:21:54.669: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:21:54.675: INFO: Waiting for pod pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:21:54.676: INFO: Pod pod-projected-configmaps-e94c2660-5aa8-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:21:54.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-875" for this suite.
Apr  9 09:22:00.682: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:22:00.720: INFO: namespace projected-875 deletion completed in 6.042423264s

• [SLOW TEST:8.085 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:22:00.720: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-t6hd
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 09:22:00.749: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-t6hd" in namespace "subpath-1217" to be "success or failure"
Apr  9 09:22:00.751: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Pending", Reason="", readiness=false. Elapsed: 1.990863ms
Apr  9 09:22:02.753: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 2.003918003s
Apr  9 09:22:04.756: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 4.006104378s
Apr  9 09:22:06.758: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 6.008191115s
Apr  9 09:22:08.760: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 8.010376023s
Apr  9 09:22:10.762: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 10.012481255s
Apr  9 09:22:12.764: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 12.014189208s
Apr  9 09:22:14.766: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 14.016189608s
Apr  9 09:22:16.768: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 16.018232715s
Apr  9 09:22:18.770: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 18.020265041s
Apr  9 09:22:20.772: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Running", Reason="", readiness=true. Elapsed: 20.022376118s
Apr  9 09:22:22.774: INFO: Pod "pod-subpath-test-downwardapi-t6hd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.02450846s
STEP: Saw pod success
Apr  9 09:22:22.774: INFO: Pod "pod-subpath-test-downwardapi-t6hd" satisfied condition "success or failure"
Apr  9 09:22:22.775: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-subpath-test-downwardapi-t6hd container test-container-subpath-downwardapi-t6hd: <nil>
STEP: delete the pod
Apr  9 09:22:22.782: INFO: Waiting for pod pod-subpath-test-downwardapi-t6hd to disappear
Apr  9 09:22:22.783: INFO: Pod pod-subpath-test-downwardapi-t6hd no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-t6hd
Apr  9 09:22:22.783: INFO: Deleting pod "pod-subpath-test-downwardapi-t6hd" in namespace "subpath-1217"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:22:22.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1217" for this suite.
Apr  9 09:22:28.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:22:28.836: INFO: namespace subpath-1217 deletion completed in 6.047123537s

• [SLOW TEST:28.116 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:22:28.836: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Apr  9 09:22:31.865: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:22:31.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1159" for this suite.
Apr  9 09:22:53.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:22:53.937: INFO: namespace replicaset-1159 deletion completed in 22.055690859s

• [SLOW TEST:25.101 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:22:53.937: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0409 09:23:33.971189      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 09:23:33.971: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:23:33.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1003" for this suite.
Apr  9 09:23:39.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:23:40.020: INFO: namespace gc-1003 deletion completed in 6.048342784s

• [SLOW TEST:46.084 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:23:40.021: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:23:40.060: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Apr  9 09:23:40.063: INFO: Number of nodes with available pods: 0
Apr  9 09:23:40.063: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Apr  9 09:23:40.073: INFO: Number of nodes with available pods: 0
Apr  9 09:23:40.073: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:41.075: INFO: Number of nodes with available pods: 0
Apr  9 09:23:41.075: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:42.075: INFO: Number of nodes with available pods: 1
Apr  9 09:23:42.075: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Apr  9 09:23:42.090: INFO: Number of nodes with available pods: 1
Apr  9 09:23:42.090: INFO: Number of running nodes: 0, number of available pods: 1
Apr  9 09:23:43.092: INFO: Number of nodes with available pods: 0
Apr  9 09:23:43.092: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Apr  9 09:23:43.099: INFO: Number of nodes with available pods: 0
Apr  9 09:23:43.099: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:44.101: INFO: Number of nodes with available pods: 0
Apr  9 09:23:44.101: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:45.101: INFO: Number of nodes with available pods: 0
Apr  9 09:23:45.101: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:46.101: INFO: Number of nodes with available pods: 0
Apr  9 09:23:46.101: INFO: Node ip-172-31-13-147 is running more than one daemon pod
Apr  9 09:23:47.101: INFO: Number of nodes with available pods: 1
Apr  9 09:23:47.101: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2965, will wait for the garbage collector to delete the pods
Apr  9 09:23:47.157: INFO: Deleting DaemonSet.extensions daemon-set took: 2.454084ms
Apr  9 09:23:47.458: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.257412ms
Apr  9 09:23:59.259: INFO: Number of nodes with available pods: 0
Apr  9 09:23:59.259: INFO: Number of running nodes: 0, number of available pods: 0
Apr  9 09:23:59.260: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2965/daemonsets","resourceVersion":"13365"},"items":null}

Apr  9 09:23:59.261: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2965/pods","resourceVersion":"13365"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:23:59.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2965" for this suite.
Apr  9 09:24:05.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:24:05.328: INFO: namespace daemonsets-2965 deletion completed in 6.058238366s

• [SLOW TEST:25.307 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:24:05.328: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-3864b86c-5aa9-11e9-b737-ea708a3858a3
STEP: Creating configMap with name cm-test-opt-upd-3864b8a9-5aa9-11e9-b737-ea708a3858a3
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-3864b86c-5aa9-11e9-b737-ea708a3858a3
STEP: Updating configmap cm-test-opt-upd-3864b8a9-5aa9-11e9-b737-ea708a3858a3
STEP: Creating configMap with name cm-test-opt-create-3864b8ea-5aa9-11e9-b737-ea708a3858a3
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:25:11.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7337" for this suite.
Apr  9 09:25:41.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:25:41.558: INFO: namespace projected-7337 deletion completed in 30.051427498s

• [SLOW TEST:96.230 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:25:41.558: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:25:41.584: INFO: Waiting up to 5m0s for pod "downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3" in namespace "downward-api-6078" to be "success or failure"
Apr  9 09:25:41.586: INFO: Pod "downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.887014ms
Apr  9 09:25:43.588: INFO: Pod "downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004202619s
STEP: Saw pod success
Apr  9 09:25:43.588: INFO: Pod "downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:25:43.589: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:25:43.599: INFO: Waiting for pod downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:25:43.606: INFO: Pod downwardapi-volume-71bdc1a3-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:25:43.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6078" for this suite.
Apr  9 09:25:49.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:25:49.652: INFO: namespace downward-api-6078 deletion completed in 6.043865868s

• [SLOW TEST:8.094 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:25:49.652: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Apr  9 09:25:52.183: INFO: Successfully updated pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3"
Apr  9 09:25:52.183: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3" in namespace "pods-3779" to be "terminated due to deadline exceeded"
Apr  9 09:25:52.184: INFO: Pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 1.181724ms
Apr  9 09:25:54.186: INFO: Pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003256783s
Apr  9 09:25:56.189: INFO: Pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005345535s
Apr  9 09:25:56.189: INFO: Pod "pod-update-activedeadlineseconds-7690af54-5aa9-11e9-b737-ea708a3858a3" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:25:56.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3779" for this suite.
Apr  9 09:26:02.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:02.231: INFO: namespace pods-3779 deletion completed in 6.041105525s

• [SLOW TEST:12.579 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:02.231: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:26:02.256: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3" in namespace "projected-8334" to be "success or failure"
Apr  9 09:26:02.257: INFO: Pod "downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.358205ms
Apr  9 09:26:04.259: INFO: Pod "downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003487209s
STEP: Saw pod success
Apr  9 09:26:04.259: INFO: Pod "downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:26:04.261: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:26:04.267: INFO: Waiting for pod downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:26:04.268: INFO: Pod downwardapi-volume-7e102925-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:04.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8334" for this suite.
Apr  9 09:26:10.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:10.312: INFO: namespace projected-8334 deletion completed in 6.042665477s

• [SLOW TEST:8.081 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:10.313: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-82e14772-5aa9-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:26:10.339: INFO: Waiting up to 5m0s for pod "pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3" in namespace "secrets-3264" to be "success or failure"
Apr  9 09:26:10.340: INFO: Pod "pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.150978ms
Apr  9 09:26:12.342: INFO: Pod "pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.002970779s
STEP: Saw pod success
Apr  9 09:26:12.342: INFO: Pod "pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:26:12.343: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:26:12.350: INFO: Waiting for pod pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:26:12.351: INFO: Pod pod-secrets-82e28a30-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:12.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3264" for this suite.
Apr  9 09:26:18.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:18.395: INFO: namespace secrets-3264 deletion completed in 6.04259411s

• [SLOW TEST:8.083 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:18.395: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-87b3fd23-5aa9-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:26:18.424: INFO: Waiting up to 5m0s for pod "pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3" in namespace "configmap-6188" to be "success or failure"
Apr  9 09:26:18.427: INFO: Pod "pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.742352ms
Apr  9 09:26:20.429: INFO: Pod "pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004780106s
STEP: Saw pod success
Apr  9 09:26:20.429: INFO: Pod "pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:26:20.431: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:26:20.441: INFO: Waiting for pod pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:26:20.449: INFO: Pod pod-configmaps-87b43f65-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:20.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6188" for this suite.
Apr  9 09:26:26.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:26.496: INFO: namespace configmap-6188 deletion completed in 6.045349311s

• [SLOW TEST:8.101 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:26.496: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:26:26.521: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3" in namespace "downward-api-5291" to be "success or failure"
Apr  9 09:26:26.522: INFO: Pod "downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.081117ms
Apr  9 09:26:28.524: INFO: Pod "downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003162761s
STEP: Saw pod success
Apr  9 09:26:28.524: INFO: Pod "downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:26:28.526: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:26:28.535: INFO: Waiting for pod downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:26:28.543: INFO: Pod downwardapi-volume-8c86ac7b-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:28.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5291" for this suite.
Apr  9 09:26:34.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:34.589: INFO: namespace downward-api-5291 deletion completed in 6.045047474s

• [SLOW TEST:8.093 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:34.590: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:26:34.615: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3" in namespace "downward-api-9077" to be "success or failure"
Apr  9 09:26:34.621: INFO: Pod "downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.37031ms
Apr  9 09:26:36.623: INFO: Pod "downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007639564s
STEP: Saw pod success
Apr  9 09:26:36.623: INFO: Pod "downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:26:36.624: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:26:36.632: INFO: Waiting for pod downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:26:36.633: INFO: Pod downwardapi-volume-9159c90e-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:36.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9077" for this suite.
Apr  9 09:26:42.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:26:42.685: INFO: namespace downward-api-9077 deletion completed in 6.050318584s

• [SLOW TEST:8.095 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:26:42.685: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Apr  9 09:26:42.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-1643'
Apr  9 09:26:43.636: INFO: stderr: ""
Apr  9 09:26:43.636: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Apr  9 09:26:44.642: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:26:44.642: INFO: Found 0 / 1
Apr  9 09:26:45.638: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:26:45.638: INFO: Found 1 / 1
Apr  9 09:26:45.638: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Apr  9 09:26:45.640: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:26:45.640: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Apr  9 09:26:45.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 patch pod redis-master-9pn6m --namespace=kubectl-1643 -p {"metadata":{"annotations":{"x":"y"}}}'
Apr  9 09:26:45.702: INFO: stderr: ""
Apr  9 09:26:45.702: INFO: stdout: "pod/redis-master-9pn6m patched\n"
STEP: checking annotations
Apr  9 09:26:45.704: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:26:45.704: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:26:45.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1643" for this suite.
Apr  9 09:27:07.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:27:07.754: INFO: namespace kubectl-1643 deletion completed in 22.049170422s

• [SLOW TEST:25.069 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:27:07.754: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0409 09:27:17.786600      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 09:27:17.786: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:27:17.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1475" for this suite.
Apr  9 09:27:23.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:27:23.831: INFO: namespace gc-1475 deletion completed in 6.043777468s

• [SLOW TEST:16.077 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:27:23.831: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Apr  9 09:27:23.856: INFO: Waiting up to 5m0s for pod "pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3" in namespace "emptydir-1355" to be "success or failure"
Apr  9 09:27:23.857: INFO: Pod "pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.297631ms
Apr  9 09:27:25.859: INFO: Pod "pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003438278s
STEP: Saw pod success
Apr  9 09:27:25.859: INFO: Pod "pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:27:25.860: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:27:25.869: INFO: Waiting for pod pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:27:25.877: INFO: Pod pod-aeb34e9b-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:27:25.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1355" for this suite.
Apr  9 09:27:31.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:27:31.919: INFO: namespace emptydir-1355 deletion completed in 6.041148977s

• [SLOW TEST:8.088 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:27:31.920: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Apr  9 09:27:31.944: INFO: Waiting up to 5m0s for pod "var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3" in namespace "var-expansion-2875" to be "success or failure"
Apr  9 09:27:31.947: INFO: Pod "var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.394755ms
Apr  9 09:27:33.949: INFO: Pod "var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00442336s
STEP: Saw pod success
Apr  9 09:27:33.949: INFO: Pod "var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:27:33.950: INFO: Trying to get logs from node ip-172-31-13-147 pod var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 09:27:33.959: INFO: Waiting for pod var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:27:33.967: INFO: Pod var-expansion-b38578ac-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:27:33.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2875" for this suite.
Apr  9 09:27:39.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:27:40.011: INFO: namespace var-expansion-2875 deletion completed in 6.042875743s

• [SLOW TEST:8.091 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:27:40.011: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Apr  9 09:27:40.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 --namespace=kubectl-8698 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Apr  9 09:27:41.628: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Apr  9 09:27:41.628: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:27:43.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8698" for this suite.
Apr  9 09:27:49.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:27:49.675: INFO: namespace kubectl-8698 deletion completed in 6.043324913s

• [SLOW TEST:9.664 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:27:49.675: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:27:51.711: INFO: Waiting up to 5m0s for pod "client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3" in namespace "pods-1365" to be "success or failure"
Apr  9 09:27:51.714: INFO: Pod "client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.453489ms
Apr  9 09:27:53.716: INFO: Pod "client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004467306s
STEP: Saw pod success
Apr  9 09:27:53.716: INFO: Pod "client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:27:53.717: INFO: Trying to get logs from node ip-172-31-13-147 pod client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3 container env3cont: <nil>
STEP: delete the pod
Apr  9 09:27:53.724: INFO: Waiting for pod client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:27:53.725: INFO: Pod client-envvars-bf4eabc9-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:27:53.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1365" for this suite.
Apr  9 09:28:43.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:28:43.769: INFO: namespace pods-1365 deletion completed in 50.042829843s

• [SLOW TEST:54.094 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:28:43.769: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-de58e1d8-5aa9-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:28:43.796: INFO: Waiting up to 5m0s for pod "pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3" in namespace "configmap-8204" to be "success or failure"
Apr  9 09:28:43.798: INFO: Pod "pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.115742ms
Apr  9 09:28:45.800: INFO: Pod "pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00318912s
STEP: Saw pod success
Apr  9 09:28:45.800: INFO: Pod "pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:28:45.801: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:28:45.808: INFO: Waiting for pod pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:28:45.809: INFO: Pod pod-configmaps-de5a4983-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:28:45.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8204" for this suite.
Apr  9 09:28:51.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:28:51.854: INFO: namespace configmap-8204 deletion completed in 6.044562432s

• [SLOW TEST:8.085 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:28:51.855: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0409 09:28:52.445154      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 09:28:52.445: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:28:52.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6199" for this suite.
Apr  9 09:28:58.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:28:58.489: INFO: namespace gc-6199 deletion completed in 6.042978671s

• [SLOW TEST:6.634 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:28:58.489: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Apr  9 09:28:58.515: INFO: Waiting up to 5m0s for pod "downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3" in namespace "downward-api-1666" to be "success or failure"
Apr  9 09:28:58.516: INFO: Pod "downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.094039ms
Apr  9 09:29:00.518: INFO: Pod "downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003099025s
STEP: Saw pod success
Apr  9 09:29:00.518: INFO: Pod "downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:00.519: INFO: Trying to get logs from node ip-172-31-13-147 pod downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3 container dapi-container: <nil>
STEP: delete the pod
Apr  9 09:29:00.526: INFO: Waiting for pod downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:00.528: INFO: Pod downward-api-e71f0fca-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:00.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1666" for this suite.
Apr  9 09:29:06.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:06.571: INFO: namespace downward-api-1666 deletion completed in 6.041561748s

• [SLOW TEST:8.082 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:06.571: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:29:06.596: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3" in namespace "downward-api-4709" to be "success or failure"
Apr  9 09:29:06.597: INFO: Pod "downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.157411ms
Apr  9 09:29:08.599: INFO: Pod "downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003315576s
Apr  9 09:29:10.601: INFO: Pod "downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005294944s
STEP: Saw pod success
Apr  9 09:29:10.601: INFO: Pod "downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:10.602: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:29:10.609: INFO: Waiting for pod downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:10.610: INFO: Pod downwardapi-volume-ebf02f50-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:10.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4709" for this suite.
Apr  9 09:29:16.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:16.653: INFO: namespace downward-api-4709 deletion completed in 6.042191747s

• [SLOW TEST:10.082 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:16.654: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-f1f4027a-5aa9-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:29:16.689: INFO: Waiting up to 5m0s for pod "pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3" in namespace "configmap-6945" to be "success or failure"
Apr  9 09:29:16.691: INFO: Pod "pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.869314ms
Apr  9 09:29:18.693: INFO: Pod "pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003882337s
Apr  9 09:29:20.695: INFO: Pod "pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005994693s
STEP: Saw pod success
Apr  9 09:29:20.695: INFO: Pod "pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:20.696: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:29:20.703: INFO: Waiting for pod pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:20.704: INFO: Pod pod-configmaps-f1f53a2a-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:20.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6945" for this suite.
Apr  9 09:29:26.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:26.748: INFO: namespace configmap-6945 deletion completed in 6.042318601s

• [SLOW TEST:10.094 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:26.748: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-f7f6fd0f-5aa9-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:29:26.775: INFO: Waiting up to 5m0s for pod "pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3" in namespace "secrets-5914" to be "success or failure"
Apr  9 09:29:26.778: INFO: Pod "pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.37181ms
Apr  9 09:29:28.780: INFO: Pod "pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00437415s
Apr  9 09:29:30.782: INFO: Pod "pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00652116s
STEP: Saw pod success
Apr  9 09:29:30.782: INFO: Pod "pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:30.783: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:29:30.793: INFO: Waiting for pod pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:30.800: INFO: Pod pod-secrets-f7f845b5-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:30.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5914" for this suite.
Apr  9 09:29:36.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:36.844: INFO: namespace secrets-5914 deletion completed in 6.042164608s

• [SLOW TEST:10.096 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:36.844: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Apr  9 09:29:36.869: INFO: Waiting up to 5m0s for pod "pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3" in namespace "emptydir-7732" to be "success or failure"
Apr  9 09:29:36.870: INFO: Pod "pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.194767ms
Apr  9 09:29:38.872: INFO: Pod "pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003211474s
Apr  9 09:29:40.874: INFO: Pod "pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005355666s
STEP: Saw pod success
Apr  9 09:29:40.874: INFO: Pod "pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:40.875: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:29:40.882: INFO: Waiting for pod pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:40.883: INFO: Pod pod-fdfb6f3b-5aa9-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:40.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7732" for this suite.
Apr  9 09:29:46.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:46.930: INFO: namespace emptydir-7732 deletion completed in 6.045930563s

• [SLOW TEST:10.086 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:46.930: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  9 09:29:46.957: INFO: Waiting up to 5m0s for pod "pod-03fedb20-5aaa-11e9-b737-ea708a3858a3" in namespace "emptydir-4007" to be "success or failure"
Apr  9 09:29:46.959: INFO: Pod "pod-03fedb20-5aaa-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.747712ms
Apr  9 09:29:48.961: INFO: Pod "pod-03fedb20-5aaa-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.003587394s
Apr  9 09:29:50.963: INFO: Pod "pod-03fedb20-5aaa-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005695277s
STEP: Saw pod success
Apr  9 09:29:50.963: INFO: Pod "pod-03fedb20-5aaa-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:29:50.964: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-03fedb20-5aaa-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:29:50.971: INFO: Waiting for pod pod-03fedb20-5aaa-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:29:50.974: INFO: Pod pod-03fedb20-5aaa-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:29:50.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4007" for this suite.
Apr  9 09:29:56.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:29:57.022: INFO: namespace emptydir-4007 deletion completed in 6.047347503s

• [SLOW TEST:10.092 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:29:57.023: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-0a0408b4-5aaa-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:29:57.053: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3" in namespace "projected-4158" to be "success or failure"
Apr  9 09:29:57.056: INFO: Pod "pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017222ms
Apr  9 09:29:59.058: INFO: Pod "pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3": Phase="Running", Reason="", readiness=true. Elapsed: 2.004858689s
Apr  9 09:30:01.059: INFO: Pod "pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006834155s
STEP: Saw pod success
Apr  9 09:30:01.060: INFO: Pod "pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:30:01.061: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:30:01.070: INFO: Waiting for pod pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:30:01.078: INFO: Pod pod-projected-configmaps-0a043fed-5aaa-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:30:01.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4158" for this suite.
Apr  9 09:30:07.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:30:07.121: INFO: namespace projected-4158 deletion completed in 6.042156125s

• [SLOW TEST:10.099 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:30:07.121: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-210
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Apr  9 09:30:07.151: INFO: Found 0 stateful pods, waiting for 3
Apr  9 09:30:17.153: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 09:30:17.153: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 09:30:17.153: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 09:30:17.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-210 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:30:17.345: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:30:17.345: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:30:17.345: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Apr  9 09:30:27.365: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Apr  9 09:30:37.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-210 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 09:30:37.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 09:30:37.555: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 09:30:37.555: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 09:30:47.565: INFO: Waiting for StatefulSet statefulset-210/ss2 to complete update
Apr  9 09:30:47.565: INFO: Waiting for Pod statefulset-210/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 09:30:47.565: INFO: Waiting for Pod statefulset-210/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 09:30:47.565: INFO: Waiting for Pod statefulset-210/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 09:30:57.568: INFO: Waiting for StatefulSet statefulset-210/ss2 to complete update
Apr  9 09:30:57.568: INFO: Waiting for Pod statefulset-210/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 09:30:57.568: INFO: Waiting for Pod statefulset-210/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Apr  9 09:31:07.568: INFO: Waiting for StatefulSet statefulset-210/ss2 to complete update
Apr  9 09:31:07.568: INFO: Waiting for Pod statefulset-210/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Apr  9 09:31:17.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-210 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:31:17.736: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:31:17.736: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:31:17.736: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 09:31:27.756: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Apr  9 09:31:37.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-210 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 09:31:37.943: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 09:31:37.943: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 09:31:37.943: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 09:31:47.952: INFO: Waiting for StatefulSet statefulset-210/ss2 to complete update
Apr  9 09:31:47.952: INFO: Waiting for Pod statefulset-210/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 09:31:47.952: INFO: Waiting for Pod statefulset-210/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 09:31:47.952: INFO: Waiting for Pod statefulset-210/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Apr  9 09:31:57.956: INFO: Waiting for StatefulSet statefulset-210/ss2 to complete update
Apr  9 09:31:57.956: INFO: Waiting for Pod statefulset-210/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 09:32:07.956: INFO: Deleting all statefulset in ns statefulset-210
Apr  9 09:32:07.957: INFO: Scaling statefulset ss2 to 0
Apr  9 09:32:17.965: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 09:32:17.967: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:32:17.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-210" for this suite.
Apr  9 09:32:23.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:32:24.020: INFO: namespace statefulset-210 deletion completed in 6.046744185s

• [SLOW TEST:136.899 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:32:24.021: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:32:24.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9487" for this suite.
Apr  9 09:32:46.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:32:46.114: INFO: namespace pods-9487 deletion completed in 22.046141961s

• [SLOW TEST:22.094 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:32:46.114: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:33:10.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8215" for this suite.
Apr  9 09:33:16.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:33:16.246: INFO: namespace namespaces-8215 deletion completed in 6.052534658s
STEP: Destroying namespace "nsdeletetest-4576" for this suite.
Apr  9 09:33:16.247: INFO: Namespace nsdeletetest-4576 was already deleted
STEP: Destroying namespace "nsdeletetest-5608" for this suite.
Apr  9 09:33:22.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:33:22.290: INFO: namespace nsdeletetest-5608 deletion completed in 6.042638815s

• [SLOW TEST:36.175 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:33:22.290: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Apr  9 09:33:22.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-6518'
Apr  9 09:33:22.460: INFO: stderr: ""
Apr  9 09:33:22.460: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Apr  9 09:33:23.462: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:33:23.462: INFO: Found 0 / 1
Apr  9 09:33:24.462: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:33:24.462: INFO: Found 1 / 1
Apr  9 09:33:24.462: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Apr  9 09:33:24.463: INFO: Selector matched 1 pods for map[app:redis]
Apr  9 09:33:24.464: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Apr  9 09:33:24.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 logs redis-master-clpt4 redis-master --namespace=kubectl-6518'
Apr  9 09:33:24.538: INFO: stderr: ""
Apr  9 09:33:24.538: INFO: stdout: "1:M 09 Apr 09:33:23.695 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 09 Apr 09:33:23.695 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 09 Apr 09:33:23.695 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 09:33:23.695 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 09:33:23.695 # Server started, Redis version 3.2.12\n1:M 09 Apr 09:33:23.695 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 09:33:23.695 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Apr  9 09:33:24.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 log redis-master-clpt4 redis-master --namespace=kubectl-6518 --tail=1'
Apr  9 09:33:24.606: INFO: stderr: ""
Apr  9 09:33:24.606: INFO: stdout: "1:M 09 Apr 09:33:23.695 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Apr  9 09:33:24.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 log redis-master-clpt4 redis-master --namespace=kubectl-6518 --limit-bytes=1'
Apr  9 09:33:24.672: INFO: stderr: ""
Apr  9 09:33:24.672: INFO: stdout: "1"
STEP: exposing timestamps
Apr  9 09:33:24.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 log redis-master-clpt4 redis-master --namespace=kubectl-6518 --tail=1 --timestamps'
Apr  9 09:33:24.738: INFO: stderr: ""
Apr  9 09:33:24.738: INFO: stdout: "2019-04-09T09:33:23.695924674Z 1:M 09 Apr 09:33:23.695 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Apr  9 09:33:27.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 log redis-master-clpt4 redis-master --namespace=kubectl-6518 --since=1s'
Apr  9 09:33:27.323: INFO: stderr: ""
Apr  9 09:33:27.323: INFO: stdout: ""
Apr  9 09:33:27.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 log redis-master-clpt4 redis-master --namespace=kubectl-6518 --since=24h'
Apr  9 09:33:27.397: INFO: stderr: ""
Apr  9 09:33:27.397: INFO: stdout: "1:M 09 Apr 09:33:23.695 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 09 Apr 09:33:23.695 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 09 Apr 09:33:23.695 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 09 Apr 09:33:23.695 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 09 Apr 09:33:23.695 # Server started, Redis version 3.2.12\n1:M 09 Apr 09:33:23.695 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 09 Apr 09:33:23.695 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Apr  9 09:33:27.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-6518'
Apr  9 09:33:27.460: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 09:33:27.460: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Apr  9 09:33:27.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6518'
Apr  9 09:33:27.522: INFO: stderr: "No resources found.\n"
Apr  9 09:33:27.522: INFO: stdout: ""
Apr  9 09:33:27.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=nginx --namespace=kubectl-6518 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 09:33:27.583: INFO: stderr: ""
Apr  9 09:33:27.583: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:33:27.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6518" for this suite.
Apr  9 09:33:33.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:33:33.633: INFO: namespace kubectl-6518 deletion completed in 6.0480076s

• [SLOW TEST:11.343 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:33:33.633: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Apr  9 09:33:36.165: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4630 pod-service-account-8b6c1d74-5aaa-11e9-b737-ea708a3858a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Apr  9 09:33:36.330: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4630 pod-service-account-8b6c1d74-5aaa-11e9-b737-ea708a3858a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Apr  9 09:33:36.494: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4630 pod-service-account-8b6c1d74-5aaa-11e9-b737-ea708a3858a3 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:33:36.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4630" for this suite.
Apr  9 09:33:42.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:33:42.710: INFO: namespace svcaccounts-4630 deletion completed in 6.044845522s

• [SLOW TEST:9.077 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:33:42.710: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Apr  9 09:33:42.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6228,SelfLink:/api/v1/namespaces/watch-6228/configmaps/e2e-watch-test-watch-closed,UID:9087fac5-5aaa-11e9-bcbd-0af1119c727a,ResourceVersion:15268,Generation:0,CreationTimestamp:2019-04-09 09:33:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 09:33:42.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6228,SelfLink:/api/v1/namespaces/watch-6228/configmaps/e2e-watch-test-watch-closed,UID:9087fac5-5aaa-11e9-bcbd-0af1119c727a,ResourceVersion:15269,Generation:0,CreationTimestamp:2019-04-09 09:33:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Apr  9 09:33:42.743: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6228,SelfLink:/api/v1/namespaces/watch-6228/configmaps/e2e-watch-test-watch-closed,UID:9087fac5-5aaa-11e9-bcbd-0af1119c727a,ResourceVersion:15270,Generation:0,CreationTimestamp:2019-04-09 09:33:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 09:33:42.743: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6228,SelfLink:/api/v1/namespaces/watch-6228/configmaps/e2e-watch-test-watch-closed,UID:9087fac5-5aaa-11e9-bcbd-0af1119c727a,ResourceVersion:15271,Generation:0,CreationTimestamp:2019-04-09 09:33:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:33:42.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6228" for this suite.
Apr  9 09:33:48.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:33:48.794: INFO: namespace watch-6228 deletion completed in 6.04832589s

• [SLOW TEST:6.084 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:33:48.794: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Apr  9 09:33:51.335: INFO: Successfully updated pod "labelsupdate942960c4-5aaa-11e9-b737-ea708a3858a3"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:33:55.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1910" for this suite.
Apr  9 09:34:17.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:34:17.391: INFO: namespace projected-1910 deletion completed in 22.044322056s

• [SLOW TEST:28.597 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:34:17.391: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-361
Apr  9 09:34:19.420: INFO: Started pod liveness-exec in namespace container-probe-361
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 09:34:19.422: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:38:19.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-361" for this suite.
Apr  9 09:38:25.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:38:25.724: INFO: namespace container-probe-361 deletion completed in 6.043515425s

• [SLOW TEST:248.333 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:38:25.725: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Apr  9 09:38:25.742: INFO: Waiting up to 5m0s for pod "pod-39381503-5aab-11e9-b737-ea708a3858a3" in namespace "emptydir-3775" to be "success or failure"
Apr  9 09:38:25.745: INFO: Pod "pod-39381503-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297512ms
Apr  9 09:38:27.747: INFO: Pod "pod-39381503-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005281333s
STEP: Saw pod success
Apr  9 09:38:27.747: INFO: Pod "pod-39381503-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:38:27.749: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-39381503-5aab-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:38:27.755: INFO: Waiting for pod pod-39381503-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:38:27.756: INFO: Pod pod-39381503-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:38:27.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3775" for this suite.
Apr  9 09:38:33.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:38:33.800: INFO: namespace emptydir-3775 deletion completed in 6.042758183s

• [SLOW TEST:8.075 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:38:33.800: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  9 09:38:33.827: INFO: Waiting up to 5m0s for pod "pod-3e09c516-5aab-11e9-b737-ea708a3858a3" in namespace "emptydir-1460" to be "success or failure"
Apr  9 09:38:33.830: INFO: Pod "pod-3e09c516-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.872764ms
Apr  9 09:38:35.831: INFO: Pod "pod-3e09c516-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004552925s
STEP: Saw pod success
Apr  9 09:38:35.831: INFO: Pod "pod-3e09c516-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:38:35.833: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-3e09c516-5aab-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:38:35.839: INFO: Waiting for pod pod-3e09c516-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:38:35.840: INFO: Pod pod-3e09c516-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:38:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1460" for this suite.
Apr  9 09:38:41.846: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:38:41.885: INFO: namespace emptydir-1460 deletion completed in 6.043302272s

• [SLOW TEST:8.085 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:38:41.885: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:38:41.909: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3" in namespace "downward-api-4752" to be "success or failure"
Apr  9 09:38:41.913: INFO: Pod "downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125572ms
Apr  9 09:38:43.915: INFO: Pod "downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005928659s
STEP: Saw pod success
Apr  9 09:38:43.915: INFO: Pod "downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:38:43.916: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:38:43.925: INFO: Waiting for pod downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:38:43.927: INFO: Pod downwardapi-volume-42dae3cd-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:38:43.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4752" for this suite.
Apr  9 09:38:49.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:38:49.976: INFO: namespace downward-api-4752 deletion completed in 6.047563401s

• [SLOW TEST:8.091 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:38:49.976: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5270
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-5270
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5270
Apr  9 09:38:50.003: INFO: Found 0 stateful pods, waiting for 1
Apr  9 09:39:00.005: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Apr  9 09:39:00.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:39:00.173: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:39:00.173: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:39:00.173: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 09:39:00.175: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Apr  9 09:39:10.177: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 09:39:10.177: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 09:39:10.184: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Apr  9 09:39:10.184: INFO: ss-0  ip-172-31-13-147  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  }]
Apr  9 09:39:10.184: INFO: 
Apr  9 09:39:10.184: INFO: StatefulSet ss has not reached scale 3, at 1
Apr  9 09:39:11.187: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.99826941s
Apr  9 09:39:12.188: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995666161s
Apr  9 09:39:13.191: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.993816911s
Apr  9 09:39:14.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.991532012s
Apr  9 09:39:15.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.989566435s
Apr  9 09:39:16.197: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.987241233s
Apr  9 09:39:17.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.985286231s
Apr  9 09:39:18.201: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.983040731s
Apr  9 09:39:19.203: INFO: Verifying statefulset ss doesn't scale past 3 for another 981.120339ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5270
Apr  9 09:39:20.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 09:39:20.391: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Apr  9 09:39:20.391: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 09:39:20.391: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 09:39:20.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 09:39:20.554: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  9 09:39:20.554: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 09:39:20.554: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 09:39:20.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Apr  9 09:39:20.721: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Apr  9 09:39:20.721: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Apr  9 09:39:20.721: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Apr  9 09:39:20.724: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Apr  9 09:39:30.726: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 09:39:30.726: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Apr  9 09:39:30.726: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Apr  9 09:39:30.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:39:30.897: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:39:30.897: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:39:30.897: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 09:39:30.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:39:31.067: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:39:31.067: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:39:31.067: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 09:39:31.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 exec --namespace=statefulset-5270 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Apr  9 09:39:31.247: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Apr  9 09:39:31.247: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Apr  9 09:39:31.247: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Apr  9 09:39:31.247: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 09:39:31.248: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Apr  9 09:39:41.252: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 09:39:41.252: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 09:39:41.252: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Apr  9 09:39:41.257: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Apr  9 09:39:41.257: INFO: ss-0  ip-172-31-13-147  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  }]
Apr  9 09:39:41.257: INFO: ss-1  ip-172-31-13-147  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:41.257: INFO: ss-2  ip-172-31-13-147  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:41.257: INFO: 
Apr  9 09:39:41.257: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 09:39:42.259: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Apr  9 09:39:42.259: INFO: ss-0  ip-172-31-13-147  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  }]
Apr  9 09:39:42.259: INFO: ss-1  ip-172-31-13-147  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:42.259: INFO: ss-2  ip-172-31-13-147  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:42.259: INFO: 
Apr  9 09:39:42.259: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 09:39:43.261: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Apr  9 09:39:43.261: INFO: ss-0  ip-172-31-13-147  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:38:50 +0000 UTC  }]
Apr  9 09:39:43.261: INFO: ss-1  ip-172-31-13-147  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:43.261: INFO: ss-2  ip-172-31-13-147  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:31 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:39:10 +0000 UTC  }]
Apr  9 09:39:43.261: INFO: 
Apr  9 09:39:43.261: INFO: StatefulSet ss has not reached scale 0, at 3
Apr  9 09:39:44.263: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.9934026s
Apr  9 09:39:45.265: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.991732474s
Apr  9 09:39:46.267: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.989830228s
Apr  9 09:39:47.269: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.988138443s
Apr  9 09:39:48.270: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.986248524s
Apr  9 09:39:49.272: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.984543058s
Apr  9 09:39:50.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 982.618932ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5270
Apr  9 09:39:51.276: INFO: Scaling statefulset ss to 0
Apr  9 09:39:51.280: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Apr  9 09:39:51.282: INFO: Deleting all statefulset in ns statefulset-5270
Apr  9 09:39:51.283: INFO: Scaling statefulset ss to 0
Apr  9 09:39:51.287: INFO: Waiting for statefulset status.replicas updated to 0
Apr  9 09:39:51.288: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:39:51.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5270" for this suite.
Apr  9 09:39:57.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:39:57.341: INFO: namespace statefulset-5270 deletion completed in 6.045492612s

• [SLOW TEST:67.365 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:39:57.341: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0409 09:40:27.871812      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Apr  9 09:40:27.871: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:40:27.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1753" for this suite.
Apr  9 09:40:33.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:40:33.917: INFO: namespace gc-1753 deletion completed in 6.04352838s

• [SLOW TEST:36.576 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:40:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-667
Apr  9 09:40:35.945: INFO: Started pod liveness-exec in namespace container-probe-667
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 09:40:35.947: INFO: Initial restart count of pod liveness-exec is 0
Apr  9 09:41:25.998: INFO: Restart count of pod container-probe-667/liveness-exec is now 1 (50.051389683s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:41:26.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-667" for this suite.
Apr  9 09:41:32.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:41:32.057: INFO: namespace container-probe-667 deletion completed in 6.050719052s

• [SLOW TEST:58.140 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:41:32.057: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 09:41:32.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8968'
Apr  9 09:41:32.887: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr  9 09:41:32.887: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Apr  9 09:41:34.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8968'
Apr  9 09:41:34.973: INFO: stderr: ""
Apr  9 09:41:34.973: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:41:34.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8968" for this suite.
Apr  9 09:41:56.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:41:57.017: INFO: namespace kubectl-8968 deletion completed in 22.04171822s

• [SLOW TEST:24.960 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:41:57.017: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:41:57.040: INFO: Pod name rollover-pod: Found 0 pods out of 1
Apr  9 09:42:02.042: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Apr  9 09:42:02.042: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Apr  9 09:42:04.044: INFO: Creating deployment "test-rollover-deployment"
Apr  9 09:42:04.048: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Apr  9 09:42:06.056: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Apr  9 09:42:06.058: INFO: Ensure that both replica sets have 1 created replica
Apr  9 09:42:06.061: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Apr  9 09:42:06.064: INFO: Updating deployment test-rollover-deployment
Apr  9 09:42:06.064: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Apr  9 09:42:08.067: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Apr  9 09:42:08.070: INFO: Make sure deployment "test-rollover-deployment" is complete
Apr  9 09:42:08.073: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 09:42:08.073: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399728, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 09:42:10.076: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 09:42:10.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399728, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 09:42:12.076: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 09:42:12.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399728, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 09:42:14.076: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 09:42:14.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399728, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 09:42:16.076: INFO: all replica sets need to contain the pod-template-hash label
Apr  9 09:42:16.076: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399728, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63690399724, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Apr  9 09:42:18.076: INFO: 
Apr  9 09:42:18.076: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Apr  9 09:42:18.080: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-155,SelfLink:/apis/apps/v1/namespaces/deployment-155/deployments/test-rollover-deployment,UID:bb56ecad-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16409,Generation:2,CreationTimestamp:2019-04-09 09:42:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-04-09 09:42:04 +0000 UTC 2019-04-09 09:42:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-04-09 09:42:18 +0000 UTC 2019-04-09 09:42:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Apr  9 09:42:18.082: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-155,SelfLink:/apis/apps/v1/namespaces/deployment-155/replicasets/test-rollover-deployment-766b4d6c9d,UID:bc8b1eb8-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16398,Generation:2,CreationTimestamp:2019-04-09 09:42:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bb56ecad-5aab-11e9-bcbd-0af1119c727a 0xc002774e07 0xc002774e08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Apr  9 09:42:18.082: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Apr  9 09:42:18.082: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-155,SelfLink:/apis/apps/v1/namespaces/deployment-155/replicasets/test-rollover-controller,UID:b729ac2b-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16407,Generation:2,CreationTimestamp:2019-04-09 09:41:57 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bb56ecad-5aab-11e9-bcbd-0af1119c727a 0xc002774c57 0xc002774c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 09:42:18.082: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-155,SelfLink:/apis/apps/v1/namespaces/deployment-155/replicasets/test-rollover-deployment-6455657675,UID:bb57a66d-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16371,Generation:2,CreationTimestamp:2019-04-09 09:42:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment bb56ecad-5aab-11e9-bcbd-0af1119c727a 0xc002774d27 0xc002774d28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Apr  9 09:42:18.084: INFO: Pod "test-rollover-deployment-766b4d6c9d-r2l57" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-r2l57,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-155,SelfLink:/api/v1/namespaces/deployment-155/pods/test-rollover-deployment-766b4d6c9d-r2l57,UID:bc8dfcd6-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16383,Generation:0,CreationTimestamp:2019-04-09 09:42:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d bc8b1eb8-5aab-11e9-bcbd-0af1119c727a 0xc0027759a7 0xc0027759a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9ptdh {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9ptdh,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9ptdh true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002775a20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002775a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:42:06 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:42:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:42:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:42:06 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.200,StartTime:2019-04-09 09:42:06 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-04-09 09:42:07 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://d9363cebfd1f6688a10e2990b13a8b2b9f1d461e4bbbd8bf8e0a41daf6a933a0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:42:18.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-155" for this suite.
Apr  9 09:42:24.091: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:42:24.127: INFO: namespace deployment-155 deletion completed in 6.041617911s

• [SLOW TEST:27.110 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:42:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:42:24.153: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3" in namespace "downward-api-5656" to be "success or failure"
Apr  9 09:42:24.155: INFO: Pod "downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.116691ms
Apr  9 09:42:26.157: INFO: Pod "downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004270981s
STEP: Saw pod success
Apr  9 09:42:26.157: INFO: Pod "downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:42:26.159: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:42:26.168: INFO: Waiting for pod downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:42:26.177: INFO: Pod downwardapi-volume-c751b102-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:42:26.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5656" for this suite.
Apr  9 09:42:32.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:42:32.224: INFO: namespace downward-api-5656 deletion completed in 6.044937646s

• [SLOW TEST:8.096 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:42:32.224: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Apr  9 09:42:32.251: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2649" to be "success or failure"
Apr  9 09:42:32.254: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.122986ms
Apr  9 09:42:34.256: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004992411s
STEP: Saw pod success
Apr  9 09:42:34.256: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Apr  9 09:42:34.257: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Apr  9 09:42:34.267: INFO: Waiting for pod pod-host-path-test to disappear
Apr  9 09:42:34.275: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:42:34.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2649" for this suite.
Apr  9 09:42:40.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:42:40.320: INFO: namespace hostpath-2649 deletion completed in 6.043019717s

• [SLOW TEST:8.096 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:42:40.320: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:42:40.372: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3" in namespace "downward-api-2275" to be "success or failure"
Apr  9 09:42:40.377: INFO: Pod "downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.108801ms
Apr  9 09:42:42.379: INFO: Pod "downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007140973s
STEP: Saw pod success
Apr  9 09:42:42.379: INFO: Pod "downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:42:42.380: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:42:42.388: INFO: Waiting for pod downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:42:42.389: INFO: Pod downwardapi-volume-d0fd5970-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:42:42.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2275" for this suite.
Apr  9 09:42:48.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:42:48.433: INFO: namespace downward-api-2275 deletion completed in 6.043333597s

• [SLOW TEST:8.113 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:42:48.433: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d5ce6921-5aab-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:42:48.482: INFO: Waiting up to 5m0s for pod "pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3" in namespace "secrets-9117" to be "success or failure"
Apr  9 09:42:48.483: INFO: Pod "pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.149597ms
Apr  9 09:42:50.485: INFO: Pod "pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003172261s
STEP: Saw pod success
Apr  9 09:42:50.485: INFO: Pod "pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:42:50.487: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:42:50.497: INFO: Waiting for pod pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:42:50.505: INFO: Pod pod-secrets-d5d31a80-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:42:50.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9117" for this suite.
Apr  9 09:42:56.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:42:56.552: INFO: namespace secrets-9117 deletion completed in 6.044526921s
STEP: Destroying namespace "secret-namespace-4616" for this suite.
Apr  9 09:43:02.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:43:02.595: INFO: namespace secret-namespace-4616 deletion completed in 6.043141404s

• [SLOW TEST:14.162 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:43:02.596: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Apr  9 09:43:04.627: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-de3f804b-5aab-11e9-b737-ea708a3858a3,GenerateName:,Namespace:events-6792,SelfLink:/api/v1/namespaces/events-6792/pods/send-events-de3f804b-5aab-11e9-b737-ea708a3858a3,UID:de40b7c9-5aab-11e9-bcbd-0af1119c727a,ResourceVersion:16630,Generation:0,CreationTimestamp:2019-04-09 09:43:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 612184826,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k5d57 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k5d57,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-k5d57 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-172-31-13-147,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00280db80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00280dba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:43:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:43:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:43:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-04-09 09:43:02 +0000 UTC  }],Message:,Reason:,HostIP:172.31.13.147,PodIP:10.1.1.205,StartTime:2019-04-09 09:43:02 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-04-09 09:43:03 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://c85a230ea498c1c356cfdfde7153ded231a42708d3b6fac0fe74a8fae6001981}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Apr  9 09:43:06.630: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Apr  9 09:43:08.631: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:43:08.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6792" for this suite.
Apr  9 09:43:52.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:43:52.685: INFO: namespace events-6792 deletion completed in 44.04517314s

• [SLOW TEST:50.089 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:43:52.685: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:43:52.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 version'
Apr  9 09:43:52.782: INFO: stderr: ""
Apr  9 09:43:52.782: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:43:52.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2202" for this suite.
Apr  9 09:43:58.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:43:58.827: INFO: namespace kubectl-2202 deletion completed in 6.042579565s

• [SLOW TEST:6.141 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:43:58.827: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-ffc439b6-5aab-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:43:58.851: INFO: Waiting up to 5m0s for pod "pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3" in namespace "secrets-2118" to be "success or failure"
Apr  9 09:43:58.854: INFO: Pod "pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.524308ms
Apr  9 09:44:00.856: INFO: Pod "pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00458866s
STEP: Saw pod success
Apr  9 09:44:00.856: INFO: Pod "pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:44:00.857: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:44:00.866: INFO: Waiting for pod pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:44:00.874: INFO: Pod pod-secrets-ffc47c1f-5aab-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:44:00.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2118" for this suite.
Apr  9 09:44:06.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:44:06.920: INFO: namespace secrets-2118 deletion completed in 6.044444924s

• [SLOW TEST:8.093 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:44:06.920: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Apr  9 09:44:06.938: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3" in namespace "projected-612" to be "success or failure"
Apr  9 09:44:06.941: INFO: Pod "downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.7626ms
Apr  9 09:44:08.944: INFO: Pod "downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005880157s
STEP: Saw pod success
Apr  9 09:44:08.944: INFO: Pod "downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:44:08.945: INFO: Trying to get logs from node ip-172-31-13-147 pod downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3 container client-container: <nil>
STEP: delete the pod
Apr  9 09:44:08.954: INFO: Waiting for pod downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:44:08.962: INFO: Pod downwardapi-volume-0496616b-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:44:08.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-612" for this suite.
Apr  9 09:44:14.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:44:15.009: INFO: namespace projected-612 deletion completed in 6.04566102s

• [SLOW TEST:8.089 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:44:15.009: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:44:17.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7124" for this suite.
Apr  9 09:44:55.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:44:55.088: INFO: namespace kubelet-test-7124 deletion completed in 38.043830252s

• [SLOW TEST:40.079 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:44:55.088: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-47
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Apr  9 09:44:55.104: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Apr  9 09:45:11.147: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.1.1.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-47 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Apr  9 09:45:11.147: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
Apr  9 09:45:12.248: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:45:12.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-47" for this suite.
Apr  9 09:45:26.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:45:26.296: INFO: namespace pod-network-test-47 deletion completed in 14.045908033s

• [SLOW TEST:31.208 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:45:26.296: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Apr  9 09:45:26.323: INFO: Waiting up to 5m0s for pod "pod-33e79e18-5aac-11e9-b737-ea708a3858a3" in namespace "emptydir-2557" to be "success or failure"
Apr  9 09:45:26.325: INFO: Pod "pod-33e79e18-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.176093ms
Apr  9 09:45:28.327: INFO: Pod "pod-33e79e18-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00417181s
STEP: Saw pod success
Apr  9 09:45:28.327: INFO: Pod "pod-33e79e18-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:45:28.329: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-33e79e18-5aac-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:45:28.335: INFO: Waiting for pod pod-33e79e18-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:45:28.337: INFO: Pod pod-33e79e18-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:45:28.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2557" for this suite.
Apr  9 09:45:34.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:45:34.381: INFO: namespace emptydir-2557 deletion completed in 6.042766149s

• [SLOW TEST:8.084 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:45:34.381: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-38b816a0-5aac-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume configMaps
Apr  9 09:45:34.408: INFO: Waiting up to 5m0s for pod "pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3" in namespace "configmap-4430" to be "success or failure"
Apr  9 09:45:34.413: INFO: Pod "pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332984ms
Apr  9 09:45:36.415: INFO: Pod "pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006434525s
STEP: Saw pod success
Apr  9 09:45:36.415: INFO: Pod "pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:45:36.416: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3 container configmap-volume-test: <nil>
STEP: delete the pod
Apr  9 09:45:36.423: INFO: Waiting for pod pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:45:36.425: INFO: Pod pod-configmaps-38b956b2-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:45:36.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4430" for this suite.
Apr  9 09:45:42.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:45:42.470: INFO: namespace configmap-4430 deletion completed in 6.042945159s

• [SLOW TEST:8.089 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:45:42.470: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Apr  9 09:45:42.489: INFO: Creating ReplicaSet my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3
Apr  9 09:45:42.493: INFO: Pod name my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3: Found 0 pods out of 1
Apr  9 09:45:47.495: INFO: Pod name my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3: Found 1 pods out of 1
Apr  9 09:45:47.495: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3" is running
Apr  9 09:45:47.496: INFO: Pod "my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3-jwp4q" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:45:42 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:45:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:45:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-04-09 09:45:42 +0000 UTC Reason: Message:}])
Apr  9 09:45:47.496: INFO: Trying to dial the pod
Apr  9 09:45:52.502: INFO: Controller my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3: Got expected result from replica 1 [my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3-jwp4q]: "my-hostname-basic-3d8ad6cd-5aac-11e9-b737-ea708a3858a3-jwp4q", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:45:52.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2962" for this suite.
Apr  9 09:45:58.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:45:58.545: INFO: namespace replicaset-2962 deletion completed in 6.042167335s

• [SLOW TEST:16.076 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:45:58.545: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-606
Apr  9 09:46:00.574: INFO: Started pod liveness-http in namespace container-probe-606
STEP: checking the pod's current state and verifying that restartCount is present
Apr  9 09:46:00.575: INFO: Initial restart count of pod liveness-http is 0
Apr  9 09:46:20.597: INFO: Restart count of pod container-probe-606/liveness-http is now 1 (20.021158355s elapsed)
Apr  9 09:46:40.616: INFO: Restart count of pod container-probe-606/liveness-http is now 2 (40.040196734s elapsed)
Apr  9 09:47:00.635: INFO: Restart count of pod container-probe-606/liveness-http is now 3 (1m0.059440545s elapsed)
Apr  9 09:47:20.655: INFO: Restart count of pod container-probe-606/liveness-http is now 4 (1m20.07930291s elapsed)
Apr  9 09:48:20.715: INFO: Restart count of pod container-probe-606/liveness-http is now 5 (2m20.139419552s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:48:20.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-606" for this suite.
Apr  9 09:48:26.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:48:26.772: INFO: namespace container-probe-606 deletion completed in 6.041811654s

• [SLOW TEST:148.227 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:48:26.772: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Apr  9 09:48:26.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-2215'
Apr  9 09:48:26.855: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Apr  9 09:48:26.855: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Apr  9 09:48:26.867: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-dk8bb]
Apr  9 09:48:26.867: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-dk8bb" in namespace "kubectl-2215" to be "running and ready"
Apr  9 09:48:26.868: INFO: Pod "e2e-test-nginx-rc-dk8bb": Phase="Pending", Reason="", readiness=false. Elapsed: 1.127288ms
Apr  9 09:48:28.870: INFO: Pod "e2e-test-nginx-rc-dk8bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.003071869s
Apr  9 09:48:30.871: INFO: Pod "e2e-test-nginx-rc-dk8bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.004755425s
Apr  9 09:48:30.871: INFO: Pod "e2e-test-nginx-rc-dk8bb" satisfied condition "running and ready"
Apr  9 09:48:30.872: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-dk8bb]
Apr  9 09:48:30.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 logs rc/e2e-test-nginx-rc --namespace=kubectl-2215'
Apr  9 09:48:30.953: INFO: stderr: ""
Apr  9 09:48:30.953: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Apr  9 09:48:30.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete rc e2e-test-nginx-rc --namespace=kubectl-2215'
Apr  9 09:48:31.015: INFO: stderr: ""
Apr  9 09:48:31.015: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:48:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2215" for this suite.
Apr  9 09:48:53.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:48:53.061: INFO: namespace kubectl-2215 deletion completed in 22.043251014s

• [SLOW TEST:26.289 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:48:53.061: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Apr  9 09:48:53.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 create -f - --namespace=kubectl-9645'
Apr  9 09:48:53.221: INFO: stderr: ""
Apr  9 09:48:53.221: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 09:48:53.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:48:53.291: INFO: stderr: ""
Apr  9 09:48:53.291: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-phjpq "
Apr  9 09:48:53.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:48:53.350: INFO: stderr: ""
Apr  9 09:48:53.351: INFO: stdout: ""
Apr  9 09:48:53.351: INFO: update-demo-nautilus-kbxm5 is created but not running
Apr  9 09:48:58.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:48:58.421: INFO: stderr: ""
Apr  9 09:48:58.421: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-phjpq "
Apr  9 09:48:58.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:48:58.498: INFO: stderr: ""
Apr  9 09:48:58.498: INFO: stdout: "true"
Apr  9 09:48:58.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:48:58.558: INFO: stderr: ""
Apr  9 09:48:58.558: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:48:58.558: INFO: validating pod update-demo-nautilus-kbxm5
Apr  9 09:48:58.561: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:48:58.561: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:48:58.561: INFO: update-demo-nautilus-kbxm5 is verified up and running
Apr  9 09:48:58.561: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-phjpq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:48:58.620: INFO: stderr: ""
Apr  9 09:48:58.620: INFO: stdout: "true"
Apr  9 09:48:58.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-phjpq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:48:58.679: INFO: stderr: ""
Apr  9 09:48:58.679: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:48:58.679: INFO: validating pod update-demo-nautilus-phjpq
Apr  9 09:48:58.681: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:48:58.681: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:48:58.681: INFO: update-demo-nautilus-phjpq is verified up and running
STEP: scaling down the replication controller
Apr  9 09:48:58.682: INFO: scanned /root for discovery docs: <nil>
Apr  9 09:48:58.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-9645'
Apr  9 09:48:59.762: INFO: stderr: ""
Apr  9 09:48:59.762: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 09:48:59.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:48:59.824: INFO: stderr: ""
Apr  9 09:48:59.824: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-phjpq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  9 09:49:04.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:49:04.890: INFO: stderr: ""
Apr  9 09:49:04.891: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-phjpq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Apr  9 09:49:09.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:49:09.951: INFO: stderr: ""
Apr  9 09:49:09.951: INFO: stdout: "update-demo-nautilus-kbxm5 "
Apr  9 09:49:09.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:10.011: INFO: stderr: ""
Apr  9 09:49:10.011: INFO: stdout: "true"
Apr  9 09:49:10.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:10.072: INFO: stderr: ""
Apr  9 09:49:10.072: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:49:10.072: INFO: validating pod update-demo-nautilus-kbxm5
Apr  9 09:49:10.074: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:49:10.074: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:49:10.074: INFO: update-demo-nautilus-kbxm5 is verified up and running
STEP: scaling up the replication controller
Apr  9 09:49:10.075: INFO: scanned /root for discovery docs: <nil>
Apr  9 09:49:10.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-9645'
Apr  9 09:49:10.158: INFO: stderr: ""
Apr  9 09:49:10.158: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Apr  9 09:49:10.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:49:10.220: INFO: stderr: ""
Apr  9 09:49:10.220: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-zslqf "
Apr  9 09:49:10.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:10.283: INFO: stderr: ""
Apr  9 09:49:10.283: INFO: stdout: "true"
Apr  9 09:49:10.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:10.346: INFO: stderr: ""
Apr  9 09:49:10.346: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:49:10.346: INFO: validating pod update-demo-nautilus-kbxm5
Apr  9 09:49:10.348: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:49:10.348: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:49:10.348: INFO: update-demo-nautilus-kbxm5 is verified up and running
Apr  9 09:49:10.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-zslqf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:10.414: INFO: stderr: ""
Apr  9 09:49:10.414: INFO: stdout: ""
Apr  9 09:49:10.414: INFO: update-demo-nautilus-zslqf is created but not running
Apr  9 09:49:15.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9645'
Apr  9 09:49:15.482: INFO: stderr: ""
Apr  9 09:49:15.482: INFO: stdout: "update-demo-nautilus-kbxm5 update-demo-nautilus-zslqf "
Apr  9 09:49:15.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:15.542: INFO: stderr: ""
Apr  9 09:49:15.542: INFO: stdout: "true"
Apr  9 09:49:15.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-kbxm5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:15.605: INFO: stderr: ""
Apr  9 09:49:15.605: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:49:15.605: INFO: validating pod update-demo-nautilus-kbxm5
Apr  9 09:49:15.607: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:49:15.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:49:15.607: INFO: update-demo-nautilus-kbxm5 is verified up and running
Apr  9 09:49:15.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-zslqf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:15.666: INFO: stderr: ""
Apr  9 09:49:15.666: INFO: stdout: "true"
Apr  9 09:49:15.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods update-demo-nautilus-zslqf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9645'
Apr  9 09:49:15.725: INFO: stderr: ""
Apr  9 09:49:15.725: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Apr  9 09:49:15.725: INFO: validating pod update-demo-nautilus-zslqf
Apr  9 09:49:15.727: INFO: got data: {
  "image": "nautilus.jpg"
}

Apr  9 09:49:15.727: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Apr  9 09:49:15.727: INFO: update-demo-nautilus-zslqf is verified up and running
STEP: using delete to clean up resources
Apr  9 09:49:15.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 delete --grace-period=0 --force -f - --namespace=kubectl-9645'
Apr  9 09:49:15.789: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Apr  9 09:49:15.789: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Apr  9 09:49:15.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9645'
Apr  9 09:49:15.853: INFO: stderr: "No resources found.\n"
Apr  9 09:49:15.853: INFO: stdout: ""
Apr  9 09:49:15.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=update-demo --namespace=kubectl-9645 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 09:49:15.918: INFO: stderr: ""
Apr  9 09:49:15.918: INFO: stdout: "update-demo-nautilus-kbxm5\nupdate-demo-nautilus-zslqf\n"
Apr  9 09:49:16.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9645'
Apr  9 09:49:16.488: INFO: stderr: "No resources found.\n"
Apr  9 09:49:16.488: INFO: stdout: ""
Apr  9 09:49:16.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393566007 get pods -l name=update-demo --namespace=kubectl-9645 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Apr  9 09:49:16.550: INFO: stderr: ""
Apr  9 09:49:16.550: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:49:16.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9645" for this suite.
Apr  9 09:49:38.557: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:49:38.595: INFO: namespace kubectl-9645 deletion completed in 22.043038022s

• [SLOW TEST:45.534 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:49:38.595: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ca4813e4-5aac-11e9-b737-ea708a3858a3
STEP: Creating a pod to test consume secrets
Apr  9 09:49:38.616: INFO: Waiting up to 5m0s for pod "pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3" in namespace "secrets-6226" to be "success or failure"
Apr  9 09:49:38.624: INFO: Pod "pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.72615ms
Apr  9 09:49:40.632: INFO: Pod "pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016008297s
STEP: Saw pod success
Apr  9 09:49:40.632: INFO: Pod "pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:49:40.634: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3 container secret-volume-test: <nil>
STEP: delete the pod
Apr  9 09:49:40.640: INFO: Waiting for pod pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:49:40.642: INFO: Pod pod-secrets-ca4853c8-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:49:40.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6226" for this suite.
Apr  9 09:49:46.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:49:46.691: INFO: namespace secrets-6226 deletion completed in 6.047252942s

• [SLOW TEST:8.096 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:49:46.691: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Apr  9 09:49:46.716: INFO: Waiting up to 5m0s for pod "client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3" in namespace "containers-156" to be "success or failure"
Apr  9 09:49:46.717: INFO: Pod "client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.266965ms
Apr  9 09:49:48.719: INFO: Pod "client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003275318s
STEP: Saw pod success
Apr  9 09:49:48.720: INFO: Pod "client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:49:48.721: INFO: Trying to get logs from node ip-172-31-13-147 pod client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:49:48.728: INFO: Waiting for pod client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:49:48.729: INFO: Pod client-containers-cf1b82e0-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:49:48.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-156" for this suite.
Apr  9 09:49:54.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:49:54.773: INFO: namespace containers-156 deletion completed in 6.042471316s

• [SLOW TEST:8.082 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:49:54.773: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Apr  9 09:49:54.798: INFO: Waiting up to 5m0s for pod "pod-d3ed2134-5aac-11e9-b737-ea708a3858a3" in namespace "emptydir-1476" to be "success or failure"
Apr  9 09:49:54.803: INFO: Pod "pod-d3ed2134-5aac-11e9-b737-ea708a3858a3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264985ms
Apr  9 09:49:56.805: INFO: Pod "pod-d3ed2134-5aac-11e9-b737-ea708a3858a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006367738s
STEP: Saw pod success
Apr  9 09:49:56.805: INFO: Pod "pod-d3ed2134-5aac-11e9-b737-ea708a3858a3" satisfied condition "success or failure"
Apr  9 09:49:56.806: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-d3ed2134-5aac-11e9-b737-ea708a3858a3 container test-container: <nil>
STEP: delete the pod
Apr  9 09:49:56.816: INFO: Waiting for pod pod-d3ed2134-5aac-11e9-b737-ea708a3858a3 to disappear
Apr  9 09:49:56.824: INFO: Pod pod-d3ed2134-5aac-11e9-b737-ea708a3858a3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:49:56.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1476" for this suite.
Apr  9 09:50:02.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:50:02.867: INFO: namespace emptydir-1476 deletion completed in 6.041375987s

• [SLOW TEST:8.094 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:50:02.867: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-djrd
STEP: Creating a pod to test atomic-volume-subpath
Apr  9 09:50:02.896: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-djrd" in namespace "subpath-7918" to be "success or failure"
Apr  9 09:50:02.900: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.28588ms
Apr  9 09:50:04.903: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 2.006395268s
Apr  9 09:50:06.905: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 4.008478503s
Apr  9 09:50:08.907: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 6.01056255s
Apr  9 09:50:10.909: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 8.012351777s
Apr  9 09:50:12.911: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 10.014513155s
Apr  9 09:50:14.913: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 12.016549158s
Apr  9 09:50:16.915: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 14.018367746s
Apr  9 09:50:18.917: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 16.02049763s
Apr  9 09:50:20.919: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 18.022435036s
Apr  9 09:50:22.921: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Running", Reason="", readiness=true. Elapsed: 20.024458885s
Apr  9 09:50:24.923: INFO: Pod "pod-subpath-test-secret-djrd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.026557016s
STEP: Saw pod success
Apr  9 09:50:24.923: INFO: Pod "pod-subpath-test-secret-djrd" satisfied condition "success or failure"
Apr  9 09:50:24.924: INFO: Trying to get logs from node ip-172-31-13-147 pod pod-subpath-test-secret-djrd container test-container-subpath-secret-djrd: <nil>
STEP: delete the pod
Apr  9 09:50:24.932: INFO: Waiting for pod pod-subpath-test-secret-djrd to disappear
Apr  9 09:50:24.933: INFO: Pod pod-subpath-test-secret-djrd no longer exists
STEP: Deleting pod pod-subpath-test-secret-djrd
Apr  9 09:50:24.933: INFO: Deleting pod "pod-subpath-test-secret-djrd" in namespace "subpath-7918"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:50:24.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7918" for this suite.
Apr  9 09:50:30.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:50:30.981: INFO: namespace subpath-7918 deletion completed in 6.045680149s

• [SLOW TEST:28.114 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Apr  9 09:50:30.982: INFO: >>> kubeConfig: /tmp/kubeconfig-393566007
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Apr  9 09:50:31.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17680,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 09:50:31.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17680,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Apr  9 09:50:41.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17693,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Apr  9 09:50:41.012: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17693,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Apr  9 09:50:51.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17707,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 09:50:51.016: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17707,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Apr  9 09:51:01.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17720,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Apr  9 09:51:01.019: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-a,UID:e9828d91-5aac-11e9-bcbd-0af1119c727a,ResourceVersion:17720,Generation:0,CreationTimestamp:2019-04-09 09:50:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Apr  9 09:51:11.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-b,UID:015cb851-5aad-11e9-bcbd-0af1119c727a,ResourceVersion:17733,Generation:0,CreationTimestamp:2019-04-09 09:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 09:51:11.023: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-b,UID:015cb851-5aad-11e9-bcbd-0af1119c727a,ResourceVersion:17733,Generation:0,CreationTimestamp:2019-04-09 09:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Apr  9 09:51:21.026: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-b,UID:015cb851-5aad-11e9-bcbd-0af1119c727a,ResourceVersion:17746,Generation:0,CreationTimestamp:2019-04-09 09:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Apr  9 09:51:21.026: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-1687,SelfLink:/api/v1/namespaces/watch-1687/configmaps/e2e-watch-test-configmap-b,UID:015cb851-5aad-11e9-bcbd-0af1119c727a,ResourceVersion:17746,Generation:0,CreationTimestamp:2019-04-09 09:51:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Apr  9 09:51:31.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1687" for this suite.
Apr  9 09:51:37.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Apr  9 09:51:37.071: INFO: namespace watch-1687 deletion completed in 6.04343218s

• [SLOW TEST:66.090 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSApr  9 09:51:37.071: INFO: Running AfterSuite actions on all nodes
Apr  9 09:51:37.071: INFO: Running AfterSuite actions on node 1
Apr  9 09:51:37.071: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5110.473 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h25m11.548713693s
Test Suite Passed
