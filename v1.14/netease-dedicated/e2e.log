I0522 07:05:26.266953      19 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-393362560
I0522 07:05:26.267053      19 e2e.go:240] Starting e2e run "f8fdfde3-7c5f-11e9-aefd-da1a35f02de1" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1558508725 - Will randomize all specs
Will run 204 of 3584 specs

May 22 07:05:27.708: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:05:27.710: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 22 07:05:30.578: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 22 07:05:30.818: INFO: 32 / 32 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 22 07:05:30.839: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
May 22 07:05:30.839: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 22 07:05:30.855: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cleanlog' (0 seconds elapsed)
May 22 07:05:30.855: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'cleanlog-master' (0 seconds elapsed)
May 22 07:05:30.855: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'ds-vpccni' (0 seconds elapsed)
May 22 07:05:30.855: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
May 22 07:05:30.859: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-device-plugin-daemonset' (0 seconds elapsed)
May 22 07:05:30.859: INFO: e2e test version: v1.14.1
May 22 07:05:30.864: INFO: kube-apiserver version: v1.14.1
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:05:30.864: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
May 22 07:05:30.919: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0522 07:06:11.025643      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 07:06:11.025: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:06:11.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1483" for this suite.
May 22 07:06:17.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:06:17.115: INFO: namespace gc-1483 deletion completed in 6.08509053s

• [SLOW TEST:46.251 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:06:17.115: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-17ebc970-7c60-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:06:17.164: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1" in namespace "projected-9290" to be "success or failure"
May 22 07:06:17.166: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.297557ms
May 22 07:06:19.171: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007150222s
May 22 07:06:21.191: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027286277s
May 22 07:06:23.196: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.032514224s
May 22 07:06:25.199: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.035812903s
May 22 07:06:27.202: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.038623281s
May 22 07:06:29.206: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.0419234s
STEP: Saw pod success
May 22 07:06:29.206: INFO: Pod "pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:06:29.208: INFO: Trying to get logs from node node1 pod pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 07:06:29.540: INFO: Waiting for pod pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:06:29.542: INFO: Pod pod-projected-secrets-17ec614b-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:06:29.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9290" for this suite.
May 22 07:06:36.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:06:37.219: INFO: namespace projected-9290 deletion completed in 7.672517272s

• [SLOW TEST:20.104 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:06:37.220: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /root/workspace/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:06:37.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5670" for this suite.
May 22 07:07:17.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:07:18.032: INFO: namespace pods-5670 deletion completed in 40.216553888s

• [SLOW TEST:40.812 seconds]
[k8s.io] [sig-node] Pods Extended
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:07:18.032: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:07:18.090: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1" in namespace "projected-3017" to be "success or failure"
May 22 07:07:18.093: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794998ms
May 22 07:07:20.097: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00630219s
May 22 07:07:22.102: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011547055s
May 22 07:07:24.106: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015835626s
May 22 07:07:26.110: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019593035s
May 22 07:07:28.115: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024250754s
May 22 07:07:30.119: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.027998105s
STEP: Saw pod success
May 22 07:07:30.119: INFO: Pod "downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:07:30.124: INFO: Trying to get logs from node node1 pod downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:07:30.147: INFO: Waiting for pod downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:07:30.149: INFO: Pod downwardapi-volume-3c3bee43-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:07:30.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3017" for this suite.
May 22 07:07:36.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:07:36.281: INFO: namespace projected-3017 deletion completed in 6.126618971s

• [SLOW TEST:18.249 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:07:36.282: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 22 07:07:36.424: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6766,SelfLink:/api/v1/namespaces/watch-6766/configmaps/e2e-watch-test-resource-version,UID:4725681f-7c60-11e9-9cf4-fa163ecd1d63,ResourceVersion:12802782,Generation:0,CreationTimestamp:2019-05-22 07:07:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 07:07:36.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6766,SelfLink:/api/v1/namespaces/watch-6766/configmaps/e2e-watch-test-resource-version,UID:4725681f-7c60-11e9-9cf4-fa163ecd1d63,ResourceVersion:12802783,Generation:0,CreationTimestamp:2019-05-22 07:07:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:07:36.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6766" for this suite.
May 22 07:07:42.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:07:42.536: INFO: namespace watch-6766 deletion completed in 6.106673855s

• [SLOW TEST:6.254 seconds]
[sig-api-machinery] Watchers
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:07:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 22 07:07:52.693: INFO: Pod pod-hostip-4ad6a598-7c60-11e9-aefd-da1a35f02de1 has hostIP: 10.177.10.19
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:07:52.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9201" for this suite.
May 22 07:08:14.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:08:14.818: INFO: namespace pods-9201 deletion completed in 22.118451721s

• [SLOW TEST:32.281 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:08:14.818: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 22 07:08:14.925: INFO: Waiting up to 5m0s for pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1" in namespace "containers-7105" to be "success or failure"
May 22 07:08:14.932: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.163277ms
May 22 07:08:16.935: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009188641s
May 22 07:08:18.938: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012679943s
May 22 07:08:20.942: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016995726s
May 22 07:08:22.946: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020706291s
May 22 07:08:24.950: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024077423s
May 22 07:08:26.952: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027042911s
May 22 07:08:28.957: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.03125504s
May 22 07:08:30.960: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.035039271s
May 22 07:08:32.964: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039023292s
May 22 07:08:34.969: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.043429763s
May 22 07:08:36.973: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.047585012s
May 22 07:08:38.978: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.052182804s
May 22 07:08:40.982: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.056873087s
May 22 07:08:42.987: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.061520273s
May 22 07:08:44.991: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.066002713s
STEP: Saw pod success
May 22 07:08:44.992: INFO: Pod "client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:08:44.995: INFO: Trying to get logs from node node1 pod client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:08:45.022: INFO: Waiting for pod client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:08:45.025: INFO: Pod client-containers-5e1cf8de-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:08:45.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7105" for this suite.
May 22 07:08:51.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:08:51.193: INFO: namespace containers-7105 deletion completed in 6.16251595s

• [SLOW TEST:36.375 seconds]
[k8s.io] Docker Containers
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:08:51.194: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 22 07:09:27.806: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:27.809: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:29.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:29.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:31.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:31.816: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:33.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:33.817: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:35.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:35.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:37.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:37.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:39.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:39.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:41.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:41.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:43.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:43.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:45.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:45.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:47.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:47.813: INFO: Pod pod-with-poststart-exec-hook still exists
May 22 07:09:49.809: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 22 07:09:49.813: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:09:49.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5590" for this suite.
May 22 07:10:11.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:10:11.916: INFO: namespace container-lifecycle-hook-5590 deletion completed in 22.098269359s

• [SLOW TEST:80.722 seconds]
[k8s.io] Container Lifecycle Hook
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:10:11.917: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-a3ec4c95-7c60-11e9-aefd-da1a35f02de1
STEP: Creating secret with name s-test-opt-upd-a3ec4cdf-7c60-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a3ec4c95-7c60-11e9-aefd-da1a35f02de1
STEP: Updating secret s-test-opt-upd-a3ec4cdf-7c60-11e9-aefd-da1a35f02de1
STEP: Creating secret with name s-test-opt-create-a3ec4d04-7c60-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:10:20.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2415" for this suite.
May 22 07:10:42.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:10:42.333: INFO: namespace secrets-2415 deletion completed in 22.115722929s

• [SLOW TEST:30.416 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:10:42.334: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 22 07:10:42.381: INFO: Waiting up to 5m0s for pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1" in namespace "emptydir-8982" to be "success or failure"
May 22 07:10:42.384: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.497046ms
May 22 07:10:44.387: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006180717s
May 22 07:10:46.391: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010127768s
May 22 07:10:48.489: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.108342377s
May 22 07:10:50.497: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.115793431s
May 22 07:10:52.501: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.119911323s
May 22 07:10:54.505: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.123908473s
STEP: Saw pod success
May 22 07:10:54.505: INFO: Pod "pod-b60131a2-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:10:54.509: INFO: Trying to get logs from node node2 pod pod-b60131a2-7c60-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:10:54.901: INFO: Waiting for pod pod-b60131a2-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:10:54.905: INFO: Pod pod-b60131a2-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:10:54.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8982" for this suite.
May 22 07:11:00.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:11:01.021: INFO: namespace emptydir-8982 deletion completed in 6.109822013s

• [SLOW TEST:18.688 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:11:01.021: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:11:01.069: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1" in namespace "downward-api-2440" to be "success or failure"
May 22 07:11:01.071: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.268391ms
May 22 07:11:03.074: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005709754s
May 22 07:11:05.077: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008834097s
May 22 07:11:07.082: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013155272s
May 22 07:11:09.091: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.02198652s
STEP: Saw pod success
May 22 07:11:09.091: INFO: Pod "downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:11:09.093: INFO: Trying to get logs from node node1 pod downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:11:09.116: INFO: Waiting for pod downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:11:09.118: INFO: Pod downwardapi-volume-c124ce55-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:11:09.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2440" for this suite.
May 22 07:11:15.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:11:16.097: INFO: namespace downward-api-2440 deletion completed in 6.974182029s

• [SLOW TEST:15.075 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:11:16.097: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-ca6a8705-7c60-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:11:16.666: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1" in namespace "projected-7703" to be "success or failure"
May 22 07:11:16.687: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.118379ms
May 22 07:11:18.697: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031081738s
May 22 07:11:20.701: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034521877s
May 22 07:11:22.705: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038472272s
May 22 07:11:24.709: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.042195545s
May 22 07:11:26.712: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045147091s
May 22 07:11:28.720: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.053175163s
May 22 07:11:30.723: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.056433546s
May 22 07:11:32.726: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.059877625s
May 22 07:11:34.731: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.064541334s
May 22 07:11:36.735: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.068976542s
May 22 07:11:38.739: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.073029652s
May 22 07:11:40.744: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.077624204s
STEP: Saw pod success
May 22 07:11:40.744: INFO: Pod "pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:11:40.747: INFO: Trying to get logs from node node1 pod pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 07:11:40.771: INFO: Waiting for pod pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:11:40.774: INFO: Pod pod-projected-secrets-ca6f5e8d-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:11:40.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7703" for this suite.
May 22 07:11:46.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:11:46.885: INFO: namespace projected-7703 deletion completed in 6.106754851s

• [SLOW TEST:30.789 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:11:46.889: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-dc7ca112-7c60-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:11:46.950: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1" in namespace "projected-4757" to be "success or failure"
May 22 07:11:46.953: INFO: Pod "pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.2752ms
May 22 07:11:48.957: INFO: Pod "pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00675806s
STEP: Saw pod success
May 22 07:11:48.957: INFO: Pod "pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:11:48.961: INFO: Trying to get logs from node node1 pod pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 07:11:48.986: INFO: Waiting for pod pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:11:48.988: INFO: Pod pod-projected-secrets-dc7d5429-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:11:48.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4757" for this suite.
May 22 07:11:55.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:11:55.088: INFO: namespace projected-4757 deletion completed in 6.094065003s

• [SLOW TEST:8.199 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:11:55.088: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 07:11:55.129: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 07:11:55.137: INFO: Waiting for terminating namespaces to be deleted...
May 22 07:11:55.140: INFO: 
Logging pods the kubelet thinks is on node node1 before test
May 22 07:11:55.151: INFO: lctest0-65749587df-t29rr from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-vlkxh from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-gfcfs from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: cleanlog-9dn5v from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container cleanlog ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-fc9wf from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-6plsf from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-8gn8r from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 07:05:07 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 22 07:11:55.151: INFO: kube-proxy-q4fz8 from kube-system started at 2019-05-17 07:21:31 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container kube-proxy ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-mbnhq from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-q5s65 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-2c429 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-xdcfx from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-m7ckd from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-g9qf9 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-kfgd8 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-bcpct from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-t59p9 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-twg2l from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: lctest0-65749587df-2t8xf from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.151: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-lvq4t from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:11:55.151: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:11:55.151: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-qqxct from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-qwjmr from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-8ghbx from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-c7kxn from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-mrfmz from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-9t27n from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: ds-vpccni-sc86k from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-fwp7d from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-xgjbf from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-9lr68 from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-zqg7f from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-47g56 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: lctest0-65749587df-pznfm from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.152: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.152: INFO: 
Logging pods the kubelet thinks is on node node2 before test
May 22 07:11:55.174: INFO: lctest0-65749587df-lg6gj from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-krvcx from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: kube-proxy-6dqrx from kube-system started at 2019-05-17 07:22:10 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container kube-proxy ready: true, restart count 1
May 22 07:11:55.174: INFO: lctest0-65749587df-2fdr6 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-lslzb from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-z7fz9 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: coredns-d549755c8-nfhl9 from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container coredns ready: true, restart count 1
May 22 07:11:55.174: INFO: lctest0-65749587df-k9d5s from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-mtm65 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-44npn from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: sonobuoy-e2e-job-2312d67a818b448f from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container e2e ready: true, restart count 0
May 22 07:11:55.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:11:55.174: INFO: coredns-d549755c8-l428d from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container coredns ready: true, restart count 1
May 22 07:11:55.174: INFO: lctest0-65749587df-8bbbr from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-zwxm5 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: ds-vpccni-xmznc from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-5g75l from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-hsb6m from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:11:55.174: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-2vd7m from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-94msv from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: lctest0-65749587df-9wtp5 from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.174: INFO: cleanlog-55lgl from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.174: INFO: 	Container cleanlog ready: true, restart count 2
May 22 07:11:55.175: INFO: lctest0-65749587df-lsdsl from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-bklfq from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-qvlzl from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-7rlv5 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-jvhkh from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-splfw from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-5g8gz from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-85swp from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-p7zvr from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-f2v2l from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-4t9v6 from default started at 2019-05-22 07:11:11 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
May 22 07:11:55.175: INFO: lctest0-65749587df-59lz7 from default started at 2019-05-22 07:11:12 +0000 UTC (1 container statuses recorded)
May 22 07:11:55.175: INFO: 	Container debian ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node node1
STEP: verifying the node has the label node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-2c429 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-2fdr6 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-2t8xf requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-2vd7m requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-44npn requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-47g56 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-4t9v6 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-59lz7 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-5g75l requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-5g8gz requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-6plsf requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-7rlv5 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-85swp requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-8bbbr requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-8ghbx requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-8gn8r requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-94msv requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-9lr68 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-9t27n requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-9wtp5 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-bcpct requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-bklfq requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-c7kxn requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-f2v2l requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-fc9wf requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-fwp7d requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-g9qf9 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-gfcfs requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-jvhkh requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-k9d5s requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-kfgd8 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-krvcx requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-lg6gj requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-lsdsl requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-lslzb requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-m7ckd requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-mbnhq requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-mrfmz requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-mtm65 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-p7zvr requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-pznfm requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-q5s65 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-qqxct requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-qvlzl requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-qwjmr requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-splfw requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-t29rr requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-t59p9 requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-twg2l requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-vlkxh requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-xdcfx requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-xgjbf requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-z7fz9 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-zqg7f requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod lctest0-65749587df-zwxm5 requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod sonobuoy requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod sonobuoy-e2e-job-2312d67a818b448f requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-hsb6m requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-lvq4t requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod cleanlog-55lgl requesting resource cpu=100m on Node node2
May 22 07:11:55.232: INFO: Pod cleanlog-9dn5v requesting resource cpu=100m on Node node1
May 22 07:11:55.232: INFO: Pod coredns-d549755c8-l428d requesting resource cpu=100m on Node node2
May 22 07:11:55.232: INFO: Pod coredns-d549755c8-nfhl9 requesting resource cpu=100m on Node node2
May 22 07:11:55.232: INFO: Pod ds-vpccni-sc86k requesting resource cpu=0m on Node node1
May 22 07:11:55.232: INFO: Pod ds-vpccni-xmznc requesting resource cpu=0m on Node node2
May 22 07:11:55.232: INFO: Pod kube-proxy-6dqrx requesting resource cpu=100m on Node node2
May 22 07:11:55.232: INFO: Pod kube-proxy-q4fz8 requesting resource cpu=100m on Node node1
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1.15a0efc87485689a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8907/filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1 to node1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1.15a0efc8b02cce43], Reason = [Pulled], Message = [Container image "hub.c.163.com/combk8s/pause-amd64:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1.15a0efc8b542351a], Reason = [Created], Message = [Created container filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1.15a0efc8c3b8a9d7], Reason = [Started], Message = [Started container filler-pod-e16e77d1-7c60-11e9-aefd-da1a35f02de1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1.15a0efc874808bf2], Reason = [Scheduled], Message = [Successfully assigned sched-pred-8907/filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1 to node2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1.15a0efc8b27b29b2], Reason = [Pulled], Message = [Container image "hub.c.163.com/combk8s/pause-amd64:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1.15a0efc8b894c839], Reason = [Created], Message = [Created container filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1.15a0efc8c246738c], Reason = [Started], Message = [Started container filler-pod-e16f7f78-7c60-11e9-aefd-da1a35f02de1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15a0efca4fb895e6], Reason = [FailedScheduling], Message = [0/6 nodes are available: 1 node(s) were unschedulable, 2 Insufficient cpu, 3 node(s) had taints that the pod didn't tolerate.]
STEP: removing the label node off the node node1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node node2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:12:04.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8907" for this suite.
May 22 07:12:10.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:12:10.416: INFO: namespace sched-pred-8907 deletion completed in 6.097082837s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:15.328 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:12:10.417: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 22 07:12:10.464: INFO: Waiting up to 5m0s for pod "client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1" in namespace "containers-7213" to be "success or failure"
May 22 07:12:10.471: INFO: Pod "client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.702978ms
May 22 07:12:12.475: INFO: Pod "client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010512201s
STEP: Saw pod success
May 22 07:12:12.475: INFO: Pod "client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:12:12.478: INFO: Trying to get logs from node node1 pod client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:12:12.498: INFO: Waiting for pod client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:12:12.500: INFO: Pod client-containers-ea817e8a-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:12:12.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7213" for this suite.
May 22 07:12:18.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:12:18.613: INFO: namespace containers-7213 deletion completed in 6.108178816s

• [SLOW TEST:8.196 seconds]
[k8s.io] Docker Containers
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:12:18.613: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:12:18.663: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1" in namespace "projected-1262" to be "success or failure"
May 22 07:12:18.666: INFO: Pod "downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.368813ms
May 22 07:12:20.670: INFO: Pod "downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006675284s
STEP: Saw pod success
May 22 07:12:20.670: INFO: Pod "downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:12:20.673: INFO: Trying to get logs from node node1 pod downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:12:20.697: INFO: Waiting for pod downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:12:20.699: INFO: Pod downwardapi-volume-ef646fa2-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:12:20.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1262" for this suite.
May 22 07:12:26.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:12:26.816: INFO: namespace projected-1262 deletion completed in 6.098695971s

• [SLOW TEST:8.203 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:12:26.817: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 22 07:12:26.918: INFO: Waiting up to 5m0s for pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1" in namespace "emptydir-3538" to be "success or failure"
May 22 07:12:26.921: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.5156ms
May 22 07:12:28.925: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006493533s
May 22 07:12:30.929: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010486563s
May 22 07:12:32.934: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016227632s
May 22 07:12:34.939: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020735607s
May 22 07:12:36.948: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.029547548s
May 22 07:12:38.951: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.032555565s
May 22 07:12:40.954: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.035891614s
May 22 07:12:42.959: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.040582525s
May 22 07:12:44.968: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.049463038s
May 22 07:12:46.972: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.053661874s
STEP: Saw pod success
May 22 07:12:46.972: INFO: Pod "pod-f45049a8-7c60-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:12:46.975: INFO: Trying to get logs from node node1 pod pod-f45049a8-7c60-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:12:47.001: INFO: Waiting for pod pod-f45049a8-7c60-11e9-aefd-da1a35f02de1 to disappear
May 22 07:12:47.004: INFO: Pod pod-f45049a8-7c60-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:12:47.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3538" for this suite.
May 22 07:12:53.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:12:53.127: INFO: namespace emptydir-3538 deletion completed in 6.118222769s

• [SLOW TEST:26.311 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:12:53.128: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4513
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 07:12:53.174: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 07:13:57.270: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.177.11.199:8080/dial?request=hostName&protocol=udp&host=10.177.10.126&port=8081&tries=1'] Namespace:pod-network-test-4513 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:13:57.270: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:13:58.050: INFO: Waiting for endpoints: map[]
May 22 07:13:58.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.177.11.199:8080/dial?request=hostName&protocol=udp&host=10.177.11.245&port=8081&tries=1'] Namespace:pod-network-test-4513 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:13:58.056: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:13:58.959: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:13:58.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4513" for this suite.
May 22 07:14:20.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:14:21.079: INFO: namespace pod-network-test-4513 deletion completed in 22.113715299s

• [SLOW TEST:87.951 seconds]
[sig-network] Networking
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:14:21.079: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7094
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-7094
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7094
May 22 07:14:21.154: INFO: Found 0 stateful pods, waiting for 1
May 22 07:14:31.159: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 22 07:14:41.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 22 07:14:51.158: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 22 07:14:51.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 07:14:54.410: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 07:14:54.410: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 07:14:54.410: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 07:14:54.413: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 22 07:15:04.417: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 07:15:04.417: INFO: Waiting for statefulset status.replicas updated to 0
May 22 07:15:04.431: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:15:04.431: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:54 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:15:04.431: INFO: 
May 22 07:15:04.431: INFO: StatefulSet ss has not reached scale 3, at 1
May 22 07:15:05.434: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997158829s
May 22 07:15:06.438: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993813351s
May 22 07:15:07.442: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990430543s
May 22 07:15:08.446: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986509492s
May 22 07:15:09.450: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982391966s
May 22 07:15:10.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.978337544s
May 22 07:15:11.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.97452856s
May 22 07:15:12.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969804008s
May 22 07:15:13.467: INFO: Verifying statefulset ss doesn't scale past 3 for another 964.85102ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7094
May 22 07:15:14.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:15:14.701: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 07:15:14.701: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 07:15:14.701: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 07:15:14.701: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:15:15.240: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 22 07:15:15.240: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 07:15:15.240: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 07:15:15.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:15:15.440: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 22 07:15:15.440: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 07:15:15.440: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 07:15:15.444: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 22 07:15:25.450: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 22 07:15:35.448: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
May 22 07:15:45.449: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 07:15:45.449: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 07:15:45.449: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 22 07:15:45.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 07:15:45.659: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 07:15:45.659: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 07:15:45.659: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 07:15:45.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 07:15:45.866: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 07:15:45.866: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 07:15:45.866: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 07:15:45.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 07:15:46.048: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 07:15:46.048: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 07:15:46.048: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 07:15:46.048: INFO: Waiting for statefulset status.replicas updated to 0
May 22 07:15:46.053: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 22 07:15:56.149: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 07:15:56.149: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 22 07:15:56.149: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 22 07:15:56.203: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:15:56.203: INFO: ss-0  node2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:15:56.203: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:56.203: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:56.203: INFO: 
May 22 07:15:56.203: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:15:57.240: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:15:57.240: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:15:57.240: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:57.240: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:57.240: INFO: 
May 22 07:15:57.240: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:15:58.246: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:15:58.246: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:15:58.246: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:58.246: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:58.246: INFO: 
May 22 07:15:58.246: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:15:59.284: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:15:59.284: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:15:59.284: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:59.284: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:15:59.284: INFO: 
May 22 07:15:59.284: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:00.379: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:00.379: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:00.379: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:00.379: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:00.379: INFO: 
May 22 07:16:00.379: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:01.440: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:01.440: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:01.440: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:01.440: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:01.440: INFO: 
May 22 07:16:01.440: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:02.476: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:02.476: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:02.476: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:02.476: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:02.476: INFO: 
May 22 07:16:02.476: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:03.596: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:03.596: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:03.596: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:03.596: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:03.596: INFO: 
May 22 07:16:03.596: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:04.614: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:04.614: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:04.614: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:04.614: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:04.614: INFO: 
May 22 07:16:04.614: INFO: StatefulSet ss has not reached scale 0, at 3
May 22 07:16:05.726: INFO: POD   NODE   PHASE    GRACE  CONDITIONS
May 22 07:16:05.726: INFO: ss-0  node2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:14:21 +0000 UTC  }]
May 22 07:16:05.726: INFO: ss-1  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:05.727: INFO: ss-2  node1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:15:04 +0000 UTC  }]
May 22 07:16:05.727: INFO: 
May 22 07:16:05.727: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7094
May 22 07:16:06.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:07.675: INFO: rc: 1
May 22 07:16:07.675: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000af3710 exit status 1 <nil> <nil> true [0xc00261b100 0xc00261b118 0xc00261b130] [0xc00261b100 0xc00261b118 0xc00261b130] [0xc00261b110 0xc00261b128] [0x980420 0x980420] 0xc000d924e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 22 07:16:17.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:17.862: INFO: rc: 1
May 22 07:16:17.863: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000af3aa0 exit status 1 <nil> <nil> true [0xc00261b138 0xc00261b150 0xc00261b168] [0xc00261b138 0xc00261b150 0xc00261b168] [0xc00261b148 0xc00261b160] [0x980420 0x980420] 0xc000d92a80 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 22 07:16:27.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:28.016: INFO: rc: 1
May 22 07:16:28.016: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc000fb6330 exit status 1 <nil> <nil> true [0xc002342040 0xc0023420a0 0xc0023420f8] [0xc002342040 0xc0023420a0 0xc0023420f8] [0xc002342080 0xc0023420c0] [0x980420 0x980420] 0xc00267e4e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 22 07:16:38.016: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:38.094: INFO: rc: 1
May 22 07:16:38.094: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6810 exit status 1 <nil> <nil> true [0xc002342108 0xc002342190 0xc002342288] [0xc002342108 0xc002342190 0xc002342288] [0xc002342168 0xc002342208] [0x980420 0x980420] 0xc00267ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:16:48.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:48.185: INFO: rc: 1
May 22 07:16:48.185: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000966330 exit status 1 <nil> <nil> true [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a010 0xc00261a040] [0x980420 0x980420] 0xc00125e5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:16:58.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:16:58.270: INFO: rc: 1
May 22 07:16:58.271: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6ba0 exit status 1 <nil> <nil> true [0xc0023422a0 0xc002342390 0xc002342498] [0xc0023422a0 0xc002342390 0xc002342498] [0xc002342338 0xc002342420] [0x980420 0x980420] 0xc00267f1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:08.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:08.350: INFO: rc: 1
May 22 07:17:08.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009666c0 exit status 1 <nil> <nil> true [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a060 0xc00261a078] [0x980420 0x980420] 0xc00125ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:18.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:18.516: INFO: rc: 1
May 22 07:17:18.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6f30 exit status 1 <nil> <nil> true [0xc002342508 0xc002342578 0xc002342660] [0xc002342508 0xc002342578 0xc002342660] [0xc002342558 0xc002342600] [0x980420 0x980420] 0xc00267f8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:28.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:29.148: INFO: rc: 1
May 22 07:17:29.148: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb73e0 exit status 1 <nil> <nil> true [0xc002342690 0xc0023426d8 0xc002342758] [0xc002342690 0xc0023426d8 0xc002342758] [0xc0023426b8 0xc002342750] [0x980420 0x980420] 0xc00267fe60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:39.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:39.242: INFO: rc: 1
May 22 07:17:39.243: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb77a0 exit status 1 <nil> <nil> true [0xc002342790 0xc0023427f0 0xc002342840] [0xc002342790 0xc0023427f0 0xc002342840] [0xc0023427e8 0xc002342810] [0x980420 0x980420] 0xc0008a02a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:49.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:49.344: INFO: rc: 1
May 22 07:17:49.344: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000966a50 exit status 1 <nil> <nil> true [0xc00261a088 0xc00261a0a0 0xc00261a0b8] [0xc00261a088 0xc00261a0a0 0xc00261a0b8] [0xc00261a098 0xc00261a0b0] [0x980420 0x980420] 0xc00125f2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:17:59.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:17:59.455: INFO: rc: 1
May 22 07:17:59.455: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb7b60 exit status 1 <nil> <nil> true [0xc002342850 0xc0023428b8 0xc002342938] [0xc002342850 0xc0023428b8 0xc002342938] [0xc0023428b0 0xc0023428e0] [0x980420 0x980420] 0xc0008a0600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:18:09.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:18:09.545: INFO: rc: 1
May 22 07:18:09.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb7ef0 exit status 1 <nil> <nil> true [0xc002342990 0xc0023429f0 0xc002342a88] [0xc002342990 0xc0023429f0 0xc002342a88] [0xc0023429e8 0xc002342a50] [0x980420 0x980420] 0xc0008a0a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:18:19.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:18:21.212: INFO: rc: 1
May 22 07:18:21.213: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001401aa0 exit status 1 <nil> <nil> true [0xc002342aa8 0xc002342bb8 0xc002342be0] [0xc002342aa8 0xc002342bb8 0xc002342be0] [0xc002342b20 0xc002342bd0] [0x980420 0x980420] 0xc0008a10e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:18:31.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:18:32.307: INFO: rc: 1
May 22 07:18:32.307: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6360 exit status 1 <nil> <nil> true [0xc002342040 0xc0023420a0 0xc0023420f8] [0xc002342040 0xc0023420a0 0xc0023420f8] [0xc002342080 0xc0023420c0] [0x980420 0x980420] 0xc00267e4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:18:42.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:18:42.389: INFO: rc: 1
May 22 07:18:42.389: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6840 exit status 1 <nil> <nil> true [0xc002342108 0xc002342190 0xc002342288] [0xc002342108 0xc002342190 0xc002342288] [0xc002342168 0xc002342208] [0x980420 0x980420] 0xc00267ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:18:52.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:18:52.471: INFO: rc: 1
May 22 07:18:52.471: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6c00 exit status 1 <nil> <nil> true [0xc0023422a0 0xc002342390 0xc002342498] [0xc0023422a0 0xc002342390 0xc002342498] [0xc002342338 0xc002342420] [0x980420 0x980420] 0xc00267f1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:02.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:02.558: INFO: rc: 1
May 22 07:19:02.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6fc0 exit status 1 <nil> <nil> true [0xc002342508 0xc002342578 0xc002342660] [0xc002342508 0xc002342578 0xc002342660] [0xc002342558 0xc002342600] [0x980420 0x980420] 0xc00267f8c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:12.648: INFO: rc: 1
May 22 07:19:12.648: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb74d0 exit status 1 <nil> <nil> true [0xc002342690 0xc0023426d8 0xc002342758] [0xc002342690 0xc0023426d8 0xc002342758] [0xc0023426b8 0xc002342750] [0x980420 0x980420] 0xc00267fe60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:22.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:22.811: INFO: rc: 1
May 22 07:19:22.811: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb7890 exit status 1 <nil> <nil> true [0xc002342790 0xc0023427f0 0xc002342840] [0xc002342790 0xc0023427f0 0xc002342840] [0xc0023427e8 0xc002342810] [0x980420 0x980420] 0xc0008a02a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:32.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:32.907: INFO: rc: 1
May 22 07:19:32.907: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001401b90 exit status 1 <nil> <nil> true [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a010 0xc00261a040] [0x980420 0x980420] 0xc00125e5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:42.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:43.040: INFO: rc: 1
May 22 07:19:43.040: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001401f20 exit status 1 <nil> <nil> true [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a060 0xc00261a078] [0x980420 0x980420] 0xc00125eb40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:19:53.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:19:53.115: INFO: rc: 1
May 22 07:19:53.115: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb7c80 exit status 1 <nil> <nil> true [0xc002342850 0xc0023428b8 0xc002342938] [0xc002342850 0xc0023428b8 0xc002342938] [0xc0023428b0 0xc0023428e0] [0x980420 0x980420] 0xc0008a0660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:03.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:03.514: INFO: rc: 1
May 22 07:20:03.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000966060 exit status 1 <nil> <nil> true [0xc002342990 0xc0023429f0 0xc002342a88] [0xc002342990 0xc0023429f0 0xc002342a88] [0xc0023429e8 0xc002342a50] [0x980420 0x980420] 0xc0008a0ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:13.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:13.607: INFO: rc: 1
May 22 07:20:13.607: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000966420 exit status 1 <nil> <nil> true [0xc002342aa8 0xc002342bb8 0xc002342be0] [0xc002342aa8 0xc002342bb8 0xc002342be0] [0xc002342b20 0xc002342bd0] [0x980420 0x980420] 0xc0008a11a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:23.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:23.726: INFO: rc: 1
May 22 07:20:23.726: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0009667e0 exit status 1 <nil> <nil> true [0xc002342c40 0xc002342cc0 0xc002342cf8] [0xc002342c40 0xc002342cc0 0xc002342cf8] [0xc002342c98 0xc002342cf0] [0x980420 0x980420] 0xc0008a1800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:33.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:33.839: INFO: rc: 1
May 22 07:20:33.839: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb64b0 exit status 1 <nil> <nil> true [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a000 0xc00261a018 0xc00261a048] [0xc00261a010 0xc00261a040] [0x980420 0x980420] 0xc00267e4e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:43.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:44.001: INFO: rc: 1
May 22 07:20:44.001: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fb6870 exit status 1 <nil> <nil> true [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a050 0xc00261a068 0xc00261a080] [0xc00261a060 0xc00261a078] [0x980420 0x980420] 0xc00267ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:20:54.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:20:54.085: INFO: rc: 1
May 22 07:20:54.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001401b60 exit status 1 <nil> <nil> true [0xc002342000 0xc002342080 0xc0023420c0] [0xc002342000 0xc002342080 0xc0023420c0] [0xc002342068 0xc0023420b8] [0x980420 0x980420] 0xc00125e5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:21:04.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:21:04.184: INFO: rc: 1
May 22 07:21:04.184: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001401f80 exit status 1 <nil> <nil> true [0xc0023420f8 0xc002342168 0xc002342208] [0xc0023420f8 0xc002342168 0xc002342208] [0xc002342148 0xc0023421b0] [0x980420 0x980420] 0xc00125ec00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

May 22 07:21:14.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-7094 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 07:21:15.983: INFO: rc: 1
May 22 07:21:15.992: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
May 22 07:21:15.992: INFO: Scaling statefulset ss to 0
May 22 07:21:16.045: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 07:21:16.049: INFO: Deleting all statefulset in ns statefulset-7094
May 22 07:21:16.053: INFO: Scaling statefulset ss to 0
May 22 07:21:16.069: INFO: Waiting for statefulset status.replicas updated to 0
May 22 07:21:16.072: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:21:16.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7094" for this suite.
May 22 07:21:22.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:21:23.425: INFO: namespace statefulset-7094 deletion completed in 7.303271179s

• [SLOW TEST:422.349 seconds]
[sig-apps] StatefulSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:21:23.429: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 22 07:21:23.803: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-4441" to be "success or failure"
May 22 07:21:23.825: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 21.703663ms
May 22 07:21:25.862: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05908241s
May 22 07:21:27.895: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.092231317s
May 22 07:21:29.904: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100898604s
May 22 07:21:31.913: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 8.109728092s
May 22 07:21:33.920: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 10.117011404s
May 22 07:21:35.924: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 12.120939755s
May 22 07:21:37.928: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 14.125226187s
May 22 07:21:39.932: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 16.129324492s
May 22 07:21:41.936: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 18.132739641s
May 22 07:21:43.940: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13702188s
May 22 07:21:45.943: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 22.140238926s
May 22 07:21:47.949: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 24.145616112s
May 22 07:21:49.952: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 26.148718036s
May 22 07:21:51.955: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 28.152072683s
May 22 07:21:53.959: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 30.15562486s
May 22 07:21:55.962: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 32.15920974s
May 22 07:21:57.966: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 34.163015503s
May 22 07:21:59.970: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.166438058s
STEP: Saw pod success
May 22 07:21:59.970: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 22 07:21:59.972: INFO: Trying to get logs from node node2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 22 07:22:00.370: INFO: Waiting for pod pod-host-path-test to disappear
May 22 07:22:00.372: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:22:00.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-4441" for this suite.
May 22 07:22:06.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:22:06.474: INFO: namespace hostpath-4441 deletion completed in 6.097896022s

• [SLOW TEST:43.046 seconds]
[sig-storage] HostPath
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:22:06.475: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 07:22:11.429: INFO: Successfully updated pod "annotationupdate4dca7e61-7c62-11e9-aefd-da1a35f02de1"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:22:13.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2108" for this suite.
May 22 07:22:43.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:22:43.599: INFO: namespace projected-2108 deletion completed in 30.143207856s

• [SLOW TEST:37.124 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:22:43.599: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0522 07:22:49.675350      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 07:22:49.675: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:22:49.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1540" for this suite.
May 22 07:22:55.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:22:55.774: INFO: namespace gc-1540 deletion completed in 6.094488023s

• [SLOW TEST:12.175 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:22:55.774: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-6b2b400b-7c62-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:22:55.831: INFO: Waiting up to 5m0s for pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1" in namespace "secrets-6739" to be "success or failure"
May 22 07:22:55.833: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.069402ms
May 22 07:22:57.838: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006189514s
May 22 07:22:59.842: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010258919s
May 22 07:23:01.846: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014854392s
May 22 07:23:03.850: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018998802s
May 22 07:23:05.854: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022270826s
May 22 07:23:07.910: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.078392574s
May 22 07:23:09.914: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.082170106s
May 22 07:23:11.917: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.085407775s
May 22 07:23:13.921: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.089665101s
May 22 07:23:15.924: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.093054493s
May 22 07:23:17.928: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.096257477s
STEP: Saw pod success
May 22 07:23:17.928: INFO: Pod "pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:23:17.931: INFO: Trying to get logs from node node2 pod pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 07:23:17.952: INFO: Waiting for pod pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1 to disappear
May 22 07:23:17.955: INFO: Pod pod-secrets-6b2cdf70-7c62-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:23:17.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6739" for this suite.
May 22 07:23:23.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:23:24.073: INFO: namespace secrets-6739 deletion completed in 6.113437774s

• [SLOW TEST:28.299 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:23:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 07:23:24.196: INFO: Waiting up to 5m0s for pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1" in namespace "downward-api-8458" to be "success or failure"
May 22 07:23:24.200: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.30854ms
May 22 07:23:26.203: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007173137s
May 22 07:23:28.208: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011285663s
May 22 07:23:30.211: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014809128s
May 22 07:23:32.215: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.018433887s
May 22 07:23:34.219: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.022299382s
STEP: Saw pod success
May 22 07:23:34.219: INFO: Pod "downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:23:34.221: INFO: Trying to get logs from node node1 pod downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 07:23:34.246: INFO: Waiting for pod downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1 to disappear
May 22 07:23:34.248: INFO: Pod downward-api-7c145f94-7c62-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:23:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8458" for this suite.
May 22 07:23:40.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:23:40.386: INFO: namespace downward-api-8458 deletion completed in 6.13215633s

• [SLOW TEST:16.313 seconds]
[sig-node] Downward API
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:23:40.387: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-wd29
STEP: Creating a pod to test atomic-volume-subpath
May 22 07:23:40.468: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wd29" in namespace "subpath-1865" to be "success or failure"
May 22 07:23:40.470: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.530554ms
May 22 07:23:42.475: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00708923s
May 22 07:23:44.480: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011684164s
May 22 07:23:46.483: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01556081s
May 22 07:23:48.603: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Pending", Reason="", readiness=false. Elapsed: 8.135371484s
May 22 07:23:50.611: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 10.142848921s
May 22 07:23:52.615: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 12.147242852s
May 22 07:23:54.619: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 14.151154557s
May 22 07:23:56.624: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 16.156101208s
May 22 07:23:58.629: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 18.161173557s
May 22 07:24:00.634: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 20.16585441s
May 22 07:24:02.639: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 22.170784399s
May 22 07:24:04.643: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 24.175325278s
May 22 07:24:06.647: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 26.179167377s
May 22 07:24:08.651: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Running", Reason="", readiness=true. Elapsed: 28.183078451s
May 22 07:24:10.655: INFO: Pod "pod-subpath-test-configmap-wd29": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.186968202s
STEP: Saw pod success
May 22 07:24:10.655: INFO: Pod "pod-subpath-test-configmap-wd29" satisfied condition "success or failure"
May 22 07:24:10.658: INFO: Trying to get logs from node node1 pod pod-subpath-test-configmap-wd29 container test-container-subpath-configmap-wd29: <nil>
STEP: delete the pod
May 22 07:24:10.681: INFO: Waiting for pod pod-subpath-test-configmap-wd29 to disappear
May 22 07:24:10.684: INFO: Pod pod-subpath-test-configmap-wd29 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wd29
May 22 07:24:10.684: INFO: Deleting pod "pod-subpath-test-configmap-wd29" in namespace "subpath-1865"
[AfterEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:24:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1865" for this suite.
May 22 07:24:16.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:24:16.799: INFO: namespace subpath-1865 deletion completed in 6.107364349s

• [SLOW TEST:36.413 seconds]
[sig-storage] Subpath
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:24:16.801: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-9b7be4d2-7c62-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-9b7be4d2-7c62-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:25:29.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-542" for this suite.
May 22 07:25:51.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:25:51.819: INFO: namespace configmap-542 deletion completed in 22.11541414s

• [SLOW TEST:95.019 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:25:51.820: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 22 07:25:51.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-9077'
May 22 07:25:52.485: INFO: stderr: ""
May 22 07:25:52.485: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 07:25:52.485: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9077'
May 22 07:25:52.615: INFO: stderr: ""
May 22 07:25:52.615: INFO: stdout: "update-demo-nautilus-6mnm4 update-demo-nautilus-gghpw "
May 22 07:25:52.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-6mnm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:25:52.705: INFO: stderr: ""
May 22 07:25:52.705: INFO: stdout: ""
May 22 07:25:52.705: INFO: update-demo-nautilus-6mnm4 is created but not running
May 22 07:25:57.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9077'
May 22 07:25:57.817: INFO: stderr: ""
May 22 07:25:57.817: INFO: stdout: "update-demo-nautilus-6mnm4 update-demo-nautilus-gghpw "
May 22 07:25:57.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-6mnm4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:25:57.922: INFO: stderr: ""
May 22 07:25:57.922: INFO: stdout: "true"
May 22 07:25:57.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-6mnm4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:25:58.009: INFO: stderr: ""
May 22 07:25:58.009: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 07:25:58.009: INFO: validating pod update-demo-nautilus-6mnm4
May 22 07:25:58.118: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 07:25:58.118: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 07:25:58.118: INFO: update-demo-nautilus-6mnm4 is verified up and running
May 22 07:25:58.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-gghpw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:25:58.205: INFO: stderr: ""
May 22 07:25:58.205: INFO: stdout: "true"
May 22 07:25:58.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-gghpw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:25:58.293: INFO: stderr: ""
May 22 07:25:58.293: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 07:25:58.293: INFO: validating pod update-demo-nautilus-gghpw
May 22 07:25:58.860: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 07:25:58.860: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 07:25:58.860: INFO: update-demo-nautilus-gghpw is verified up and running
STEP: rolling-update to new replication controller
May 22 07:25:58.861: INFO: scanned /root for discovery docs: <nil>
May 22 07:25:58.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9077'
May 22 07:26:23.377: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 22 07:26:23.377: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 07:26:23.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9077'
May 22 07:26:23.477: INFO: stderr: ""
May 22 07:26:23.477: INFO: stdout: "update-demo-kitten-qfdkr update-demo-kitten-xdjbh "
May 22 07:26:23.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-kitten-qfdkr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:26:23.570: INFO: stderr: ""
May 22 07:26:23.570: INFO: stdout: "true"
May 22 07:26:23.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-kitten-qfdkr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:26:23.650: INFO: stderr: ""
May 22 07:26:23.650: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/kitten-amd64:1.0"
May 22 07:26:23.650: INFO: validating pod update-demo-kitten-qfdkr
May 22 07:26:24.144: INFO: got data: {
  "image": "kitten.jpg"
}

May 22 07:26:24.144: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 22 07:26:24.144: INFO: update-demo-kitten-qfdkr is verified up and running
May 22 07:26:24.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-kitten-xdjbh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:26:24.235: INFO: stderr: ""
May 22 07:26:24.235: INFO: stdout: "true"
May 22 07:26:24.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-kitten-xdjbh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9077'
May 22 07:26:24.317: INFO: stderr: ""
May 22 07:26:24.317: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/kitten-amd64:1.0"
May 22 07:26:24.317: INFO: validating pod update-demo-kitten-xdjbh
May 22 07:26:24.442: INFO: got data: {
  "image": "kitten.jpg"
}

May 22 07:26:24.442: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 22 07:26:24.442: INFO: update-demo-kitten-xdjbh is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:26:24.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9077" for this suite.
May 22 07:27:06.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:27:06.563: INFO: namespace kubectl-9077 deletion completed in 42.113832301s

• [SLOW TEST:74.743 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:27:06.564: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 07:27:06.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-pod --generator=run-pod/v1 --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5494'
May 22 07:27:06.729: INFO: stderr: ""
May 22 07:27:06.729: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 22 07:27:26.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pod e2e-test-nginx-pod --namespace=kubectl-5494 -o json'
May 22 07:27:26.882: INFO: stderr: ""
May 22 07:27:26.882: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-22T07:27:06Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5494\",\n        \"resourceVersion\": \"12820439\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5494/pods/e2e-test-nginx-pod\",\n        \"uid\": \"00b79b29-7c63-11e9-8476-fa163e387bbe\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"hub.c.163.com/combk8s/nginx-amd64:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-hj5bx\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"node1\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-hj5bx\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-hj5bx\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T07:27:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T07:27:08Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T07:27:08Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-22T07:27:06Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://7d3c71c06a87badf417df5d0cc8ca955fe7d42623c146df70992bee766aaa8ad\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-22T07:27:08Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.177.10.19\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.177.10.62\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-22T07:27:06Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 22 07:27:26.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 replace -f - --namespace=kubectl-5494'
May 22 07:27:27.252: INFO: stderr: ""
May 22 07:27:27.252: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image hub.c.163.com/combk8s/busybox-amd64:1.29
[AfterEach] [k8s.io] Kubectl replace
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 22 07:27:27.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete pods e2e-test-nginx-pod --namespace=kubectl-5494'
May 22 07:27:38.342: INFO: stderr: ""
May 22 07:27:38.342: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:27:38.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5494" for this suite.
May 22 07:27:44.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:27:44.462: INFO: namespace kubectl-5494 deletion completed in 6.112060287s

• [SLOW TEST:37.898 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:27:44.462: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-173ee6db-7c63-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:27:44.526: INFO: Waiting up to 5m0s for pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1" in namespace "secrets-1009" to be "success or failure"
May 22 07:27:44.529: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51626ms
May 22 07:27:46.536: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009255701s
May 22 07:27:48.540: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013038534s
May 22 07:27:50.543: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016246834s
May 22 07:27:52.547: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020100656s
May 22 07:27:54.594: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.06707979s
May 22 07:27:56.602: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.075235252s
May 22 07:27:58.606: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079742385s
May 22 07:28:00.611: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.084075053s
STEP: Saw pod success
May 22 07:28:00.611: INFO: Pod "pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:28:00.614: INFO: Trying to get logs from node node2 pod pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 07:28:00.998: INFO: Waiting for pod pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:28:01.001: INFO: Pod pod-secrets-173fa8fa-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:28:01.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1009" for this suite.
May 22 07:28:07.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:28:07.110: INFO: namespace secrets-1009 deletion completed in 6.10280717s

• [SLOW TEST:22.648 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:28:07.110: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-24cb22b6-7c63-11e9-aefd-da1a35f02de1
STEP: Creating secret with name secret-projected-all-test-volume-24cb2298-7c63-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test Check all projections for projected volume plugin
May 22 07:28:07.267: INFO: Waiting up to 5m0s for pod "projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1" in namespace "projected-2515" to be "success or failure"
May 22 07:28:07.270: INFO: Pod "projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.229832ms
May 22 07:28:09.279: INFO: Pod "projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012044114s
May 22 07:28:11.284: INFO: Pod "projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016819712s
STEP: Saw pod success
May 22 07:28:11.284: INFO: Pod "projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:28:11.287: INFO: Trying to get logs from node node1 pod projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1 container projected-all-volume-test: <nil>
STEP: delete the pod
May 22 07:28:11.356: INFO: Waiting for pod projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:28:11.358: INFO: Pod projected-volume-24cb2265-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected combined
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:28:11.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2515" for this suite.
May 22 07:28:17.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:28:17.537: INFO: namespace projected-2515 deletion completed in 6.109127879s

• [SLOW TEST:10.427 seconds]
[sig-storage] Projected combined
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:28:17.537: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 22 07:28:17.613: INFO: Waiting up to 5m0s for pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1" in namespace "var-expansion-9299" to be "success or failure"
May 22 07:28:17.616: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.857841ms
May 22 07:28:19.620: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006561105s
May 22 07:28:21.625: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012132462s
May 22 07:28:23.629: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015937561s
May 22 07:28:25.633: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019797841s
May 22 07:28:27.637: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023430245s
May 22 07:28:29.641: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027813783s
May 22 07:28:31.645: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032375624s
May 22 07:28:33.650: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036680151s
May 22 07:28:35.654: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040853456s
May 22 07:28:37.658: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.044746063s
May 22 07:28:39.662: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.049101174s
May 22 07:28:41.666: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.053053196s
May 22 07:28:43.670: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.057082616s
May 22 07:28:45.674: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.060913964s
May 22 07:28:47.678: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.064764338s
May 22 07:28:49.682: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.06936251s
May 22 07:28:51.687: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.07356201s
May 22 07:28:53.691: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.077816824s
May 22 07:28:55.696: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.082978965s
May 22 07:28:57.700: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.087173118s
STEP: Saw pod success
May 22 07:28:57.700: INFO: Pod "var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:28:57.704: INFO: Trying to get logs from node node2 pod var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 07:28:57.727: INFO: Waiting for pod var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:28:57.729: INFO: Pod var-expansion-2af8c511-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:28:57.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9299" for this suite.
May 22 07:29:03.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:29:03.830: INFO: namespace var-expansion-9299 deletion completed in 6.09523978s

• [SLOW TEST:46.293 seconds]
[k8s.io] Variable Expansion
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:29:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-468c969d-7c63-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 07:29:03.888: INFO: Waiting up to 5m0s for pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1" in namespace "secrets-2806" to be "success or failure"
May 22 07:29:03.894: INFO: Pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.275817ms
May 22 07:29:05.898: INFO: Pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010444504s
May 22 07:29:07.902: INFO: Pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014504593s
May 22 07:29:09.907: INFO: Pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019585818s
STEP: Saw pod success
May 22 07:29:09.907: INFO: Pod "pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:29:09.910: INFO: Trying to get logs from node node1 pod pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 07:29:09.934: INFO: Waiting for pod pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:29:09.938: INFO: Pod pod-secrets-468d78f0-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:29:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2806" for this suite.
May 22 07:29:15.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:29:16.213: INFO: namespace secrets-2806 deletion completed in 6.268732719s

• [SLOW TEST:12.382 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:29:16.215: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 22 07:29:16.351: INFO: Waiting up to 5m0s for pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1" in namespace "emptydir-7095" to be "success or failure"
May 22 07:29:16.354: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.971716ms
May 22 07:29:18.358: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00690699s
May 22 07:29:20.362: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011067913s
May 22 07:29:22.367: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016053523s
May 22 07:29:24.372: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020452243s
May 22 07:29:26.374: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023206559s
May 22 07:29:28.378: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.026888622s
May 22 07:29:30.383: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.031662082s
STEP: Saw pod success
May 22 07:29:30.383: INFO: Pod "pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:29:30.386: INFO: Trying to get logs from node node1 pod pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:29:30.412: INFO: Waiting for pod pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:29:30.414: INFO: Pod pod-4dfb3aa2-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:29:30.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7095" for this suite.
May 22 07:29:36.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:29:36.548: INFO: namespace emptydir-7095 deletion completed in 6.128307249s

• [SLOW TEST:20.334 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:29:36.555: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 07:29:36.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --namespace=kubectl-367'
May 22 07:29:36.703: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 07:29:36.703: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 22 07:29:36.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete jobs e2e-test-nginx-job --namespace=kubectl-367'
May 22 07:29:36.819: INFO: stderr: ""
May 22 07:29:36.819: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:29:36.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-367" for this suite.
May 22 07:29:42.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:29:42.924: INFO: namespace kubectl-367 deletion completed in 6.099986418s

• [SLOW TEST:6.370 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:29:42.924: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 07:29:42.984: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 22 07:29:47.988: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 07:29:49.996: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 22 07:29:52.002: INFO: Creating deployment "test-rollover-deployment"
May 22 07:29:52.013: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 22 07:29:54.021: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 22 07:29:54.028: INFO: Ensure that both replica sets have 1 created replica
May 22 07:29:54.033: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 22 07:29:54.040: INFO: Updating deployment test-rollover-deployment
May 22 07:29:54.041: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 22 07:29:56.047: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 22 07:29:56.055: INFO: Make sure deployment "test-rollover-deployment" is complete
May 22 07:29:56.062: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:29:56.062: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106994, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:29:58.071: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:29:58.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106994, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:00.072: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:00.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106994, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:02.070: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:02.070: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106994, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:04.071: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:04.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106994, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:06.071: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:06.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694107004, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:08.072: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:08.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694107004, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:10.071: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:10.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694107004, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:12.071: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:12.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694107004, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:14.070: INFO: all replica sets need to contain the pod-template-hash label
May 22 07:30:14.071: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694107004, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694106992, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-5445cf55f4\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 07:30:16.071: INFO: 
May 22 07:30:16.071: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 07:30:16.083: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-5118,SelfLink:/apis/apps/v1/namespaces/deployment-5118/deployments/test-rollover-deployment,UID:633c8105-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12822392,Generation:2,CreationTimestamp:2019-05-22 07:29:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-22 07:29:52 +0000 UTC 2019-05-22 07:29:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-22 07:30:14 +0000 UTC 2019-05-22 07:29:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-5445cf55f4" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 22 07:30:16.087: INFO: New ReplicaSet "test-rollover-deployment-5445cf55f4" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5445cf55f4,GenerateName:,Namespace:deployment-5118,SelfLink:/apis/apps/v1/namespaces/deployment-5118/replicasets/test-rollover-deployment-5445cf55f4,UID:64737a66-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12822382,Generation:2,CreationTimestamp:2019-05-22 07:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5445cf55f4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 633c8105-7c63-11e9-9cf4-fa163ecd1d63 0xc000d2aab7 0xc000d2aab8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 5445cf55f4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5445cf55f4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 22 07:30:16.087: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 22 07:30:16.087: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-5118,SelfLink:/apis/apps/v1/namespaces/deployment-5118/replicasets/test-rollover-controller,UID:5ddb5d52-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12822391,Generation:2,CreationTimestamp:2019-05-22 07:29:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 633c8105-7c63-11e9-9cf4-fa163ecd1d63 0xc000d2a9e7 0xc000d2a9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 07:30:16.087: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-5118,SelfLink:/apis/apps/v1/namespaces/deployment-5118/replicasets/test-rollover-deployment-6455657675,UID:633eb7cc-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12822240,Generation:2,CreationTimestamp:2019-05-22 07:29:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 633c8105-7c63-11e9-9cf4-fa163ecd1d63 0xc000d2ab87 0xc000d2ab88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 07:30:16.091: INFO: Pod "test-rollover-deployment-5445cf55f4-b2lpt" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-5445cf55f4-b2lpt,GenerateName:test-rollover-deployment-5445cf55f4-,Namespace:deployment-5118,SelfLink:/api/v1/namespaces/deployment-5118/pods/test-rollover-deployment-5445cf55f4-b2lpt,UID:6481b3e6-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12822353,Generation:0,CreationTimestamp:2019-05-22 07:29:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 5445cf55f4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-5445cf55f4 64737a66-7c63-11e9-8476-fa163e387bbe 0xc000d2b727 0xc000d2b728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-d2fv7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-d2fv7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [{default-token-d2fv7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000d2b7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000d2b7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:29:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:29:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:29:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:29:54 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.3,StartTime:2019-05-22 07:29:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-22 07:29:55 +0000 UTC,} nil} {nil nil nil} true 0 hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 docker-pullable://hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64@sha256:9b5b1c1ec462abb4b89145a23a1fbf7eb3b2bb25927fc94e820f89a73029889f docker://70ef815020e1fc562a4637f397ca4b330d489eeaa419e0099711690c081de4a9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:30:16.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5118" for this suite.
May 22 07:30:22.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:30:22.266: INFO: namespace deployment-5118 deletion completed in 6.162056656s

• [SLOW TEST:39.341 seconds]
[sig-apps] Deployment
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:30:22.268: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-91
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 07:30:22.410: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 07:30:50.491: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.177.10.143:8080/dial?request=hostName&protocol=http&host=10.177.10.67&port=8080&tries=1'] Namespace:pod-network-test-91 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:30:50.492: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:30:50.794: INFO: Waiting for endpoints: map[]
May 22 07:30:50.798: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.177.10.143:8080/dial?request=hostName&protocol=http&host=10.177.11.94&port=8080&tries=1'] Namespace:pod-network-test-91 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:30:50.798: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:30:51.277: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:30:51.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-91" for this suite.
May 22 07:31:13.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:31:13.389: INFO: namespace pod-network-test-91 deletion completed in 22.10401285s

• [SLOW TEST:51.121 seconds]
[sig-network] Networking
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:31:13.389: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:31:13.449: INFO: Waiting up to 5m0s for pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1" in namespace "projected-1447" to be "success or failure"
May 22 07:31:13.452: INFO: Pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.216454ms
May 22 07:31:15.456: INFO: Pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007532118s
May 22 07:31:17.490: INFO: Pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04129393s
May 22 07:31:19.495: INFO: Pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04612877s
STEP: Saw pod success
May 22 07:31:19.495: INFO: Pod "downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:31:19.499: INFO: Trying to get logs from node node1 pod downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:31:19.601: INFO: Waiting for pod downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:31:19.603: INFO: Pod downwardapi-volume-93c66909-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:31:19.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1447" for this suite.
May 22 07:31:25.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:31:25.700: INFO: namespace projected-1447 deletion completed in 6.09120605s

• [SLOW TEST:12.311 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:31:25.701: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 22 07:31:25.757: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823472,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 07:31:25.757: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823473,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 22 07:31:25.757: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823474,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 22 07:31:35.790: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823620,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 07:31:35.791: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823621,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 22 07:31:35.791: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5888,SelfLink:/api/v1/namespaces/watch-5888/configmaps/e2e-watch-test-label-changed,UID:9b1bdd7d-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12823622,Generation:0,CreationTimestamp:2019-05-22 07:31:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:31:35.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5888" for this suite.
May 22 07:31:41.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:31:41.899: INFO: namespace watch-5888 deletion completed in 6.103757339s

• [SLOW TEST:16.199 seconds]
[sig-api-machinery] Watchers
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:31:41.900: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 22 07:31:41.948: INFO: Waiting up to 5m0s for pod "pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1" in namespace "emptydir-9060" to be "success or failure"
May 22 07:31:41.951: INFO: Pod "pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.160661ms
May 22 07:31:43.955: INFO: Pod "pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006456528s
May 22 07:31:45.960: INFO: Pod "pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011340309s
STEP: Saw pod success
May 22 07:31:45.960: INFO: Pod "pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:31:45.963: INFO: Trying to get logs from node node1 pod pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:31:46.022: INFO: Waiting for pod pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:31:46.025: INFO: Pod pod-a4c3aa5b-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:31:46.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9060" for this suite.
May 22 07:31:52.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:31:52.220: INFO: namespace emptydir-9060 deletion completed in 6.190098616s

• [SLOW TEST:10.320 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:31:52.221: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0522 07:32:02.417883      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 07:32:02.417: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:32:02.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6628" for this suite.
May 22 07:32:08.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:32:08.531: INFO: namespace gc-6628 deletion completed in 6.109932093s

• [SLOW TEST:16.310 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:32:08.531: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:32:08.616: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1" in namespace "downward-api-2297" to be "success or failure"
May 22 07:32:08.618: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3354ms
May 22 07:32:10.622: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005973954s
May 22 07:32:12.626: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010207692s
May 22 07:32:14.629: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013623259s
May 22 07:32:16.638: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021771239s
May 22 07:32:18.642: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02589706s
May 22 07:32:20.646: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029756445s
May 22 07:32:22.650: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033799258s
May 22 07:32:24.653: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.037204464s
STEP: Saw pod success
May 22 07:32:24.653: INFO: Pod "downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:32:24.656: INFO: Trying to get logs from node node1 pod downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:32:24.674: INFO: Waiting for pod downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:32:24.677: INFO: Pod downwardapi-volume-b4a8abb6-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:32:24.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2297" for this suite.
May 22 07:32:30.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:32:30.785: INFO: namespace downward-api-2297 deletion completed in 6.101790474s

• [SLOW TEST:22.254 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:32:30.785: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 22 07:32:30.857: INFO: Waiting up to 5m0s for pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1" in namespace "var-expansion-2997" to be "success or failure"
May 22 07:32:30.860: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.311865ms
May 22 07:32:32.864: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006233605s
May 22 07:32:34.867: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009489953s
May 22 07:32:36.870: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012742165s
May 22 07:32:38.874: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016040579s
May 22 07:32:40.881: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023885366s
May 22 07:32:42.899: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.04128093s
May 22 07:32:44.978: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.120651601s
STEP: Saw pod success
May 22 07:32:44.978: INFO: Pod "var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:32:45.008: INFO: Trying to get logs from node node1 pod var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 07:32:45.056: INFO: Waiting for pod var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:32:45.061: INFO: Pod var-expansion-c1eae26a-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:32:45.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2997" for this suite.
May 22 07:32:51.185: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:32:52.617: INFO: namespace var-expansion-2997 deletion completed in 7.543694756s

• [SLOW TEST:21.831 seconds]
[k8s.io] Variable Expansion
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:32:52.617: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 07:32:52.699: INFO: Creating deployment "nginx-deployment"
May 22 07:32:52.710: INFO: Waiting for observed generation 1
May 22 07:32:54.727: INFO: Waiting for all required pods to come up
May 22 07:32:54.737: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 22 07:33:44.758: INFO: Waiting for deployment "nginx-deployment" to complete
May 22 07:33:44.765: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 22 07:33:44.774: INFO: Updating deployment nginx-deployment
May 22 07:33:44.774: INFO: Waiting for observed generation 2
May 22 07:33:46.780: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 22 07:33:46.783: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 22 07:33:46.786: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 22 07:33:46.793: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 22 07:33:46.793: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 22 07:33:46.796: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 22 07:33:46.802: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 22 07:33:46.802: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 22 07:33:46.819: INFO: Updating deployment nginx-deployment
May 22 07:33:46.819: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 22 07:33:46.828: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 22 07:33:46.832: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 07:33:48.844: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-3159,SelfLink:/apis/apps/v1/namespaces/deployment-3159/deployments/nginx-deployment,UID:cef08021-7c63-11e9-9cf4-fa163ecd1d63,ResourceVersion:12826516,Generation:3,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-05-22 07:33:46 +0000 UTC 2019-05-22 07:33:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-22 07:33:46 +0000 UTC 2019-05-22 07:32:52 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

May 22 07:33:48.851: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-3159,SelfLink:/apis/apps/v1/namespaces/deployment-3159/replicasets/nginx-deployment-5f9595f595,UID:edfa5c92-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826512,Generation:3,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cef08021-7c63-11e9-9cf4-fa163ecd1d63 0xc0014a6107 0xc0014a6108}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 07:33:48.851: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 22 07:33:48.851: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4,GenerateName:,Namespace:deployment-3159,SelfLink:/apis/apps/v1/namespaces/deployment-3159/replicasets/nginx-deployment-588c9cdcf4,UID:cef19a2b-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826509,Generation:3,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment cef08021-7c63-11e9-9cf4-fa163ecd1d63 0xc0014a6027 0xc0014a6028}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 22 07:33:48.856: INFO: Pod "nginx-deployment-588c9cdcf4-49k9k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-49k9k,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-49k9k,UID:ef3c65de-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826503,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346ef57 0xc00346ef58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346efd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346eff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.856: INFO: Pod "nginx-deployment-588c9cdcf4-4v489" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-4v489,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-4v489,UID:cef5eb7b-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825525,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f077 0xc00346f078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f110}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:10.177.10.215,StartTime:2019-05-22 07:32:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fb2fc9945499e68ef1faed14ffc474e44f56160b0e9f8e499ed70ee4a74b90c7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.856: INFO: Pod "nginx-deployment-588c9cdcf4-54s7c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-54s7c,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-54s7c,UID:cefb802f-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825694,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f1e7 0xc00346f1e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.206,StartTime:2019-05-22 07:32:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f13f92abb93a56269cd375e7d874c478a9fc87ef2558e20e6d9bddf850d08e2d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.856: INFO: Pod "nginx-deployment-588c9cdcf4-5v59n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-5v59n,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-5v59n,UID:ef396e60-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826495,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f357 0xc00346f358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.856: INFO: Pod "nginx-deployment-588c9cdcf4-6pdgl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-6pdgl,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-6pdgl,UID:ef395263-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826499,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f547 0xc00346f548}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f6f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-9sdhf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-9sdhf,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-9sdhf,UID:ef3c4e18-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826500,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f8a7 0xc00346f8a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346f940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346f960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-cb77c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-cb77c,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-cb77c,UID:ceff3736-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826257,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346f9e7 0xc00346f9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fa60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346faf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.41,StartTime:2019-05-22 07:32:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c3e050057bcb49ce5397ca30cda716d45b8fa898228badf82a67ff91f36a9c70}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-d8n8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-d8n8n,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-d8n8n,UID:ef3c2acd-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826501,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346fbc7 0xc00346fbc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fc40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fc70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-dkfjl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-dkfjl,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-dkfjl,UID:cefb8ee6-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825773,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346fd97 0xc00346fd98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00346fe50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00346fea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:10.177.10.45,StartTime:2019-05-22 07:32:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e770bb94410267865424f8ae699d71ec70dc4d9ad877d8792ada66ca36ba1e89}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-jtnlx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-jtnlx,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-jtnlx,UID:ef395e59-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826493,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc00346ffe7 0xc00346ffe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32070} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32090}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-kvzss" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-kvzss,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-kvzss,UID:ef375ce2-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826555,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32117 0xc000f32118}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f321b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 hub.c.163.com/combk8s/nginx-amd64:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-mkcf8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-mkcf8,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-mkcf8,UID:ef374e9c-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826484,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32277 0xc000f32278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f322f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32310}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-nnzlv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-nnzlv,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-nnzlv,UID:cef84018-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825809,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32397 0xc000f32398}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:08 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:08 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.132,StartTime:2019-05-22 07:32:52 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:07 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5b82a39a0c6e1488021b77b842efa08806429945d9423329d4e2ad760a5c3709}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.857: INFO: Pod "nginx-deployment-588c9cdcf4-rhh92" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-rhh92,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-rhh92,UID:ef393b35-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826489,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32507 0xc000f32508}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f325a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.858: INFO: Pod "nginx-deployment-588c9cdcf4-s6ngc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-s6ngc,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-s6ngc,UID:ef3bdeb3-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826498,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32627 0xc000f32628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f326a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f326c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.859: INFO: Pod "nginx-deployment-588c9cdcf4-t6p78" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-t6p78,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-t6p78,UID:ceff2bdf-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826198,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32747 0xc000f32748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f327c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f327e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:10.177.10.168,StartTime:2019-05-22 07:32:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0b3ab879e5937ff4e2548c6b5428e99e5dd0f2a4c2427c1e7656c33a897d0add}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.859: INFO: Pod "nginx-deployment-588c9cdcf4-vd9jd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-vd9jd,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-vd9jd,UID:ef33ddf3-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826458,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f328b7 0xc000f328b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-588c9cdcf4-x2sfj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-x2sfj,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-x2sfj,UID:cef84f0c-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825541,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f329d7 0xc000f329d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:10.177.10.214,StartTime:2019-05-22 07:32:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a47e13ef30b094cb65bb1b43599fdab73d0147e6b6d09d1f5f8e46d5cbdac108}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-588c9cdcf4-xr88v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-xr88v,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-xr88v,UID:ef3c4fbe-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826502,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32b47 0xc000f32b48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-588c9cdcf4-zmjmk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-588c9cdcf4-zmjmk,GenerateName:nginx-deployment-588c9cdcf4-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-588c9cdcf4-zmjmk,UID:cefb7805-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12825954,Generation:0,CreationTimestamp:2019-05-22 07:32:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 588c9cdcf4,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-588c9cdcf4 cef19a2b-7c63-11e9-8476-fa163e387bbe 0xc000f32c87 0xc000f32c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:07 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:32:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:10.177.10.92,StartTime:2019-05-22 07:32:53 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-22 07:33:05 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5fcda5069b2a84fef27ca67b42158d04e3c04b34b882e180663698c5d70f0a6e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-257bw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-257bw,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-257bw,UID:ee0f3249-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826542,Generation:0,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f32df7 0xc000f32df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f32e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f32e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-2pk8p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-2pk8p,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-2pk8p,UID:ef353b99-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826485,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f32f87 0xc000f32f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33000} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33020}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-5g7qs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5g7qs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-5g7qs,UID:ee0e1555-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826398,Generation:0,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f330b7 0xc000f330b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-6ss6k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-6ss6k,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-6ss6k,UID:ef3a0512-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826560,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f331d7 0xc000f331d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f332b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-76wnf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-76wnf,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-76wnf,UID:ef39f791-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826494,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33387 0xc000f33388}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-bqgk9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bqgk9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-bqgk9,UID:ee0078c7-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826393,Generation:0,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f334b7 0xc000f334b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33530} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.860: INFO: Pod "nginx-deployment-5f9595f595-brcjr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-brcjr,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-brcjr,UID:ee007ccb-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826451,Generation:0,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f335d7 0xc000f335d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.861: INFO: Pod "nginx-deployment-5f9595f595-j2j7d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j2j7d,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-j2j7d,UID:ef3a02aa-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826564,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33747 0xc000f33748}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f337c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f337e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.861: INFO: Pod "nginx-deployment-5f9595f595-j59j7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j59j7,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-j59j7,UID:edfe02e8-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826443,Generation:0,CreationTimestamp:2019-05-22 07:33:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f338b7 0xc000f338b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33930} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:44 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.861: INFO: Pod "nginx-deployment-5f9595f595-mhbps" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-mhbps,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-mhbps,UID:ef3a0c31-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826497,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33a27 0xc000f33a28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.861: INFO: Pod "nginx-deployment-5f9595f595-pkv44" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pkv44,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-pkv44,UID:ef3cf593-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826506,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33b47 0xc000f33b48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.861: INFO: Pod "nginx-deployment-5f9595f595-tj9p8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tj9p8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-tj9p8,UID:ef37b1ac-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826488,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33c67 0xc000f33c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 22 07:33:48.864: INFO: Pod "nginx-deployment-5f9595f595-wwnz2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wwnz2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-3159,SelfLink:/api/v1/namespaces/deployment-3159/pods/nginx-deployment-5f9595f595-wwnz2,UID:ef37b599-7c63-11e9-8476-fa163e387bbe,ResourceVersion:12826571,Generation:0,CreationTimestamp:2019-05-22 07:33:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 edfa5c92-7c63-11e9-8476-fa163e387bbe 0xc000f33d87 0xc000f33d88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-stkrg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-stkrg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-stkrg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000f33e10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000f33e30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 07:33:46 +0000 UTC  }],Message:,Reason:,HostIP:10.177.10.19,PodIP:,StartTime:2019-05-22 07:33:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:33:48.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3159" for this suite.
May 22 07:33:56.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:33:57.310: INFO: namespace deployment-3159 deletion completed in 8.441268866s

• [SLOW TEST:64.693 seconds]
[sig-apps] Deployment
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:33:57.317: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 22 07:33:57.391: INFO: Waiting up to 5m0s for pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1" in namespace "emptydir-2467" to be "success or failure"
May 22 07:33:57.394: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.414956ms
May 22 07:33:59.400: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008540589s
May 22 07:34:01.403: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012294085s
May 22 07:34:03.408: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016531884s
May 22 07:34:05.413: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021651285s
May 22 07:34:07.417: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025409326s
May 22 07:34:09.420: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.029006748s
STEP: Saw pod success
May 22 07:34:09.420: INFO: Pod "pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:34:09.423: INFO: Trying to get logs from node node2 pod pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:34:09.788: INFO: Waiting for pod pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1 to disappear
May 22 07:34:09.791: INFO: Pod pod-f57ea97c-7c63-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:34:09.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2467" for this suite.
May 22 07:34:15.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:34:15.904: INFO: namespace emptydir-2467 deletion completed in 6.108374741s

• [SLOW TEST:18.587 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:34:15.906: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 22 07:34:32.483: INFO: Successfully updated pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1"
May 22 07:34:32.483: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1" in namespace "pods-3991" to be "terminated due to deadline exceeded"
May 22 07:34:32.486: INFO: Pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1": Phase="Running", Reason="", readiness=true. Elapsed: 2.806455ms
May 22 07:34:34.490: INFO: Pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006604971s
May 22 07:34:36.494: INFO: Pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01039707s
May 22 07:34:36.494: INFO: Pod "pod-update-activedeadlineseconds-008f9070-7c64-11e9-aefd-da1a35f02de1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:34:36.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3991" for this suite.
May 22 07:34:42.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:34:42.610: INFO: namespace pods-3991 deletion completed in 6.111095114s

• [SLOW TEST:26.704 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:34:42.610: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 22 07:34:42.954: INFO: Pod name wrapped-volume-race-10a4c1c5-7c64-11e9-aefd-da1a35f02de1: Found 0 pods out of 5
May 22 07:34:47.962: INFO: Pod name wrapped-volume-race-10a4c1c5-7c64-11e9-aefd-da1a35f02de1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-10a4c1c5-7c64-11e9-aefd-da1a35f02de1 in namespace emptydir-wrapper-6099, will wait for the garbage collector to delete the pods
May 22 07:35:36.068: INFO: Deleting ReplicationController wrapped-volume-race-10a4c1c5-7c64-11e9-aefd-da1a35f02de1 took: 9.482223ms
May 22 07:35:36.368: INFO: Terminating ReplicationController wrapped-volume-race-10a4c1c5-7c64-11e9-aefd-da1a35f02de1 pods took: 300.255139ms
STEP: Creating RC which spawns configmap-volume pods
May 22 07:36:40.394: INFO: Pod name wrapped-volume-race-56a432e6-7c64-11e9-aefd-da1a35f02de1: Found 0 pods out of 5
May 22 07:36:45.401: INFO: Pod name wrapped-volume-race-56a432e6-7c64-11e9-aefd-da1a35f02de1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-56a432e6-7c64-11e9-aefd-da1a35f02de1 in namespace emptydir-wrapper-6099, will wait for the garbage collector to delete the pods
May 22 07:37:01.501: INFO: Deleting ReplicationController wrapped-volume-race-56a432e6-7c64-11e9-aefd-da1a35f02de1 took: 9.126887ms
May 22 07:37:01.801: INFO: Terminating ReplicationController wrapped-volume-race-56a432e6-7c64-11e9-aefd-da1a35f02de1 pods took: 300.322329ms
STEP: Creating RC which spawns configmap-volume pods
May 22 07:37:39.120: INFO: Pod name wrapped-volume-race-79a62930-7c64-11e9-aefd-da1a35f02de1: Found 0 pods out of 5
May 22 07:37:44.126: INFO: Pod name wrapped-volume-race-79a62930-7c64-11e9-aefd-da1a35f02de1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-79a62930-7c64-11e9-aefd-da1a35f02de1 in namespace emptydir-wrapper-6099, will wait for the garbage collector to delete the pods
May 22 07:38:36.225: INFO: Deleting ReplicationController wrapped-volume-race-79a62930-7c64-11e9-aefd-da1a35f02de1 took: 14.307101ms
May 22 07:38:36.526: INFO: Terminating ReplicationController wrapped-volume-race-79a62930-7c64-11e9-aefd-da1a35f02de1 pods took: 300.29609ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:39:44.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6099" for this suite.
May 22 07:39:52.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:39:52.379: INFO: namespace emptydir-wrapper-6099 deletion completed in 8.120070084s

• [SLOW TEST:309.768 seconds]
[sig-storage] EmptyDir wrapper volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:39:52.379: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 07:39:52.457: INFO: Waiting up to 5m0s for pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1" in namespace "downward-api-7532" to be "success or failure"
May 22 07:39:52.460: INFO: Pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523171ms
May 22 07:39:54.464: INFO: Pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007294881s
May 22 07:39:56.472: INFO: Pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015097993s
May 22 07:39:58.476: INFO: Pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018866786s
STEP: Saw pod success
May 22 07:39:58.476: INFO: Pod "downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:39:58.479: INFO: Trying to get logs from node node2 pod downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 07:39:58.513: INFO: Waiting for pod downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1 to disappear
May 22 07:39:58.516: INFO: Pod downward-api-c9213307-7c64-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:39:58.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7532" for this suite.
May 22 07:40:04.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:40:04.643: INFO: namespace downward-api-7532 deletion completed in 6.106785152s

• [SLOW TEST:12.264 seconds]
[sig-node] Downward API
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:40:04.643: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 07:40:04.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --namespace=kubectl-4582'
May 22 07:40:05.109: INFO: stderr: ""
May 22 07:40:05.110: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 22 07:40:05.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete pods e2e-test-nginx-pod --namespace=kubectl-4582'
May 22 07:40:14.281: INFO: stderr: ""
May 22 07:40:14.281: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:40:14.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4582" for this suite.
May 22 07:40:20.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:40:20.395: INFO: namespace kubectl-4582 deletion completed in 6.10834687s

• [SLOW TEST:15.752 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:40:20.395: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 22 07:40:20.448: INFO: Waiting up to 5m0s for pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1" in namespace "emptydir-8485" to be "success or failure"
May 22 07:40:20.450: INFO: Pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.055928ms
May 22 07:40:22.455: INFO: Pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006624511s
May 22 07:40:24.459: INFO: Pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010635924s
May 22 07:40:26.494: INFO: Pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.04596999s
STEP: Saw pod success
May 22 07:40:26.494: INFO: Pod "pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:40:26.497: INFO: Trying to get logs from node node2 pod pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:40:26.599: INFO: Waiting for pod pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1 to disappear
May 22 07:40:26.602: INFO: Pod pod-d9d03a11-7c64-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:40:26.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8485" for this suite.
May 22 07:40:32.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:40:32.729: INFO: namespace emptydir-8485 deletion completed in 6.121438464s

• [SLOW TEST:12.333 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:40:32.729: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 07:41:27.610: INFO: Successfully updated pod "labelsupdatee12c8af1-7c64-11e9-aefd-da1a35f02de1"
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:41:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7516" for this suite.
May 22 07:41:51.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:41:51.766: INFO: namespace downward-api-7516 deletion completed in 22.124370857s

• [SLOW TEST:79.036 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:41:51.766: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 22 07:41:51.814: INFO: namespace kubectl-713
May 22 07:41:51.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-713'
May 22 07:41:52.050: INFO: stderr: ""
May 22 07:41:52.050: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 07:41:53.058: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:53.058: INFO: Found 0 / 1
May 22 07:41:54.055: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:54.055: INFO: Found 0 / 1
May 22 07:41:55.093: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:55.093: INFO: Found 0 / 1
May 22 07:41:56.090: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:56.090: INFO: Found 0 / 1
May 22 07:41:57.096: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:57.096: INFO: Found 0 / 1
May 22 07:41:58.056: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:58.056: INFO: Found 0 / 1
May 22 07:41:59.099: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:41:59.099: INFO: Found 0 / 1
May 22 07:42:00.055: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:00.055: INFO: Found 0 / 1
May 22 07:42:01.055: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:01.055: INFO: Found 0 / 1
May 22 07:42:02.102: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:02.102: INFO: Found 0 / 1
May 22 07:42:03.054: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:03.054: INFO: Found 0 / 1
May 22 07:42:04.093: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:04.093: INFO: Found 0 / 1
May 22 07:42:05.090: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:05.090: INFO: Found 0 / 1
May 22 07:42:06.055: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:06.055: INFO: Found 1 / 1
May 22 07:42:06.055: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 07:42:06.058: INFO: Selector matched 1 pods for map[app:redis]
May 22 07:42:06.058: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 07:42:06.058: INFO: wait on redis-master startup in kubectl-713 
May 22 07:42:06.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 logs redis-master-xm7ld redis-master --namespace=kubectl-713'
May 22 07:42:06.157: INFO: stderr: ""
May 22 07:42:06.157: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 07:41:53.728 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 07:41:53.729 # Server started, Redis version 3.2.12\n1:M 22 May 07:41:53.729 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 07:41:53.729 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 22 07:42:06.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-713'
May 22 07:42:06.278: INFO: stderr: ""
May 22 07:42:06.278: INFO: stdout: "service/rm2 exposed\n"
May 22 07:42:06.281: INFO: Service rm2 in namespace kubectl-713 found.
STEP: exposing service
May 22 07:42:08.291: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-713'
May 22 07:42:08.425: INFO: stderr: ""
May 22 07:42:08.425: INFO: stdout: "service/rm3 exposed\n"
May 22 07:42:08.429: INFO: Service rm3 in namespace kubectl-713 found.
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:42:10.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-713" for this suite.
May 22 07:42:32.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:42:32.631: INFO: namespace kubectl-713 deletion completed in 22.12361518s

• [SLOW TEST:40.866 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:42:32.635: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0522 07:43:03.279126      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 07:43:03.279: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:43:03.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5968" for this suite.
May 22 07:43:09.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:43:09.381: INFO: namespace gc-5968 deletion completed in 6.09694776s

• [SLOW TEST:36.746 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:43:09.381: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 22 07:43:09.529: INFO: Waiting up to 5m0s for pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1" in namespace "var-expansion-4828" to be "success or failure"
May 22 07:43:09.532: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.253203ms
May 22 07:43:11.535: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00582701s
May 22 07:43:13.540: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010122309s
May 22 07:43:15.543: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014010705s
May 22 07:43:17.547: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017122456s
May 22 07:43:19.552: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.022044628s
May 22 07:43:21.555: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025731076s
May 22 07:43:23.559: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.029230871s
May 22 07:43:25.562: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032272427s
May 22 07:43:27.565: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.035637251s
May 22 07:43:29.569: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.039676895s
STEP: Saw pod success
May 22 07:43:29.569: INFO: Pod "var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:43:29.573: INFO: Trying to get logs from node node1 pod var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 07:43:29.606: INFO: Waiting for pod var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1 to disappear
May 22 07:43:29.610: INFO: Pod var-expansion-3e97f411-7c65-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:43:29.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4828" for this suite.
May 22 07:43:35.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:43:35.714: INFO: namespace var-expansion-4828 deletion completed in 6.098594164s

• [SLOW TEST:26.333 seconds]
[k8s.io] Variable Expansion
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:43:35.714: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:43:35.767: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1" in namespace "projected-2778" to be "success or failure"
May 22 07:43:35.770: INFO: Pod "downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.574193ms
May 22 07:43:37.774: INFO: Pod "downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006520184s
May 22 07:43:39.779: INFO: Pod "downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011459391s
STEP: Saw pod success
May 22 07:43:39.779: INFO: Pod "downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:43:39.782: INFO: Trying to get logs from node node1 pod downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:43:39.831: INFO: Waiting for pod downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1 to disappear
May 22 07:43:39.834: INFO: Pod downwardapi-volume-4e3ba78c-7c65-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:43:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2778" for this suite.
May 22 07:43:45.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:43:45.949: INFO: namespace projected-2778 deletion completed in 6.108866647s

• [SLOW TEST:10.234 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:43:45.949: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:43:45.998: INFO: Waiting up to 5m0s for pod "downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1" in namespace "downward-api-5668" to be "success or failure"
May 22 07:43:46.001: INFO: Pod "downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413119ms
May 22 07:43:48.005: INFO: Pod "downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007119253s
May 22 07:43:50.009: INFO: Pod "downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011159772s
STEP: Saw pod success
May 22 07:43:50.009: INFO: Pod "downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:43:50.012: INFO: Trying to get logs from node node1 pod downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:43:50.048: INFO: Waiting for pod downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1 to disappear
May 22 07:43:50.092: INFO: Pod downwardapi-volume-54551928-7c65-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:43:50.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5668" for this suite.
May 22 07:43:56.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:43:56.211: INFO: namespace downward-api-5668 deletion completed in 6.112761751s

• [SLOW TEST:10.262 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:43:56.211: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3562
[It] Should recreate evicted statefulset [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3562
STEP: Creating statefulset with conflicting port in namespace statefulset-3562
STEP: Waiting until pod test-pod will start running in namespace statefulset-3562
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3562
May 22 07:44:00.373: INFO: Observed stateful pod in namespace: statefulset-3562, name: ss-0, uid: 5c28cfdb-7c65-11e9-8476-fa163e387bbe, status phase: Pending. Waiting for statefulset controller to delete.
May 22 07:44:11.676: INFO: Observed stateful pod in namespace: statefulset-3562, name: ss-0, uid: 5c28cfdb-7c65-11e9-8476-fa163e387bbe, status phase: Failed. Waiting for statefulset controller to delete.
May 22 07:44:11.691: INFO: Observed stateful pod in namespace: statefulset-3562, name: ss-0, uid: 5c28cfdb-7c65-11e9-8476-fa163e387bbe, status phase: Failed. Waiting for statefulset controller to delete.
May 22 07:44:11.699: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3562
STEP: Removing pod with conflicting port in namespace statefulset-3562
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3562 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 07:44:29.755: INFO: Deleting all statefulset in ns statefulset-3562
May 22 07:44:29.759: INFO: Scaling statefulset ss to 0
May 22 07:44:39.778: INFO: Waiting for statefulset status.replicas updated to 0
May 22 07:44:39.783: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:44:39.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3562" for this suite.
May 22 07:44:45.834: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:44:45.929: INFO: namespace statefulset-3562 deletion completed in 6.112223227s

• [SLOW TEST:49.718 seconds]
[sig-apps] StatefulSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:44:45.929: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 22 07:44:46.032: INFO: Waiting up to 5m0s for pod "pod-781dad8e-7c65-11e9-aefd-da1a35f02de1" in namespace "emptydir-1717" to be "success or failure"
May 22 07:44:46.035: INFO: Pod "pod-781dad8e-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.508615ms
May 22 07:44:48.040: INFO: Pod "pod-781dad8e-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007697795s
May 22 07:44:50.046: INFO: Pod "pod-781dad8e-7c65-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013874689s
STEP: Saw pod success
May 22 07:44:50.046: INFO: Pod "pod-781dad8e-7c65-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:44:50.049: INFO: Trying to get logs from node node2 pod pod-781dad8e-7c65-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:44:50.074: INFO: Waiting for pod pod-781dad8e-7c65-11e9-aefd-da1a35f02de1 to disappear
May 22 07:44:50.077: INFO: Pod pod-781dad8e-7c65-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:44:50.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1717" for this suite.
May 22 07:44:56.095: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:44:56.194: INFO: namespace emptydir-1717 deletion completed in 6.112671769s

• [SLOW TEST:10.265 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:44:56.199: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 07:46:04.258: INFO: Container started at 2019-05-22 07:44:57 +0000 UTC, pod became ready at 2019-05-22 07:45:26 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:46:04.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4093" for this suite.
May 22 07:46:42.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:46:42.366: INFO: namespace container-probe-4093 deletion completed in 38.102414733s

• [SLOW TEST:106.168 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:46:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-h7h6
STEP: Creating a pod to test atomic-volume-subpath
May 22 07:46:42.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-h7h6" in namespace "subpath-9757" to be "success or failure"
May 22 07:46:42.490: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829059ms
May 22 07:46:44.494: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00779996s
May 22 07:46:46.500: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01329378s
May 22 07:46:48.503: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016893133s
May 22 07:46:50.507: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 8.020493762s
May 22 07:46:52.510: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 10.023950395s
May 22 07:46:54.514: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 12.027336986s
May 22 07:46:56.518: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 14.031114177s
May 22 07:46:58.522: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 16.035273276s
May 22 07:47:00.526: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 18.039329461s
May 22 07:47:02.530: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 20.043948778s
May 22 07:47:04.534: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 22.047297241s
May 22 07:47:06.538: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 24.051475192s
May 22 07:47:08.541: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 26.054864305s
May 22 07:47:10.549: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 28.062414356s
May 22 07:47:12.553: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 30.066887985s
May 22 07:47:14.557: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 32.070644518s
May 22 07:47:16.562: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 34.075167272s
May 22 07:47:18.566: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 36.079590069s
May 22 07:47:20.570: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 38.083933533s
May 22 07:47:22.575: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 40.088453918s
May 22 07:47:24.579: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 42.092297535s
May 22 07:47:26.587: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 44.100200652s
May 22 07:47:28.592: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 46.105648008s
May 22 07:47:30.596: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 48.109073306s
May 22 07:47:32.694: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 50.207877406s
May 22 07:47:34.698: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 52.211637816s
May 22 07:47:36.702: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Running", Reason="", readiness=true. Elapsed: 54.215667099s
May 22 07:47:38.706: INFO: Pod "pod-subpath-test-configmap-h7h6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 56.219947416s
STEP: Saw pod success
May 22 07:47:38.707: INFO: Pod "pod-subpath-test-configmap-h7h6" satisfied condition "success or failure"
May 22 07:47:38.710: INFO: Trying to get logs from node node1 pod pod-subpath-test-configmap-h7h6 container test-container-subpath-configmap-h7h6: <nil>
STEP: delete the pod
May 22 07:47:39.113: INFO: Waiting for pod pod-subpath-test-configmap-h7h6 to disappear
May 22 07:47:39.116: INFO: Pod pod-subpath-test-configmap-h7h6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-h7h6
May 22 07:47:39.116: INFO: Deleting pod "pod-subpath-test-configmap-h7h6" in namespace "subpath-9757"
[AfterEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:47:39.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9757" for this suite.
May 22 07:47:45.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:47:45.237: INFO: namespace subpath-9757 deletion completed in 6.107399374s

• [SLOW TEST:62.871 seconds]
[sig-storage] Subpath
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:47:45.238: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 07:47:45.671: INFO: (0) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 385.924476ms)
May 22 07:47:45.677: INFO: (1) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.498522ms)
May 22 07:47:45.682: INFO: (2) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.227452ms)
May 22 07:47:45.692: INFO: (3) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.146578ms)
May 22 07:47:45.696: INFO: (4) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.033183ms)
May 22 07:47:45.700: INFO: (5) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.548117ms)
May 22 07:47:45.705: INFO: (6) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.31703ms)
May 22 07:47:45.709: INFO: (7) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.985785ms)
May 22 07:47:45.713: INFO: (8) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.944387ms)
May 22 07:47:45.717: INFO: (9) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.806101ms)
May 22 07:47:45.721: INFO: (10) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.795543ms)
May 22 07:47:45.725: INFO: (11) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.043827ms)
May 22 07:47:45.729: INFO: (12) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.695909ms)
May 22 07:47:45.732: INFO: (13) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.831237ms)
May 22 07:47:45.737: INFO: (14) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.79164ms)
May 22 07:47:45.741: INFO: (15) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.750311ms)
May 22 07:47:45.745: INFO: (16) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.774259ms)
May 22 07:47:45.749: INFO: (17) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.859457ms)
May 22 07:47:45.753: INFO: (18) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.950057ms)
May 22 07:47:45.756: INFO: (19) /api/v1/nodes/node1/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 3.406234ms)
[AfterEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:47:45.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2492" for this suite.
May 22 07:47:51.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:47:51.872: INFO: namespace proxy-2492 deletion completed in 6.110738128s

• [SLOW TEST:6.634 seconds]
[sig-network] Proxy
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:47:51.873: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 07:47:51.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-deployment --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9081'
May 22 07:47:52.098: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 07:47:52.098: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 22 07:47:56.142: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9081'
May 22 07:47:57.308: INFO: stderr: ""
May 22 07:47:57.308: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:47:57.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9081" for this suite.
May 22 07:48:03.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:48:03.418: INFO: namespace kubectl-9081 deletion completed in 6.097244025s

• [SLOW TEST:11.546 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:48:03.421: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-edcbda4e-7c65-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 07:48:03.473: INFO: Waiting up to 5m0s for pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1" in namespace "configmap-5733" to be "success or failure"
May 22 07:48:03.476: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.030786ms
May 22 07:48:05.480: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006710047s
May 22 07:48:07.485: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011325618s
May 22 07:48:09.489: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01605897s
May 22 07:48:11.493: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019436049s
May 22 07:48:13.499: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026037436s
May 22 07:48:15.503: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02946703s
May 22 07:48:17.507: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033434955s
May 22 07:48:19.511: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.03791944s
May 22 07:48:21.515: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.041915058s
May 22 07:48:23.519: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.045605869s
STEP: Saw pod success
May 22 07:48:23.519: INFO: Pod "pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:48:23.522: INFO: Trying to get logs from node node1 pod pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 07:48:23.544: INFO: Waiting for pod pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1 to disappear
May 22 07:48:23.546: INFO: Pod pod-configmaps-edcc95b3-7c65-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:48:23.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5733" for this suite.
May 22 07:48:29.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:48:29.658: INFO: namespace configmap-5733 deletion completed in 6.106886258s

• [SLOW TEST:26.238 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:48:29.661: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 22 07:48:51.733: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:51.733: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:51.877: INFO: Exec stderr: ""
May 22 07:48:51.877: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:51.877: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.024: INFO: Exec stderr: ""
May 22 07:48:52.024: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.024: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.205: INFO: Exec stderr: ""
May 22 07:48:52.205: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.205: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.312: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 22 07:48:52.312: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.312: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.420: INFO: Exec stderr: ""
May 22 07:48:52.420: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.420: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.537: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 22 07:48:52.537: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.537: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:52.985: INFO: Exec stderr: ""
May 22 07:48:52.985: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:52.985: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:53.108: INFO: Exec stderr: ""
May 22 07:48:53.108: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:53.108: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:53.211: INFO: Exec stderr: ""
May 22 07:48:53.212: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8941 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 07:48:53.212: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 07:48:53.331: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:48:53.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8941" for this suite.
May 22 07:49:37.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:49:37.449: INFO: namespace e2e-kubelet-etc-hosts-8941 deletion completed in 44.112341981s

• [SLOW TEST:67.788 seconds]
[k8s.io] KubeletManagedEtcHosts
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:49:37.450: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:50:37.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7121" for this suite.
May 22 07:50:59.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:50:59.671: INFO: namespace container-probe-7121 deletion completed in 22.152954094s

• [SLOW TEST:82.221 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:50:59.672: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-56e2c9a5-7c66-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 07:50:59.785: INFO: Waiting up to 5m0s for pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1" in namespace "configmap-9755" to be "success or failure"
May 22 07:50:59.788: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.427371ms
May 22 07:51:01.792: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006752377s
May 22 07:51:03.797: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011241444s
May 22 07:51:05.801: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015704621s
May 22 07:51:07.806: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020706923s
May 22 07:51:09.811: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.025588586s
STEP: Saw pod success
May 22 07:51:09.811: INFO: Pod "pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:51:09.814: INFO: Trying to get logs from node node1 pod pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 07:51:09.839: INFO: Waiting for pod pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1 to disappear
May 22 07:51:09.842: INFO: Pod pod-configmaps-56e37c16-7c66-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:51:09.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9755" for this suite.
May 22 07:51:15.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:51:16.921: INFO: namespace configmap-9755 deletion completed in 7.073163344s

• [SLOW TEST:17.250 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:51:16.922: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6247
May 22 07:51:39.665: INFO: Started pod liveness-http in namespace container-probe-6247
STEP: checking the pod's current state and verifying that restartCount is present
May 22 07:51:39.670: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:55:40.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6247" for this suite.
May 22 07:55:46.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:55:46.589: INFO: namespace container-probe-6247 deletion completed in 6.122847609s

• [SLOW TEST:269.667 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:55:46.589: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 22 07:56:13.031: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:13.033: INFO: Pod pod-with-poststart-http-hook still exists
May 22 07:56:15.033: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:15.037: INFO: Pod pod-with-poststart-http-hook still exists
May 22 07:56:17.033: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:17.044: INFO: Pod pod-with-poststart-http-hook still exists
May 22 07:56:19.033: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:19.038: INFO: Pod pod-with-poststart-http-hook still exists
May 22 07:56:21.033: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:21.091: INFO: Pod pod-with-poststart-http-hook still exists
May 22 07:56:23.038: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 22 07:56:23.148: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:56:23.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8846" for this suite.
May 22 07:56:45.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:56:45.327: INFO: namespace container-lifecycle-hook-8846 deletion completed in 22.123683021s

• [SLOW TEST:58.739 seconds]
[k8s.io] Container Lifecycle Hook
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:56:45.328: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-24e0d932-7c67-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 07:56:45.382: INFO: Waiting up to 5m0s for pod "pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1" in namespace "configmap-4195" to be "success or failure"
May 22 07:56:45.385: INFO: Pod "pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.740048ms
May 22 07:56:47.390: INFO: Pod "pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007406666s
May 22 07:56:49.395: INFO: Pod "pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012162008s
STEP: Saw pod success
May 22 07:56:49.395: INFO: Pod "pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:56:49.398: INFO: Trying to get logs from node node1 pod pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 07:56:49.683: INFO: Waiting for pod pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 07:56:49.686: INFO: Pod pod-configmaps-24e195eb-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:56:49.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4195" for this suite.
May 22 07:56:55.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:56:55.879: INFO: namespace configmap-4195 deletion completed in 6.186703922s

• [SLOW TEST:10.551 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:56:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 07:56:55.921: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 07:56:55.929: INFO: Waiting for terminating namespaces to be deleted...
May 22 07:56:55.931: INFO: 
Logging pods the kubelet thinks is on node node1 before test
May 22 07:56:55.940: INFO: lctest0-65749587df-q2k7f from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-xdqsj from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-wbbxl from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-6zljq from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-qxkwg from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-d5b7h from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: ds-vpccni-sc86k from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-ggkcv from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-h5h5t from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-dnjdn from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-qcsfh from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-6p7bt from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-fk4rh from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-pw6sn from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-blsdg from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-7rx2z from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-6s8ps from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-445zl from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: cleanlog-9dn5v from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container cleanlog ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-7s6hq from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-f692w from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: kube-proxy-q4fz8 from kube-system started at 2019-05-17 07:21:31 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container kube-proxy ready: true, restart count 0
May 22 07:56:55.940: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 07:05:07 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-kzn9g from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-4b28l from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-xxspc from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-ll99p from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-lnmsg from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-6557v from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: lctest0-65749587df-ndmp8 from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.940: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-lvq4t from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:56:55.940: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:56:55.940: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 07:56:55.940: INFO: 
Logging pods the kubelet thinks is on node node2 before test
May 22 07:56:55.949: INFO: lctest0-65749587df-g27qj from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: kube-proxy-6dqrx from kube-system started at 2019-05-17 07:22:10 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container kube-proxy ready: true, restart count 1
May 22 07:56:55.949: INFO: lctest0-65749587df-wh6gf from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: coredns-d549755c8-nfhl9 from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container coredns ready: true, restart count 1
May 22 07:56:55.949: INFO: lctest0-65749587df-b4jwz from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: sonobuoy-e2e-job-2312d67a818b448f from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container e2e ready: true, restart count 0
May 22 07:56:55.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-7lf6s from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-57nzp from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: coredns-d549755c8-l428d from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container coredns ready: true, restart count 1
May 22 07:56:55.949: INFO: lctest0-65749587df-t9gqh from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: ds-vpccni-xmznc from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-rlrrw from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-g55kg from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-hsb6m from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 07:56:55.949: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-b88dn from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: cleanlog-55lgl from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container cleanlog ready: true, restart count 2
May 22 07:56:55.949: INFO: lctest0-65749587df-2jdvl from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-2hnjz from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-k9c2t from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-c2f68 from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-b696g from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-pcgjt from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-gxqfn from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-x2wg9 from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-nmmx4 from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-dr7h5 from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-fd78f from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-m4bh2 from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-k6448 from default started at 2019-05-22 07:56:19 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.949: INFO: 	Container debian ready: true, restart count 0
May 22 07:56:55.949: INFO: lctest0-65749587df-ctm2r from default started at 2019-05-22 07:56:20 +0000 UTC (1 container statuses recorded)
May 22 07:56:55.950: INFO: 	Container debian ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15a0f23d418ce91c], Reason = [FailedScheduling], Message = [0/6 nodes are available: 1 node(s) were unschedulable, 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:56:56.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2422" for this suite.
May 22 07:57:03.001: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:57:03.085: INFO: namespace sched-pred-2422 deletion completed in 6.100694548s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.206 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:57:03.086: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 07:57:05.671: INFO: Successfully updated pod "annotationupdate2f765d39-7c67-11e9-aefd-da1a35f02de1"
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:57:07.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5458" for this suite.
May 22 07:57:29.734: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:57:29.879: INFO: namespace downward-api-5458 deletion completed in 22.159985587s

• [SLOW TEST:26.793 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:57:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-3258
May 22 07:57:53.942: INFO: Started pod liveness-http in namespace container-probe-3258
STEP: checking the pod's current state and verifying that restartCount is present
May 22 07:57:53.946: INFO: Initial restart count of pod liveness-http is 0
May 22 07:57:59.964: INFO: Restart count of pod container-probe-3258/liveness-http is now 1 (6.017861972s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:57:59.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3258" for this suite.
May 22 07:58:06.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:58:06.089: INFO: namespace container-probe-3258 deletion completed in 6.105888828s

• [SLOW TEST:36.209 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:58:06.089: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-345 to expose endpoints map[]
May 22 07:58:06.144: INFO: Get endpoints failed (3.465358ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
May 22 07:58:07.150: INFO: successfully validated that service multi-endpoint-test in namespace services-345 exposes endpoints map[] (1.009343578s elapsed)
STEP: Creating pod pod1 in namespace services-345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-345 to expose endpoints map[pod1:[100]]
May 22 07:58:11.712: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (4.544487311s elapsed, will retry)
May 22 07:58:17.301: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (10.133392668s elapsed, will retry)
May 22 07:58:22.337: INFO: Unexpected endpoints: found map[], expected map[pod1:[100]] (15.169491282s elapsed, will retry)
May 22 07:58:24.352: INFO: successfully validated that service multi-endpoint-test in namespace services-345 exposes endpoints map[pod1:[100]] (17.184743595s elapsed)
STEP: Creating pod pod2 in namespace services-345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-345 to expose endpoints map[pod1:[100] pod2:[101]]
May 22 07:58:28.411: INFO: Unexpected endpoints: found map[55a05c32-7c67-11e9-9cf4-fa163ecd1d63:[100]], expected map[pod1:[100] pod2:[101]] (4.049684522s elapsed, will retry)
May 22 07:58:29.423: INFO: successfully validated that service multi-endpoint-test in namespace services-345 exposes endpoints map[pod1:[100] pod2:[101]] (5.06169028s elapsed)
STEP: Deleting pod pod1 in namespace services-345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-345 to expose endpoints map[pod2:[101]]
May 22 07:58:30.444: INFO: successfully validated that service multi-endpoint-test in namespace services-345 exposes endpoints map[pod2:[101]] (1.013280436s elapsed)
STEP: Deleting pod pod2 in namespace services-345
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-345 to expose endpoints map[]
May 22 07:58:30.495: INFO: successfully validated that service multi-endpoint-test in namespace services-345 exposes endpoints map[] (4.724727ms elapsed)
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:58:30.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-345" for this suite.
May 22 07:58:52.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:58:52.657: INFO: namespace services-345 deletion completed in 22.12898444s
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:46.568 seconds]
[sig-network] Services
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:58:52.657: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 07:58:53.065: INFO: (0) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 357.657476ms)
May 22 07:58:53.073: INFO: (1) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.978688ms)
May 22 07:58:53.079: INFO: (2) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.940696ms)
May 22 07:58:53.086: INFO: (3) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.9058ms)
May 22 07:58:53.092: INFO: (4) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.875968ms)
May 22 07:58:53.098: INFO: (5) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.111642ms)
May 22 07:58:53.104: INFO: (6) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.773565ms)
May 22 07:58:53.110: INFO: (7) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.868604ms)
May 22 07:58:53.116: INFO: (8) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.653322ms)
May 22 07:58:53.121: INFO: (9) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.010618ms)
May 22 07:58:53.126: INFO: (10) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.434428ms)
May 22 07:58:53.132: INFO: (11) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.023272ms)
May 22 07:58:53.137: INFO: (12) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.434071ms)
May 22 07:58:53.145: INFO: (13) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.289694ms)
May 22 07:58:53.151: INFO: (14) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.15867ms)
May 22 07:58:53.155: INFO: (15) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.459725ms)
May 22 07:58:53.161: INFO: (16) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.792292ms)
May 22 07:58:53.166: INFO: (17) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.895193ms)
May 22 07:58:53.170: INFO: (18) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.684319ms)
May 22 07:58:53.175: INFO: (19) /api/v1/nodes/node1:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 4.538032ms)
[AfterEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:58:53.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2241" for this suite.
May 22 07:58:59.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:58:59.286: INFO: namespace proxy-2241 deletion completed in 6.105659036s

• [SLOW TEST:6.629 seconds]
[sig-network] Proxy
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:58:59.287: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 22 07:58:59.416: INFO: Waiting up to 5m0s for pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1" in namespace "emptydir-9239" to be "success or failure"
May 22 07:58:59.419: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200273ms
May 22 07:59:01.424: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008616441s
May 22 07:59:03.429: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012928817s
May 22 07:59:05.433: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016962556s
May 22 07:59:07.437: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021139185s
May 22 07:59:09.489: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.073272102s
STEP: Saw pod success
May 22 07:59:09.489: INFO: Pod "pod-74c554c3-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:59:09.492: INFO: Trying to get logs from node node1 pod pod-74c554c3-7c67-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 07:59:09.515: INFO: Waiting for pod pod-74c554c3-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 07:59:09.517: INFO: Pod pod-74c554c3-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:59:09.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9239" for this suite.
May 22 07:59:15.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:59:15.641: INFO: namespace emptydir-9239 deletion completed in 6.119153283s

• [SLOW TEST:16.355 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:59:15.642: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6637/configmap-test-7e812484-7c67-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 07:59:15.748: INFO: Waiting up to 5m0s for pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1" in namespace "configmap-6637" to be "success or failure"
May 22 07:59:15.750: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.834886ms
May 22 07:59:17.754: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006211948s
May 22 07:59:19.758: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009959028s
May 22 07:59:21.762: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014078233s
May 22 07:59:23.767: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.018854094s
STEP: Saw pod success
May 22 07:59:23.767: INFO: Pod "pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:59:23.772: INFO: Trying to get logs from node node2 pod pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1 container env-test: <nil>
STEP: delete the pod
May 22 07:59:23.799: INFO: Waiting for pod pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 07:59:23.802: INFO: Pod pod-configmaps-7e81ac86-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:59:23.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6637" for this suite.
May 22 07:59:29.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:59:29.940: INFO: namespace configmap-6637 deletion completed in 6.132578218s

• [SLOW TEST:14.299 seconds]
[sig-node] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:59:29.941: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 07:59:29.995: INFO: Waiting up to 5m0s for pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1" in namespace "projected-3008" to be "success or failure"
May 22 07:59:29.998: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.709272ms
May 22 07:59:32.001: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006305627s
May 22 07:59:34.004: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009221605s
May 22 07:59:36.013: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018066829s
May 22 07:59:38.018: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022816895s
May 22 07:59:40.022: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.026818382s
STEP: Saw pod success
May 22 07:59:40.022: INFO: Pod "downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 07:59:40.025: INFO: Trying to get logs from node node1 pod downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 07:59:40.101: INFO: Waiting for pod downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 07:59:40.104: INFO: Pod downwardapi-volume-86ff9d86-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 07:59:40.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3008" for this suite.
May 22 07:59:46.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 07:59:46.220: INFO: namespace projected-3008 deletion completed in 6.110692693s

• [SLOW TEST:16.279 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 07:59:46.221: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 22 07:59:46.271: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 22 07:59:46.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:46.901: INFO: stderr: ""
May 22 07:59:46.901: INFO: stdout: "service/redis-slave created\n"
May 22 07:59:46.901: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 22 07:59:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:47.208: INFO: stderr: ""
May 22 07:59:47.208: INFO: stdout: "service/redis-master created\n"
May 22 07:59:47.211: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 22 07:59:47.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:47.518: INFO: stderr: ""
May 22 07:59:47.518: INFO: stdout: "service/frontend created\n"
May 22 07:59:47.518: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: hub.c.163.com/combk8s/google-samples/gb-frontend-amd64:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 22 07:59:47.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:47.727: INFO: stderr: ""
May 22 07:59:47.727: INFO: stdout: "deployment.apps/frontend created\n"
May 22 07:59:47.727: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 22 07:59:47.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:47.930: INFO: stderr: ""
May 22 07:59:47.930: INFO: stdout: "deployment.apps/redis-master created\n"
May 22 07:59:47.930: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: hub.c.163.com/combk8s/google-samples/gb-redisslave-amd64:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 22 07:59:47.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-2380'
May 22 07:59:48.139: INFO: stderr: ""
May 22 07:59:48.139: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 22 07:59:48.139: INFO: Waiting for all frontend pods to be Running.
May 22 07:59:58.636: INFO: Waiting for frontend to serve content.
May 22 08:00:00.428: INFO: Trying to add a new entry to the guestbook.
May 22 08:00:00.547: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 22 08:00:00.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:03.457: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:03.457: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 22 08:00:03.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:04.199: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:04.199: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 22 08:00:04.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:04.918: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:04.918: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 22 08:00:04.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:05.564: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:05.564: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 22 08:00:05.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:05.933: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:05.933: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 22 08:00:05.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-2380'
May 22 08:00:06.613: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:00:06.613: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:00:06.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2380" for this suite.
May 22 08:00:44.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:00:44.730: INFO: namespace kubectl-2380 deletion completed in 38.106767606s

• [SLOW TEST:58.509 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:00:44.730: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 22 08:00:44.779: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-393362560 proxy --unix-socket=/tmp/kubectl-proxy-unix568314847/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:00:44.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3430" for this suite.
May 22 08:00:50.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:00:50.972: INFO: namespace kubectl-3430 deletion completed in 6.108281599s

• [SLOW TEST:6.241 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:00:50.972: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-b74ab5d2-7c67-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:00:51.027: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1" in namespace "projected-404" to be "success or failure"
May 22 08:00:51.029: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.319706ms
May 22 08:00:53.033: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00636435s
May 22 08:00:55.038: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010996949s
May 22 08:00:57.043: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015898139s
May 22 08:00:59.046: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019542224s
May 22 08:01:01.051: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023788993s
May 22 08:01:03.055: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02775346s
May 22 08:01:05.059: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032146491s
May 22 08:01:07.088: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.061164479s
STEP: Saw pod success
May 22 08:01:07.088: INFO: Pod "pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:01:07.091: INFO: Trying to get logs from node node2 pod pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 08:01:07.142: INFO: Waiting for pod pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 08:01:07.144: INFO: Pod pod-projected-secrets-b74c2436-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:01:07.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-404" for this suite.
May 22 08:01:13.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:01:13.257: INFO: namespace projected-404 deletion completed in 6.109104969s

• [SLOW TEST:22.285 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:01:13.258: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0522 08:01:23.324457      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 08:01:23.324: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:01:23.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-174" for this suite.
May 22 08:01:29.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:01:29.510: INFO: namespace gc-174 deletion completed in 6.180703642s

• [SLOW TEST:16.253 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:01:29.511: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 08:01:29.574: INFO: Waiting up to 5m0s for pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1" in namespace "downward-api-3839" to be "success or failure"
May 22 08:01:29.577: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.616151ms
May 22 08:01:31.581: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006980273s
May 22 08:01:33.586: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011248659s
May 22 08:01:35.589: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014863908s
May 22 08:01:37.593: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01881454s
May 22 08:01:39.599: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024184424s
May 22 08:01:41.603: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028719206s
May 22 08:01:43.607: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.032811981s
May 22 08:01:45.611: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.036264789s
May 22 08:01:47.615: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.040170979s
May 22 08:01:49.620: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.045163322s
May 22 08:01:51.623: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.048623758s
May 22 08:01:53.631: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.056179632s
May 22 08:01:55.634: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.059233204s
May 22 08:01:57.638: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.063450087s
May 22 08:01:59.643: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.068084925s
STEP: Saw pod success
May 22 08:01:59.643: INFO: Pod "downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:01:59.646: INFO: Trying to get logs from node node2 pod downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 08:02:00.035: INFO: Waiting for pod downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1 to disappear
May 22 08:02:00.100: INFO: Pod downward-api-ce45aeb6-7c67-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:02:00.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3839" for this suite.
May 22 08:02:06.125: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:02:06.198: INFO: namespace downward-api-3839 deletion completed in 6.089238104s

• [SLOW TEST:36.687 seconds]
[sig-node] Downward API
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:02:06.198: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:02:06.368: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 22 08:02:06.380: INFO: Number of nodes with available pods: 0
May 22 08:02:06.380: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 22 08:02:06.406: INFO: Number of nodes with available pods: 0
May 22 08:02:06.406: INFO: Node node1 is running more than one daemon pod
May 22 08:02:07.409: INFO: Number of nodes with available pods: 0
May 22 08:02:07.409: INFO: Node node1 is running more than one daemon pod
May 22 08:02:08.410: INFO: Number of nodes with available pods: 0
May 22 08:02:08.410: INFO: Node node1 is running more than one daemon pod
May 22 08:02:09.410: INFO: Number of nodes with available pods: 0
May 22 08:02:09.410: INFO: Node node1 is running more than one daemon pod
May 22 08:02:10.411: INFO: Number of nodes with available pods: 0
May 22 08:02:10.411: INFO: Node node1 is running more than one daemon pod
May 22 08:02:11.410: INFO: Number of nodes with available pods: 0
May 22 08:02:11.411: INFO: Node node1 is running more than one daemon pod
May 22 08:02:12.410: INFO: Number of nodes with available pods: 0
May 22 08:02:12.410: INFO: Node node1 is running more than one daemon pod
May 22 08:02:13.410: INFO: Number of nodes with available pods: 0
May 22 08:02:13.410: INFO: Node node1 is running more than one daemon pod
May 22 08:02:14.409: INFO: Number of nodes with available pods: 0
May 22 08:02:14.409: INFO: Node node1 is running more than one daemon pod
May 22 08:02:15.409: INFO: Number of nodes with available pods: 0
May 22 08:02:15.409: INFO: Node node1 is running more than one daemon pod
May 22 08:02:16.409: INFO: Number of nodes with available pods: 0
May 22 08:02:16.409: INFO: Node node1 is running more than one daemon pod
May 22 08:02:17.424: INFO: Number of nodes with available pods: 0
May 22 08:02:17.424: INFO: Node node1 is running more than one daemon pod
May 22 08:02:18.436: INFO: Number of nodes with available pods: 1
May 22 08:02:18.436: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 22 08:02:18.542: INFO: Number of nodes with available pods: 1
May 22 08:02:18.542: INFO: Number of running nodes: 0, number of available pods: 1
May 22 08:02:19.545: INFO: Number of nodes with available pods: 0
May 22 08:02:19.545: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 22 08:02:19.554: INFO: Number of nodes with available pods: 0
May 22 08:02:19.554: INFO: Node node1 is running more than one daemon pod
May 22 08:02:20.559: INFO: Number of nodes with available pods: 0
May 22 08:02:20.560: INFO: Node node1 is running more than one daemon pod
May 22 08:02:21.559: INFO: Number of nodes with available pods: 0
May 22 08:02:21.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:22.559: INFO: Number of nodes with available pods: 0
May 22 08:02:22.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:23.558: INFO: Number of nodes with available pods: 0
May 22 08:02:23.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:24.558: INFO: Number of nodes with available pods: 0
May 22 08:02:24.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:25.560: INFO: Number of nodes with available pods: 0
May 22 08:02:25.560: INFO: Node node1 is running more than one daemon pod
May 22 08:02:26.558: INFO: Number of nodes with available pods: 0
May 22 08:02:26.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:27.558: INFO: Number of nodes with available pods: 0
May 22 08:02:27.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:28.588: INFO: Number of nodes with available pods: 0
May 22 08:02:28.588: INFO: Node node1 is running more than one daemon pod
May 22 08:02:29.558: INFO: Number of nodes with available pods: 0
May 22 08:02:29.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:30.559: INFO: Number of nodes with available pods: 0
May 22 08:02:30.560: INFO: Node node1 is running more than one daemon pod
May 22 08:02:31.558: INFO: Number of nodes with available pods: 0
May 22 08:02:31.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:32.559: INFO: Number of nodes with available pods: 0
May 22 08:02:32.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:33.558: INFO: Number of nodes with available pods: 0
May 22 08:02:33.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:34.558: INFO: Number of nodes with available pods: 0
May 22 08:02:34.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:35.559: INFO: Number of nodes with available pods: 0
May 22 08:02:35.559: INFO: Node node1 is running more than one daemon pod
May 22 08:02:36.558: INFO: Number of nodes with available pods: 0
May 22 08:02:36.558: INFO: Node node1 is running more than one daemon pod
May 22 08:02:37.559: INFO: Number of nodes with available pods: 1
May 22 08:02:37.559: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2966, will wait for the garbage collector to delete the pods
May 22 08:02:37.631: INFO: Deleting DaemonSet.extensions daemon-set took: 11.170069ms
May 22 08:02:37.931: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.338107ms
May 22 08:02:48.435: INFO: Number of nodes with available pods: 0
May 22 08:02:48.435: INFO: Number of running nodes: 0, number of available pods: 0
May 22 08:02:48.440: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2966/daemonsets","resourceVersion":"12850995"},"items":null}

May 22 08:02:48.443: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2966/pods","resourceVersion":"12850995"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:02:48.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2966" for this suite.
May 22 08:02:54.485: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:02:54.591: INFO: namespace daemonsets-2966 deletion completed in 6.120490598s

• [SLOW TEST:48.392 seconds]
[sig-apps] Daemon set [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:02:54.591: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 22 08:02:54.699: INFO: Waiting up to 5m0s for pod "pod-0102f30a-7c68-11e9-aefd-da1a35f02de1" in namespace "emptydir-1913" to be "success or failure"
May 22 08:02:54.701: INFO: Pod "pod-0102f30a-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.824637ms
May 22 08:02:56.709: INFO: Pod "pod-0102f30a-7c68-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010378727s
STEP: Saw pod success
May 22 08:02:56.709: INFO: Pod "pod-0102f30a-7c68-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:02:56.712: INFO: Trying to get logs from node node2 pod pod-0102f30a-7c68-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:02:56.733: INFO: Waiting for pod pod-0102f30a-7c68-11e9-aefd-da1a35f02de1 to disappear
May 22 08:02:56.736: INFO: Pod pod-0102f30a-7c68-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:02:56.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1913" for this suite.
May 22 08:03:02.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:03:02.892: INFO: namespace emptydir-1913 deletion completed in 6.151279589s

• [SLOW TEST:8.301 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:03:02.892: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 22 08:03:02.941: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 22 08:03:02.953: INFO: Waiting for terminating namespaces to be deleted...
May 22 08:03:02.957: INFO: 
Logging pods the kubelet thinks is on node node1 before test
May 22 08:03:02.971: INFO: lctest0-65749587df-n44g7 from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.971: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.971: INFO: lctest0-65749587df-sb4q9 from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.971: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.971: INFO: lctest0-65749587df-n8l7l from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.971: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.971: INFO: lctest0-65749587df-f8l9b from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.972: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.972: INFO: lctest0-65749587df-tfcdf from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.972: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.972: INFO: ds-vpccni-sc86k from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.972: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 08:03:02.972: INFO: lctest0-65749587df-868rc from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.972: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.972: INFO: lctest0-65749587df-b8ksx from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.973: INFO: lctest0-65749587df-wmbhc from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.973: INFO: lctest0-65749587df-v58p7 from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.973: INFO: lctest0-65749587df-9wv7w from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.973: INFO: lctest0-65749587df-sqhwv from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.973: INFO: cleanlog-9dn5v from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.973: INFO: 	Container cleanlog ready: true, restart count 0
May 22 08:03:02.974: INFO: lctest0-65749587df-vnlsl from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.974: INFO: lctest0-65749587df-7jfxr from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.974: INFO: kube-proxy-q4fz8 from kube-system started at 2019-05-17 07:21:31 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container kube-proxy ready: true, restart count 0
May 22 08:03:02.974: INFO: lctest0-65749587df-bt5st from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.974: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-22 07:05:07 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 22 08:03:02.974: INFO: lctest0-65749587df-lfwch from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.974: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.974: INFO: lctest0-65749587df-zfcrk from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.975: INFO: lctest0-65749587df-gf8b4 from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.975: INFO: lctest0-65749587df-gcndz from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.975: INFO: lctest0-65749587df-22c2t from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.975: INFO: lctest0-65749587df-gw67f from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.975: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-lvq4t from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 08:03:02.975: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 08:03:02.975: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 08:03:02.976: INFO: 
Logging pods the kubelet thinks is on node node2 before test
May 22 08:03:02.988: INFO: coredns-d549755c8-nfhl9 from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.988: INFO: 	Container coredns ready: true, restart count 1
May 22 08:03:02.988: INFO: lctest0-65749587df-gq2ql from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.988: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.988: INFO: sonobuoy-e2e-job-2312d67a818b448f from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 08:03:02.988: INFO: 	Container e2e ready: true, restart count 0
May 22 08:03:02.988: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 08:03:02.988: INFO: lctest0-65749587df-gsdxc from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.988: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.988: INFO: lctest0-65749587df-sbgxg from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.989: INFO: lctest0-65749587df-4d8nx from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.989: INFO: lctest0-65749587df-66srn from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.989: INFO: coredns-d549755c8-l428d from kube-system started at 2019-05-15 04:40:07 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container coredns ready: true, restart count 1
May 22 08:03:02.989: INFO: ds-vpccni-xmznc from kube-system started at 2019-05-21 13:00:58 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container ds-vpccni ready: true, restart count 0
May 22 08:03:02.989: INFO: sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-hsb6m from heptio-sonobuoy started at 2019-05-22 07:05:11 +0000 UTC (2 container statuses recorded)
May 22 08:03:02.989: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 22 08:03:02.989: INFO: 	Container systemd-logs ready: true, restart count 0
May 22 08:03:02.990: INFO: lctest0-65749587df-f96mq from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.990: INFO: lctest0-65749587df-gqnvs from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.990: INFO: cleanlog-55lgl from kube-system started at 2019-04-17 06:09:38 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container cleanlog ready: true, restart count 2
May 22 08:03:02.990: INFO: lctest0-65749587df-mgwnq from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.990: INFO: lctest0-65749587df-nrfwf from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.990: INFO: lctest0-65749587df-hkkfz from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.990: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.990: INFO: lctest0-65749587df-kwbhd from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.991: INFO: lctest0-65749587df-msbls from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.991: INFO: lctest0-65749587df-zswss from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.991: INFO: lctest0-65749587df-w4c5d from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.991: INFO: lctest0-65749587df-6tmf6 from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.991: INFO: lctest0-65749587df-r5rkd from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.991: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.992: INFO: lctest0-65749587df-gj795 from default started at 2019-05-22 08:02:14 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.992: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.992: INFO: kube-proxy-6dqrx from kube-system started at 2019-05-17 07:22:10 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.992: INFO: 	Container kube-proxy ready: true, restart count 1
May 22 08:03:02.992: INFO: lctest0-65749587df-5sq4z from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.992: INFO: 	Container debian ready: true, restart count 0
May 22 08:03:02.992: INFO: lctest0-65749587df-wss4r from default started at 2019-05-22 08:02:13 +0000 UTC (1 container statuses recorded)
May 22 08:03:02.992: INFO: 	Container debian ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-0869437a-7c68-11e9-aefd-da1a35f02de1 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-0869437a-7c68-11e9-aefd-da1a35f02de1 off the node node1
STEP: verifying the node doesn't have the label kubernetes.io/e2e-0869437a-7c68-11e9-aefd-da1a35f02de1
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:03:11.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3934" for this suite.
May 22 08:03:21.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:03:21.295: INFO: namespace sched-pred-3934 deletion completed in 10.130358699s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:18.403 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:03:21.295: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 22 08:03:21.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-58'
May 22 08:03:22.432: INFO: stderr: ""
May 22 08:03:22.432: INFO: stdout: "pod/pause created\n"
May 22 08:03:22.432: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 22 08:03:22.432: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-58" to be "running and ready"
May 22 08:03:22.436: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.96822ms
May 22 08:03:24.440: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008617596s
May 22 08:03:26.444: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012485346s
May 22 08:03:28.448: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 6.016567684s
May 22 08:03:28.448: INFO: Pod "pause" satisfied condition "running and ready"
May 22 08:03:28.448: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 22 08:03:28.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 label pods pause testing-label=testing-label-value --namespace=kubectl-58'
May 22 08:03:28.569: INFO: stderr: ""
May 22 08:03:28.569: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 22 08:03:28.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pod pause -L testing-label --namespace=kubectl-58'
May 22 08:03:28.653: INFO: stderr: ""
May 22 08:03:28.653: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 22 08:03:28.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 label pods pause testing-label- --namespace=kubectl-58'
May 22 08:03:28.772: INFO: stderr: ""
May 22 08:03:28.772: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 22 08:03:28.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pod pause -L testing-label --namespace=kubectl-58'
May 22 08:03:28.887: INFO: stderr: ""
May 22 08:03:28.887: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          6s    \n"
[AfterEach] [k8s.io] Kubectl label
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 22 08:03:28.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-58'
May 22 08:03:29.003: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:03:29.003: INFO: stdout: "pod \"pause\" force deleted\n"
May 22 08:03:29.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=pause --no-headers --namespace=kubectl-58'
May 22 08:03:29.132: INFO: stderr: "No resources found.\n"
May 22 08:03:29.132: INFO: stdout: ""
May 22 08:03:29.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=pause --namespace=kubectl-58 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 08:03:29.249: INFO: stderr: ""
May 22 08:03:29.249: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:03:29.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-58" for this suite.
May 22 08:03:35.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:03:35.367: INFO: namespace kubectl-58 deletion completed in 6.10781864s

• [SLOW TEST:14.071 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:03:35.367: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1440
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 08:03:35.413: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 08:04:01.483: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.177.11.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1440 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 08:04:01.483: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 08:04:02.600: INFO: Found all expected endpoints: [netserver-0]
May 22 08:04:02.606: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.177.10.239 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1440 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 08:04:02.606: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 08:04:03.723: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:04:03.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1440" for this suite.
May 22 08:04:25.747: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:04:25.819: INFO: namespace pod-network-test-1440 deletion completed in 22.086306778s

• [SLOW TEST:50.452 seconds]
[sig-network] Networking
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:04:25.821: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-376278fe-7c68-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:04:25.928: INFO: Waiting up to 5m0s for pod "pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1" in namespace "secrets-2840" to be "success or failure"
May 22 08:04:25.935: INFO: Pod "pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.452527ms
May 22 08:04:27.939: INFO: Pod "pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010497717s
STEP: Saw pod success
May 22 08:04:27.939: INFO: Pod "pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:04:27.942: INFO: Trying to get logs from node node1 pod pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1 container secret-env-test: <nil>
STEP: delete the pod
May 22 08:04:27.979: INFO: Waiting for pod pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1 to disappear
May 22 08:04:27.981: INFO: Pod pod-secrets-37634a2e-7c68-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:04:27.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2840" for this suite.
May 22 08:04:34.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:04:34.171: INFO: namespace secrets-2840 deletion completed in 6.185003328s

• [SLOW TEST:8.350 seconds]
[sig-api-machinery] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:04:34.172: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 22 08:04:34.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-6262'
May 22 08:04:34.673: INFO: stderr: ""
May 22 08:04:34.673: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 22 08:04:35.677: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:04:35.677: INFO: Found 0 / 1
May 22 08:04:36.689: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:04:36.689: INFO: Found 0 / 1
May 22 08:04:37.678: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:04:37.678: INFO: Found 1 / 1
May 22 08:04:37.678: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 08:04:37.681: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:04:37.681: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 22 08:04:37.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 logs redis-master-bpq6t redis-master --namespace=kubectl-6262'
May 22 08:04:37.800: INFO: stderr: ""
May 22 08:04:37.800: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 08:04:35.895 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 08:04:35.896 # Server started, Redis version 3.2.12\n1:M 22 May 08:04:35.896 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 08:04:35.896 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 22 08:04:37.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 log redis-master-bpq6t redis-master --namespace=kubectl-6262 --tail=1'
May 22 08:04:37.892: INFO: stderr: ""
May 22 08:04:37.892: INFO: stdout: "1:M 22 May 08:04:35.896 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 22 08:04:37.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 log redis-master-bpq6t redis-master --namespace=kubectl-6262 --limit-bytes=1'
May 22 08:04:37.982: INFO: stderr: ""
May 22 08:04:37.982: INFO: stdout: " "
STEP: exposing timestamps
May 22 08:04:37.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 log redis-master-bpq6t redis-master --namespace=kubectl-6262 --tail=1 --timestamps'
May 22 08:04:38.077: INFO: stderr: ""
May 22 08:04:38.077: INFO: stdout: "2019-05-22T08:04:35.896318565Z 1:M 22 May 08:04:35.896 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 22 08:04:40.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 log redis-master-bpq6t redis-master --namespace=kubectl-6262 --since=1s'
May 22 08:04:40.691: INFO: stderr: ""
May 22 08:04:40.691: INFO: stdout: ""
May 22 08:04:40.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 log redis-master-bpq6t redis-master --namespace=kubectl-6262 --since=24h'
May 22 08:04:40.800: INFO: stderr: ""
May 22 08:04:40.800: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 May 08:04:35.895 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 May 08:04:35.896 # Server started, Redis version 3.2.12\n1:M 22 May 08:04:35.896 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 May 08:04:35.896 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 22 08:04:40.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-6262'
May 22 08:04:40.892: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 08:04:40.892: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 22 08:04:40.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=nginx --no-headers --namespace=kubectl-6262'
May 22 08:04:40.995: INFO: stderr: "No resources found.\n"
May 22 08:04:40.995: INFO: stdout: ""
May 22 08:04:40.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=nginx --namespace=kubectl-6262 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 08:04:41.070: INFO: stderr: ""
May 22 08:04:41.070: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:04:41.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6262" for this suite.
May 22 08:05:03.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:05:03.304: INFO: namespace kubectl-6262 deletion completed in 22.228757886s

• [SLOW TEST:29.132 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:05:03.305: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:05:32.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2125" for this suite.
May 22 08:05:38.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:05:38.964: INFO: namespace container-runtime-2125 deletion completed in 6.188586632s

• [SLOW TEST:35.660 seconds]
[k8s.io] Container Runtime
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /root/workspace/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:05:38.965: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 22 08:05:39.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 api-versions'
May 22 08:05:39.194: INFO: stderr: ""
May 22 08:05:39.194: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:05:39.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-804" for this suite.
May 22 08:05:45.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:05:45.374: INFO: namespace kubectl-804 deletion completed in 6.172586119s

• [SLOW TEST:6.409 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:05:45.374: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 08:05:45.417: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:05:49.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-128" for this suite.
May 22 08:06:11.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:06:11.660: INFO: namespace init-container-128 deletion completed in 22.215869002s

• [SLOW TEST:26.287 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:06:11.662: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 08:06:11.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-rc --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --generator=run/v1 --namespace=kubectl-5820'
May 22 08:06:11.806: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 08:06:11.806: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 22 08:06:11.812: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 22 08:06:11.815: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 22 08:06:11.821: INFO: scanned /root for discovery docs: <nil>
May 22 08:06:11.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 rolling-update e2e-test-nginx-rc --update-period=1s --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5820'
May 22 08:06:29.722: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 22 08:06:29.722: INFO: stdout: "Created e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086\nScaling up e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 22 08:06:29.722: INFO: stdout: "Created e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086\nScaling up e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 22 08:06:29.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5820'
May 22 08:06:29.824: INFO: stderr: ""
May 22 08:06:29.824: INFO: stdout: "e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086-7p6xf "
May 22 08:06:29.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086-7p6xf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5820'
May 22 08:06:29.911: INFO: stderr: ""
May 22 08:06:29.911: INFO: stdout: "true"
May 22 08:06:29.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086-7p6xf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5820'
May 22 08:06:29.991: INFO: stderr: ""
May 22 08:06:29.991: INFO: stdout: "hub.c.163.com/combk8s/nginx-amd64:1.14-alpine"
May 22 08:06:29.991: INFO: e2e-test-nginx-rc-558f1e07d9a5373716e0334f3ff2e086-7p6xf is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 22 08:06:29.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete rc e2e-test-nginx-rc --namespace=kubectl-5820'
May 22 08:06:30.086: INFO: stderr: ""
May 22 08:06:30.086: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:06:30.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5820" for this suite.
May 22 08:06:52.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:06:52.235: INFO: namespace kubectl-5820 deletion completed in 22.136816161s

• [SLOW TEST:40.573 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:06:52.235: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-5725/configmap-test-8e9f9226-7c68-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:06:52.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1" in namespace "configmap-5725" to be "success or failure"
May 22 08:06:52.298: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.369836ms
May 22 08:06:54.304: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014382252s
May 22 08:06:56.308: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018476991s
May 22 08:06:58.311: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021877495s
May 22 08:07:00.315: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025980433s
May 22 08:07:02.320: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030886553s
May 22 08:07:04.325: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.035436381s
STEP: Saw pod success
May 22 08:07:04.325: INFO: Pod "pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:07:04.329: INFO: Trying to get logs from node node2 pod pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1 container env-test: <nil>
STEP: delete the pod
May 22 08:07:04.638: INFO: Waiting for pod pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1 to disappear
May 22 08:07:04.640: INFO: Pod pod-configmaps-8ea03b46-7c68-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:07:04.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5725" for this suite.
May 22 08:07:10.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:07:10.825: INFO: namespace configmap-5725 deletion completed in 6.179029269s

• [SLOW TEST:18.590 seconds]
[sig-node] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:07:10.826: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 08:07:10.931: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:07:13.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4663" for this suite.
May 22 08:07:19.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:07:20.100: INFO: namespace init-container-4663 deletion completed in 6.162005441s

• [SLOW TEST:9.275 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:07:20.106: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:07:32.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2459" for this suite.
May 22 08:07:38.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:07:38.324: INFO: namespace kubelet-test-2459 deletion completed in 6.142149435s

• [SLOW TEST:18.218 seconds]
[k8s.io] Kubelet
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:07:38.327: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 08:07:38.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-deployment --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --namespace=kubectl-5017'
May 22 08:07:38.492: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 08:07:38.492: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 22 08:07:38.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete deployment e2e-test-nginx-deployment --namespace=kubectl-5017'
May 22 08:07:38.596: INFO: stderr: ""
May 22 08:07:38.596: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:07:38.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5017" for this suite.
May 22 08:07:44.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:07:44.706: INFO: namespace kubectl-5017 deletion completed in 6.105914479s

• [SLOW TEST:6.380 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:07:44.707: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-adeec41d-7c68-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-adeec41d-7c68-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:07:48.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6081" for this suite.
May 22 08:08:26.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:08:26.963: INFO: namespace projected-6081 deletion completed in 38.098493428s

• [SLOW TEST:42.256 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:08:26.963: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 22 08:08:27.068: INFO: Waiting up to 5m0s for pod "pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1" in namespace "emptydir-4313" to be "success or failure"
May 22 08:08:27.071: INFO: Pod "pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.94284ms
May 22 08:08:29.075: INFO: Pod "pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007353525s
May 22 08:08:31.089: INFO: Pod "pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020893076s
STEP: Saw pod success
May 22 08:08:31.089: INFO: Pod "pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:08:31.092: INFO: Trying to get logs from node node1 pod pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:08:31.123: INFO: Waiting for pod pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1 to disappear
May 22 08:08:31.126: INFO: Pod pod-c71e3a37-7c68-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:08:31.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4313" for this suite.
May 22 08:08:37.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:08:39.160: INFO: namespace emptydir-4313 deletion completed in 8.028275087s

• [SLOW TEST:12.197 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:08:39.161: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-ce61344e-7c68-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:08:39.271: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1" in namespace "projected-2994" to be "success or failure"
May 22 08:08:39.307: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 35.681187ms
May 22 08:08:41.529: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258140354s
May 22 08:08:43.606: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.334964155s
May 22 08:08:45.616: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.344418395s
May 22 08:08:47.644: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.372503774s
May 22 08:08:49.808: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.537030111s
May 22 08:08:51.949: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.677955437s
May 22 08:08:53.965: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.693919919s
May 22 08:08:55.976: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.705219564s
May 22 08:08:57.989: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.717409167s
May 22 08:08:59.995: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.723714561s
May 22 08:09:02.000: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.728596007s
May 22 08:09:04.004: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.733035512s
May 22 08:09:06.008: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.737178612s
May 22 08:09:08.013: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.741342559s
May 22 08:09:10.017: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.745404536s
STEP: Saw pod success
May 22 08:09:10.017: INFO: Pod "pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:09:10.020: INFO: Trying to get logs from node node2 pod pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 08:09:10.090: INFO: Waiting for pod pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1 to disappear
May 22 08:09:10.096: INFO: Pod pod-projected-secrets-ce646f14-7c68-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:09:10.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2994" for this suite.
May 22 08:09:16.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:09:16.236: INFO: namespace projected-2994 deletion completed in 6.135135495s

• [SLOW TEST:37.076 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:09:16.237: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7297
I0522 08:09:16.290351      19 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7297, replica count: 1
I0522 08:09:17.340722      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:18.341083      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:19.341359      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:20.341635      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:21.341932      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:22.342234      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:23.342520      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:24.342833      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:25.343074      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:26.343270      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:27.343486      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:28.343753      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:29.344059      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:30.344367      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:09:31.344580      19 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 22 08:09:31.456: INFO: Created: latency-svc-nfgjw
May 22 08:09:31.462: INFO: Got endpoints: latency-svc-nfgjw [17.39481ms]
May 22 08:09:31.480: INFO: Created: latency-svc-md6mf
May 22 08:09:31.486: INFO: Got endpoints: latency-svc-md6mf [23.742163ms]
May 22 08:09:31.493: INFO: Created: latency-svc-hgxfh
May 22 08:09:31.497: INFO: Created: latency-svc-xrdmq
May 22 08:09:31.502: INFO: Got endpoints: latency-svc-hgxfh [39.714761ms]
May 22 08:09:31.516: INFO: Created: latency-svc-2sn6w
May 22 08:09:31.516: INFO: Got endpoints: latency-svc-2sn6w [53.923265ms]
May 22 08:09:31.517: INFO: Got endpoints: latency-svc-xrdmq [55.059195ms]
May 22 08:09:31.524: INFO: Created: latency-svc-zqc5h
May 22 08:09:31.529: INFO: Got endpoints: latency-svc-zqc5h [66.12361ms]
May 22 08:09:31.530: INFO: Created: latency-svc-lzhs8
May 22 08:09:31.537: INFO: Got endpoints: latency-svc-lzhs8 [74.251794ms]
May 22 08:09:31.538: INFO: Created: latency-svc-qrlbx
May 22 08:09:31.545: INFO: Got endpoints: latency-svc-qrlbx [81.7276ms]
May 22 08:09:31.546: INFO: Created: latency-svc-t52q8
May 22 08:09:31.550: INFO: Got endpoints: latency-svc-t52q8 [87.830464ms]
May 22 08:09:31.552: INFO: Created: latency-svc-wgg96
May 22 08:09:31.560: INFO: Got endpoints: latency-svc-wgg96 [96.830265ms]
May 22 08:09:31.562: INFO: Created: latency-svc-pqqx7
May 22 08:09:31.567: INFO: Got endpoints: latency-svc-pqqx7 [104.168533ms]
May 22 08:09:31.568: INFO: Created: latency-svc-mdrnf
May 22 08:09:31.572: INFO: Got endpoints: latency-svc-mdrnf [109.14394ms]
May 22 08:09:31.578: INFO: Created: latency-svc-qrdtl
May 22 08:09:31.582: INFO: Got endpoints: latency-svc-qrdtl [118.943927ms]
May 22 08:09:31.586: INFO: Created: latency-svc-rzgt7
May 22 08:09:31.589: INFO: Got endpoints: latency-svc-rzgt7 [126.000305ms]
May 22 08:09:31.591: INFO: Created: latency-svc-w4rvh
May 22 08:09:31.598: INFO: Got endpoints: latency-svc-w4rvh [134.621089ms]
May 22 08:09:31.599: INFO: Created: latency-svc-tbph6
May 22 08:09:31.604: INFO: Created: latency-svc-tkvvm
May 22 08:09:31.604: INFO: Got endpoints: latency-svc-tbph6 [140.814547ms]
May 22 08:09:31.609: INFO: Got endpoints: latency-svc-tkvvm [122.978291ms]
May 22 08:09:31.610: INFO: Created: latency-svc-7pknt
May 22 08:09:31.616: INFO: Created: latency-svc-5b7ff
May 22 08:09:31.618: INFO: Got endpoints: latency-svc-7pknt [116.576431ms]
May 22 08:09:31.621: INFO: Got endpoints: latency-svc-5b7ff [105.018176ms]
May 22 08:09:31.622: INFO: Created: latency-svc-7lwpk
May 22 08:09:31.630: INFO: Got endpoints: latency-svc-7lwpk [113.035225ms]
May 22 08:09:31.632: INFO: Created: latency-svc-nv97n
May 22 08:09:31.638: INFO: Got endpoints: latency-svc-nv97n [108.891903ms]
May 22 08:09:31.639: INFO: Created: latency-svc-2jcgf
May 22 08:09:31.643: INFO: Got endpoints: latency-svc-2jcgf [106.486465ms]
May 22 08:09:31.645: INFO: Created: latency-svc-xsq7l
May 22 08:09:31.657: INFO: Created: latency-svc-fdvxr
May 22 08:09:31.657: INFO: Got endpoints: latency-svc-xsq7l [112.719739ms]
May 22 08:09:31.664: INFO: Got endpoints: latency-svc-fdvxr [113.760231ms]
May 22 08:09:31.664: INFO: Created: latency-svc-x582f
May 22 08:09:31.669: INFO: Got endpoints: latency-svc-x582f [109.402241ms]
May 22 08:09:31.671: INFO: Created: latency-svc-w7p7g
May 22 08:09:31.678: INFO: Got endpoints: latency-svc-w7p7g [110.200666ms]
May 22 08:09:31.680: INFO: Created: latency-svc-htdk5
May 22 08:09:31.685: INFO: Created: latency-svc-zsdcq
May 22 08:09:31.688: INFO: Got endpoints: latency-svc-htdk5 [116.088085ms]
May 22 08:09:31.691: INFO: Got endpoints: latency-svc-zsdcq [108.593919ms]
May 22 08:09:31.692: INFO: Created: latency-svc-7vpsr
May 22 08:09:31.700: INFO: Created: latency-svc-ww5t8
May 22 08:09:31.703: INFO: Got endpoints: latency-svc-7vpsr [114.031055ms]
May 22 08:09:31.706: INFO: Got endpoints: latency-svc-ww5t8 [107.69788ms]
May 22 08:09:31.707: INFO: Created: latency-svc-hp8fd
May 22 08:09:31.713: INFO: Got endpoints: latency-svc-hp8fd [108.357862ms]
May 22 08:09:31.714: INFO: Created: latency-svc-vh7q8
May 22 08:09:31.722: INFO: Got endpoints: latency-svc-vh7q8 [113.398163ms]
May 22 08:09:31.724: INFO: Created: latency-svc-l2l8w
May 22 08:09:31.729: INFO: Created: latency-svc-wczrs
May 22 08:09:31.729: INFO: Got endpoints: latency-svc-l2l8w [110.190289ms]
May 22 08:09:31.735: INFO: Got endpoints: latency-svc-wczrs [113.472409ms]
May 22 08:09:31.740: INFO: Created: latency-svc-f89mb
May 22 08:09:31.745: INFO: Got endpoints: latency-svc-f89mb [114.767155ms]
May 22 08:09:31.746: INFO: Created: latency-svc-zj4kz
May 22 08:09:31.751: INFO: Created: latency-svc-v7nv8
May 22 08:09:31.756: INFO: Created: latency-svc-9g8vh
May 22 08:09:31.763: INFO: Got endpoints: latency-svc-zj4kz [125.220931ms]
May 22 08:09:31.765: INFO: Created: latency-svc-2j2d8
May 22 08:09:31.769: INFO: Created: latency-svc-gl5b8
May 22 08:09:31.774: INFO: Created: latency-svc-ptxkc
May 22 08:09:31.779: INFO: Created: latency-svc-t664j
May 22 08:09:31.787: INFO: Created: latency-svc-7wqkm
May 22 08:09:31.793: INFO: Created: latency-svc-hktz4
May 22 08:09:31.800: INFO: Created: latency-svc-485h7
May 22 08:09:31.806: INFO: Created: latency-svc-m79k9
May 22 08:09:31.813: INFO: Created: latency-svc-p48h4
May 22 08:09:31.813: INFO: Got endpoints: latency-svc-v7nv8 [169.946384ms]
May 22 08:09:31.818: INFO: Created: latency-svc-pwq9x
May 22 08:09:31.824: INFO: Created: latency-svc-cqjrf
May 22 08:09:31.829: INFO: Created: latency-svc-99xds
May 22 08:09:31.839: INFO: Created: latency-svc-8n5zp
May 22 08:09:31.846: INFO: Created: latency-svc-tvwbv
May 22 08:09:31.861: INFO: Got endpoints: latency-svc-9g8vh [203.155533ms]
May 22 08:09:31.869: INFO: Created: latency-svc-2cgnm
May 22 08:09:31.911: INFO: Got endpoints: latency-svc-2j2d8 [246.912319ms]
May 22 08:09:31.919: INFO: Created: latency-svc-x5ps8
May 22 08:09:31.963: INFO: Got endpoints: latency-svc-gl5b8 [293.893367ms]
May 22 08:09:31.972: INFO: Created: latency-svc-k2k7c
May 22 08:09:32.011: INFO: Got endpoints: latency-svc-ptxkc [333.25179ms]
May 22 08:09:32.025: INFO: Created: latency-svc-z5fdn
May 22 08:09:32.063: INFO: Got endpoints: latency-svc-t664j [374.384924ms]
May 22 08:09:32.076: INFO: Created: latency-svc-5ndvb
May 22 08:09:32.109: INFO: Got endpoints: latency-svc-7wqkm [418.453369ms]
May 22 08:09:32.117: INFO: Created: latency-svc-dtmcf
May 22 08:09:32.162: INFO: Got endpoints: latency-svc-hktz4 [458.366016ms]
May 22 08:09:32.177: INFO: Created: latency-svc-t2qb5
May 22 08:09:32.210: INFO: Got endpoints: latency-svc-485h7 [504.353774ms]
May 22 08:09:32.222: INFO: Created: latency-svc-8n5r2
May 22 08:09:32.260: INFO: Got endpoints: latency-svc-m79k9 [547.314086ms]
May 22 08:09:32.270: INFO: Created: latency-svc-dlxl8
May 22 08:09:32.310: INFO: Got endpoints: latency-svc-p48h4 [587.618003ms]
May 22 08:09:32.323: INFO: Created: latency-svc-qb6ln
May 22 08:09:32.364: INFO: Got endpoints: latency-svc-pwq9x [635.230661ms]
May 22 08:09:32.377: INFO: Created: latency-svc-9t28z
May 22 08:09:32.411: INFO: Got endpoints: latency-svc-cqjrf [675.79966ms]
May 22 08:09:32.420: INFO: Created: latency-svc-dkbbq
May 22 08:09:32.464: INFO: Got endpoints: latency-svc-99xds [719.621752ms]
May 22 08:09:32.475: INFO: Created: latency-svc-wvvrk
May 22 08:09:32.511: INFO: Got endpoints: latency-svc-8n5zp [747.706592ms]
May 22 08:09:32.520: INFO: Created: latency-svc-bdjld
May 22 08:09:32.564: INFO: Got endpoints: latency-svc-tvwbv [750.147285ms]
May 22 08:09:32.575: INFO: Created: latency-svc-9jj2l
May 22 08:09:32.611: INFO: Got endpoints: latency-svc-2cgnm [750.562485ms]
May 22 08:09:32.632: INFO: Created: latency-svc-shgt2
May 22 08:09:32.660: INFO: Got endpoints: latency-svc-x5ps8 [749.196162ms]
May 22 08:09:32.674: INFO: Created: latency-svc-rsrxg
May 22 08:09:32.712: INFO: Got endpoints: latency-svc-k2k7c [748.452172ms]
May 22 08:09:32.723: INFO: Created: latency-svc-gsxhw
May 22 08:09:32.764: INFO: Got endpoints: latency-svc-z5fdn [752.622915ms]
May 22 08:09:32.778: INFO: Created: latency-svc-mc8kt
May 22 08:09:32.811: INFO: Got endpoints: latency-svc-5ndvb [747.521162ms]
May 22 08:09:32.820: INFO: Created: latency-svc-z8krb
May 22 08:09:32.864: INFO: Got endpoints: latency-svc-dtmcf [754.715711ms]
May 22 08:09:32.877: INFO: Created: latency-svc-qpfq9
May 22 08:09:32.911: INFO: Got endpoints: latency-svc-t2qb5 [748.609212ms]
May 22 08:09:32.924: INFO: Created: latency-svc-td2sf
May 22 08:09:32.962: INFO: Got endpoints: latency-svc-8n5r2 [751.125687ms]
May 22 08:09:32.974: INFO: Created: latency-svc-xjdl9
May 22 08:09:33.012: INFO: Got endpoints: latency-svc-dlxl8 [751.505486ms]
May 22 08:09:33.025: INFO: Created: latency-svc-qnjch
May 22 08:09:33.062: INFO: Got endpoints: latency-svc-qb6ln [751.419603ms]
May 22 08:09:33.072: INFO: Created: latency-svc-wdvfv
May 22 08:09:33.111: INFO: Got endpoints: latency-svc-9t28z [746.800301ms]
May 22 08:09:33.119: INFO: Created: latency-svc-6khlh
May 22 08:09:33.167: INFO: Got endpoints: latency-svc-dkbbq [756.353776ms]
May 22 08:09:33.178: INFO: Created: latency-svc-hf2hc
May 22 08:09:33.211: INFO: Got endpoints: latency-svc-wvvrk [746.599678ms]
May 22 08:09:33.220: INFO: Created: latency-svc-dtm4t
May 22 08:09:33.260: INFO: Got endpoints: latency-svc-bdjld [749.288838ms]
May 22 08:09:33.270: INFO: Created: latency-svc-hln6s
May 22 08:09:33.312: INFO: Got endpoints: latency-svc-9jj2l [747.886892ms]
May 22 08:09:33.322: INFO: Created: latency-svc-zhhvj
May 22 08:09:33.387: INFO: Got endpoints: latency-svc-shgt2 [775.809981ms]
May 22 08:09:33.402: INFO: Created: latency-svc-g25mn
May 22 08:09:33.415: INFO: Got endpoints: latency-svc-rsrxg [754.808633ms]
May 22 08:09:33.424: INFO: Created: latency-svc-8j9k8
May 22 08:09:33.461: INFO: Got endpoints: latency-svc-gsxhw [748.914307ms]
May 22 08:09:33.472: INFO: Created: latency-svc-ftv95
May 22 08:09:33.511: INFO: Got endpoints: latency-svc-mc8kt [746.82127ms]
May 22 08:09:33.522: INFO: Created: latency-svc-p64jl
May 22 08:09:33.563: INFO: Got endpoints: latency-svc-z8krb [751.760166ms]
May 22 08:09:33.574: INFO: Created: latency-svc-l75qd
May 22 08:09:33.612: INFO: Got endpoints: latency-svc-qpfq9 [747.483213ms]
May 22 08:09:33.638: INFO: Created: latency-svc-2299f
May 22 08:09:33.664: INFO: Got endpoints: latency-svc-td2sf [752.896596ms]
May 22 08:09:33.675: INFO: Created: latency-svc-sw259
May 22 08:09:33.710: INFO: Got endpoints: latency-svc-xjdl9 [748.782632ms]
May 22 08:09:33.720: INFO: Created: latency-svc-hvrr7
May 22 08:09:33.767: INFO: Got endpoints: latency-svc-qnjch [755.09676ms]
May 22 08:09:33.777: INFO: Created: latency-svc-2lwp7
May 22 08:09:33.810: INFO: Got endpoints: latency-svc-wdvfv [748.479264ms]
May 22 08:09:33.822: INFO: Created: latency-svc-9f4df
May 22 08:09:33.862: INFO: Got endpoints: latency-svc-6khlh [750.947751ms]
May 22 08:09:33.872: INFO: Created: latency-svc-qxpv4
May 22 08:09:33.913: INFO: Got endpoints: latency-svc-hf2hc [745.765468ms]
May 22 08:09:33.923: INFO: Created: latency-svc-lgc69
May 22 08:09:33.963: INFO: Got endpoints: latency-svc-dtm4t [752.081666ms]
May 22 08:09:33.973: INFO: Created: latency-svc-92vx9
May 22 08:09:34.013: INFO: Got endpoints: latency-svc-hln6s [753.156703ms]
May 22 08:09:34.025: INFO: Created: latency-svc-nhksr
May 22 08:09:34.061: INFO: Got endpoints: latency-svc-zhhvj [749.484151ms]
May 22 08:09:34.070: INFO: Created: latency-svc-mhjd2
May 22 08:09:34.113: INFO: Got endpoints: latency-svc-g25mn [726.073782ms]
May 22 08:09:34.130: INFO: Created: latency-svc-vg2v8
May 22 08:09:34.166: INFO: Got endpoints: latency-svc-8j9k8 [750.271628ms]
May 22 08:09:34.178: INFO: Created: latency-svc-gx6kf
May 22 08:09:34.210: INFO: Got endpoints: latency-svc-ftv95 [749.202055ms]
May 22 08:09:34.222: INFO: Created: latency-svc-kwth2
May 22 08:09:34.261: INFO: Got endpoints: latency-svc-p64jl [750.406429ms]
May 22 08:09:34.276: INFO: Created: latency-svc-xg6bl
May 22 08:09:34.313: INFO: Got endpoints: latency-svc-l75qd [750.723414ms]
May 22 08:09:34.323: INFO: Created: latency-svc-2kmtk
May 22 08:09:34.363: INFO: Got endpoints: latency-svc-2299f [751.210641ms]
May 22 08:09:34.376: INFO: Created: latency-svc-pztxw
May 22 08:09:34.415: INFO: Got endpoints: latency-svc-sw259 [750.908555ms]
May 22 08:09:34.425: INFO: Created: latency-svc-lxxb4
May 22 08:09:34.460: INFO: Got endpoints: latency-svc-hvrr7 [749.13095ms]
May 22 08:09:34.474: INFO: Created: latency-svc-m8j22
May 22 08:09:34.510: INFO: Got endpoints: latency-svc-2lwp7 [742.631985ms]
May 22 08:09:34.520: INFO: Created: latency-svc-gkf5x
May 22 08:09:34.560: INFO: Got endpoints: latency-svc-9f4df [749.942962ms]
May 22 08:09:34.573: INFO: Created: latency-svc-hl8j9
May 22 08:09:34.616: INFO: Got endpoints: latency-svc-qxpv4 [753.34333ms]
May 22 08:09:34.652: INFO: Created: latency-svc-ntmnd
May 22 08:09:34.660: INFO: Got endpoints: latency-svc-lgc69 [746.47218ms]
May 22 08:09:34.669: INFO: Created: latency-svc-jlw99
May 22 08:09:34.713: INFO: Got endpoints: latency-svc-92vx9 [749.800953ms]
May 22 08:09:34.723: INFO: Created: latency-svc-b9xl7
May 22 08:09:34.762: INFO: Got endpoints: latency-svc-nhksr [748.852716ms]
May 22 08:09:34.772: INFO: Created: latency-svc-dzbz8
May 22 08:09:34.810: INFO: Got endpoints: latency-svc-mhjd2 [748.965025ms]
May 22 08:09:34.820: INFO: Created: latency-svc-r55d9
May 22 08:09:34.866: INFO: Got endpoints: latency-svc-vg2v8 [752.140628ms]
May 22 08:09:34.879: INFO: Created: latency-svc-9tgl8
May 22 08:09:34.999: INFO: Got endpoints: latency-svc-gx6kf [833.333522ms]
May 22 08:09:35.009: INFO: Got endpoints: latency-svc-kwth2 [798.320602ms]
May 22 08:09:35.021: INFO: Created: latency-svc-cf6mq
May 22 08:09:35.028: INFO: Created: latency-svc-zw9nk
May 22 08:09:35.099: INFO: Got endpoints: latency-svc-2kmtk [785.973452ms]
May 22 08:09:35.100: INFO: Got endpoints: latency-svc-xg6bl [838.257567ms]
May 22 08:09:35.112: INFO: Created: latency-svc-t75wf
May 22 08:09:35.136: INFO: Created: latency-svc-vk75t
May 22 08:09:35.194: INFO: Got endpoints: latency-svc-lxxb4 [779.112617ms]
May 22 08:09:35.195: INFO: Got endpoints: latency-svc-pztxw [831.743639ms]
May 22 08:09:35.204: INFO: Created: latency-svc-sxd2w
May 22 08:09:35.215: INFO: Created: latency-svc-2lgpj
May 22 08:09:35.322: INFO: Got endpoints: latency-svc-m8j22 [861.646133ms]
May 22 08:09:35.326: INFO: Got endpoints: latency-svc-gkf5x [816.081968ms]
May 22 08:09:35.326: INFO: Got endpoints: latency-svc-hl8j9 [765.937291ms]
May 22 08:09:35.340: INFO: Created: latency-svc-mdz75
May 22 08:09:35.355: INFO: Created: latency-svc-2v5rw
May 22 08:09:35.362: INFO: Got endpoints: latency-svc-ntmnd [745.81825ms]
May 22 08:09:35.362: INFO: Created: latency-svc-8fvqx
May 22 08:09:35.370: INFO: Created: latency-svc-hxqwb
May 22 08:09:35.411: INFO: Got endpoints: latency-svc-jlw99 [750.239727ms]
May 22 08:09:35.423: INFO: Created: latency-svc-tjlxz
May 22 08:09:35.460: INFO: Got endpoints: latency-svc-b9xl7 [746.769508ms]
May 22 08:09:35.471: INFO: Created: latency-svc-f7hcq
May 22 08:09:35.512: INFO: Got endpoints: latency-svc-dzbz8 [749.183195ms]
May 22 08:09:35.528: INFO: Created: latency-svc-xz6b6
May 22 08:09:35.560: INFO: Got endpoints: latency-svc-r55d9 [749.689629ms]
May 22 08:09:35.571: INFO: Created: latency-svc-jw9zs
May 22 08:09:35.623: INFO: Got endpoints: latency-svc-9tgl8 [757.172441ms]
May 22 08:09:35.632: INFO: Created: latency-svc-x2pgz
May 22 08:09:35.665: INFO: Got endpoints: latency-svc-cf6mq [665.833149ms]
May 22 08:09:35.676: INFO: Created: latency-svc-g5lhp
May 22 08:09:35.712: INFO: Got endpoints: latency-svc-zw9nk [702.953285ms]
May 22 08:09:35.728: INFO: Created: latency-svc-q8jgf
May 22 08:09:35.762: INFO: Got endpoints: latency-svc-t75wf [662.202809ms]
May 22 08:09:35.773: INFO: Created: latency-svc-rqv7h
May 22 08:09:35.810: INFO: Got endpoints: latency-svc-vk75t [711.035715ms]
May 22 08:09:35.831: INFO: Created: latency-svc-tzflb
May 22 08:09:35.861: INFO: Got endpoints: latency-svc-sxd2w [667.125716ms]
May 22 08:09:35.872: INFO: Created: latency-svc-9qpj8
May 22 08:09:35.911: INFO: Got endpoints: latency-svc-2lgpj [716.072667ms]
May 22 08:09:35.921: INFO: Created: latency-svc-frcch
May 22 08:09:35.963: INFO: Got endpoints: latency-svc-mdz75 [641.467785ms]
May 22 08:09:35.978: INFO: Created: latency-svc-xhcj2
May 22 08:09:36.010: INFO: Got endpoints: latency-svc-2v5rw [684.494584ms]
May 22 08:09:36.019: INFO: Created: latency-svc-wz8mf
May 22 08:09:36.105: INFO: Got endpoints: latency-svc-8fvqx [779.148375ms]
May 22 08:09:36.110: INFO: Got endpoints: latency-svc-hxqwb [748.189196ms]
May 22 08:09:36.118: INFO: Created: latency-svc-sg7dh
May 22 08:09:36.123: INFO: Created: latency-svc-2b5cm
May 22 08:09:36.161: INFO: Got endpoints: latency-svc-tjlxz [749.683331ms]
May 22 08:09:36.170: INFO: Created: latency-svc-jxhpj
May 22 08:09:36.211: INFO: Got endpoints: latency-svc-f7hcq [750.566792ms]
May 22 08:09:36.222: INFO: Created: latency-svc-6g6vb
May 22 08:09:36.260: INFO: Got endpoints: latency-svc-xz6b6 [748.71847ms]
May 22 08:09:36.269: INFO: Created: latency-svc-j2cfd
May 22 08:09:36.311: INFO: Got endpoints: latency-svc-jw9zs [751.410935ms]
May 22 08:09:36.321: INFO: Created: latency-svc-v8dhp
May 22 08:09:36.364: INFO: Got endpoints: latency-svc-x2pgz [741.37732ms]
May 22 08:09:36.375: INFO: Created: latency-svc-brc4z
May 22 08:09:36.411: INFO: Got endpoints: latency-svc-g5lhp [745.761838ms]
May 22 08:09:36.432: INFO: Created: latency-svc-j6xlt
May 22 08:09:36.459: INFO: Got endpoints: latency-svc-q8jgf [747.236659ms]
May 22 08:09:36.471: INFO: Created: latency-svc-sfqnd
May 22 08:09:36.511: INFO: Got endpoints: latency-svc-rqv7h [749.655154ms]
May 22 08:09:36.522: INFO: Created: latency-svc-smfvt
May 22 08:09:36.561: INFO: Got endpoints: latency-svc-tzflb [750.256085ms]
May 22 08:09:36.570: INFO: Created: latency-svc-rsxlc
May 22 08:09:36.611: INFO: Got endpoints: latency-svc-9qpj8 [749.838015ms]
May 22 08:09:36.627: INFO: Created: latency-svc-6tbss
May 22 08:09:36.661: INFO: Got endpoints: latency-svc-frcch [749.929821ms]
May 22 08:09:36.670: INFO: Created: latency-svc-zc6kt
May 22 08:09:36.712: INFO: Got endpoints: latency-svc-xhcj2 [748.667487ms]
May 22 08:09:36.725: INFO: Created: latency-svc-rmj96
May 22 08:09:36.763: INFO: Got endpoints: latency-svc-wz8mf [752.198915ms]
May 22 08:09:36.772: INFO: Created: latency-svc-dxhfc
May 22 08:09:36.811: INFO: Got endpoints: latency-svc-sg7dh [705.170454ms]
May 22 08:09:36.820: INFO: Created: latency-svc-mf59l
May 22 08:09:36.861: INFO: Got endpoints: latency-svc-2b5cm [750.801516ms]
May 22 08:09:36.869: INFO: Created: latency-svc-hr6jb
May 22 08:09:36.913: INFO: Got endpoints: latency-svc-jxhpj [752.033198ms]
May 22 08:09:36.922: INFO: Created: latency-svc-6pvc6
May 22 08:09:36.960: INFO: Got endpoints: latency-svc-6g6vb [749.132293ms]
May 22 08:09:36.968: INFO: Created: latency-svc-dgssh
May 22 08:09:37.010: INFO: Got endpoints: latency-svc-j2cfd [749.293896ms]
May 22 08:09:37.020: INFO: Created: latency-svc-lc2s4
May 22 08:09:37.060: INFO: Got endpoints: latency-svc-v8dhp [748.51439ms]
May 22 08:09:37.069: INFO: Created: latency-svc-bhh6v
May 22 08:09:37.110: INFO: Got endpoints: latency-svc-brc4z [745.94408ms]
May 22 08:09:37.120: INFO: Created: latency-svc-f5m92
May 22 08:09:37.161: INFO: Got endpoints: latency-svc-j6xlt [749.872264ms]
May 22 08:09:37.173: INFO: Created: latency-svc-rmfzz
May 22 08:09:37.211: INFO: Got endpoints: latency-svc-sfqnd [752.020309ms]
May 22 08:09:37.221: INFO: Created: latency-svc-vzpmq
May 22 08:09:37.261: INFO: Got endpoints: latency-svc-smfvt [749.211543ms]
May 22 08:09:37.273: INFO: Created: latency-svc-8zhjn
May 22 08:09:37.310: INFO: Got endpoints: latency-svc-rsxlc [748.758972ms]
May 22 08:09:37.324: INFO: Created: latency-svc-knqx7
May 22 08:09:37.360: INFO: Got endpoints: latency-svc-6tbss [748.374389ms]
May 22 08:09:37.368: INFO: Created: latency-svc-rm2gn
May 22 08:09:37.410: INFO: Got endpoints: latency-svc-zc6kt [749.084563ms]
May 22 08:09:37.428: INFO: Created: latency-svc-ngsdw
May 22 08:09:37.462: INFO: Got endpoints: latency-svc-rmj96 [750.207203ms]
May 22 08:09:37.471: INFO: Created: latency-svc-6qkd9
May 22 08:09:37.511: INFO: Got endpoints: latency-svc-dxhfc [747.958874ms]
May 22 08:09:37.519: INFO: Created: latency-svc-zgq2x
May 22 08:09:37.562: INFO: Got endpoints: latency-svc-mf59l [750.906057ms]
May 22 08:09:37.572: INFO: Created: latency-svc-ftskz
May 22 08:09:37.617: INFO: Got endpoints: latency-svc-hr6jb [756.623924ms]
May 22 08:09:37.628: INFO: Created: latency-svc-vctnh
May 22 08:09:37.661: INFO: Got endpoints: latency-svc-6pvc6 [748.701272ms]
May 22 08:09:37.674: INFO: Created: latency-svc-8xdqq
May 22 08:09:37.711: INFO: Got endpoints: latency-svc-dgssh [751.101436ms]
May 22 08:09:37.736: INFO: Created: latency-svc-gbwvw
May 22 08:09:37.762: INFO: Got endpoints: latency-svc-lc2s4 [752.181591ms]
May 22 08:09:37.775: INFO: Created: latency-svc-qlwg5
May 22 08:09:37.811: INFO: Got endpoints: latency-svc-bhh6v [750.504948ms]
May 22 08:09:37.819: INFO: Created: latency-svc-fcp8v
May 22 08:09:37.863: INFO: Got endpoints: latency-svc-f5m92 [752.026395ms]
May 22 08:09:37.895: INFO: Created: latency-svc-qbt7x
May 22 08:09:37.911: INFO: Got endpoints: latency-svc-rmfzz [750.132209ms]
May 22 08:09:37.920: INFO: Created: latency-svc-fc5fz
May 22 08:09:37.968: INFO: Got endpoints: latency-svc-vzpmq [756.679868ms]
May 22 08:09:38.031: INFO: Got endpoints: latency-svc-8zhjn [769.705621ms]
May 22 08:09:38.032: INFO: Created: latency-svc-zw9v8
May 22 08:09:38.046: INFO: Created: latency-svc-drr7s
May 22 08:09:38.061: INFO: Got endpoints: latency-svc-knqx7 [750.668779ms]
May 22 08:09:38.069: INFO: Created: latency-svc-746dz
May 22 08:09:38.111: INFO: Got endpoints: latency-svc-rm2gn [751.074523ms]
May 22 08:09:38.120: INFO: Created: latency-svc-gc956
May 22 08:09:38.166: INFO: Got endpoints: latency-svc-ngsdw [755.506926ms]
May 22 08:09:38.180: INFO: Created: latency-svc-tpmcr
May 22 08:09:38.211: INFO: Got endpoints: latency-svc-6qkd9 [748.658752ms]
May 22 08:09:38.221: INFO: Created: latency-svc-5sh59
May 22 08:09:38.269: INFO: Got endpoints: latency-svc-zgq2x [757.745229ms]
May 22 08:09:38.280: INFO: Created: latency-svc-4zm4d
May 22 08:09:38.314: INFO: Got endpoints: latency-svc-ftskz [751.583073ms]
May 22 08:09:38.324: INFO: Created: latency-svc-9z7dn
May 22 08:09:38.360: INFO: Got endpoints: latency-svc-vctnh [741.930477ms]
May 22 08:09:38.370: INFO: Created: latency-svc-hkfhf
May 22 08:09:38.432: INFO: Got endpoints: latency-svc-8xdqq [770.363757ms]
May 22 08:09:38.442: INFO: Created: latency-svc-d889c
May 22 08:09:38.466: INFO: Got endpoints: latency-svc-gbwvw [754.42654ms]
May 22 08:09:38.481: INFO: Created: latency-svc-c9wc2
May 22 08:09:38.510: INFO: Got endpoints: latency-svc-qlwg5 [748.300982ms]
May 22 08:09:38.524: INFO: Created: latency-svc-5qflv
May 22 08:09:38.561: INFO: Got endpoints: latency-svc-fcp8v [750.467775ms]
May 22 08:09:38.585: INFO: Created: latency-svc-dmjxh
May 22 08:09:38.614: INFO: Got endpoints: latency-svc-qbt7x [751.495672ms]
May 22 08:09:38.625: INFO: Created: latency-svc-9nwrw
May 22 08:09:38.660: INFO: Got endpoints: latency-svc-fc5fz [748.873417ms]
May 22 08:09:38.675: INFO: Created: latency-svc-gpmd9
May 22 08:09:38.711: INFO: Got endpoints: latency-svc-zw9v8 [742.654011ms]
May 22 08:09:38.721: INFO: Created: latency-svc-qf6n6
May 22 08:09:38.761: INFO: Got endpoints: latency-svc-drr7s [730.465616ms]
May 22 08:09:38.770: INFO: Created: latency-svc-b6ps5
May 22 08:09:38.814: INFO: Got endpoints: latency-svc-746dz [753.249114ms]
May 22 08:09:38.825: INFO: Created: latency-svc-pv5xl
May 22 08:09:38.861: INFO: Got endpoints: latency-svc-gc956 [749.673518ms]
May 22 08:09:38.870: INFO: Created: latency-svc-vf8fs
May 22 08:09:38.912: INFO: Got endpoints: latency-svc-tpmcr [745.939274ms]
May 22 08:09:38.924: INFO: Created: latency-svc-9ft8q
May 22 08:09:38.960: INFO: Got endpoints: latency-svc-5sh59 [749.383371ms]
May 22 08:09:38.983: INFO: Created: latency-svc-qrdwx
May 22 08:09:39.010: INFO: Got endpoints: latency-svc-4zm4d [741.53057ms]
May 22 08:09:39.019: INFO: Created: latency-svc-svl6m
May 22 08:09:39.060: INFO: Got endpoints: latency-svc-9z7dn [746.25974ms]
May 22 08:09:39.072: INFO: Created: latency-svc-rxmbj
May 22 08:09:39.116: INFO: Got endpoints: latency-svc-hkfhf [756.937768ms]
May 22 08:09:39.127: INFO: Created: latency-svc-72jh4
May 22 08:09:39.161: INFO: Got endpoints: latency-svc-d889c [728.764679ms]
May 22 08:09:39.170: INFO: Created: latency-svc-ggbcf
May 22 08:09:39.212: INFO: Got endpoints: latency-svc-c9wc2 [746.356646ms]
May 22 08:09:39.225: INFO: Created: latency-svc-6ptz7
May 22 08:09:39.261: INFO: Got endpoints: latency-svc-5qflv [750.292299ms]
May 22 08:09:39.271: INFO: Created: latency-svc-67nt4
May 22 08:09:39.311: INFO: Got endpoints: latency-svc-dmjxh [748.978178ms]
May 22 08:09:39.372: INFO: Got endpoints: latency-svc-9nwrw [757.773062ms]
May 22 08:09:39.410: INFO: Got endpoints: latency-svc-gpmd9 [750.199664ms]
May 22 08:09:39.471: INFO: Got endpoints: latency-svc-qf6n6 [759.326738ms]
May 22 08:09:39.511: INFO: Got endpoints: latency-svc-b6ps5 [749.487526ms]
May 22 08:09:39.562: INFO: Got endpoints: latency-svc-pv5xl [748.18423ms]
May 22 08:09:39.612: INFO: Got endpoints: latency-svc-vf8fs [750.953579ms]
May 22 08:09:39.660: INFO: Got endpoints: latency-svc-9ft8q [747.88927ms]
May 22 08:09:39.712: INFO: Got endpoints: latency-svc-qrdwx [752.070369ms]
May 22 08:09:39.774: INFO: Got endpoints: latency-svc-svl6m [763.164113ms]
May 22 08:09:39.830: INFO: Got endpoints: latency-svc-rxmbj [769.807814ms]
May 22 08:09:39.861: INFO: Got endpoints: latency-svc-72jh4 [744.38947ms]
May 22 08:09:39.916: INFO: Got endpoints: latency-svc-ggbcf [754.69407ms]
May 22 08:09:39.962: INFO: Got endpoints: latency-svc-6ptz7 [749.697925ms]
May 22 08:09:40.013: INFO: Got endpoints: latency-svc-67nt4 [752.414666ms]
May 22 08:09:40.013: INFO: Latencies: [23.742163ms 39.714761ms 53.923265ms 55.059195ms 66.12361ms 74.251794ms 81.7276ms 87.830464ms 96.830265ms 104.168533ms 105.018176ms 106.486465ms 107.69788ms 108.357862ms 108.593919ms 108.891903ms 109.14394ms 109.402241ms 110.190289ms 110.200666ms 112.719739ms 113.035225ms 113.398163ms 113.472409ms 113.760231ms 114.031055ms 114.767155ms 116.088085ms 116.576431ms 118.943927ms 122.978291ms 125.220931ms 126.000305ms 134.621089ms 140.814547ms 169.946384ms 203.155533ms 246.912319ms 293.893367ms 333.25179ms 374.384924ms 418.453369ms 458.366016ms 504.353774ms 547.314086ms 587.618003ms 635.230661ms 641.467785ms 662.202809ms 665.833149ms 667.125716ms 675.79966ms 684.494584ms 702.953285ms 705.170454ms 711.035715ms 716.072667ms 719.621752ms 726.073782ms 728.764679ms 730.465616ms 741.37732ms 741.53057ms 741.930477ms 742.631985ms 742.654011ms 744.38947ms 745.761838ms 745.765468ms 745.81825ms 745.939274ms 745.94408ms 746.25974ms 746.356646ms 746.47218ms 746.599678ms 746.769508ms 746.800301ms 746.82127ms 747.236659ms 747.483213ms 747.521162ms 747.706592ms 747.886892ms 747.88927ms 747.958874ms 748.18423ms 748.189196ms 748.300982ms 748.374389ms 748.452172ms 748.479264ms 748.51439ms 748.609212ms 748.658752ms 748.667487ms 748.701272ms 748.71847ms 748.758972ms 748.782632ms 748.852716ms 748.873417ms 748.914307ms 748.965025ms 748.978178ms 749.084563ms 749.13095ms 749.132293ms 749.183195ms 749.196162ms 749.202055ms 749.211543ms 749.288838ms 749.293896ms 749.383371ms 749.484151ms 749.487526ms 749.655154ms 749.673518ms 749.683331ms 749.689629ms 749.697925ms 749.800953ms 749.838015ms 749.872264ms 749.929821ms 749.942962ms 750.132209ms 750.147285ms 750.199664ms 750.207203ms 750.239727ms 750.256085ms 750.271628ms 750.292299ms 750.406429ms 750.467775ms 750.504948ms 750.562485ms 750.566792ms 750.668779ms 750.723414ms 750.801516ms 750.906057ms 750.908555ms 750.947751ms 750.953579ms 751.074523ms 751.101436ms 751.125687ms 751.210641ms 751.410935ms 751.419603ms 751.495672ms 751.505486ms 751.583073ms 751.760166ms 752.020309ms 752.026395ms 752.033198ms 752.070369ms 752.081666ms 752.140628ms 752.181591ms 752.198915ms 752.414666ms 752.622915ms 752.896596ms 753.156703ms 753.249114ms 753.34333ms 754.42654ms 754.69407ms 754.715711ms 754.808633ms 755.09676ms 755.506926ms 756.353776ms 756.623924ms 756.679868ms 756.937768ms 757.172441ms 757.745229ms 757.773062ms 759.326738ms 763.164113ms 765.937291ms 769.705621ms 769.807814ms 770.363757ms 775.809981ms 779.112617ms 779.148375ms 785.973452ms 798.320602ms 816.081968ms 831.743639ms 833.333522ms 838.257567ms 861.646133ms]
May 22 08:09:40.013: INFO: 50 %ile: 748.852716ms
May 22 08:09:40.013: INFO: 90 %ile: 756.937768ms
May 22 08:09:40.013: INFO: 99 %ile: 838.257567ms
May 22 08:09:40.013: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:09:40.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7297" for this suite.
May 22 08:09:52.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:09:52.122: INFO: namespace svc-latency-7297 deletion completed in 12.101567202s

• [SLOW TEST:35.885 seconds]
[sig-network] Service endpoints latency
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:09:52.122: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 22 08:10:12.218: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-f9d81982-7c68-11e9-aefd-da1a35f02de1,GenerateName:,Namespace:events-9371,SelfLink:/api/v1/namespaces/events-9371/pods/send-events-f9d81982-7c68-11e9-aefd-da1a35f02de1,UID:f9d895ef-7c68-11e9-9cf4-fa163ecd1d63,ResourceVersion:12858214,Generation:0,CreationTimestamp:2019-05-22 08:09:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 164304805,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-txrjk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-txrjk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p hub.c.163.com/combk8s/kubernetes-e2e-test-images/serve-hostname-amd64:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-txrjk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0007c19c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007c1a40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:09:52 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:09:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:09:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:09:52 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.80,StartTime:2019-05-22 08:09:52 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-22 08:09:53 +0000 UTC,} nil} {nil nil nil} true 0 hub.c.163.com/combk8s/kubernetes-e2e-test-images/serve-hostname-amd64:1.1 docker-pullable://hub.c.163.com/combk8s/kubernetes-e2e-test-images/serve-hostname-amd64@sha256:53c28beabd3509fb5b1d1185b2962e8204384cef7562982d8b216b71292aabf9 docker://e85c4c7ffb63f07d4d9ee37c27beacd16e15de756e8433f3d1ffd3f50d0a7fe7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 22 08:10:14.223: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 22 08:10:16.228: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:10:16.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9371" for this suite.
May 22 08:11:38.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:11:38.789: INFO: namespace events-9371 deletion completed in 1m22.547033967s

• [SLOW TEST:106.667 seconds]
[k8s.io] [sig-node] Events
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:11:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 22 08:11:38.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859486,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 08:11:38.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859486,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 22 08:11:49.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859735,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 22 08:11:49.935: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859735,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 22 08:11:59.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859972,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 08:11:59.955: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12859972,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 22 08:12:09.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860180,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 08:12:09.963: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-a,UID:397d8318-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860180,Generation:0,CreationTimestamp:2019-05-22 08:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 22 08:12:20.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-b,UID:51f0ca20-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860414,Generation:0,CreationTimestamp:2019-05-22 08:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 08:12:20.021: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-b,UID:51f0ca20-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860414,Generation:0,CreationTimestamp:2019-05-22 08:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 22 08:12:30.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-b,UID:51f0ca20-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860590,Generation:0,CreationTimestamp:2019-05-22 08:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 08:12:30.028: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8906,SelfLink:/api/v1/namespaces/watch-8906/configmaps/e2e-watch-test-configmap-b,UID:51f0ca20-7c69-11e9-9cf4-fa163ecd1d63,ResourceVersion:12860590,Generation:0,CreationTimestamp:2019-05-22 08:12:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:12:40.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8906" for this suite.
May 22 08:12:46.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:12:46.167: INFO: namespace watch-8906 deletion completed in 6.131115055s

• [SLOW TEST:67.378 seconds]
[sig-api-machinery] Watchers
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:12:46.168: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 22 08:12:46.852: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
May 22 08:12:48.898: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:12:50.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:12:52.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:12:54.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:12:56.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:12:58.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:00.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:02.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:04.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:06.904: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:08.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:10.901: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:12.902: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694109566, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-54647ddb87\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:13:16.802: INFO: Waited 1.892110704s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:13:17.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6518" for this suite.
May 22 08:13:23.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:13:23.586: INFO: namespace aggregator-6518 deletion completed in 6.421800391s

• [SLOW TEST:37.418 seconds]
[sig-api-machinery] Aggregator
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:13:23.586: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:13:23.635: INFO: Creating ReplicaSet my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1
May 22 08:13:23.644: INFO: Pod name my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1: Found 0 pods out of 1
May 22 08:13:28.649: INFO: Pod name my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1: Found 1 pods out of 1
May 22 08:13:28.649: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1" is running
May 22 08:13:44.656: INFO: Pod "my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1-cbp6x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 08:13:23 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 08:13:23 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 08:13:23 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-22 08:13:23 +0000 UTC Reason: Message:}])
May 22 08:13:44.657: INFO: Trying to dial the pod
May 22 08:13:50.113: INFO: Controller my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1: Got expected result from replica 1 [my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1-cbp6x]: "my-hostname-basic-77e3fc1a-7c69-11e9-aefd-da1a35f02de1-cbp6x", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:13:50.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7594" for this suite.
May 22 08:13:56.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:13:56.234: INFO: namespace replicaset-7594 deletion completed in 6.115508234s

• [SLOW TEST:32.648 seconds]
[sig-apps] ReplicaSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:13:56.235: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:13:56.300: INFO: Create a RollingUpdate DaemonSet
May 22 08:13:56.306: INFO: Check that daemon pods launch on every node of the cluster
May 22 08:13:56.315: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:56.315: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:56.315: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:56.320: INFO: Number of nodes with available pods: 0
May 22 08:13:56.320: INFO: Node node-mini is running more than one daemon pod
May 22 08:13:57.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:57.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:57.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:57.330: INFO: Number of nodes with available pods: 0
May 22 08:13:57.330: INFO: Node node-mini is running more than one daemon pod
May 22 08:13:58.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:58.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:58.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:58.330: INFO: Number of nodes with available pods: 1
May 22 08:13:58.330: INFO: Node node1 is running more than one daemon pod
May 22 08:13:59.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:59.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:59.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:13:59.330: INFO: Number of nodes with available pods: 1
May 22 08:13:59.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:00.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:00.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:00.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:00.330: INFO: Number of nodes with available pods: 1
May 22 08:14:00.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:01.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:01.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:01.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:01.328: INFO: Number of nodes with available pods: 1
May 22 08:14:01.328: INFO: Node node1 is running more than one daemon pod
May 22 08:14:02.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:02.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:02.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:02.329: INFO: Number of nodes with available pods: 1
May 22 08:14:02.329: INFO: Node node1 is running more than one daemon pod
May 22 08:14:03.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:03.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:03.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:03.328: INFO: Number of nodes with available pods: 1
May 22 08:14:03.328: INFO: Node node1 is running more than one daemon pod
May 22 08:14:04.334: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:04.334: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:04.335: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:04.342: INFO: Number of nodes with available pods: 1
May 22 08:14:04.342: INFO: Node node1 is running more than one daemon pod
May 22 08:14:05.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:05.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:05.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:05.330: INFO: Number of nodes with available pods: 1
May 22 08:14:05.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:06.325: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:06.325: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:06.325: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:06.328: INFO: Number of nodes with available pods: 1
May 22 08:14:06.328: INFO: Node node1 is running more than one daemon pod
May 22 08:14:07.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:07.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:07.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:07.329: INFO: Number of nodes with available pods: 1
May 22 08:14:07.329: INFO: Node node1 is running more than one daemon pod
May 22 08:14:08.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:08.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:08.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:08.330: INFO: Number of nodes with available pods: 1
May 22 08:14:08.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:09.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:09.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:09.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:09.330: INFO: Number of nodes with available pods: 1
May 22 08:14:09.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:10.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:10.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:10.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:10.331: INFO: Number of nodes with available pods: 1
May 22 08:14:10.331: INFO: Node node1 is running more than one daemon pod
May 22 08:14:11.328: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:11.328: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:11.328: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:11.334: INFO: Number of nodes with available pods: 1
May 22 08:14:11.334: INFO: Node node1 is running more than one daemon pod
May 22 08:14:12.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:12.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:12.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:12.329: INFO: Number of nodes with available pods: 1
May 22 08:14:12.329: INFO: Node node1 is running more than one daemon pod
May 22 08:14:13.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:13.326: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:13.326: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:13.329: INFO: Number of nodes with available pods: 2
May 22 08:14:13.329: INFO: Node node1 is running more than one daemon pod
May 22 08:14:14.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:14.328: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:14.328: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:14.336: INFO: Number of nodes with available pods: 2
May 22 08:14:14.336: INFO: Node node1 is running more than one daemon pod
May 22 08:14:15.327: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:15.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:15.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:15.330: INFO: Number of nodes with available pods: 2
May 22 08:14:15.330: INFO: Node node1 is running more than one daemon pod
May 22 08:14:16.326: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:16.327: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:16.327: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:16.330: INFO: Number of nodes with available pods: 3
May 22 08:14:16.330: INFO: Number of running nodes: 3, number of available pods: 3
May 22 08:14:16.330: INFO: Update the DaemonSet to trigger a rollout
May 22 08:14:16.341: INFO: Updating DaemonSet daemon-set
May 22 08:14:22.355: INFO: Roll back the DaemonSet before rollout is complete
May 22 08:14:22.367: INFO: Updating DaemonSet daemon-set
May 22 08:14:22.367: INFO: Make sure DaemonSet rollback is complete
May 22 08:14:22.374: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:22.374: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:22.379: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:22.379: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:22.379: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:23.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:23.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:23.390: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:23.390: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:23.390: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:24.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:24.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:24.389: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:24.389: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:24.389: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:25.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:25.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:25.389: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:25.389: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:25.389: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:26.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:26.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:26.389: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:26.389: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:26.389: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:27.383: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:27.383: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:27.393: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:27.393: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:27.393: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:28.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:28.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:28.391: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:28.391: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:28.391: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:29.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:29.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:29.392: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:29.392: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:29.393: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:30.384: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:30.384: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:30.392: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:30.392: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:30.392: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:31.470: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:31.475: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:31.512: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:31.512: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:31.512: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:32.976: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:32.976: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:33.203: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:33.203: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:33.203: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:33.387: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:33.388: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:33.415: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:33.415: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:33.415: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:34.390: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:34.390: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:34.438: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:34.438: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:34.438: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:35.488: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:35.488: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:35.522: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:35.557: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:35.557: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:36.389: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:36.389: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:36.441: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:36.441: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:36.441: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:37.564: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:37.564: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:37.724: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:37.824: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:37.824: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:38.388: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:38.388: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:38.434: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:38.434: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:38.434: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:39.403: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:39.403: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:39.444: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:39.444: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:39.444: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:40.657: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:40.657: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:40.678: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:40.678: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:40.678: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:41.628: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:41.794: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:41.904: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:41.904: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:41.911: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:42.434: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:42.434: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:42.446: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:42.446: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:42.446: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:43.459: INFO: Wrong image for pod: daemon-set-2dlhj. Expected: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine, got: foo:non-existent.
May 22 08:14:43.459: INFO: Pod daemon-set-2dlhj is not available
May 22 08:14:43.465: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:43.465: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:43.465: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:44.398: INFO: Pod daemon-set-f5lv5 is not available
May 22 08:14:44.405: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:44.405: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:14:44.405: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3540, will wait for the garbage collector to delete the pods
May 22 08:14:44.556: INFO: Deleting DaemonSet.extensions daemon-set took: 24.114307ms
May 22 08:14:44.785: INFO: Terminating DaemonSet.extensions daemon-set pods took: 229.005433ms
May 22 08:15:08.905: INFO: Number of nodes with available pods: 0
May 22 08:15:08.905: INFO: Number of running nodes: 0, number of available pods: 0
May 22 08:15:08.907: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3540/daemonsets","resourceVersion":"12863218"},"items":null}

May 22 08:15:08.909: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3540/pods","resourceVersion":"12863218"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:15:08.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3540" for this suite.
May 22 08:15:14.936: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:15:15.032: INFO: namespace daemonsets-3540 deletion completed in 6.107413153s

• [SLOW TEST:78.797 seconds]
[sig-apps] Daemon set [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:15:15.032: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 22 08:15:15.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-4524'
May 22 08:15:16.208: INFO: stderr: ""
May 22 08:15:16.208: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 08:15:17.212: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:15:17.212: INFO: Found 0 / 1
May 22 08:15:18.212: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:15:18.212: INFO: Found 0 / 1
May 22 08:15:19.212: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:15:19.212: INFO: Found 1 / 1
May 22 08:15:19.212: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 22 08:15:19.214: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:15:19.214: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 08:15:19.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 patch pod redis-master-cmz82 --namespace=kubectl-4524 -p {"metadata":{"annotations":{"x":"y"}}}'
May 22 08:15:19.374: INFO: stderr: ""
May 22 08:15:19.374: INFO: stdout: "pod/redis-master-cmz82 patched\n"
STEP: checking annotations
May 22 08:15:19.377: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:15:19.377: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:15:19.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4524" for this suite.
May 22 08:15:41.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:15:41.548: INFO: namespace kubectl-4524 deletion completed in 22.120527196s

• [SLOW TEST:26.516 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:15:41.556: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-ca20c049-7c69-11e9-aefd-da1a35f02de1
STEP: Creating secret with name s-test-opt-upd-ca20c098-7c69-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-ca20c049-7c69-11e9-aefd-da1a35f02de1
STEP: Updating secret s-test-opt-upd-ca20c098-7c69-11e9-aefd-da1a35f02de1
STEP: Creating secret with name s-test-opt-create-ca20c0b5-7c69-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:17:07.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6750" for this suite.
May 22 08:18:07.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:18:07.147: INFO: namespace projected-6750 deletion completed in 1m0.09513499s

• [SLOW TEST:145.591 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:18:07.148: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:18:07.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 version --client'
May 22 08:18:07.694: INFO: stderr: ""
May 22 08:18:07.694: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 22 08:18:07.732: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-4001'
May 22 08:18:08.076: INFO: stderr: ""
May 22 08:18:08.076: INFO: stdout: "replicationcontroller/redis-master created\n"
May 22 08:18:08.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-4001'
May 22 08:18:08.298: INFO: stderr: ""
May 22 08:18:08.298: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 22 08:18:09.301: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:18:09.301: INFO: Found 0 / 1
May 22 08:18:10.302: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:18:10.302: INFO: Found 0 / 1
May 22 08:18:11.302: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:18:11.302: INFO: Found 0 / 1
May 22 08:18:12.302: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:18:12.302: INFO: Found 1 / 1
May 22 08:18:12.302: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 22 08:18:12.311: INFO: Selector matched 1 pods for map[app:redis]
May 22 08:18:12.311: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 22 08:18:12.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 describe pod redis-master-2pvmx --namespace=kubectl-4001'
May 22 08:18:12.414: INFO: stderr: ""
May 22 08:18:12.414: INFO: stdout: "Name:               redis-master-2pvmx\nNamespace:          kubectl-4001\nPriority:           0\nPriorityClassName:  <none>\nNode:               node2/10.177.11.2\nStart Time:         Wed, 22 May 2019 08:18:08 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.177.11.70\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://2ebb0b57f6da1634f9dfae23616ef0d44a6979567e93908ef50185a9ce7ce739\n    Image:          hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0\n    Image ID:       docker-pullable://hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64@sha256:9b5b1c1ec462abb4b89145a23a1fbf7eb3b2bb25927fc94e820f89a73029889f\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 22 May 2019 08:18:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x85tn (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-x85tn:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-x85tn\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  4s    default-scheduler  Successfully assigned kubectl-4001/redis-master-2pvmx to node2\n  Normal  Pulled     3s    kubelet, node2     Container image \"hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0\" already present on machine\n  Normal  Created    3s    kubelet, node2     Created container redis-master\n  Normal  Started    3s    kubelet, node2     Started container redis-master\n"
May 22 08:18:12.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 describe rc redis-master --namespace=kubectl-4001'
May 22 08:18:12.532: INFO: stderr: ""
May 22 08:18:12.532: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4001\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  4s    replication-controller  Created pod: redis-master-2pvmx\n"
May 22 08:18:12.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 describe service redis-master --namespace=kubectl-4001'
May 22 08:18:12.636: INFO: stderr: ""
May 22 08:18:12.636: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4001\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.178.6.138\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.177.11.70:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 22 08:18:12.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 describe node k8s-master1'
May 22 08:18:12.950: INFO: stderr: ""
May 22 08:18:12.950: INFO: stdout: "Name:               k8s-master1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    cloud-controller=true\n                    failure-domain.beta.kubernetes.io/zone=betayun\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node.netease.com/port_id=35cc7bb0-cdca-44d3-93e7-46e79080ed6f\n                    node.netease.com/role=offline\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 17 Apr 2019 06:09:43 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\n                    node.netease.com/role=offline:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Wed, 22 May 2019 08:18:09 +0000   Wed, 17 Apr 2019 06:09:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Wed, 22 May 2019 08:18:09 +0000   Wed, 17 Apr 2019 06:09:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Wed, 22 May 2019 08:18:09 +0000   Wed, 17 Apr 2019 06:09:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Wed, 22 May 2019 08:18:09 +0000   Mon, 13 May 2019 06:12:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.177.10.17\n  Hostname:    k8s-master1\nCapacity:\n cpu:                4\n ephemeral-storage:  61893400Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8170300Ki\n pods:               110\nAllocatable:\n cpu:                3850m\n ephemeral-storage:  44156055458\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             7351100Ki\n pods:               110\nSystem Info:\n Machine ID:                 698d50801e7346909a80f22ebc561d93\n System UUID:                0ADA9105-7450-472C-84BA-1AB9873E88B9\n Boot ID:                    3a296850-83d6-4efe-90f3-6ef76794bf8b\n Kernel Version:             4.9.0-8-amd64\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.14.1\n Kube-Proxy Version:         v1.14.1\nNon-terminated Pods:         (8 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-368b445c2a8e4133-kpx8f    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\n  kube-system                betayun-dev-kube-controller-manager-k8s-master1            250m (6%)     250m (6%)   400Mi (5%)       400Mi (5%)     5d\n  kube-system                betayun-dev-kube-scheduler-k8s-master1                     200m (5%)     200m (5%)   200Mi (2%)       200Mi (2%)     5d\n  kube-system                cleanlog-zdjzk                                             100m (2%)     100m (2%)   40Mi (0%)        40Mi (0%)      35d\n  kube-system                ds-vpccni-r8qgv                                            0 (0%)        0 (0%)      40Mi (0%)        40Mi (0%)      19h\n  kube-system                etcd-betayun-dev-k8s-master1                               1 (25%)       1 (25%)     2Gi (28%)        2Gi (28%)      5d\n  kube-system                kube-proxy-ldsf9                                           100m (2%)     100m (2%)   100Mi (1%)       100Mi (1%)     5d\n  kube-system                proton-proxy-65c657cc46-zvpkk                              0 (0%)        0 (0%)      100Mi (1%)       100Mi (1%)     20h\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1650m (42%)   1650m (42%)\n  memory             2928Mi (40%)  2928Mi (40%)\n  ephemeral-storage  0 (0%)        0 (0%)\nEvents:              <none>\n"
May 22 08:18:12.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 describe namespace kubectl-4001'
May 22 08:18:13.104: INFO: stderr: ""
May 22 08:18:13.104: INFO: stdout: "Name:         kubectl-4001\nLabels:       e2e-framework=kubectl\n              e2e-run=f8fdfde3-7c5f-11e9-aefd-da1a35f02de1\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:18:13.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4001" for this suite.
May 22 08:18:37.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:18:37.240: INFO: namespace kubectl-4001 deletion completed in 24.131502152s

• [SLOW TEST:30.092 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:18:37.247: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-9459/secret-test-32de8082-7c6a-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:18:37.353: INFO: Waiting up to 5m0s for pod "pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1" in namespace "secrets-9459" to be "success or failure"
May 22 08:18:37.357: INFO: Pod "pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.77068ms
May 22 08:18:39.362: INFO: Pod "pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007718202s
May 22 08:18:41.366: INFO: Pod "pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01179105s
STEP: Saw pod success
May 22 08:18:41.366: INFO: Pod "pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:18:41.370: INFO: Trying to get logs from node node1 pod pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1 container env-test: <nil>
STEP: delete the pod
May 22 08:18:41.422: INFO: Waiting for pod pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1 to disappear
May 22 08:18:41.425: INFO: Pod pod-configmaps-32df3871-7c6a-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:18:41.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9459" for this suite.
May 22 08:18:47.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:18:47.547: INFO: namespace secrets-9459 deletion completed in 6.116962481s

• [SLOW TEST:10.301 seconds]
[sig-api-machinery] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:18:47.549: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-38ffd8ef-7c6a-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:18:47.633: INFO: Waiting up to 5m0s for pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1" in namespace "secrets-8640" to be "success or failure"
May 22 08:18:47.635: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.840585ms
May 22 08:18:49.688: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054655369s
May 22 08:18:51.697: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.063652982s
May 22 08:18:53.705: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071885043s
May 22 08:18:55.709: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.075641157s
May 22 08:18:57.711: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.078325207s
STEP: Saw pod success
May 22 08:18:57.711: INFO: Pod "pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:18:57.714: INFO: Trying to get logs from node node2 pod pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 08:18:57.742: INFO: Waiting for pod pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1 to disappear
May 22 08:18:57.744: INFO: Pod pod-secrets-3900c6de-7c6a-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:18:57.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8640" for this suite.
May 22 08:19:03.762: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:19:03.848: INFO: namespace secrets-8640 deletion completed in 6.099113426s

• [SLOW TEST:16.299 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:19:03.853: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-kz2h
STEP: Creating a pod to test atomic-volume-subpath
May 22 08:19:03.964: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kz2h" in namespace "subpath-5914" to be "success or failure"
May 22 08:19:03.967: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.627697ms
May 22 08:19:05.972: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008233365s
May 22 08:19:07.976: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012528083s
May 22 08:19:09.981: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017476962s
May 22 08:19:11.990: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 8.026271147s
May 22 08:19:13.993: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02943367s
May 22 08:19:15.997: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 12.033099996s
May 22 08:19:18.000: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 14.0363231s
May 22 08:19:20.004: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 16.040061082s
May 22 08:19:22.022: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 18.057924123s
May 22 08:19:24.027: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 20.062808237s
May 22 08:19:26.031: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 22.066793562s
May 22 08:19:28.035: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Pending", Reason="", readiness=false. Elapsed: 24.071336977s
May 22 08:19:30.039: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Running", Reason="", readiness=true. Elapsed: 26.075313018s
May 22 08:19:32.044: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Running", Reason="", readiness=true. Elapsed: 28.079996777s
May 22 08:19:34.048: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Running", Reason="", readiness=true. Elapsed: 30.08399369s
May 22 08:19:36.053: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Running", Reason="", readiness=true. Elapsed: 32.088808645s
May 22 08:19:38.058: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Running", Reason="", readiness=true. Elapsed: 34.093668323s
May 22 08:19:40.063: INFO: Pod "pod-subpath-test-downwardapi-kz2h": Phase="Succeeded", Reason="", readiness=false. Elapsed: 36.098735437s
STEP: Saw pod success
May 22 08:19:40.063: INFO: Pod "pod-subpath-test-downwardapi-kz2h" satisfied condition "success or failure"
May 22 08:19:40.066: INFO: Trying to get logs from node node1 pod pod-subpath-test-downwardapi-kz2h container test-container-subpath-downwardapi-kz2h: <nil>
STEP: delete the pod
May 22 08:19:40.094: INFO: Waiting for pod pod-subpath-test-downwardapi-kz2h to disappear
May 22 08:19:40.098: INFO: Pod pod-subpath-test-downwardapi-kz2h no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kz2h
May 22 08:19:40.099: INFO: Deleting pod "pod-subpath-test-downwardapi-kz2h" in namespace "subpath-5914"
[AfterEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:19:40.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5914" for this suite.
May 22 08:19:46.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:19:46.225: INFO: namespace subpath-5914 deletion completed in 6.116217759s

• [SLOW TEST:42.372 seconds]
[sig-storage] Subpath
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:19:46.225: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 22 08:19:46.349: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5024,SelfLink:/api/v1/namespaces/watch-5024/configmaps/e2e-watch-test-watch-closed,UID:5c003850-7c6a-11e9-9cf4-fa163ecd1d63,ResourceVersion:12867166,Generation:0,CreationTimestamp:2019-05-22 08:19:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 22 08:19:46.350: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5024,SelfLink:/api/v1/namespaces/watch-5024/configmaps/e2e-watch-test-watch-closed,UID:5c003850-7c6a-11e9-9cf4-fa163ecd1d63,ResourceVersion:12867167,Generation:0,CreationTimestamp:2019-05-22 08:19:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 22 08:19:46.365: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5024,SelfLink:/api/v1/namespaces/watch-5024/configmaps/e2e-watch-test-watch-closed,UID:5c003850-7c6a-11e9-9cf4-fa163ecd1d63,ResourceVersion:12867168,Generation:0,CreationTimestamp:2019-05-22 08:19:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 22 08:19:46.365: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-5024,SelfLink:/api/v1/namespaces/watch-5024/configmaps/e2e-watch-test-watch-closed,UID:5c003850-7c6a-11e9-9cf4-fa163ecd1d63,ResourceVersion:12867169,Generation:0,CreationTimestamp:2019-05-22 08:19:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:19:46.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5024" for this suite.
May 22 08:19:52.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:19:52.474: INFO: namespace watch-5024 deletion completed in 6.103729554s

• [SLOW TEST:6.249 seconds]
[sig-api-machinery] Watchers
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:19:52.475: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-5faebeac-7c6a-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:19:52.529: INFO: Waiting up to 5m0s for pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1" in namespace "configmap-3838" to be "success or failure"
May 22 08:19:52.532: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.103634ms
May 22 08:19:54.537: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007456447s
May 22 08:19:56.541: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011235918s
May 22 08:19:58.543: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014147323s
May 22 08:20:00.858: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.329149242s
May 22 08:20:03.162: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.632807649s
May 22 08:20:05.226: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.696940128s
May 22 08:20:07.254: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.724977648s
May 22 08:20:09.259: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 16.729662177s
STEP: Saw pod success
May 22 08:20:09.259: INFO: Pod "pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:20:09.266: INFO: Trying to get logs from node node1 pod pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:20:09.290: INFO: Waiting for pod pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1 to disappear
May 22 08:20:09.293: INFO: Pod pod-configmaps-5faf6719-7c6a-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:20:09.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3838" for this suite.
May 22 08:20:15.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:20:15.401: INFO: namespace configmap-3838 deletion completed in 6.103292596s

• [SLOW TEST:22.927 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:20:15.402: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 22 08:20:15.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 cluster-info'
May 22 08:20:15.579: INFO: stderr: ""
May 22 08:20:15.579: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.178.4.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.178.4.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mProtonProxy\x1b[0m is running at \x1b[0;33mhttps://10.178.4.1:443/api/v1/namespaces/kube-system/services/proton-proxy:proxy/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:20:15.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7656" for this suite.
May 22 08:20:21.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:20:21.701: INFO: namespace kubectl-7656 deletion completed in 6.11687453s

• [SLOW TEST:6.299 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:20:21.702: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-8330
May 22 08:20:33.773: INFO: Started pod liveness-exec in namespace container-probe-8330
STEP: checking the pod's current state and verifying that restartCount is present
May 22 08:20:33.776: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:24:35.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8330" for this suite.
May 22 08:24:41.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:24:43.586: INFO: namespace container-probe-8330 deletion completed in 8.324487733s

• [SLOW TEST:261.884 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:24:43.586: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:24:44.138: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1" in namespace "downward-api-3436" to be "success or failure"
May 22 08:24:44.266: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 127.829848ms
May 22 08:24:46.346: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.208354363s
May 22 08:24:48.352: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2148281s
May 22 08:24:50.489: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.351598665s
May 22 08:24:52.497: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.359125706s
May 22 08:24:54.501: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.3631738s
May 22 08:24:56.504: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.366787791s
May 22 08:24:58.512: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.374185558s
May 22 08:25:00.517: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379386618s
May 22 08:25:02.521: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.383315585s
May 22 08:25:04.525: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.387666381s
May 22 08:25:06.529: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.390832183s
May 22 08:25:08.533: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.395470226s
May 22 08:25:10.537: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.399288279s
May 22 08:25:12.541: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.403071514s
May 22 08:25:14.545: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 30.407795025s
STEP: Saw pod success
May 22 08:25:14.546: INFO: Pod "downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:25:14.549: INFO: Trying to get logs from node node1 pod downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:25:14.810: INFO: Waiting for pod downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1 to disappear
May 22 08:25:14.814: INFO: Pod downwardapi-volume-0d5269da-7c6b-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:25:14.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3436" for this suite.
May 22 08:25:20.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:25:20.997: INFO: namespace downward-api-3436 deletion completed in 6.177106165s

• [SLOW TEST:37.411 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:25:20.997: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 22 08:25:21.049: INFO: Waiting up to 5m0s for pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1" in namespace "emptydir-8147" to be "success or failure"
May 22 08:25:21.054: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.476004ms
May 22 08:25:23.058: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008910084s
May 22 08:25:25.062: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013574764s
May 22 08:25:27.066: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01698326s
May 22 08:25:29.069: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020446775s
May 22 08:25:31.079: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.030069814s
May 22 08:25:33.084: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.035055837s
STEP: Saw pod success
May 22 08:25:33.084: INFO: Pod "pod-237fca91-7c6b-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:25:33.088: INFO: Trying to get logs from node node2 pod pod-237fca91-7c6b-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:25:33.505: INFO: Waiting for pod pod-237fca91-7c6b-11e9-aefd-da1a35f02de1 to disappear
May 22 08:25:33.510: INFO: Pod pod-237fca91-7c6b-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:25:33.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8147" for this suite.
May 22 08:25:39.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:25:39.816: INFO: namespace emptydir-8147 deletion completed in 6.301653409s

• [SLOW TEST:18.819 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:25:39.817: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:25:42.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5233" for this suite.
May 22 08:27:04.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:27:04.248: INFO: namespace kubelet-test-5233 deletion completed in 1m22.11006598s

• [SLOW TEST:84.432 seconds]
[k8s.io] Kubelet
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:27:04.249: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-6109ba65-7c6b-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:27:04.301: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1" in namespace "projected-7872" to be "success or failure"
May 22 08:27:04.304: INFO: Pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.39642ms
May 22 08:27:06.308: INFO: Pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007109701s
May 22 08:27:08.313: INFO: Pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011396848s
May 22 08:27:10.317: INFO: Pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01598489s
STEP: Saw pod success
May 22 08:27:10.317: INFO: Pod "pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:27:10.321: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:27:10.341: INFO: Waiting for pod pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1 to disappear
May 22 08:27:10.343: INFO: Pod pod-projected-configmaps-610ab61d-7c6b-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:27:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7872" for this suite.
May 22 08:27:16.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:27:16.460: INFO: namespace projected-7872 deletion completed in 6.111990806s

• [SLOW TEST:12.211 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:27:16.460: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 22 08:27:16.513: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:27:58.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3299" for this suite.
May 22 08:28:04.398: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:28:04.478: INFO: namespace pods-3299 deletion completed in 6.092471257s

• [SLOW TEST:48.018 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:28:04.478: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:28:04.614: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 22 08:28:04.627: INFO: Pod name sample-pod: Found 0 pods out of 1
May 22 08:28:09.632: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 08:28:09.633: INFO: Creating deployment "test-rolling-update-deployment"
May 22 08:28:09.641: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 22 08:28:09.646: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 22 08:28:11.654: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 22 08:28:11.657: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110489, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110489, loc:(*time.Location)(0x80d5f20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110489, loc:(*time.Location)(0x80d5f20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110489, loc:(*time.Location)(0x80d5f20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-7867447499\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 22 08:28:13.662: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 08:28:13.686: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-5936,SelfLink:/apis/apps/v1/namespaces/deployment-5936/deployments/test-rolling-update-deployment,UID:87fc9777-7c6b-11e9-9cf4-fa163ecd1d63,ResourceVersion:12875047,Generation:1,CreationTimestamp:2019-05-22 08:28:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-22 08:28:09 +0000 UTC 2019-05-22 08:28:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-22 08:28:13 +0000 UTC 2019-05-22 08:28:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-7867447499" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 22 08:28:13.690: INFO: New ReplicaSet "test-rolling-update-deployment-7867447499" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-7867447499,GenerateName:,Namespace:deployment-5936,SelfLink:/apis/apps/v1/namespaces/deployment-5936/replicasets/test-rolling-update-deployment-7867447499,UID:880893ef-7c6b-11e9-8476-fa163e387bbe,ResourceVersion:12875036,Generation:1,CreationTimestamp:2019-05-22 08:28:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 7867447499,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 87fc9777-7c6b-11e9-9cf4-fa163ecd1d63 0xc0015bf077 0xc0015bf078}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7867447499,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 7867447499,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 22 08:28:13.690: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 22 08:28:13.690: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-5936,SelfLink:/apis/apps/v1/namespaces/deployment-5936/replicasets/test-rolling-update-controller,UID:84fedef5-7c6b-11e9-9cf4-fa163ecd1d63,ResourceVersion:12875046,Generation:2,CreationTimestamp:2019-05-22 08:28:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 87fc9777-7c6b-11e9-9cf4-fa163ecd1d63 0xc0015bef97 0xc0015bef98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 08:28:13.716: INFO: Pod "test-rolling-update-deployment-7867447499-pk7tb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-7867447499-pk7tb,GenerateName:test-rolling-update-deployment-7867447499-,Namespace:deployment-5936,SelfLink:/api/v1/namespaces/deployment-5936/pods/test-rolling-update-deployment-7867447499-pk7tb,UID:880971a2-7c6b-11e9-8476-fa163e387bbe,ResourceVersion:12875035,Generation:0,CreationTimestamp:2019-05-22 08:28:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 7867447499,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-7867447499 880893ef-7c6b-11e9-8476-fa163e387bbe 0xc0015bf977 0xc0015bf978}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-trbps {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-trbps,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [{default-token-trbps true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:node2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0015bf9f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0015bfa10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:28:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:28:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:28:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-22 08:28:09 +0000 UTC  }],Message:,Reason:,HostIP:10.177.11.2,PodIP:10.177.11.108,StartTime:2019-05-22 08:28:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-22 08:28:10 +0000 UTC,} nil} {nil nil nil} true 0 hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 docker-pullable://hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64@sha256:9b5b1c1ec462abb4b89145a23a1fbf7eb3b2bb25927fc94e820f89a73029889f docker://2372708e686bce0b2a2c64b2dadb0240db8188802cf5b41a2fcce8de40517560}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:28:13.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5936" for this suite.
May 22 08:28:19.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:28:19.820: INFO: namespace deployment-5936 deletion completed in 6.097135494s

• [SLOW TEST:15.342 seconds]
[sig-apps] Deployment
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:28:19.821: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:28:26.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1486" for this suite.
May 22 08:28:32.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:28:32.152: INFO: namespace namespaces-1486 deletion completed in 6.128135926s
STEP: Destroying namespace "nsdeletetest-7617" for this suite.
May 22 08:28:32.155: INFO: Namespace nsdeletetest-7617 was already deleted
STEP: Destroying namespace "nsdeletetest-4262" for this suite.
May 22 08:28:38.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:28:38.281: INFO: namespace nsdeletetest-4262 deletion completed in 6.125227647s

• [SLOW TEST:18.460 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:28:38.282: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /root/workspace/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-6438
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-6438
STEP: Deleting pre-stop pod
May 22 08:28:51.387: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": null,
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
May 22 08:28:56.384: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:28:56.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-6438" for this suite.
May 22 08:29:36.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:29:36.501: INFO: namespace prestop-6438 deletion completed in 40.105601168s

• [SLOW TEST:58.218 seconds]
[k8s.io] [sig-node] PreStop
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:29:36.501: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:29:36.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1061" for this suite.
May 22 08:29:42.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:29:43.236: INFO: namespace kubelet-test-1061 deletion completed in 6.604041495s

• [SLOW TEST:6.735 seconds]
[k8s.io] Kubelet
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:29:43.236: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-85mxs in namespace proxy-6067
I0522 08:29:43.352962      19 runners.go:184] Created replication controller with name: proxy-service-85mxs, namespace: proxy-6067, replica count: 1
I0522 08:29:44.418546      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:45.425240      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:46.472997      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:47.490376      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:48.610099      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:49.620573      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:50.631677      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:51.632913      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:52.636729      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:53.854307      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:54.856578      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:55.884766      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:56.895590      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:57.895776      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:58.896053      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:29:59.896236      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:00.896507      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:01.896771      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:02.897047      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:03.897271      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:04.897494      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:05.897768      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:06.898054      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:07.898327      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:08.898563      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:09.898833      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:10.899135      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:11.899420      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0522 08:30:12.899682      19 runners.go:184] proxy-service-85mxs Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 22 08:30:12.904: INFO: setup took 29.60854082s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 10.140576ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 10.481091ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 10.398027ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 9.745707ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 11.282935ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 9.829658ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 10.46031ms)
May 22 08:30:12.920: INFO: (0) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 10.045609ms)
May 22 08:30:12.934: INFO: (0) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 23.99847ms)
May 22 08:30:12.934: INFO: (0) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 23.673894ms)
May 22 08:30:12.934: INFO: (0) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 23.912248ms)
May 22 08:30:12.935: INFO: (0) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 24.66781ms)
May 22 08:30:12.935: INFO: (0) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 24.727169ms)
May 22 08:30:12.941: INFO: (0) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 30.779364ms)
May 22 08:30:12.943: INFO: (0) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 34.074253ms)
May 22 08:30:12.944: INFO: (0) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 33.966971ms)
May 22 08:30:12.948: INFO: (1) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 4.181326ms)
May 22 08:30:12.950: INFO: (1) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 5.853761ms)
May 22 08:30:12.951: INFO: (1) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.888173ms)
May 22 08:30:12.952: INFO: (1) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 6.874819ms)
May 22 08:30:12.952: INFO: (1) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.841095ms)
May 22 08:30:12.952: INFO: (1) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 7.573806ms)
May 22 08:30:12.952: INFO: (1) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 7.334941ms)
May 22 08:30:12.953: INFO: (1) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 7.751691ms)
May 22 08:30:12.954: INFO: (1) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 9.210089ms)
May 22 08:30:12.954: INFO: (1) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 9.014206ms)
May 22 08:30:12.954: INFO: (1) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 8.792989ms)
May 22 08:30:12.954: INFO: (1) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 9.201576ms)
May 22 08:30:12.954: INFO: (1) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 9.282085ms)
May 22 08:30:12.956: INFO: (1) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 11.058959ms)
May 22 08:30:12.956: INFO: (1) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 10.275506ms)
May 22 08:30:12.956: INFO: (1) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 10.449331ms)
May 22 08:30:12.959: INFO: (2) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 3.607731ms)
May 22 08:30:12.960: INFO: (2) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.274547ms)
May 22 08:30:12.961: INFO: (2) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.522797ms)
May 22 08:30:12.962: INFO: (2) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 5.686446ms)
May 22 08:30:12.962: INFO: (2) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 5.815976ms)
May 22 08:30:12.962: INFO: (2) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 5.760719ms)
May 22 08:30:12.963: INFO: (2) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 6.577865ms)
May 22 08:30:12.963: INFO: (2) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.833513ms)
May 22 08:30:12.964: INFO: (2) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 7.342047ms)
May 22 08:30:12.964: INFO: (2) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 7.520823ms)
May 22 08:30:12.964: INFO: (2) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 8.069036ms)
May 22 08:30:12.964: INFO: (2) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 7.945129ms)
May 22 08:30:12.966: INFO: (2) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 9.6317ms)
May 22 08:30:12.966: INFO: (2) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 9.211915ms)
May 22 08:30:12.966: INFO: (2) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 9.778721ms)
May 22 08:30:12.966: INFO: (2) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 9.323098ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 5.89263ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 5.89882ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 6.243959ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.123816ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.06732ms)
May 22 08:30:12.972: INFO: (3) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.039575ms)
May 22 08:30:12.973: INFO: (3) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 6.546029ms)
May 22 08:30:12.973: INFO: (3) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.512742ms)
May 22 08:30:12.973: INFO: (3) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.305491ms)
May 22 08:30:12.973: INFO: (3) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 6.728162ms)
May 22 08:30:12.977: INFO: (3) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 10.42724ms)
May 22 08:30:12.978: INFO: (3) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 11.087899ms)
May 22 08:30:12.978: INFO: (3) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 11.221889ms)
May 22 08:30:12.978: INFO: (3) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 11.547741ms)
May 22 08:30:12.978: INFO: (3) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 11.62366ms)
May 22 08:30:12.978: INFO: (3) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 11.797227ms)
May 22 08:30:12.989: INFO: (4) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 11.055661ms)
May 22 08:30:12.990: INFO: (4) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 11.315648ms)
May 22 08:30:12.990: INFO: (4) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 11.66816ms)
May 22 08:30:12.990: INFO: (4) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 11.846191ms)
May 22 08:30:12.990: INFO: (4) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 11.870096ms)
May 22 08:30:12.990: INFO: (4) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 12.093803ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 12.472292ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 12.517961ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 12.19754ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 12.480021ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 12.160195ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 12.600166ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 12.484911ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 12.992521ms)
May 22 08:30:12.991: INFO: (4) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 12.925022ms)
May 22 08:30:12.992: INFO: (4) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 13.290798ms)
May 22 08:30:12.999: INFO: (5) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 7.614789ms)
May 22 08:30:13.001: INFO: (5) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 9.662315ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 9.578581ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 9.598939ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 10.080192ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 10.092521ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 10.218654ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 10.085456ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 10.115699ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 10.36977ms)
May 22 08:30:13.002: INFO: (5) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 10.332283ms)
May 22 08:30:13.004: INFO: (5) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 11.640764ms)
May 22 08:30:13.004: INFO: (5) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 11.735441ms)
May 22 08:30:13.004: INFO: (5) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 11.924122ms)
May 22 08:30:13.004: INFO: (5) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 11.989932ms)
May 22 08:30:13.004: INFO: (5) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 12.496893ms)
May 22 08:30:13.007: INFO: (6) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 2.657147ms)
May 22 08:30:13.009: INFO: (6) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 4.983806ms)
May 22 08:30:13.013: INFO: (6) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 7.734197ms)
May 22 08:30:13.013: INFO: (6) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 7.955146ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 8.93527ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 8.518308ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 9.363768ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 9.192728ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 8.701777ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 8.436516ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 9.369672ms)
May 22 08:30:13.014: INFO: (6) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 9.174444ms)
May 22 08:30:13.017: INFO: (6) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 12.038582ms)
May 22 08:30:13.017: INFO: (6) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 12.182004ms)
May 22 08:30:13.017: INFO: (6) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 12.598625ms)
May 22 08:30:13.018: INFO: (6) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 12.775951ms)
May 22 08:30:13.021: INFO: (7) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 2.894124ms)
May 22 08:30:13.021: INFO: (7) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 3.140638ms)
May 22 08:30:13.022: INFO: (7) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 4.144333ms)
May 22 08:30:13.023: INFO: (7) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.546292ms)
May 22 08:30:13.024: INFO: (7) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 5.180032ms)
May 22 08:30:13.024: INFO: (7) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 6.36628ms)
May 22 08:30:13.025: INFO: (7) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 6.795161ms)
May 22 08:30:13.025: INFO: (7) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.775607ms)
May 22 08:30:13.025: INFO: (7) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 6.960113ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 7.794564ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 7.56572ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 7.688209ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 7.701222ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 8.099697ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 7.904854ms)
May 22 08:30:13.026: INFO: (7) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 8.026847ms)
May 22 08:30:13.031: INFO: (8) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.810103ms)
May 22 08:30:13.032: INFO: (8) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 5.273675ms)
May 22 08:30:13.033: INFO: (8) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 6.237851ms)
May 22 08:30:13.033: INFO: (8) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.580773ms)
May 22 08:30:13.033: INFO: (8) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 6.99217ms)
May 22 08:30:13.033: INFO: (8) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.48041ms)
May 22 08:30:13.033: INFO: (8) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 6.628114ms)
May 22 08:30:13.035: INFO: (8) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 7.977737ms)
May 22 08:30:13.035: INFO: (8) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 8.176034ms)
May 22 08:30:13.036: INFO: (8) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 8.994217ms)
May 22 08:30:13.036: INFO: (8) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 8.744868ms)
May 22 08:30:13.036: INFO: (8) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 9.358418ms)
May 22 08:30:13.036: INFO: (8) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 9.430499ms)
May 22 08:30:13.036: INFO: (8) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 9.474749ms)
May 22 08:30:13.037: INFO: (8) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 9.836022ms)
May 22 08:30:13.038: INFO: (8) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 10.781834ms)
May 22 08:30:13.040: INFO: (9) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 2.377457ms)
May 22 08:30:13.041: INFO: (9) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 3.679998ms)
May 22 08:30:13.042: INFO: (9) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 3.368733ms)
May 22 08:30:13.042: INFO: (9) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 3.92958ms)
May 22 08:30:13.044: INFO: (9) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 5.450485ms)
May 22 08:30:13.044: INFO: (9) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 5.440954ms)
May 22 08:30:13.044: INFO: (9) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 6.135194ms)
May 22 08:30:13.044: INFO: (9) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 6.46245ms)
May 22 08:30:13.044: INFO: (9) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 5.868466ms)
May 22 08:30:13.045: INFO: (9) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 6.039263ms)
May 22 08:30:13.045: INFO: (9) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.158653ms)
May 22 08:30:13.045: INFO: (9) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 6.236335ms)
May 22 08:30:13.045: INFO: (9) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 6.842082ms)
May 22 08:30:13.046: INFO: (9) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 7.075173ms)
May 22 08:30:13.046: INFO: (9) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 7.375656ms)
May 22 08:30:13.048: INFO: (9) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 10.711304ms)
May 22 08:30:13.052: INFO: (10) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 3.722642ms)
May 22 08:30:13.053: INFO: (10) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 4.044823ms)
May 22 08:30:13.053: INFO: (10) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 4.274437ms)
May 22 08:30:13.053: INFO: (10) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 4.673074ms)
May 22 08:30:13.059: INFO: (10) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 10.199644ms)
May 22 08:30:13.059: INFO: (10) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 10.023508ms)
May 22 08:30:13.060: INFO: (10) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 11.022162ms)
May 22 08:30:13.061: INFO: (10) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 11.293598ms)
May 22 08:30:13.062: INFO: (10) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 12.466582ms)
May 22 08:30:13.062: INFO: (10) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 12.447344ms)
May 22 08:30:13.063: INFO: (10) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 13.18632ms)
May 22 08:30:13.065: INFO: (10) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 15.256971ms)
May 22 08:30:13.066: INFO: (10) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 16.235045ms)
May 22 08:30:13.066: INFO: (10) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 16.799127ms)
May 22 08:30:13.066: INFO: (10) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 16.76983ms)
May 22 08:30:13.066: INFO: (10) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 17.214114ms)
May 22 08:30:13.072: INFO: (11) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 4.883172ms)
May 22 08:30:13.072: INFO: (11) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 5.578079ms)
May 22 08:30:13.074: INFO: (11) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 7.421768ms)
May 22 08:30:13.075: INFO: (11) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 7.821348ms)
May 22 08:30:13.075: INFO: (11) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 8.097087ms)
May 22 08:30:13.075: INFO: (11) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 7.922864ms)
May 22 08:30:13.075: INFO: (11) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 8.457723ms)
May 22 08:30:13.087: INFO: (11) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 20.349207ms)
May 22 08:30:13.088: INFO: (11) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 21.438215ms)
May 22 08:30:13.088: INFO: (11) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 21.275489ms)
May 22 08:30:13.088: INFO: (11) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 21.375655ms)
May 22 08:30:13.089: INFO: (11) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 21.628874ms)
May 22 08:30:13.089: INFO: (11) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 21.757762ms)
May 22 08:30:13.089: INFO: (11) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 22.109507ms)
May 22 08:30:13.089: INFO: (11) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 22.134247ms)
May 22 08:30:13.089: INFO: (11) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 22.364323ms)
May 22 08:30:13.093: INFO: (12) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 2.924212ms)
May 22 08:30:13.093: INFO: (12) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 3.234641ms)
May 22 08:30:13.094: INFO: (12) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.308584ms)
May 22 08:30:13.097: INFO: (12) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.317717ms)
May 22 08:30:13.097: INFO: (12) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 6.696721ms)
May 22 08:30:13.097: INFO: (12) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 7.793383ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 6.528306ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 6.371206ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.081917ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 7.992199ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 6.925605ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 6.011964ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 6.250879ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 8.222666ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 6.440541ms)
May 22 08:30:13.098: INFO: (12) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 6.623833ms)
May 22 08:30:13.101: INFO: (13) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 3.241197ms)
May 22 08:30:13.101: INFO: (13) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 3.383632ms)
May 22 08:30:13.102: INFO: (13) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 3.747078ms)
May 22 08:30:13.102: INFO: (13) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 4.049473ms)
May 22 08:30:13.103: INFO: (13) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 4.512308ms)
May 22 08:30:13.103: INFO: (13) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 4.780737ms)
May 22 08:30:13.104: INFO: (13) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 5.47589ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 7.457174ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 7.718768ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 7.669418ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 7.723841ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 7.713854ms)
May 22 08:30:13.106: INFO: (13) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 7.972408ms)
May 22 08:30:13.107: INFO: (13) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 9.353615ms)
May 22 08:30:13.108: INFO: (13) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 9.355715ms)
May 22 08:30:13.108: INFO: (13) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 9.452008ms)
May 22 08:30:13.111: INFO: (14) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 3.113655ms)
May 22 08:30:13.111: INFO: (14) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 3.324603ms)
May 22 08:30:13.112: INFO: (14) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 3.320527ms)
May 22 08:30:13.112: INFO: (14) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 3.361365ms)
May 22 08:30:13.112: INFO: (14) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 3.731742ms)
May 22 08:30:13.112: INFO: (14) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 4.350843ms)
May 22 08:30:13.113: INFO: (14) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 4.133881ms)
May 22 08:30:13.113: INFO: (14) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 4.663059ms)
May 22 08:30:13.113: INFO: (14) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 5.60681ms)
May 22 08:30:13.114: INFO: (14) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 5.978747ms)
May 22 08:30:13.114: INFO: (14) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 5.812789ms)
May 22 08:30:13.114: INFO: (14) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 6.033364ms)
May 22 08:30:13.114: INFO: (14) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 5.519632ms)
May 22 08:30:13.114: INFO: (14) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 5.6042ms)
May 22 08:30:13.115: INFO: (14) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 6.603324ms)
May 22 08:30:13.115: INFO: (14) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 7.093817ms)
May 22 08:30:13.130: INFO: (15) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 14.892135ms)
May 22 08:30:13.130: INFO: (15) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 15.052703ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 14.99179ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 15.207957ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 15.209723ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 15.364456ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 15.273826ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 15.388804ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 15.593607ms)
May 22 08:30:13.131: INFO: (15) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 15.258627ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 20.626179ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 20.551367ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 20.629459ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 20.62027ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 20.918386ms)
May 22 08:30:13.136: INFO: (15) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 20.785491ms)
May 22 08:30:13.139: INFO: (16) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 2.261963ms)
May 22 08:30:13.139: INFO: (16) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 2.473614ms)
May 22 08:30:13.140: INFO: (16) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 2.940222ms)
May 22 08:30:13.140: INFO: (16) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 2.915359ms)
May 22 08:30:13.141: INFO: (16) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 4.731735ms)
May 22 08:30:13.142: INFO: (16) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 5.47229ms)
May 22 08:30:13.142: INFO: (16) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 5.101723ms)
May 22 08:30:13.142: INFO: (16) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 5.435916ms)
May 22 08:30:13.142: INFO: (16) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 5.019756ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 5.234072ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 5.075166ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 5.050921ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 5.556163ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 6.633852ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 6.332793ms)
May 22 08:30:13.143: INFO: (16) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 5.799234ms)
May 22 08:30:13.146: INFO: (17) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 2.728078ms)
May 22 08:30:13.146: INFO: (17) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 2.975258ms)
May 22 08:30:13.147: INFO: (17) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 3.196869ms)
May 22 08:30:13.147: INFO: (17) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 3.546799ms)
May 22 08:30:13.148: INFO: (17) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 4.724555ms)
May 22 08:30:13.149: INFO: (17) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 5.385127ms)
May 22 08:30:13.149: INFO: (17) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 5.629624ms)
May 22 08:30:13.149: INFO: (17) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.004473ms)
May 22 08:30:13.150: INFO: (17) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 6.142559ms)
May 22 08:30:13.150: INFO: (17) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.241296ms)
May 22 08:30:13.150: INFO: (17) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 6.794032ms)
May 22 08:30:13.151: INFO: (17) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 6.924875ms)
May 22 08:30:13.151: INFO: (17) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 6.932097ms)
May 22 08:30:13.151: INFO: (17) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 7.134909ms)
May 22 08:30:13.151: INFO: (17) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 7.651737ms)
May 22 08:30:13.151: INFO: (17) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 7.908588ms)
May 22 08:30:13.155: INFO: (18) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 3.354344ms)
May 22 08:30:13.158: INFO: (18) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 6.858087ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 6.061661ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 6.65109ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 6.600563ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 7.015985ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 6.88735ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.075865ms)
May 22 08:30:13.159: INFO: (18) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 6.618873ms)
May 22 08:30:13.160: INFO: (18) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 6.863866ms)
May 22 08:30:13.161: INFO: (18) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 8.762543ms)
May 22 08:30:13.161: INFO: (18) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 8.975266ms)
May 22 08:30:13.162: INFO: (18) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 9.290149ms)
May 22 08:30:13.162: INFO: (18) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 9.609061ms)
May 22 08:30:13.162: INFO: (18) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 9.297894ms)
May 22 08:30:13.162: INFO: (18) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 9.646965ms)
May 22 08:30:13.188: INFO: (19) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:1080/proxy/rewriteme">test<... (200; 25.904614ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:443/proxy/tlsrewritem... (200; 26.22904ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname1/proxy/: tls baz (200; 26.315431ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:1080/proxy/rewriteme">... (200; 26.229638ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:460/proxy/: tls baz (200; 26.570219ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/: <a href="/api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59/proxy/rewriteme">test</a> (200; 26.251049ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname2/proxy/: bar (200; 26.665921ms)
May 22 08:30:13.189: INFO: (19) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:162/proxy/: bar (200; 26.776751ms)
May 22 08:30:13.191: INFO: (19) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:162/proxy/: bar (200; 28.48472ms)
May 22 08:30:13.193: INFO: (19) /api/v1/namespaces/proxy-6067/pods/proxy-service-85mxs-gbp59:160/proxy/: foo (200; 30.961993ms)
May 22 08:30:13.194: INFO: (19) /api/v1/namespaces/proxy-6067/pods/http:proxy-service-85mxs-gbp59:160/proxy/: foo (200; 31.495957ms)
May 22 08:30:13.194: INFO: (19) /api/v1/namespaces/proxy-6067/pods/https:proxy-service-85mxs-gbp59:462/proxy/: tls qux (200; 31.481791ms)
May 22 08:30:13.196: INFO: (19) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname2/proxy/: bar (200; 32.990913ms)
May 22 08:30:13.196: INFO: (19) /api/v1/namespaces/proxy-6067/services/http:proxy-service-85mxs:portname1/proxy/: foo (200; 33.846517ms)
May 22 08:30:13.196: INFO: (19) /api/v1/namespaces/proxy-6067/services/https:proxy-service-85mxs:tlsportname2/proxy/: tls qux (200; 33.89926ms)
May 22 08:30:13.197: INFO: (19) /api/v1/namespaces/proxy-6067/services/proxy-service-85mxs:portname1/proxy/: foo (200; 34.279629ms)
STEP: deleting ReplicationController proxy-service-85mxs in namespace proxy-6067, will wait for the garbage collector to delete the pods
May 22 08:30:13.257: INFO: Deleting ReplicationController proxy-service-85mxs took: 7.806164ms
May 22 08:30:13.557: INFO: Terminating ReplicationController proxy-service-85mxs pods took: 300.212216ms
[AfterEach] version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:30:27.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6067" for this suite.
May 22 08:30:33.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:30:33.673: INFO: namespace proxy-6067 deletion completed in 6.179634536s

• [SLOW TEST:50.437 seconds]
[sig-network] Proxy
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:30:33.673: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 22 08:30:33.726: INFO: Waiting up to 5m0s for pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1" in namespace "emptydir-200" to be "success or failure"
May 22 08:30:33.729: INFO: Pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.56689ms
May 22 08:30:35.733: INFO: Pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006358909s
May 22 08:30:37.737: INFO: Pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010403721s
May 22 08:30:39.742: INFO: Pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015219312s
STEP: Saw pod success
May 22 08:30:39.742: INFO: Pod "pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:30:39.745: INFO: Trying to get logs from node node2 pod pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:30:39.774: INFO: Waiting for pod pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1 to disappear
May 22 08:30:39.776: INFO: Pod pod-ddde793b-7c6b-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:30:39.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-200" for this suite.
May 22 08:30:45.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:30:45.893: INFO: namespace emptydir-200 deletion completed in 6.109784988s

• [SLOW TEST:12.220 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:30:45.894: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:30:51.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7885" for this suite.
May 22 08:31:13.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:31:13.176: INFO: namespace replication-controller-7885 deletion completed in 22.13066299s

• [SLOW TEST:27.282 seconds]
[sig-apps] ReplicationController
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:31:13.177: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 22 08:31:13.314: INFO: Waiting up to 5m0s for pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1" in namespace "emptydir-3631" to be "success or failure"
May 22 08:31:13.317: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589832ms
May 22 08:31:15.320: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006238857s
May 22 08:31:17.324: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010123287s
May 22 08:31:19.328: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013559397s
May 22 08:31:21.331: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.017379348s
May 22 08:31:23.336: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.021723659s
May 22 08:31:25.339: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.025160165s
May 22 08:31:27.342: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.028390054s
May 22 08:31:29.347: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.032890242s
May 22 08:31:31.353: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.038425121s
STEP: Saw pod success
May 22 08:31:31.353: INFO: Pod "pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:31:31.357: INFO: Trying to get logs from node node2 pod pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:31:31.382: INFO: Waiting for pod pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1 to disappear
May 22 08:31:31.387: INFO: Pod pod-f5770f4e-7c6b-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:31:31.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3631" for this suite.
May 22 08:31:37.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:31:37.608: INFO: namespace emptydir-3631 deletion completed in 6.215220008s

• [SLOW TEST:24.432 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:31:37.611: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-195 to expose endpoints map[]
May 22 08:31:37.689: INFO: successfully validated that service endpoint-test2 in namespace services-195 exposes endpoints map[] (3.194965ms elapsed)
STEP: Creating pod pod1 in namespace services-195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-195 to expose endpoints map[pod1:[80]]
May 22 08:31:41.733: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (4.037765674s elapsed, will retry)
May 22 08:31:46.772: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (9.076320831s elapsed, will retry)
May 22 08:31:51.812: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (14.116683278s elapsed, will retry)
May 22 08:31:56.904: INFO: Unexpected endpoints: found map[], expected map[pod1:[80]] (19.208830534s elapsed, will retry)
May 22 08:32:00.934: INFO: successfully validated that service endpoint-test2 in namespace services-195 exposes endpoints map[pod1:[80]] (23.238459219s elapsed)
STEP: Creating pod pod2 in namespace services-195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-195 to expose endpoints map[pod1:[80] pod2:[80]]
May 22 08:32:05.001: INFO: Unexpected endpoints: found map[03ffb1ef-7c6c-11e9-9cf4-fa163ecd1d63:[80]], expected map[pod1:[80] pod2:[80]] (4.06032645s elapsed, will retry)
May 22 08:32:10.141: INFO: successfully validated that service endpoint-test2 in namespace services-195 exposes endpoints map[pod1:[80] pod2:[80]] (9.19985845s elapsed)
STEP: Deleting pod pod1 in namespace services-195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-195 to expose endpoints map[pod2:[80]]
May 22 08:32:11.167: INFO: successfully validated that service endpoint-test2 in namespace services-195 exposes endpoints map[pod2:[80]] (1.015892214s elapsed)
STEP: Deleting pod pod2 in namespace services-195
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-195 to expose endpoints map[]
May 22 08:32:12.187: INFO: successfully validated that service endpoint-test2 in namespace services-195 exposes endpoints map[] (1.009408284s elapsed)
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:32:12.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-195" for this suite.
May 22 08:32:34.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:32:34.323: INFO: namespace services-195 deletion completed in 22.103827063s
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:56.712 seconds]
[sig-network] Services
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:32:34.324: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 22 08:32:34.388: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:34.388: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:34.388: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:34.390: INFO: Number of nodes with available pods: 0
May 22 08:32:34.390: INFO: Node node-mini is running more than one daemon pod
May 22 08:32:35.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:35.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:35.396: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:35.398: INFO: Number of nodes with available pods: 0
May 22 08:32:35.398: INFO: Node node-mini is running more than one daemon pod
May 22 08:32:36.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:36.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:36.396: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:36.400: INFO: Number of nodes with available pods: 1
May 22 08:32:36.400: INFO: Node node1 is running more than one daemon pod
May 22 08:32:37.399: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:37.399: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:37.399: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:37.402: INFO: Number of nodes with available pods: 1
May 22 08:32:37.402: INFO: Node node1 is running more than one daemon pod
May 22 08:32:38.398: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:38.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:38.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:38.402: INFO: Number of nodes with available pods: 1
May 22 08:32:38.402: INFO: Node node1 is running more than one daemon pod
May 22 08:32:39.397: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:39.397: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:39.397: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:39.399: INFO: Number of nodes with available pods: 1
May 22 08:32:39.399: INFO: Node node1 is running more than one daemon pod
May 22 08:32:40.398: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:40.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:40.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:40.401: INFO: Number of nodes with available pods: 1
May 22 08:32:40.401: INFO: Node node1 is running more than one daemon pod
May 22 08:32:41.403: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:41.403: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:41.406: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:41.410: INFO: Number of nodes with available pods: 1
May 22 08:32:41.410: INFO: Node node1 is running more than one daemon pod
May 22 08:32:42.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:42.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:42.397: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:42.403: INFO: Number of nodes with available pods: 1
May 22 08:32:42.403: INFO: Node node1 is running more than one daemon pod
May 22 08:32:43.398: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:43.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:43.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:43.401: INFO: Number of nodes with available pods: 1
May 22 08:32:43.401: INFO: Node node1 is running more than one daemon pod
May 22 08:32:44.400: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:44.400: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:44.400: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:44.403: INFO: Number of nodes with available pods: 2
May 22 08:32:44.404: INFO: Node node1 is running more than one daemon pod
May 22 08:32:45.405: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:45.406: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:45.406: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:45.409: INFO: Number of nodes with available pods: 2
May 22 08:32:45.409: INFO: Node node1 is running more than one daemon pod
May 22 08:32:46.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:46.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:46.396: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:46.400: INFO: Number of nodes with available pods: 2
May 22 08:32:46.400: INFO: Node node1 is running more than one daemon pod
May 22 08:32:47.400: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:47.400: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:47.400: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:47.404: INFO: Number of nodes with available pods: 2
May 22 08:32:47.404: INFO: Node node1 is running more than one daemon pod
May 22 08:32:48.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:48.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:48.396: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:48.403: INFO: Number of nodes with available pods: 2
May 22 08:32:48.403: INFO: Node node1 is running more than one daemon pod
May 22 08:32:49.397: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:49.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:49.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:49.405: INFO: Number of nodes with available pods: 2
May 22 08:32:49.406: INFO: Node node1 is running more than one daemon pod
May 22 08:32:50.397: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:50.397: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:50.397: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:50.400: INFO: Number of nodes with available pods: 2
May 22 08:32:50.400: INFO: Node node1 is running more than one daemon pod
May 22 08:32:51.488: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:51.489: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:51.489: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:51.498: INFO: Number of nodes with available pods: 2
May 22 08:32:51.498: INFO: Node node1 is running more than one daemon pod
May 22 08:32:52.396: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:52.396: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:52.396: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:52.399: INFO: Number of nodes with available pods: 2
May 22 08:32:52.399: INFO: Node node1 is running more than one daemon pod
May 22 08:32:53.398: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:53.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:53.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:53.405: INFO: Number of nodes with available pods: 2
May 22 08:32:53.405: INFO: Node node1 is running more than one daemon pod
May 22 08:32:54.399: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:54.399: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:54.399: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:54.491: INFO: Number of nodes with available pods: 2
May 22 08:32:54.491: INFO: Node node1 is running more than one daemon pod
May 22 08:32:55.398: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:55.398: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:55.398: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:55.405: INFO: Number of nodes with available pods: 2
May 22 08:32:55.405: INFO: Node node1 is running more than one daemon pod
May 22 08:32:56.397: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:56.397: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:56.397: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:56.401: INFO: Number of nodes with available pods: 2
May 22 08:32:56.401: INFO: Node node1 is running more than one daemon pod
May 22 08:32:57.399: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.399: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.399: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.403: INFO: Number of nodes with available pods: 3
May 22 08:32:57.403: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 22 08:32:57.428: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.428: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.428: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:57.431: INFO: Number of nodes with available pods: 2
May 22 08:32:57.431: INFO: Node node1 is running more than one daemon pod
May 22 08:32:58.438: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:58.438: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:58.438: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:58.441: INFO: Number of nodes with available pods: 2
May 22 08:32:58.441: INFO: Node node1 is running more than one daemon pod
May 22 08:32:59.440: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:59.440: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:59.440: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:32:59.443: INFO: Number of nodes with available pods: 2
May 22 08:32:59.443: INFO: Node node1 is running more than one daemon pod
May 22 08:33:00.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:00.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:00.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:00.440: INFO: Number of nodes with available pods: 2
May 22 08:33:00.440: INFO: Node node1 is running more than one daemon pod
May 22 08:33:01.436: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:01.436: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:01.436: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:01.439: INFO: Number of nodes with available pods: 2
May 22 08:33:01.439: INFO: Node node1 is running more than one daemon pod
May 22 08:33:02.436: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:02.436: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:02.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:02.439: INFO: Number of nodes with available pods: 2
May 22 08:33:02.439: INFO: Node node1 is running more than one daemon pod
May 22 08:33:03.436: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:03.436: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:03.436: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:03.439: INFO: Number of nodes with available pods: 2
May 22 08:33:03.439: INFO: Node node1 is running more than one daemon pod
May 22 08:33:04.436: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:04.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:04.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:04.439: INFO: Number of nodes with available pods: 2
May 22 08:33:04.439: INFO: Node node1 is running more than one daemon pod
May 22 08:33:05.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:05.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:05.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:05.440: INFO: Number of nodes with available pods: 2
May 22 08:33:05.440: INFO: Node node1 is running more than one daemon pod
May 22 08:33:06.493: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:06.493: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:06.493: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:06.497: INFO: Number of nodes with available pods: 2
May 22 08:33:06.497: INFO: Node node1 is running more than one daemon pod
May 22 08:33:07.441: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:07.441: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:07.441: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:07.449: INFO: Number of nodes with available pods: 2
May 22 08:33:07.449: INFO: Node node1 is running more than one daemon pod
May 22 08:33:08.438: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:08.439: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:08.439: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:08.443: INFO: Number of nodes with available pods: 2
May 22 08:33:08.443: INFO: Node node1 is running more than one daemon pod
May 22 08:33:09.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:09.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:09.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:09.440: INFO: Number of nodes with available pods: 2
May 22 08:33:09.440: INFO: Node node1 is running more than one daemon pod
May 22 08:33:10.439: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:10.439: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:10.439: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:10.443: INFO: Number of nodes with available pods: 2
May 22 08:33:10.443: INFO: Node node1 is running more than one daemon pod
May 22 08:33:11.438: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:11.438: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:11.438: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:11.441: INFO: Number of nodes with available pods: 2
May 22 08:33:11.441: INFO: Node node1 is running more than one daemon pod
May 22 08:33:12.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:12.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:12.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:12.440: INFO: Number of nodes with available pods: 2
May 22 08:33:12.441: INFO: Node node1 is running more than one daemon pod
May 22 08:33:13.436: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:13.436: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:13.436: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:13.439: INFO: Number of nodes with available pods: 2
May 22 08:33:13.439: INFO: Node node1 is running more than one daemon pod
May 22 08:33:14.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:14.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:14.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:14.442: INFO: Number of nodes with available pods: 2
May 22 08:33:14.442: INFO: Node node1 is running more than one daemon pod
May 22 08:33:15.437: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:15.437: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:15.437: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:15.440: INFO: Number of nodes with available pods: 2
May 22 08:33:15.440: INFO: Node node1 is running more than one daemon pod
May 22 08:33:16.438: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:16.438: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:16.438: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:16.442: INFO: Number of nodes with available pods: 2
May 22 08:33:16.442: INFO: Node node1 is running more than one daemon pod
May 22 08:33:17.446: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:17.446: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:17.446: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:33:17.450: INFO: Number of nodes with available pods: 3
May 22 08:33:17.450: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3396, will wait for the garbage collector to delete the pods
May 22 08:33:17.526: INFO: Deleting DaemonSet.extensions daemon-set took: 8.700529ms
May 22 08:33:17.926: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.402644ms
May 22 08:33:34.231: INFO: Number of nodes with available pods: 0
May 22 08:33:34.231: INFO: Number of running nodes: 0, number of available pods: 0
May 22 08:33:34.234: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3396/daemonsets","resourceVersion":"12879969"},"items":null}

May 22 08:33:34.236: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3396/pods","resourceVersion":"12879969"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:33:34.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3396" for this suite.
May 22 08:33:40.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:33:40.368: INFO: namespace daemonsets-3396 deletion completed in 6.113742868s

• [SLOW TEST:66.044 seconds]
[sig-apps] Daemon set [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:33:40.374: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-4d3526ee-7c6c-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:33:40.529: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1" in namespace "projected-1314" to be "success or failure"
May 22 08:33:40.531: INFO: Pod "pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.332172ms
May 22 08:33:42.590: INFO: Pod "pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.061215171s
STEP: Saw pod success
May 22 08:33:42.590: INFO: Pod "pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:33:42.593: INFO: Trying to get logs from node node2 pod pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:33:42.614: INFO: Waiting for pod pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:33:42.616: INFO: Pod pod-projected-configmaps-4d35d8d3-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:33:42.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1314" for this suite.
May 22 08:33:48.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:33:48.729: INFO: namespace projected-1314 deletion completed in 6.108678013s

• [SLOW TEST:8.355 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:33:48.730: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-522c1372-7c6c-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 08:33:48.856: INFO: Waiting up to 5m0s for pod "pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1" in namespace "secrets-3891" to be "success or failure"
May 22 08:33:48.858: INFO: Pod "pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366343ms
May 22 08:33:50.863: INFO: Pod "pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007103708s
STEP: Saw pod success
May 22 08:33:50.863: INFO: Pod "pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:33:50.866: INFO: Trying to get logs from node node1 pod pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 08:33:50.888: INFO: Waiting for pod pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:33:50.890: INFO: Pod pod-secrets-522cdb29-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:33:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3891" for this suite.
May 22 08:33:56.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:33:57.022: INFO: namespace secrets-3891 deletion completed in 6.125797092s

• [SLOW TEST:8.292 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:33:57.022: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:33:57.082: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1" in namespace "projected-2995" to be "success or failure"
May 22 08:33:57.087: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.374738ms
May 22 08:33:59.092: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010544095s
May 22 08:34:01.097: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.015389467s
May 22 08:34:03.100: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018298618s
May 22 08:34:05.104: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022863983s
May 22 08:34:07.109: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.027417282s
STEP: Saw pod success
May 22 08:34:07.109: INFO: Pod "downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:34:07.112: INFO: Trying to get logs from node node1 pod downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:34:07.206: INFO: Waiting for pod downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:34:07.208: INFO: Pod downwardapi-volume-5713a293-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:34:07.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2995" for this suite.
May 22 08:34:13.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:34:13.315: INFO: namespace projected-2995 deletion completed in 6.101875769s

• [SLOW TEST:16.293 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:34:13.315: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-60c86a42-7c6c-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:34:13.371: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1" in namespace "projected-7089" to be "success or failure"
May 22 08:34:13.373: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.315678ms
May 22 08:34:15.391: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019932746s
May 22 08:34:17.395: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024278063s
May 22 08:34:19.398: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.027316391s
May 22 08:34:21.403: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.032296444s
May 22 08:34:23.408: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.037284808s
May 22 08:34:25.412: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.041301075s
May 22 08:34:27.416: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.04472483s
May 22 08:34:29.419: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.048540302s
May 22 08:34:31.424: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.052768541s
May 22 08:34:33.428: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.056768654s
STEP: Saw pod success
May 22 08:34:33.428: INFO: Pod "pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:34:33.430: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:34:33.454: INFO: Waiting for pod pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:34:33.456: INFO: Pod pod-projected-configmaps-60c91433-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:34:33.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7089" for this suite.
May 22 08:34:39.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:34:39.560: INFO: namespace projected-7089 deletion completed in 6.099233688s

• [SLOW TEST:26.245 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:34:39.561: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:34:39.627: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 22 08:34:39.640: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:39.640: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:39.640: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:39.642: INFO: Number of nodes with available pods: 0
May 22 08:34:39.642: INFO: Node node-mini is running more than one daemon pod
May 22 08:34:40.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:40.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:40.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:40.651: INFO: Number of nodes with available pods: 0
May 22 08:34:40.651: INFO: Node node-mini is running more than one daemon pod
May 22 08:34:41.649: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:41.649: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:41.649: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:41.652: INFO: Number of nodes with available pods: 1
May 22 08:34:41.652: INFO: Node node1 is running more than one daemon pod
May 22 08:34:42.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:42.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:42.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:42.654: INFO: Number of nodes with available pods: 1
May 22 08:34:42.654: INFO: Node node1 is running more than one daemon pod
May 22 08:34:43.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:43.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:43.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:43.687: INFO: Number of nodes with available pods: 1
May 22 08:34:43.688: INFO: Node node1 is running more than one daemon pod
May 22 08:34:44.649: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:44.649: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:44.649: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:44.656: INFO: Number of nodes with available pods: 1
May 22 08:34:44.656: INFO: Node node1 is running more than one daemon pod
May 22 08:34:45.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:45.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:45.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:45.651: INFO: Number of nodes with available pods: 1
May 22 08:34:45.651: INFO: Node node1 is running more than one daemon pod
May 22 08:34:46.687: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:46.688: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:46.688: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:46.711: INFO: Number of nodes with available pods: 1
May 22 08:34:46.711: INFO: Node node1 is running more than one daemon pod
May 22 08:34:47.665: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:47.665: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:47.665: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:47.672: INFO: Number of nodes with available pods: 2
May 22 08:34:47.672: INFO: Node node1 is running more than one daemon pod
May 22 08:34:48.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:48.649: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:48.649: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:48.652: INFO: Number of nodes with available pods: 2
May 22 08:34:48.652: INFO: Node node1 is running more than one daemon pod
May 22 08:34:49.649: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:49.649: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:49.649: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:49.652: INFO: Number of nodes with available pods: 2
May 22 08:34:49.652: INFO: Node node1 is running more than one daemon pod
May 22 08:34:50.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:50.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:50.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:50.651: INFO: Number of nodes with available pods: 2
May 22 08:34:50.651: INFO: Node node1 is running more than one daemon pod
May 22 08:34:51.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:51.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:51.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:51.651: INFO: Number of nodes with available pods: 2
May 22 08:34:51.651: INFO: Node node1 is running more than one daemon pod
May 22 08:34:52.648: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:52.648: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:52.648: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:52.656: INFO: Number of nodes with available pods: 2
May 22 08:34:52.656: INFO: Node node1 is running more than one daemon pod
May 22 08:34:53.652: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:53.652: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:53.652: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:53.656: INFO: Number of nodes with available pods: 3
May 22 08:34:53.656: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 22 08:34:53.689: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:53.689: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:53.689: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:53.695: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:53.695: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:53.695: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:54.698: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:54.698: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:54.698: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:54.713: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:54.713: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:54.713: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:55.700: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:55.700: INFO: Pod daemon-set-8jrnm is not available
May 22 08:34:55.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:55.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:55.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:55.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:55.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:56.699: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:56.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:34:56.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:56.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:56.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:56.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:56.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:57.699: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:57.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:34:57.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:57.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:57.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:57.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:57.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:58.699: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:58.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:34:58.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:58.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:58.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:58.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:58.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:59.699: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:59.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:34:59.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:59.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:34:59.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:59.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:34:59.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:00.700: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:00.700: INFO: Pod daemon-set-8jrnm is not available
May 22 08:35:00.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:00.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:00.707: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:00.707: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:00.707: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:01.698: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:01.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:35:01.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:01.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:01.703: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:01.703: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:01.703: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:02.699: INFO: Wrong image for pod: daemon-set-8jrnm. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:02.699: INFO: Pod daemon-set-8jrnm is not available
May 22 08:35:02.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:02.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:02.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:02.709: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:02.709: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:03.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:03.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:03.699: INFO: Pod daemon-set-tltgl is not available
May 22 08:35:03.704: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:03.704: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:03.704: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:04.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:04.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:04.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:04.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:04.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:05.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:05.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:05.700: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:05.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:05.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:05.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:06.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:06.699: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:06.699: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:06.704: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:06.704: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:06.704: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:07.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:07.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:07.700: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:07.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:07.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:07.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:08.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:08.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:08.700: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:08.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:08.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:08.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:09.701: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:09.701: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:09.701: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:09.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:09.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:09.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:10.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:10.700: INFO: Wrong image for pod: daemon-set-s2pb4. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:10.700: INFO: Pod daemon-set-s2pb4 is not available
May 22 08:35:10.709: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:10.709: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:10.710: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:11.701: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:11.701: INFO: Pod daemon-set-l29p9 is not available
May 22 08:35:11.707: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:11.707: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:11.707: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:12.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:12.705: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:12.705: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:12.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:13.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:13.710: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:13.710: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:13.710: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:14.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:14.699: INFO: Pod daemon-set-9skj8 is not available
May 22 08:35:14.704: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:14.704: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:14.704: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:15.699: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:15.699: INFO: Pod daemon-set-9skj8 is not available
May 22 08:35:15.704: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:15.704: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:15.705: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:16.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:16.700: INFO: Pod daemon-set-9skj8 is not available
May 22 08:35:16.719: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:16.719: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:16.719: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:17.701: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:17.701: INFO: Pod daemon-set-9skj8 is not available
May 22 08:35:17.710: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:17.710: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:17.710: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:18.700: INFO: Wrong image for pod: daemon-set-9skj8. Expected: hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0, got: hub.c.163.com/combk8s/nginx-amd64:1.14-alpine.
May 22 08:35:18.700: INFO: Pod daemon-set-9skj8 is not available
May 22 08:35:18.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:18.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:18.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.699: INFO: Pod daemon-set-w8mw9 is not available
May 22 08:35:19.706: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.706: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.706: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
May 22 08:35:19.711: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.711: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.711: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:19.714: INFO: Number of nodes with available pods: 2
May 22 08:35:19.714: INFO: Node node1 is running more than one daemon pod
May 22 08:35:20.720: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:20.721: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:20.721: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:20.724: INFO: Number of nodes with available pods: 2
May 22 08:35:20.724: INFO: Node node1 is running more than one daemon pod
May 22 08:35:21.721: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:21.721: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:21.721: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:21.724: INFO: Number of nodes with available pods: 2
May 22 08:35:21.724: INFO: Node node1 is running more than one daemon pod
May 22 08:35:22.720: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:22.720: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:22.720: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:35:22.723: INFO: Number of nodes with available pods: 3
May 22 08:35:22.723: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4742, will wait for the garbage collector to delete the pods
May 22 08:35:22.801: INFO: Deleting DaemonSet.extensions daemon-set took: 7.607524ms
May 22 08:35:23.101: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.225797ms
May 22 08:35:31.005: INFO: Number of nodes with available pods: 0
May 22 08:35:31.005: INFO: Number of running nodes: 0, number of available pods: 0
May 22 08:35:31.008: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4742/daemonsets","resourceVersion":"12881251"},"items":null}

May 22 08:35:31.011: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4742/pods","resourceVersion":"12881251"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:35:31.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4742" for this suite.
May 22 08:35:37.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:35:37.192: INFO: namespace daemonsets-4742 deletion completed in 6.164533483s

• [SLOW TEST:57.632 seconds]
[sig-apps] Daemon set [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:35:37.193: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 08:35:37.249: INFO: Waiting up to 5m0s for pod "downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1" in namespace "downward-api-6855" to be "success or failure"
May 22 08:35:37.252: INFO: Pod "downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315595ms
May 22 08:35:39.256: INFO: Pod "downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00709494s
STEP: Saw pod success
May 22 08:35:39.256: INFO: Pod "downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:35:39.259: INFO: Trying to get logs from node node1 pod downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 08:35:39.282: INFO: Waiting for pod downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:35:39.284: INFO: Pod downward-api-92c83479-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:35:39.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6855" for this suite.
May 22 08:35:45.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:35:45.386: INFO: namespace downward-api-6855 deletion completed in 6.098169978s

• [SLOW TEST:8.193 seconds]
[sig-node] Downward API
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:35:45.386: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:35:45.507: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:35:52.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-327" for this suite.
May 22 08:36:36.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:36:36.293: INFO: namespace pods-327 deletion completed in 44.176694718s

• [SLOW TEST:50.906 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:36:36.295: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 08:36:36.399: INFO: PodSpec: initContainers in spec.initContainers
May 22 08:37:30.103: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-b60af64f-7c6c-11e9-aefd-da1a35f02de1", GenerateName:"", Namespace:"init-container-5281", SelfLink:"/api/v1/namespaces/init-container-5281/pods/pod-init-b60af64f-7c6c-11e9-aefd-da1a35f02de1", UID:"b60b53cf-7c6c-11e9-9cf4-fa163ecd1d63", ResourceVersion:"12882707", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63694110996, loc:(*time.Location)(0x80d5f20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"399477665"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-9vd68", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc00325c440), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"hub.c.163.com/combk8s/busybox-amd64:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9vd68", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"hub.c.163.com/combk8s/busybox-amd64:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9vd68", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"hub.c.163.com/combk8s/pause-amd64:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}, "cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-9vd68", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0027ec488), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"node1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0007ac420), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027ec510)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0027ec530)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0027ec538), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0027ec53c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110996, loc:(*time.Location)(0x80d5f20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110996, loc:(*time.Location)(0x80d5f20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110996, loc:(*time.Location)(0x80d5f20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63694110996, loc:(*time.Location)(0x80d5f20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.177.10.19", PodIP:"10.177.10.7", StartTime:(*v1.Time)(0xc0030a07c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030a0800), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000151ce0)}, Ready:false, RestartCount:3, Image:"busybox-amd64:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://0f516b8be36292f0d0364789e0b0a2a79779e9c5e5ab9731f434818c789474dc"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030a0820), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"hub.c.163.com/combk8s/busybox-amd64:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0030a07e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"hub.c.163.com/combk8s/pause-amd64:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:37:30.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5281" for this suite.
May 22 08:37:44.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:37:44.221: INFO: namespace init-container-5281 deletion completed in 14.107338178s

• [SLOW TEST:67.927 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:37:44.222: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-de876c5b-7c6c-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:37:44.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1" in namespace "projected-6959" to be "success or failure"
May 22 08:37:44.351: INFO: Pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.10919ms
May 22 08:37:46.356: INFO: Pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008196289s
May 22 08:37:48.388: INFO: Pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040314472s
May 22 08:37:50.394: INFO: Pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045366409s
STEP: Saw pod success
May 22 08:37:50.394: INFO: Pod "pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:37:50.397: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:37:50.425: INFO: Waiting for pod pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1 to disappear
May 22 08:37:50.429: INFO: Pod pod-projected-configmaps-de8a0b76-7c6c-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:37:50.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6959" for this suite.
May 22 08:37:56.459: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:37:56.557: INFO: namespace projected-6959 deletion completed in 6.122386078s

• [SLOW TEST:12.335 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:37:56.559: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 22 08:38:08.700: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 08:38:08.703: INFO: Pod pod-with-prestop-http-hook still exists
May 22 08:38:10.703: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 08:38:10.715: INFO: Pod pod-with-prestop-http-hook still exists
May 22 08:38:12.703: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 22 08:38:12.707: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:38:12.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7730" for this suite.
May 22 08:38:34.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:38:34.901: INFO: namespace container-lifecycle-hook-7730 deletion completed in 22.181853946s

• [SLOW TEST:38.343 seconds]
[k8s.io] Container Lifecycle Hook
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:38:34.902: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-fcb70639-7c6c-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:38:39.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6199" for this suite.
May 22 08:39:01.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:39:01.189: INFO: namespace configmap-6199 deletion completed in 22.170354356s

• [SLOW TEST:26.287 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:39:01.189: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-0c5ec124-7c6d-11e9-aefd-da1a35f02de1
[AfterEach] [sig-node] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:39:01.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1932" for this suite.
May 22 08:39:07.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:39:07.333: INFO: namespace configmap-1932 deletion completed in 6.095315718s

• [SLOW TEST:6.144 seconds]
[sig-node] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:39:07.333: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 22 08:39:07.417: INFO: Waiting up to 5m0s for pod "client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1" in namespace "containers-5073" to be "success or failure"
May 22 08:39:07.419: INFO: Pod "client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419613ms
May 22 08:39:09.424: INFO: Pod "client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006617739s
STEP: Saw pod success
May 22 08:39:09.424: INFO: Pod "client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:39:09.427: INFO: Trying to get logs from node node1 pod client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:39:09.451: INFO: Waiting for pod client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1 to disappear
May 22 08:39:09.453: INFO: Pod client-containers-1007ccf4-7c6d-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:39:09.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5073" for this suite.
May 22 08:39:15.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:39:15.580: INFO: namespace containers-5073 deletion completed in 6.121427201s

• [SLOW TEST:8.247 seconds]
[k8s.io] Docker Containers
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:39:15.581: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-1520eb10-7c6d-11e9-aefd-da1a35f02de1
STEP: Creating configMap with name cm-test-opt-upd-1520eb62-7c6d-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1520eb10-7c6d-11e9-aefd-da1a35f02de1
STEP: Updating configmap cm-test-opt-upd-1520eb62-7c6d-11e9-aefd-da1a35f02de1
STEP: Creating configMap with name cm-test-opt-create-1520eb77-7c6d-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:40:30.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2318" for this suite.
May 22 08:40:52.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:40:52.861: INFO: namespace projected-2318 deletion completed in 22.124587119s

• [SLOW TEST:97.281 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:40:52.867: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:40:59.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5636" for this suite.
May 22 08:41:43.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:41:43.175: INFO: namespace kubelet-test-5636 deletion completed in 44.159385745s

• [SLOW TEST:50.308 seconds]
[k8s.io] Kubelet
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:41:43.176: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:41:43.251: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1" in namespace "downward-api-790" to be "success or failure"
May 22 08:41:43.253: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.542382ms
May 22 08:41:45.257: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006456904s
May 22 08:41:47.261: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010257796s
May 22 08:41:49.265: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014409749s
May 22 08:41:51.270: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019091719s
May 22 08:41:53.274: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02374637s
May 22 08:41:55.278: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.027631011s
May 22 08:41:57.282: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.031108911s
May 22 08:41:59.285: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.034698545s
May 22 08:42:01.290: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.039454954s
May 22 08:42:03.294: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.04299138s
STEP: Saw pod success
May 22 08:42:03.294: INFO: Pod "downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:42:03.296: INFO: Trying to get logs from node node1 pod downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:42:03.316: INFO: Waiting for pod downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1 to disappear
May 22 08:42:03.319: INFO: Pod downwardapi-volume-6cef6e1f-7c6d-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:42:03.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-790" for this suite.
May 22 08:42:09.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:42:09.416: INFO: namespace downward-api-790 deletion completed in 6.092697062s

• [SLOW TEST:26.241 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:42:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-bzvq
STEP: Creating a pod to test atomic-volume-subpath
May 22 08:42:09.474: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bzvq" in namespace "subpath-2218" to be "success or failure"
May 22 08:42:09.476: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.258697ms
May 22 08:42:11.480: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006467742s
May 22 08:42:13.486: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011681853s
May 22 08:42:15.491: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017017515s
May 22 08:42:17.495: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021412123s
May 22 08:42:19.500: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.02612238s
May 22 08:42:21.506: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.032153811s
May 22 08:42:23.608: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.133866466s
May 22 08:42:25.839: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.365253065s
May 22 08:42:27.842: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.368391172s
May 22 08:42:29.847: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.372852526s
May 22 08:42:31.851: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 22.377076211s
May 22 08:42:33.860: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 24.385953111s
May 22 08:42:35.864: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Running", Reason="", readiness=true. Elapsed: 26.390036184s
May 22 08:42:37.869: INFO: Pod "pod-subpath-test-secret-bzvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 28.394760107s
STEP: Saw pod success
May 22 08:42:37.869: INFO: Pod "pod-subpath-test-secret-bzvq" satisfied condition "success or failure"
May 22 08:42:37.872: INFO: Trying to get logs from node node1 pod pod-subpath-test-secret-bzvq container test-container-subpath-secret-bzvq: <nil>
STEP: delete the pod
May 22 08:42:37.903: INFO: Waiting for pod pod-subpath-test-secret-bzvq to disappear
May 22 08:42:37.905: INFO: Pod pod-subpath-test-secret-bzvq no longer exists
STEP: Deleting pod pod-subpath-test-secret-bzvq
May 22 08:42:37.905: INFO: Deleting pod "pod-subpath-test-secret-bzvq" in namespace "subpath-2218"
[AfterEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:42:37.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2218" for this suite.
May 22 08:42:43.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:42:44.027: INFO: namespace subpath-2218 deletion completed in 6.113637105s

• [SLOW TEST:34.610 seconds]
[sig-storage] Subpath
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:42:44.028: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 22 08:42:44.616: INFO: created pod pod-service-account-defaultsa
May 22 08:42:44.616: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 22 08:42:44.622: INFO: created pod pod-service-account-mountsa
May 22 08:42:44.622: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 22 08:42:44.628: INFO: created pod pod-service-account-nomountsa
May 22 08:42:44.628: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 22 08:42:44.632: INFO: created pod pod-service-account-defaultsa-mountspec
May 22 08:42:44.633: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 22 08:42:44.637: INFO: created pod pod-service-account-mountsa-mountspec
May 22 08:42:44.637: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 22 08:42:44.641: INFO: created pod pod-service-account-nomountsa-mountspec
May 22 08:42:44.641: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 22 08:42:44.647: INFO: created pod pod-service-account-defaultsa-nomountspec
May 22 08:42:44.647: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 22 08:42:44.653: INFO: created pod pod-service-account-mountsa-nomountspec
May 22 08:42:44.653: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 22 08:42:44.658: INFO: created pod pod-service-account-nomountsa-nomountspec
May 22 08:42:44.658: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:42:44.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2675" for this suite.
May 22 08:43:06.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:43:06.771: INFO: namespace svcaccounts-2675 deletion completed in 22.107702205s

• [SLOW TEST:22.743 seconds]
[sig-auth] ServiceAccounts
/root/workspace/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:43:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9382
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 22 08:43:06.822: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 22 08:43:44.937: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.177.10.44:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9382 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 08:43:44.937: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 08:43:45.444: INFO: Found all expected endpoints: [netserver-0]
May 22 08:43:45.447: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.177.11.193:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9382 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 22 08:43:45.447: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
May 22 08:43:45.561: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:43:45.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9382" for this suite.
May 22 08:44:07.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:44:07.674: INFO: namespace pod-network-test-9382 deletion completed in 22.106336133s

• [SLOW TEST:60.901 seconds]
[sig-network] Networking
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:44:07.675: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:44:07.726: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1" in namespace "projected-2077" to be "success or failure"
May 22 08:44:07.729: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.280249ms
May 22 08:44:09.732: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005954774s
May 22 08:44:11.968: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.241422387s
May 22 08:44:14.013: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.286764111s
May 22 08:44:16.030: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.303927347s
May 22 08:44:18.064: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.337716832s
May 22 08:44:20.096: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.369353824s
May 22 08:44:22.100: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.373656123s
May 22 08:44:24.106: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.379143541s
May 22 08:44:26.109: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.382555146s
May 22 08:44:28.113: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.38683901s
May 22 08:44:30.118: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.391947307s
May 22 08:44:32.122: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.395922597s
May 22 08:44:34.126: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.399362608s
May 22 08:44:36.129: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.40262161s
May 22 08:44:38.133: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.406307442s
May 22 08:44:40.137: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.410401403s
May 22 08:44:42.142: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.415434219s
May 22 08:44:44.146: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.419965058s
May 22 08:44:46.150: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.423535106s
May 22 08:44:48.155: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 40.428272444s
STEP: Saw pod success
May 22 08:44:48.155: INFO: Pod "downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:44:48.158: INFO: Trying to get logs from node node2 pod downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:44:48.226: INFO: Waiting for pod downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1 to disappear
May 22 08:44:48.231: INFO: Pod downwardapi-volume-c30cd39c-7c6d-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:44:48.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2077" for this suite.
May 22 08:44:54.287: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:44:54.367: INFO: namespace projected-2077 deletion completed in 6.131518065s

• [SLOW TEST:46.692 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:44:54.368: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7325.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7325.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 08:44:58.493: INFO: DNS probes using dns-7325/dns-test-dee195fa-7c6d-11e9-aefd-da1a35f02de1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:44:58.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7325" for this suite.
May 22 08:45:04.538: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:45:04.631: INFO: namespace dns-7325 deletion completed in 6.114510925s

• [SLOW TEST:10.263 seconds]
[sig-network] DNS
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:45:04.632: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 22 08:45:04.681: INFO: Waiting up to 5m0s for pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1" in namespace "containers-5103" to be "success or failure"
May 22 08:45:04.684: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671974ms
May 22 08:45:06.688: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006994376s
May 22 08:45:08.692: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011325132s
May 22 08:45:10.696: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015468174s
May 22 08:45:12.700: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019320323s
May 22 08:45:14.705: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.023594286s
May 22 08:45:16.709: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.028071706s
May 22 08:45:18.713: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.032347128s
STEP: Saw pod success
May 22 08:45:18.713: INFO: Pod "client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:45:18.717: INFO: Trying to get logs from node node2 pod client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 08:45:18.741: INFO: Waiting for pod client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1 to disappear
May 22 08:45:18.744: INFO: Pod client-containers-e4ff7e31-7c6d-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:45:18.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5103" for this suite.
May 22 08:45:24.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:45:24.923: INFO: namespace containers-5103 deletion completed in 6.110564071s

• [SLOW TEST:20.292 seconds]
[k8s.io] Docker Containers
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:45:24.924: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0522 08:45:26.017422      19 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 22 08:45:26.017: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:45:26.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9674" for this suite.
May 22 08:45:32.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:45:32.117: INFO: namespace gc-9674 deletion completed in 6.095186251s

• [SLOW TEST:7.193 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:45:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image hub.c.163.com/combk8s/nginx-amd64:1.14-alpine
May 22 08:45:32.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 run e2e-test-nginx-rc --image=hub.c.163.com/combk8s/nginx-amd64:1.14-alpine --generator=run/v1 --namespace=kubectl-9961'
May 22 08:45:34.674: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 22 08:45:34.674: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 22 08:45:34.684: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rx9z5]
May 22 08:45:34.684: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rx9z5" in namespace "kubectl-9961" to be "running and ready"
May 22 08:45:34.686: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.203121ms
May 22 08:45:36.691: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006675185s
May 22 08:45:38.695: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011217563s
May 22 08:45:40.700: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016220711s
May 22 08:45:42.705: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020279153s
May 22 08:45:44.709: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02513117s
May 22 08:45:46.713: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029100013s
May 22 08:45:48.718: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033872304s
May 22 08:45:50.724: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.04002576s
May 22 08:45:52.729: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.044262675s
May 22 08:45:54.733: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.049144674s
May 22 08:45:56.737: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.053126083s
May 22 08:45:58.742: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.057496592s
May 22 08:46:00.746: INFO: Pod "e2e-test-nginx-rc-rx9z5": Phase="Running", Reason="", readiness=true. Elapsed: 26.061693311s
May 22 08:46:00.746: INFO: Pod "e2e-test-nginx-rc-rx9z5" satisfied condition "running and ready"
May 22 08:46:00.746: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rx9z5]
May 22 08:46:00.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 logs rc/e2e-test-nginx-rc --namespace=kubectl-9961'
May 22 08:46:01.212: INFO: stderr: ""
May 22 08:46:01.212: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 22 08:46:01.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete rc e2e-test-nginx-rc --namespace=kubectl-9961'
May 22 08:46:01.327: INFO: stderr: ""
May 22 08:46:01.327: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:46:01.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9961" for this suite.
May 22 08:46:23.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:46:23.479: INFO: namespace kubectl-9961 deletion completed in 22.140803212s

• [SLOW TEST:51.362 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:46:23.483: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4512
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 22 08:46:23.548: INFO: Found 0 stateful pods, waiting for 3
May 22 08:46:33.586: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:33.586: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:33.586: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 08:46:43.580: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:43.580: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:43.580: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 08:46:53.570: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:53.570: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:46:53.570: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 08:47:03.598: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:03.598: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:03.598: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 08:47:13.552: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:13.552: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:13.552: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 08:47:23.553: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:23.553: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:23.553: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 22 08:47:23.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-4512 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 08:47:23.890: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 08:47:23.890: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 08:47:23.890: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from hub.c.163.com/combk8s/nginx-amd64:1.14-alpine to hub.c.163.com/combk8s/nginx-amd64:1.15-alpine
May 22 08:47:33.933: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 22 08:47:43.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-4512 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 08:47:44.277: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 08:47:44.277: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 08:47:44.277: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 08:48:04.300: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:04.300: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:04.300: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:04.301: INFO: Waiting for Pod statefulset-4512/ss2-2 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:14.309: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:14.309: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:14.309: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:14.309: INFO: Waiting for Pod statefulset-4512/ss2-2 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:24.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:24.308: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:24.308: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:34.313: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:34.313: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:34.313: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:44.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:44.308: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:44.308: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:54.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:48:54.308: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:48:54.308: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:04.310: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:49:04.310: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:04.310: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:14.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:49:14.308: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:24.420: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:49:24.420: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:34.430: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:49:34.430: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:44.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:49:44.308: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 08:49:54.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:50:04.308: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:50:14.323: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
STEP: Rolling back to a previous revision
May 22 08:50:24.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-4512 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 08:50:24.835: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 08:50:24.835: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 08:50:24.835: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 08:50:34.875: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 22 08:50:44.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-4512 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 08:50:45.195: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 08:50:45.195: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 08:50:45.195: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 08:51:05.218: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:05.218: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:05.218: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:05.218: INFO: Waiting for Pod statefulset-4512/ss2-2 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:15.228: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:15.228: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:15.228: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:25.226: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:25.226: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:25.226: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:35.228: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:35.228: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:35.228: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:45.230: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:45.230: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:45.230: INFO: Waiting for Pod statefulset-4512/ss2-1 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:51:55.224: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:51:55.224: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:52:05.287: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
May 22 08:52:05.287: INFO: Waiting for Pod statefulset-4512/ss2-0 to have revision ss2-869d6c9d88 update revision ss2-85b57d9f75
May 22 08:52:15.508: INFO: Waiting for StatefulSet statefulset-4512/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 08:52:25.227: INFO: Deleting all statefulset in ns statefulset-4512
May 22 08:52:25.231: INFO: Scaling statefulset ss2 to 0
May 22 08:52:55.250: INFO: Waiting for statefulset status.replicas updated to 0
May 22 08:52:55.255: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:52:55.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4512" for this suite.
May 22 08:53:01.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:53:01.397: INFO: namespace statefulset-4512 deletion completed in 6.120229828s

• [SLOW TEST:397.914 seconds]
[sig-apps] StatefulSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:53:01.397: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-012c3476-7c6f-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:53:01.450: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1" in namespace "projected-9183" to be "success or failure"
May 22 08:53:01.456: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.712046ms
May 22 08:53:03.460: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009384538s
May 22 08:53:05.464: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013672313s
May 22 08:53:07.468: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.017785519s
May 22 08:53:09.472: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.021628977s
May 22 08:53:11.477: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.026489425s
May 22 08:53:13.480: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.030088983s
STEP: Saw pod success
May 22 08:53:13.480: INFO: Pod "pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:53:13.483: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:53:13.897: INFO: Waiting for pod pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1 to disappear
May 22 08:53:13.900: INFO: Pod pod-projected-configmaps-012cd3d0-7c6f-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:53:13.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9183" for this suite.
May 22 08:53:19.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:53:20.025: INFO: namespace projected-9183 deletion completed in 6.119049894s

• [SLOW TEST:18.628 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:53:20.026: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 22 08:53:24.175: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:24.178: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:26.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:26.187: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:28.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:28.183: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:30.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:30.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:32.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:32.187: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:34.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:34.183: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:36.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:36.183: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:38.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:38.191: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:40.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:40.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:42.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:42.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:44.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:44.186: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:46.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:46.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:48.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:48.185: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:50.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:50.188: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:52.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:52.183: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:54.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:54.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:56.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:56.182: INFO: Pod pod-with-prestop-exec-hook still exists
May 22 08:53:58.179: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 22 08:53:58.182: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:53:58.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4960" for this suite.
May 22 08:54:20.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:54:20.618: INFO: namespace container-lifecycle-hook-4960 deletion completed in 22.108942878s

• [SLOW TEST:60.592 seconds]
[k8s.io] Container Lifecycle Hook
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:54:20.619: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 22 08:54:29.699: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:54:30.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1103" for this suite.
May 22 08:54:52.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:54:52.844: INFO: namespace replicaset-1103 deletion completed in 22.12096698s

• [SLOW TEST:32.225 seconds]
[sig-apps] ReplicaSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:54:52.844: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 22 08:54:52.967: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:52.967: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:52.968: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:52.970: INFO: Number of nodes with available pods: 0
May 22 08:54:52.970: INFO: Node node-mini is running more than one daemon pod
May 22 08:54:53.976: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:53.977: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:53.977: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:53.984: INFO: Number of nodes with available pods: 0
May 22 08:54:53.984: INFO: Node node-mini is running more than one daemon pod
May 22 08:54:54.976: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:54.977: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:54.977: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:54.980: INFO: Number of nodes with available pods: 1
May 22 08:54:54.980: INFO: Node node1 is running more than one daemon pod
May 22 08:54:55.978: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:55.978: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:55.978: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:55.990: INFO: Number of nodes with available pods: 3
May 22 08:54:55.990: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 22 08:54:56.008: INFO: DaemonSet pods can't tolerate node k8s-master1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:56.009: INFO: DaemonSet pods can't tolerate node k8s-master2 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:56.009: INFO: DaemonSet pods can't tolerate node k8s-master3 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>} {Key:node.netease.com/role Value:offline Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
May 22 08:54:56.013: INFO: Number of nodes with available pods: 3
May 22 08:54:56.013: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2052, will wait for the garbage collector to delete the pods
May 22 08:54:57.152: INFO: Deleting DaemonSet.extensions daemon-set took: 9.589128ms
May 22 08:54:57.455: INFO: Terminating DaemonSet.extensions daemon-set pods took: 303.65601ms
May 22 08:55:11.063: INFO: Number of nodes with available pods: 0
May 22 08:55:11.063: INFO: Number of running nodes: 0, number of available pods: 0
May 22 08:55:11.066: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2052/daemonsets","resourceVersion":"12897040"},"items":null}

May 22 08:55:11.069: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2052/pods","resourceVersion":"12897040"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:55:11.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2052" for this suite.
May 22 08:55:17.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:55:17.281: INFO: namespace daemonsets-2052 deletion completed in 6.190886927s

• [SLOW TEST:24.437 seconds]
[sig-apps] Daemon set [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:55:17.282: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:55:17.333: INFO: Waiting up to 5m0s for pod "downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1" in namespace "downward-api-8899" to be "success or failure"
May 22 08:55:17.338: INFO: Pod "downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.664656ms
May 22 08:55:19.341: INFO: Pod "downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007704363s
May 22 08:55:21.349: INFO: Pod "downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015909398s
STEP: Saw pod success
May 22 08:55:21.349: INFO: Pod "downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:55:21.352: INFO: Trying to get logs from node node1 pod downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:55:21.390: INFO: Waiting for pod downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1 to disappear
May 22 08:55:21.393: INFO: Pod downwardapi-volume-522a6804-7c6f-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:55:21.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8899" for this suite.
May 22 08:55:27.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:55:27.504: INFO: namespace downward-api-8899 deletion completed in 6.105949875s

• [SLOW TEST:10.222 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:55:27.504: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 08:55:27.617: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 22 08:55:32.622: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 22 08:55:32.622: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 08:55:32.647: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-7576,SelfLink:/apis/apps/v1/namespaces/deployment-7576/deployments/test-cleanup-deployment,UID:5b49cfe4-7c6f-11e9-9cf4-fa163ecd1d63,ResourceVersion:12897285,Generation:1,CreationTimestamp:2019-05-22 08:55:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 22 08:55:32.653: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:55:32.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7576" for this suite.
May 22 08:55:38.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:55:38.780: INFO: namespace deployment-7576 deletion completed in 6.11666637s

• [SLOW TEST:11.276 seconds]
[sig-apps] Deployment
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:55:38.781: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-5f05f31a-7c6f-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 08:55:38.927: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1" in namespace "projected-8672" to be "success or failure"
May 22 08:55:38.930: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.779567ms
May 22 08:55:40.932: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005208877s
May 22 08:55:42.936: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009308421s
May 22 08:55:45.052: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.124825659s
May 22 08:55:47.061: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.133685302s
STEP: Saw pod success
May 22 08:55:47.061: INFO: Pod "pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:55:47.069: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 08:55:47.244: INFO: Waiting for pod pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1 to disappear
May 22 08:55:47.250: INFO: Pod pod-projected-configmaps-5f06c7a0-7c6f-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:55:47.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8672" for this suite.
May 22 08:55:53.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:55:54.411: INFO: namespace projected-8672 deletion completed in 6.879687748s

• [SLOW TEST:15.630 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:55:54.411: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1
May 22 08:55:54.586: INFO: Pod name my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1: Found 1 pods out of 1
May 22 08:55:54.586: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1" are running
May 22 08:56:00.596: INFO: Pod "my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1-5fhpf" is running (conditions: [])
May 22 08:56:00.596: INFO: Trying to dial the pod
May 22 08:56:05.617: INFO: Controller my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1: Got expected result from replica 1 [my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1-5fhpf]: "my-hostname-basic-685d2cae-7c6f-11e9-aefd-da1a35f02de1-5fhpf", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:56:05.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1453" for this suite.
May 22 08:56:11.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:56:11.718: INFO: namespace replication-controller-1453 deletion completed in 6.096446623s

• [SLOW TEST:17.307 seconds]
[sig-apps] ReplicationController
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:56:11.720: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1880
May 22 08:56:21.852: INFO: Started pod liveness-http in namespace container-probe-1880
STEP: checking the pod's current state and verifying that restartCount is present
May 22 08:56:21.855: INFO: Initial restart count of pod liveness-http is 0
May 22 08:56:25.876: INFO: Restart count of pod container-probe-1880/liveness-http is now 1 (4.020845127s elapsed)
May 22 08:56:55.941: INFO: Restart count of pod container-probe-1880/liveness-http is now 2 (34.085494002s elapsed)
May 22 08:57:05.976: INFO: Restart count of pod container-probe-1880/liveness-http is now 3 (44.120966874s elapsed)
May 22 08:57:52.305: INFO: Restart count of pod container-probe-1880/liveness-http is now 4 (1m30.449806346s elapsed)
May 22 08:58:40.828: INFO: Restart count of pod container-probe-1880/liveness-http is now 5 (2m18.973286825s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:58:40.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1880" for this suite.
May 22 08:58:46.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:58:47.048: INFO: namespace container-probe-1880 deletion completed in 6.129702493s

• [SLOW TEST:155.329 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:58:47.051: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2213
May 22 08:58:53.110: INFO: Started pod liveness-exec in namespace container-probe-2213
STEP: checking the pod's current state and verifying that restartCount is present
May 22 08:58:53.112: INFO: Initial restart count of pod liveness-exec is 0
May 22 08:59:41.229: INFO: Restart count of pod container-probe-2213/liveness-exec is now 1 (48.116689992s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:59:41.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2213" for this suite.
May 22 08:59:49.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 08:59:49.365: INFO: namespace container-probe-2213 deletion completed in 8.117122848s

• [SLOW TEST:62.314 seconds]
[k8s.io] Probing container
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 08:59:49.365: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 08:59:49.468: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1" in namespace "downward-api-1173" to be "success or failure"
May 22 08:59:49.470: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514449ms
May 22 08:59:51.475: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006713766s
May 22 08:59:53.480: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1": Phase="Running", Reason="", readiness=true. Elapsed: 4.012396109s
May 22 08:59:55.771: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1": Phase="Running", Reason="", readiness=true. Elapsed: 6.303360858s
May 22 08:59:57.807: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.339109151s
STEP: Saw pod success
May 22 08:59:57.807: INFO: Pod "downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 08:59:57.840: INFO: Trying to get logs from node node1 pod downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 08:59:58.256: INFO: Waiting for pod downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1 to disappear
May 22 08:59:58.266: INFO: Pod downwardapi-volume-f45f4582-7c6f-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 08:59:58.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1173" for this suite.
May 22 09:00:04.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:00:05.375: INFO: namespace downward-api-1173 deletion completed in 6.796026486s

• [SLOW TEST:16.010 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:00:05.376: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 22 09:00:05.418: INFO: Waiting up to 5m0s for pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1" in namespace "downward-api-8508" to be "success or failure"
May 22 09:00:05.420: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.137902ms
May 22 09:00:07.424: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005619028s
May 22 09:00:09.449: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031043045s
May 22 09:00:11.460: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.041549185s
May 22 09:00:13.463: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.045150523s
May 22 09:00:15.467: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.049084976s
May 22 09:00:17.471: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.052937134s
May 22 09:00:19.475: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.057119179s
May 22 09:00:21.479: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.061130282s
May 22 09:00:23.484: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.065768589s
STEP: Saw pod success
May 22 09:00:23.484: INFO: Pod "downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:00:23.487: INFO: Trying to get logs from node node2 pod downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1 container dapi-container: <nil>
STEP: delete the pod
May 22 09:00:23.746: INFO: Waiting for pod downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1 to disappear
May 22 09:00:23.748: INFO: Pod downward-api-fde144e9-7c6f-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-node] Downward API
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:00:23.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8508" for this suite.
May 22 09:00:29.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:00:29.875: INFO: namespace downward-api-8508 deletion completed in 6.120820004s

• [SLOW TEST:24.499 seconds]
[sig-node] Downward API
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:00:29.877: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-g2f7
STEP: Creating a pod to test atomic-volume-subpath
May 22 09:00:29.973: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-g2f7" in namespace "subpath-6284" to be "success or failure"
May 22 09:00:29.976: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147085ms
May 22 09:00:31.980: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006667186s
May 22 09:00:33.984: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011349399s
May 22 09:00:35.988: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015436846s
May 22 09:00:37.992: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019179615s
May 22 09:00:39.997: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024381334s
May 22 09:00:42.001: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 12.02853891s
May 22 09:00:44.006: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 14.033367952s
May 22 09:00:46.011: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 16.038070888s
May 22 09:00:48.016: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 18.042699045s
May 22 09:00:50.021: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 20.0485465s
May 22 09:00:52.026: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Running", Reason="", readiness=true. Elapsed: 22.053491874s
May 22 09:00:54.031: INFO: Pod "pod-subpath-test-projected-g2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.05794031s
STEP: Saw pod success
May 22 09:00:54.031: INFO: Pod "pod-subpath-test-projected-g2f7" satisfied condition "success or failure"
May 22 09:00:54.034: INFO: Trying to get logs from node node2 pod pod-subpath-test-projected-g2f7 container test-container-subpath-projected-g2f7: <nil>
STEP: delete the pod
May 22 09:00:54.057: INFO: Waiting for pod pod-subpath-test-projected-g2f7 to disappear
May 22 09:00:54.061: INFO: Pod pod-subpath-test-projected-g2f7 no longer exists
STEP: Deleting pod pod-subpath-test-projected-g2f7
May 22 09:00:54.061: INFO: Deleting pod "pod-subpath-test-projected-g2f7" in namespace "subpath-6284"
[AfterEach] [sig-storage] Subpath
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:00:54.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6284" for this suite.
May 22 09:01:00.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:01:00.185: INFO: namespace subpath-6284 deletion completed in 6.116847774s

• [SLOW TEST:30.308 seconds]
[sig-storage] Subpath
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:01:00.188: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 22 09:01:00.239: INFO: Pod name pod-release: Found 0 pods out of 1
May 22 09:01:05.243: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:01:06.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3002" for this suite.
May 22 09:01:12.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:01:12.379: INFO: namespace replication-controller-3002 deletion completed in 6.112065335s

• [SLOW TEST:12.191 seconds]
[sig-apps] ReplicationController
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:01:12.379: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 22 09:01:12.635: INFO: Waiting up to 5m0s for pod "pod-25f1a181-7c70-11e9-aefd-da1a35f02de1" in namespace "emptydir-4323" to be "success or failure"
May 22 09:01:12.638: INFO: Pod "pod-25f1a181-7c70-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.487276ms
May 22 09:01:14.642: INFO: Pod "pod-25f1a181-7c70-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006419733s
May 22 09:01:16.648: INFO: Pod "pod-25f1a181-7c70-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012477733s
STEP: Saw pod success
May 22 09:01:16.648: INFO: Pod "pod-25f1a181-7c70-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:01:16.688: INFO: Trying to get logs from node node1 pod pod-25f1a181-7c70-11e9-aefd-da1a35f02de1 container test-container: <nil>
STEP: delete the pod
May 22 09:01:16.712: INFO: Waiting for pod pod-25f1a181-7c70-11e9-aefd-da1a35f02de1 to disappear
May 22 09:01:16.716: INFO: Pod pod-25f1a181-7c70-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:01:16.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4323" for this suite.
May 22 09:01:22.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:01:22.875: INFO: namespace emptydir-4323 deletion completed in 6.154225778s

• [SLOW TEST:10.496 seconds]
[sig-storage] EmptyDir volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:01:22.876: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:01:22.992: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:01:24.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5687" for this suite.
May 22 09:01:30.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:01:30.411: INFO: namespace custom-resource-definition-5687 deletion completed in 6.108391166s

• [SLOW TEST:7.535 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:01:30.411: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 22 09:01:30.453: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:01:51.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7857" for this suite.
May 22 09:01:57.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:01:57.645: INFO: namespace init-container-7857 deletion completed in 6.11656351s

• [SLOW TEST:27.234 seconds]
[k8s.io] InitContainer [NodeConformance]
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:01:57.646: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 22 09:02:10.304: INFO: Successfully updated pod "pod-update-40cf7b11-7c70-11e9-aefd-da1a35f02de1"
STEP: verifying the updated pod is in kubernetes
May 22 09:02:10.309: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:02:10.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6276" for this suite.
May 22 09:02:32.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:02:32.473: INFO: namespace pods-6276 deletion completed in 22.159481683s

• [SLOW TEST:34.828 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:02:32.476: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 22 09:02:43.255: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-188 pod-service-account-55fc7d31-7c70-11e9-aefd-da1a35f02de1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 22 09:02:43.817: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-188 pod-service-account-55fc7d31-7c70-11e9-aefd-da1a35f02de1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 22 09:02:43.995: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-188 pod-service-account-55fc7d31-7c70-11e9-aefd-da1a35f02de1 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:02:44.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-188" for this suite.
May 22 09:02:50.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:02:50.357: INFO: namespace svcaccounts-188 deletion completed in 6.136772872s

• [SLOW TEST:17.882 seconds]
[sig-auth] ServiceAccounts
/root/workspace/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:02:50.359: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-604c4375-7c70-11e9-aefd-da1a35f02de1
STEP: Creating configMap with name cm-test-opt-upd-604c43b9-7c70-11e9-aefd-da1a35f02de1
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-604c4375-7c70-11e9-aefd-da1a35f02de1
STEP: Updating configmap cm-test-opt-upd-604c43b9-7c70-11e9-aefd-da1a35f02de1
STEP: Creating configMap with name cm-test-opt-create-604c43d7-7c70-11e9-aefd-da1a35f02de1
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:04:03.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9075" for this suite.
May 22 09:04:25.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:04:25.708: INFO: namespace configmap-9075 deletion completed in 22.149682093s

• [SLOW TEST:95.350 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:04:25.713: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 09:04:25.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1" in namespace "projected-4418" to be "success or failure"
May 22 09:04:25.819: INFO: Pod "downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.252992ms
May 22 09:04:27.823: INFO: Pod "downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006326134s
STEP: Saw pod success
May 22 09:04:27.823: INFO: Pod "downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:04:27.826: INFO: Trying to get logs from node node1 pod downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 09:04:27.856: INFO: Waiting for pod downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1 to disappear
May 22 09:04:27.859: INFO: Pod downwardapi-volume-99168b2e-7c70-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:04:27.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4418" for this suite.
May 22 09:04:33.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:04:33.970: INFO: namespace projected-4418 deletion completed in 6.106245928s

• [SLOW TEST:8.258 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:04:33.974: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-9dfa4e8e-7c70-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 09:04:34.022: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1" in namespace "configmap-9569" to be "success or failure"
May 22 09:04:34.024: INFO: Pod "pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.233396ms
May 22 09:04:36.029: INFO: Pod "pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007252135s
May 22 09:04:38.091: INFO: Pod "pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.069273729s
STEP: Saw pod success
May 22 09:04:38.091: INFO: Pod "pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:04:38.096: INFO: Trying to get logs from node node2 pod pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 09:04:38.127: INFO: Waiting for pod pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1 to disappear
May 22 09:04:38.130: INFO: Pod pod-configmaps-9dfaec8b-7c70-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:04:38.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9569" for this suite.
May 22 09:04:44.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:04:44.267: INFO: namespace configmap-9569 deletion completed in 6.131849955s

• [SLOW TEST:10.293 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:04:44.267: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 22 09:04:47.026: INFO: Successfully updated pod "labelsupdatea4274f42-7c70-11e9-aefd-da1a35f02de1"
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:04:49.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7753" for this suite.
May 22 09:05:11.124: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:05:11.207: INFO: namespace projected-7753 deletion completed in 22.098227494s

• [SLOW TEST:26.940 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:05:11.208: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:05:11.284: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:05:23.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3717" for this suite.
May 22 09:06:45.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:06:45.444: INFO: namespace pods-3717 deletion completed in 1m22.116058018s

• [SLOW TEST:94.237 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:06:45.444: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6188
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6188
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6188
May 22 09:06:45.545: INFO: Found 0 stateful pods, waiting for 1
May 22 09:06:55.549: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 22 09:07:05.590: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
May 22 09:07:15.549: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 22 09:07:15.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 09:07:20.800: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 09:07:20.800: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 09:07:20.800: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 09:07:20.804: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 09:07:20.804: INFO: Waiting for statefulset status.replicas updated to 0
May 22 09:07:20.818: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999379s
May 22 09:07:21.822: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997111701s
May 22 09:07:22.826: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.993130583s
May 22 09:07:23.830: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.989595747s
May 22 09:07:24.834: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.985984914s
May 22 09:07:25.838: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981140517s
May 22 09:07:26.844: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977723486s
May 22 09:07:27.848: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.971094899s
May 22 09:07:28.853: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967713361s
May 22 09:07:29.857: INFO: Verifying statefulset ss doesn't scale past 1 for another 962.464052ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6188
May 22 09:07:30.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 09:07:31.046: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 09:07:31.046: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 09:07:31.046: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 09:07:31.051: INFO: Found 1 stateful pods, waiting for 3
May 22 09:07:41.055: INFO: Found 1 stateful pods, waiting for 3
May 22 09:07:51.082: INFO: Found 1 stateful pods, waiting for 3
May 22 09:08:01.055: INFO: Found 1 stateful pods, waiting for 3
May 22 09:08:11.055: INFO: Found 2 stateful pods, waiting for 3
May 22 09:08:21.056: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:08:21.056: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:08:21.056: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 22 09:08:21.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 09:08:21.284: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 09:08:21.284: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 09:08:21.284: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 09:08:21.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 09:08:21.743: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 09:08:21.743: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 09:08:21.743: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 09:08:21.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 22 09:08:21.943: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 22 09:08:21.943: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 22 09:08:21.943: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 22 09:08:21.943: INFO: Waiting for statefulset status.replicas updated to 0
May 22 09:08:21.947: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 22 09:08:31.955: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
May 22 09:08:44.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 22 09:08:44.240: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 22 09:08:44.240: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 22 09:08:44.366: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999906s
May 22 09:08:45.379: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.957145364s
May 22 09:08:46.414: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.925504858s
May 22 09:08:47.552: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.797363326s
May 22 09:08:48.581: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.768306737s
May 22 09:08:49.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.744139117s
May 22 09:08:51.588: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.904944977s
May 22 09:08:52.608: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.724685944s
May 22 09:08:53.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 625.145379ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6188
May 22 09:08:54.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 09:08:58.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 09:08:58.927: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 09:08:58.927: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 09:08:58.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 09:08:59.907: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 09:08:59.907: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 09:08:59.907: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 09:08:59.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 exec --namespace=statefulset-6188 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 22 09:09:00.305: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 22 09:09:00.305: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 22 09:09:00.305: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 22 09:09:00.305: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 09:10:20.327: INFO: Deleting all statefulset in ns statefulset-6188
May 22 09:10:20.332: INFO: Scaling statefulset ss to 0
May 22 09:10:20.341: INFO: Waiting for statefulset status.replicas updated to 0
May 22 09:10:20.345: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:10:20.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6188" for this suite.
May 22 09:10:26.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:10:26.483: INFO: namespace statefulset-6188 deletion completed in 6.107625849s

• [SLOW TEST:221.039 seconds]
[sig-apps] StatefulSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:10:26.486: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-701deb13-7c71-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 09:10:26.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1" in namespace "configmap-7245" to be "success or failure"
May 22 09:10:26.585: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.278909ms
May 22 09:10:28.589: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006082085s
May 22 09:10:30.688: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105275801s
May 22 09:10:32.693: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.110122246s
May 22 09:10:34.697: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.11465762s
May 22 09:10:36.703: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.120617564s
May 22 09:10:38.707: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.124531704s
May 22 09:10:40.711: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.128868871s
May 22 09:10:42.715: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.132705225s
May 22 09:10:44.719: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.136478891s
May 22 09:10:46.722: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.13967498s
May 22 09:10:48.727: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.144550529s
May 22 09:10:50.731: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.149004781s
May 22 09:10:52.736: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.153382949s
May 22 09:10:54.740: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.157441772s
May 22 09:10:56.745: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.162357864s
May 22 09:10:58.749: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.166761917s
May 22 09:11:00.753: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.17089308s
May 22 09:11:02.760: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.177902237s
May 22 09:11:04.765: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.182802066s
May 22 09:11:06.769: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 40.186561532s
May 22 09:11:08.774: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.191084855s
STEP: Saw pod success
May 22 09:11:08.774: INFO: Pod "pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:11:08.776: INFO: Trying to get logs from node node2 pod pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 09:11:09.025: INFO: Waiting for pod pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1 to disappear
May 22 09:11:09.027: INFO: Pod pod-configmaps-701eb301-7c71-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:11:09.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7245" for this suite.
May 22 09:11:15.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:11:15.133: INFO: namespace configmap-7245 deletion completed in 6.100049221s

• [SLOW TEST:48.647 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:11:15.133: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-8d16e577-7c71-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 09:11:15.187: INFO: Waiting up to 5m0s for pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1" in namespace "secrets-8453" to be "success or failure"
May 22 09:11:15.189: INFO: Pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.107711ms
May 22 09:11:17.193: INFO: Pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005820723s
May 22 09:11:19.198: INFO: Pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01075466s
May 22 09:11:21.202: INFO: Pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.015031675s
STEP: Saw pod success
May 22 09:11:21.202: INFO: Pod "pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:11:21.205: INFO: Trying to get logs from node node1 pod pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 09:11:21.501: INFO: Waiting for pod pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1 to disappear
May 22 09:11:21.504: INFO: Pod pod-secrets-8d17a84c-7c71-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:11:21.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8453" for this suite.
May 22 09:11:27.525: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:11:27.617: INFO: namespace secrets-8453 deletion completed in 6.107353004s

• [SLOW TEST:12.484 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:11:27.619: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-94898543-7c71-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 09:11:27.685: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1" in namespace "projected-8812" to be "success or failure"
May 22 09:11:27.688: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.373376ms
May 22 09:11:29.693: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007660522s
May 22 09:11:31.698: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012961525s
May 22 09:11:33.716: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.03078665s
May 22 09:11:35.726: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04138454s
May 22 09:11:37.732: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.046885481s
May 22 09:11:39.760: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.074884152s
May 22 09:11:41.765: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.079653419s
May 22 09:11:43.769: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.083868008s
May 22 09:11:45.773: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.088229609s
May 22 09:11:47.778: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 20.092679564s
STEP: Saw pod success
May 22 09:11:47.778: INFO: Pod "pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:11:47.781: INFO: Trying to get logs from node node1 pod pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1 container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 22 09:11:47.813: INFO: Waiting for pod pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1 to disappear
May 22 09:11:47.816: INFO: Pod pod-projected-configmaps-948a8528-7c71-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:11:47.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8812" for this suite.
May 22 09:11:53.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:11:53.958: INFO: namespace projected-8812 deletion completed in 6.130711685s

• [SLOW TEST:26.340 seconds]
[sig-storage] Projected configMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:11:53.959: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:12:08.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4913" for this suite.
May 22 09:12:14.100: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:12:14.188: INFO: namespace emptydir-wrapper-4913 deletion completed in 6.108713941s

• [SLOW TEST:20.229 seconds]
[sig-storage] EmptyDir wrapper volumes
/root/workspace/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:12:14.188: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 22 09:12:14.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-9603'
May 22 09:12:14.869: INFO: stderr: ""
May 22 09:12:14.869: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 09:12:14.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9603'
May 22 09:12:14.966: INFO: stderr: ""
May 22 09:12:14.966: INFO: stdout: "update-demo-nautilus-9xdbc update-demo-nautilus-ppr9j "
May 22 09:12:14.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-9xdbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:15.105: INFO: stderr: ""
May 22 09:12:15.105: INFO: stdout: ""
May 22 09:12:15.105: INFO: update-demo-nautilus-9xdbc is created but not running
May 22 09:12:20.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9603'
May 22 09:12:20.215: INFO: stderr: ""
May 22 09:12:20.215: INFO: stdout: "update-demo-nautilus-9xdbc update-demo-nautilus-ppr9j "
May 22 09:12:20.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-9xdbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:20.290: INFO: stderr: ""
May 22 09:12:20.290: INFO: stdout: "true"
May 22 09:12:20.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-9xdbc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:20.370: INFO: stderr: ""
May 22 09:12:20.370: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:12:20.370: INFO: validating pod update-demo-nautilus-9xdbc
May 22 09:12:20.442: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:12:20.442: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:12:20.442: INFO: update-demo-nautilus-9xdbc is verified up and running
May 22 09:12:20.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-ppr9j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:20.521: INFO: stderr: ""
May 22 09:12:20.521: INFO: stdout: ""
May 22 09:12:20.521: INFO: update-demo-nautilus-ppr9j is created but not running
May 22 09:12:25.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9603'
May 22 09:12:25.614: INFO: stderr: ""
May 22 09:12:25.614: INFO: stdout: "update-demo-nautilus-9xdbc update-demo-nautilus-ppr9j "
May 22 09:12:25.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-9xdbc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:25.713: INFO: stderr: ""
May 22 09:12:25.713: INFO: stdout: "true"
May 22 09:12:25.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-9xdbc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:25.795: INFO: stderr: ""
May 22 09:12:25.795: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:12:25.795: INFO: validating pod update-demo-nautilus-9xdbc
May 22 09:12:25.799: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:12:25.799: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:12:25.799: INFO: update-demo-nautilus-9xdbc is verified up and running
May 22 09:12:25.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-ppr9j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:25.877: INFO: stderr: ""
May 22 09:12:25.877: INFO: stdout: "true"
May 22 09:12:25.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-ppr9j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9603'
May 22 09:12:25.948: INFO: stderr: ""
May 22 09:12:25.948: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:12:25.948: INFO: validating pod update-demo-nautilus-ppr9j
May 22 09:12:26.644: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:12:26.644: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:12:26.644: INFO: update-demo-nautilus-ppr9j is verified up and running
STEP: using delete to clean up resources
May 22 09:12:26.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-9603'
May 22 09:12:26.727: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 09:12:26.728: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 22 09:12:26.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9603'
May 22 09:12:26.811: INFO: stderr: "No resources found.\n"
May 22 09:12:26.811: INFO: stdout: ""
May 22 09:12:26.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=update-demo --namespace=kubectl-9603 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 09:12:26.906: INFO: stderr: ""
May 22 09:12:26.906: INFO: stdout: "update-demo-nautilus-9xdbc\nupdate-demo-nautilus-ppr9j\n"
May 22 09:12:27.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9603'
May 22 09:12:27.498: INFO: stderr: "No resources found.\n"
May 22 09:12:27.498: INFO: stdout: ""
May 22 09:12:27.498: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=update-demo --namespace=kubectl-9603 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 09:12:27.582: INFO: stderr: ""
May 22 09:12:27.582: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:12:27.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9603" for this suite.
May 22 09:12:33.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:12:33.920: INFO: namespace kubectl-9603 deletion completed in 6.332564241s

• [SLOW TEST:19.732 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:12:33.921: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 22 09:12:33.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 --namespace=kubectl-6313 run e2e-test-rm-busybox-job --image=hub.c.163.com/combk8s/busybox-amd64:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 22 09:12:47.044: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 22 09:12:47.044: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:12:49.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6313" for this suite.
May 22 09:13:11.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:13:11.233: INFO: namespace kubectl-6313 deletion completed in 22.111904901s

• [SLOW TEST:37.313 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:13:11.238: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:13:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-456" for this suite.
May 22 09:14:03.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:14:03.512: INFO: namespace kubelet-test-456 deletion completed in 38.095471806s

• [SLOW TEST:52.274 seconds]
[k8s.io] Kubelet
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:14:03.514: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 09:14:03.562: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1" in namespace "downward-api-9702" to be "success or failure"
May 22 09:14:03.564: INFO: Pod "downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.404985ms
May 22 09:14:05.570: INFO: Pod "downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007817691s
STEP: Saw pod success
May 22 09:14:05.570: INFO: Pod "downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:14:05.572: INFO: Trying to get logs from node node2 pod downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 09:14:05.599: INFO: Waiting for pod downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1 to disappear
May 22 09:14:05.602: INFO: Pod downwardapi-volume-f17382de-7c71-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:14:05.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9702" for this suite.
May 22 09:14:11.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:14:11.723: INFO: namespace downward-api-9702 deletion completed in 6.116203231s

• [SLOW TEST:8.209 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:14:11.724: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 47.5.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.5.47_udp@PTR;check="$$(dig +tcp +noall +answer +search 47.5.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.5.47_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8600.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8600.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8600.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8600.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 47.5.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.5.47_udp@PTR;check="$$(dig +tcp +noall +answer +search 47.5.178.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.178.5.47_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 09:14:33.873: INFO: Unable to read wheezy_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.903: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.907: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.910: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.938: INFO: Unable to read jessie_udp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.941: INFO: Unable to read jessie_tcp@dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.944: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.948: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local from pod dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1: the server could not find the requested resource (get pods dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1)
May 22 09:14:33.965: INFO: Lookups using dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1 failed for: [wheezy_udp@dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@dns-test-service.dns-8600.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_udp@dns-test-service.dns-8600.svc.cluster.local jessie_tcp@dns-test-service.dns-8600.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8600.svc.cluster.local]

May 22 09:14:39.027: INFO: DNS probes using dns-8600/dns-test-f6640c69-7c71-11e9-aefd-da1a35f02de1 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:14:39.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8600" for this suite.
May 22 09:14:45.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:14:45.310: INFO: namespace dns-8600 deletion completed in 6.209380264s

• [SLOW TEST:33.586 seconds]
[sig-network] DNS
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:14:45.312: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:14:49.556: INFO: Waiting up to 5m0s for pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1" in namespace "pods-884" to be "success or failure"
May 22 09:14:49.560: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598641ms
May 22 09:14:51.565: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008063907s
May 22 09:14:53.662: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.105766329s
May 22 09:14:55.682: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.125055133s
May 22 09:14:57.776: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.219481366s
May 22 09:14:59.789: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.232819804s
May 22 09:15:01.814: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.257739227s
May 22 09:15:03.818: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.261517846s
May 22 09:15:05.821: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.264596924s
May 22 09:15:07.826: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 18.269686641s
STEP: Saw pod success
May 22 09:15:07.826: INFO: Pod "client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:15:07.829: INFO: Trying to get logs from node node1 pod client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1 container env3cont: <nil>
STEP: delete the pod
May 22 09:15:07.853: INFO: Waiting for pod client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1 to disappear
May 22 09:15:07.856: INFO: Pod client-envvars-0cddfc23-7c72-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [k8s.io] Pods
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:15:07.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-884" for this suite.
May 22 09:15:51.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:15:51.984: INFO: namespace pods-884 deletion completed in 44.123875154s

• [SLOW TEST:66.672 seconds]
[k8s.io] Pods
/root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:15:51.985: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-436
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 22 09:15:52.056: INFO: Found 0 stateful pods, waiting for 3
May 22 09:16:02.090: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:16:02.090: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:16:02.090: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from hub.c.163.com/combk8s/nginx-amd64:1.14-alpine to hub.c.163.com/combk8s/nginx-amd64:1.15-alpine
May 22 09:16:02.117: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 22 09:16:12.158: INFO: Updating stateful set ss2
May 22 09:16:12.171: INFO: Waiting for Pod statefulset-436/ss2-2 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 09:16:22.179: INFO: Waiting for Pod statefulset-436/ss2-2 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 09:16:32.180: INFO: Waiting for Pod statefulset-436/ss2-2 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
STEP: Restoring Pods to the correct revision when they are deleted
May 22 09:16:42.221: INFO: Found 2 stateful pods, waiting for 3
May 22 09:16:52.226: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:16:52.226: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:16:52.226: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
May 22 09:17:02.390: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:17:02.390: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 22 09:17:02.390: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 22 09:17:02.647: INFO: Updating stateful set ss2
May 22 09:17:02.656: INFO: Waiting for Pod statefulset-436/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 09:17:12.665: INFO: Waiting for Pod statefulset-436/ss2-1 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
May 22 09:17:22.685: INFO: Updating stateful set ss2
May 22 09:17:22.693: INFO: Waiting for StatefulSet statefulset-436/ss2 to complete update
May 22 09:17:22.693: INFO: Waiting for Pod statefulset-436/ss2-0 to have revision ss2-85b57d9f75 update revision ss2-869d6c9d88
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 22 09:17:32.704: INFO: Deleting all statefulset in ns statefulset-436
May 22 09:17:32.708: INFO: Scaling statefulset ss2 to 0
May 22 09:18:02.815: INFO: Waiting for statefulset status.replicas updated to 0
May 22 09:18:02.826: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:02.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-436" for this suite.
May 22 09:18:08.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:18:08.953: INFO: namespace statefulset-436 deletion completed in 6.105572554s

• [SLOW TEST:136.968 seconds]
[sig-apps] StatefulSet
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:18:08.953: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6369.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6369.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6369.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6369.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6369.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6369.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 22 09:18:23.875: INFO: DNS probes using dns-6369/dns-test-83c758fc-7c72-11e9-aefd-da1a35f02de1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:23.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6369" for this suite.
May 22 09:18:29.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:18:30.015: INFO: namespace dns-6369 deletion completed in 6.104730044s

• [SLOW TEST:21.062 seconds]
[sig-network] DNS
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:18:30.015: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 09:18:30.066: INFO: Waiting up to 5m0s for pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1" in namespace "projected-299" to be "success or failure"
May 22 09:18:30.068: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.701765ms
May 22 09:18:32.072: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006688633s
May 22 09:18:34.077: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011209438s
May 22 09:18:36.081: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014820195s
May 22 09:18:38.091: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.024946514s
STEP: Saw pod success
May 22 09:18:38.091: INFO: Pod "downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:18:38.095: INFO: Trying to get logs from node node1 pod downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 09:18:38.130: INFO: Waiting for pod downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1 to disappear
May 22 09:18:38.132: INFO: Pod downwardapi-volume-904c6184-7c72-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:38.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-299" for this suite.
May 22 09:18:44.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:18:44.314: INFO: namespace projected-299 deletion completed in 6.175202872s

• [SLOW TEST:14.299 seconds]
[sig-storage] Projected downwardAPI
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:18:44.314: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 22 09:18:44.371: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-393362560 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:44.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-818" for this suite.
May 22 09:18:50.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:18:50.876: INFO: namespace kubectl-818 deletion completed in 6.414283792s

• [SLOW TEST:6.562 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:18:50.878: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:18:50.922: INFO: Creating deployment "test-recreate-deployment"
May 22 09:18:50.932: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 22 09:18:50.946: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 22 09:18:52.951: INFO: Waiting deployment "test-recreate-deployment" to complete
May 22 09:18:52.954: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 22 09:18:52.963: INFO: Updating deployment test-recreate-deployment
May 22 09:18:52.964: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 22 09:18:53.087: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-2210,SelfLink:/apis/apps/v1/namespaces/deployment-2210/deployments/test-recreate-deployment,UID:9cbcc601-7c72-11e9-9cf4-fa163ecd1d63,ResourceVersion:12917228,Generation:2,CreationTimestamp:2019-05-22 09:18:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-22 09:18:53 +0000 UTC 2019-05-22 09:18:53 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-22 09:18:53 +0000 UTC 2019-05-22 09:18:50 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-854f7bcc8b" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 22 09:18:53.089: INFO: New ReplicaSet "test-recreate-deployment-854f7bcc8b" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-854f7bcc8b,GenerateName:,Namespace:deployment-2210,SelfLink:/apis/apps/v1/namespaces/deployment-2210/replicasets/test-recreate-deployment-854f7bcc8b,UID:9e0045f5-7c72-11e9-8476-fa163e387bbe,ResourceVersion:12917226,Generation:1,CreationTimestamp:2019-05-22 09:18:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 854f7bcc8b,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9cbcc601-7c72-11e9-9cf4-fa163ecd1d63 0xc001e71b87 0xc001e71b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 854f7bcc8b,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 854f7bcc8b,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 09:18:53.089: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 22 09:18:53.089: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-8578596799,GenerateName:,Namespace:deployment-2210,SelfLink:/apis/apps/v1/namespaces/deployment-2210/replicasets/test-recreate-deployment-8578596799,UID:9cbd47d5-7c72-11e9-8476-fa163e387bbe,ResourceVersion:12917219,Generation:2,CreationTimestamp:2019-05-22 09:18:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 8578596799,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 9cbcc601-7c72-11e9-9cf4-fa163ecd1d63 0xc001e71c57 0xc001e71c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 8578596799,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 8578596799,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis hub.c.163.com/combk8s/kubernetes-e2e-test-images/redis-amd64:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 22 09:18:53.093: INFO: Pod "test-recreate-deployment-854f7bcc8b-tn6t8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-854f7bcc8b-tn6t8,GenerateName:test-recreate-deployment-854f7bcc8b-,Namespace:deployment-2210,SelfLink:/api/v1/namespaces/deployment-2210/pods/test-recreate-deployment-854f7bcc8b-tn6t8,UID:9e00e18e-7c72-11e9-8476-fa163e387bbe,ResourceVersion:12917221,Generation:0,CreationTimestamp:2019-05-22 09:18:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 854f7bcc8b,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-854f7bcc8b 9e0045f5-7c72-11e9-8476-fa163e387bbe 0xc00187c597 0xc00187c598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kh5fk {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kh5fk,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx hub.c.163.com/combk8s/nginx-amd64:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kh5fk true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00187c620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00187c640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:53.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2210" for this suite.
May 22 09:18:59.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:18:59.280: INFO: namespace deployment-2210 deletion completed in 6.18234553s

• [SLOW TEST:8.402 seconds]
[sig-apps] Deployment
/root/workspace/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:18:59.281: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:18:59.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1267" for this suite.
May 22 09:19:05.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:19:05.462: INFO: namespace services-1267 deletion completed in 6.12897476s
[AfterEach] [sig-network] Services
  /root/workspace/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.181 seconds]
[sig-network] Services
/root/workspace/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:19:05.462: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:19:05.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 version'
May 22 09:19:05.583: INFO: stderr: ""
May 22 09:19:05.583: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:11:31Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.1\", GitCommit:\"b7394102d6ef778017f2ca4046abbaa23b88c290\", GitTreeState:\"clean\", BuildDate:\"2019-04-08T17:02:58Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:19:05.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9761" for this suite.
May 22 09:19:11.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:19:11.764: INFO: namespace kubectl-9761 deletion completed in 6.174734424s

• [SLOW TEST:6.302 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:19:11.767: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 22 09:19:11.819: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1" in namespace "downward-api-5188" to be "success or failure"
May 22 09:19:11.821: INFO: Pod "downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.361436ms
May 22 09:19:13.826: INFO: Pod "downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007198636s
STEP: Saw pod success
May 22 09:19:13.826: INFO: Pod "downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:19:13.830: INFO: Trying to get logs from node node2 pod downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1 container client-container: <nil>
STEP: delete the pod
May 22 09:19:13.867: INFO: Waiting for pod downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1 to disappear
May 22 09:19:13.871: INFO: Pod downwardapi-volume-a93012dc-7c72-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:19:13.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5188" for this suite.
May 22 09:19:19.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:19:20.052: INFO: namespace downward-api-5188 deletion completed in 6.138551919s

• [SLOW TEST:8.285 seconds]
[sig-storage] Downward API volume
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:19:20.052: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-ae284159-7c72-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 09:19:20.210: INFO: Waiting up to 5m0s for pod "pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1" in namespace "secrets-7201" to be "success or failure"
May 22 09:19:20.212: INFO: Pod "pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.25554ms
May 22 09:19:22.217: INFO: Pod "pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007028434s
May 22 09:19:24.222: INFO: Pod "pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011599501s
STEP: Saw pod success
May 22 09:19:24.222: INFO: Pod "pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:19:24.225: INFO: Trying to get logs from node node2 pod pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1 container secret-volume-test: <nil>
STEP: delete the pod
May 22 09:19:24.311: INFO: Waiting for pod pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1 to disappear
May 22 09:19:24.314: INFO: Pod pod-secrets-ae301c6c-7c72-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Secrets
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:19:24.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7201" for this suite.
May 22 09:19:30.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:19:30.436: INFO: namespace secrets-7201 deletion completed in 6.115971657s
STEP: Destroying namespace "secret-namespace-8538" for this suite.
May 22 09:19:36.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:19:36.570: INFO: namespace secret-namespace-8538 deletion completed in 6.133913221s

• [SLOW TEST:16.518 seconds]
[sig-storage] Secrets
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:19:36.570: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:20:04.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5870" for this suite.
May 22 09:20:10.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:20:11.973: INFO: namespace namespaces-5870 deletion completed in 7.168196781s
STEP: Destroying namespace "nsdeletetest-913" for this suite.
May 22 09:20:11.985: INFO: Namespace nsdeletetest-913 was already deleted
STEP: Destroying namespace "nsdeletetest-1372" for this suite.
May 22 09:20:18.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:20:18.878: INFO: namespace nsdeletetest-1372 deletion completed in 6.892460163s

• [SLOW TEST:42.308 seconds]
[sig-api-machinery] Namespaces [Serial]
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:20:18.878: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 22 09:20:18.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 create -f - --namespace=kubectl-6406'
May 22 09:20:21.900: INFO: stderr: ""
May 22 09:20:21.900: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 09:20:21.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:23.532: INFO: stderr: ""
May 22 09:20:23.532: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:23.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:23.789: INFO: stderr: ""
May 22 09:20:23.789: INFO: stdout: ""
May 22 09:20:23.789: INFO: update-demo-nautilus-2zk8v is created but not running
May 22 09:20:28.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:28.889: INFO: stderr: ""
May 22 09:20:28.889: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:28.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:29.103: INFO: stderr: ""
May 22 09:20:29.103: INFO: stdout: ""
May 22 09:20:29.103: INFO: update-demo-nautilus-2zk8v is created but not running
May 22 09:20:34.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:34.195: INFO: stderr: ""
May 22 09:20:34.195: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:34.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:34.279: INFO: stderr: ""
May 22 09:20:34.279: INFO: stdout: ""
May 22 09:20:34.279: INFO: update-demo-nautilus-2zk8v is created but not running
May 22 09:20:39.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:39.381: INFO: stderr: ""
May 22 09:20:39.381: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:39.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:39.501: INFO: stderr: ""
May 22 09:20:39.501: INFO: stdout: ""
May 22 09:20:39.501: INFO: update-demo-nautilus-2zk8v is created but not running
May 22 09:20:44.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:44.600: INFO: stderr: ""
May 22 09:20:44.600: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:44.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:44.698: INFO: stderr: ""
May 22 09:20:44.698: INFO: stdout: "true"
May 22 09:20:44.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:44.798: INFO: stderr: ""
May 22 09:20:44.798: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:20:44.798: INFO: validating pod update-demo-nautilus-2zk8v
May 22 09:20:45.518: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:20:45.518: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:20:45.518: INFO: update-demo-nautilus-2zk8v is verified up and running
May 22 09:20:45.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-nbm8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:45.596: INFO: stderr: ""
May 22 09:20:45.596: INFO: stdout: ""
May 22 09:20:45.596: INFO: update-demo-nautilus-nbm8w is created but not running
May 22 09:20:50.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:50.710: INFO: stderr: ""
May 22 09:20:50.710: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
May 22 09:20:50.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:50.831: INFO: stderr: ""
May 22 09:20:50.831: INFO: stdout: "true"
May 22 09:20:50.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:50.922: INFO: stderr: ""
May 22 09:20:50.922: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:20:50.922: INFO: validating pod update-demo-nautilus-2zk8v
May 22 09:20:50.926: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:20:50.926: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:20:50.926: INFO: update-demo-nautilus-2zk8v is verified up and running
May 22 09:20:50.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-nbm8w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:51.043: INFO: stderr: ""
May 22 09:20:51.043: INFO: stdout: "true"
May 22 09:20:51.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-nbm8w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:20:51.119: INFO: stderr: ""
May 22 09:20:51.119: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:20:51.119: INFO: validating pod update-demo-nautilus-nbm8w
May 22 09:20:51.365: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:20:51.365: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:20:51.365: INFO: update-demo-nautilus-nbm8w is verified up and running
STEP: scaling down the replication controller
May 22 09:20:51.368: INFO: scanned /root for discovery docs: <nil>
May 22 09:20:51.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6406'
May 22 09:20:52.526: INFO: stderr: ""
May 22 09:20:52.526: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 09:20:52.526: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:52.636: INFO: stderr: ""
May 22 09:20:52.636: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 22 09:20:57.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:20:57.739: INFO: stderr: ""
May 22 09:20:57.739: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-nbm8w "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 22 09:21:02.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:21:02.910: INFO: stderr: ""
May 22 09:21:02.910: INFO: stdout: "update-demo-nautilus-2zk8v "
May 22 09:21:02.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:03.038: INFO: stderr: ""
May 22 09:21:03.038: INFO: stdout: "true"
May 22 09:21:03.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:03.131: INFO: stderr: ""
May 22 09:21:03.131: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:21:03.131: INFO: validating pod update-demo-nautilus-2zk8v
May 22 09:21:03.136: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:21:03.136: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:21:03.136: INFO: update-demo-nautilus-2zk8v is verified up and running
STEP: scaling up the replication controller
May 22 09:21:03.137: INFO: scanned /root for discovery docs: <nil>
May 22 09:21:03.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6406'
May 22 09:21:04.267: INFO: stderr: ""
May 22 09:21:04.267: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 22 09:21:04.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:21:04.352: INFO: stderr: ""
May 22 09:21:04.352: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-7zhw8 "
May 22 09:21:04.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:04.427: INFO: stderr: ""
May 22 09:21:04.427: INFO: stdout: "true"
May 22 09:21:04.427: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:04.500: INFO: stderr: ""
May 22 09:21:04.500: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:21:04.500: INFO: validating pod update-demo-nautilus-2zk8v
May 22 09:21:04.505: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:21:04.505: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:21:04.505: INFO: update-demo-nautilus-2zk8v is verified up and running
May 22 09:21:04.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-7zhw8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:04.577: INFO: stderr: ""
May 22 09:21:04.577: INFO: stdout: ""
May 22 09:21:04.577: INFO: update-demo-nautilus-7zhw8 is created but not running
May 22 09:21:09.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6406'
May 22 09:21:09.676: INFO: stderr: ""
May 22 09:21:09.676: INFO: stdout: "update-demo-nautilus-2zk8v update-demo-nautilus-7zhw8 "
May 22 09:21:09.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:09.753: INFO: stderr: ""
May 22 09:21:09.753: INFO: stdout: "true"
May 22 09:21:09.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-2zk8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:09.827: INFO: stderr: ""
May 22 09:21:09.827: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:21:09.827: INFO: validating pod update-demo-nautilus-2zk8v
May 22 09:21:09.831: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:21:09.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:21:09.831: INFO: update-demo-nautilus-2zk8v is verified up and running
May 22 09:21:09.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-7zhw8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:09.909: INFO: stderr: ""
May 22 09:21:09.909: INFO: stdout: "true"
May 22 09:21:09.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods update-demo-nautilus-7zhw8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6406'
May 22 09:21:09.982: INFO: stderr: ""
May 22 09:21:09.982: INFO: stdout: "hub.c.163.com/combk8s/kubernetes-e2e-test-images/nautilus-amd64:1.0"
May 22 09:21:09.982: INFO: validating pod update-demo-nautilus-7zhw8
May 22 09:21:10.786: INFO: got data: {
  "image": "nautilus.jpg"
}

May 22 09:21:10.786: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 22 09:21:10.786: INFO: update-demo-nautilus-7zhw8 is verified up and running
STEP: using delete to clean up resources
May 22 09:21:10.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 delete --grace-period=0 --force -f - --namespace=kubectl-6406'
May 22 09:21:10.903: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 22 09:21:10.903: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 22 09:21:10.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6406'
May 22 09:21:11.004: INFO: stderr: "No resources found.\n"
May 22 09:21:11.004: INFO: stdout: ""
May 22 09:21:11.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=update-demo --namespace=kubectl-6406 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 09:21:11.084: INFO: stderr: ""
May 22 09:21:11.084: INFO: stdout: "update-demo-nautilus-2zk8v\nupdate-demo-nautilus-7zhw8\n"
May 22 09:21:11.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6406'
May 22 09:21:11.684: INFO: stderr: "No resources found.\n"
May 22 09:21:11.684: INFO: stdout: ""
May 22 09:21:11.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-393362560 get pods -l name=update-demo --namespace=kubectl-6406 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 22 09:21:11.763: INFO: stderr: ""
May 22 09:21:11.763: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:21:11.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6406" for this suite.
May 22 09:21:33.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:21:33.874: INFO: namespace kubectl-6406 deletion completed in 22.106045464s

• [SLOW TEST:74.996 seconds]
[sig-cli] Kubectl client
/root/workspace/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:21:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-fdf14f61-7c72-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume secrets
May 22 09:21:34.023: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1" in namespace "projected-9366" to be "success or failure"
May 22 09:21:34.026: INFO: Pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.756256ms
May 22 09:21:36.031: INFO: Pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007834399s
May 22 09:21:38.034: INFO: Pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011712513s
May 22 09:21:40.039: INFO: Pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016060408s
STEP: Saw pod success
May 22 09:21:40.039: INFO: Pod "pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:21:40.041: INFO: Trying to get logs from node node1 pod pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1 container projected-secret-volume-test: <nil>
STEP: delete the pod
May 22 09:21:40.067: INFO: Waiting for pod pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1 to disappear
May 22 09:21:40.070: INFO: Pod pod-projected-secrets-fdf23bb0-7c72-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] Projected secret
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:21:40.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9366" for this suite.
May 22 09:21:46.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:21:46.190: INFO: namespace projected-9366 deletion completed in 6.114701991s

• [SLOW TEST:12.313 seconds]
[sig-storage] Projected secret
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:21:46.190: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 22 09:21:46.270: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"053e3c24-7c73-11e9-9cf4-fa163ecd1d63", Controller:(*bool)(0xc000c36c1a), BlockOwnerDeletion:(*bool)(0xc000c36c1b)}}
May 22 09:21:46.275: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"053c34b8-7c73-11e9-9cf4-fa163ecd1d63", Controller:(*bool)(0xc001d7272a), BlockOwnerDeletion:(*bool)(0xc001d7272b)}}
May 22 09:21:46.280: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"053cd39c-7c73-11e9-9cf4-fa163ecd1d63", Controller:(*bool)(0xc000c370ea), BlockOwnerDeletion:(*bool)(0xc000c370eb)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:21:51.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5374" for this suite.
May 22 09:21:57.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:21:57.475: INFO: namespace gc-5374 deletion completed in 6.115524902s

• [SLOW TEST:11.285 seconds]
[sig-api-machinery] Garbage collector
/root/workspace/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 22 09:21:57.476: INFO: >>> kubeConfig: /tmp/kubeconfig-393362560
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-0bf532f0-7c73-11e9-aefd-da1a35f02de1
STEP: Creating a pod to test consume configMaps
May 22 09:21:57.532: INFO: Waiting up to 5m0s for pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1" in namespace "configmap-3195" to be "success or failure"
May 22 09:21:57.534: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.931346ms
May 22 09:21:59.589: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05737316s
May 22 09:22:01.593: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.061322103s
May 22 09:22:03.598: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.066381035s
May 22 09:22:05.602: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.070429915s
May 22 09:22:07.606: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.074683853s
May 22 09:22:09.614: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 12.082606369s
May 22 09:22:11.618: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 14.08687024s
May 22 09:22:13.623: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.091331884s
May 22 09:22:15.627: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 18.095396392s
May 22 09:22:17.631: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 20.099758398s
May 22 09:22:19.636: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 22.104012828s
May 22 09:22:21.640: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 24.108178673s
May 22 09:22:23.645: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 26.113274748s
May 22 09:22:25.649: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 28.117409894s
May 22 09:22:27.692: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 30.160444836s
May 22 09:22:29.697: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 32.165508278s
May 22 09:22:31.709: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 34.177831483s
May 22 09:22:33.713: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 36.181204069s
May 22 09:22:35.788: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 38.256341111s
May 22 09:22:37.796: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Pending", Reason="", readiness=false. Elapsed: 40.263904146s
May 22 09:22:39.800: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 42.268693298s
STEP: Saw pod success
May 22 09:22:39.800: INFO: Pod "pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1" satisfied condition "success or failure"
May 22 09:22:39.803: INFO: Trying to get logs from node node1 pod pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1 container configmap-volume-test: <nil>
STEP: delete the pod
May 22 09:22:39.826: INFO: Waiting for pod pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1 to disappear
May 22 09:22:39.829: INFO: Pod pod-configmaps-0bf5da2d-7c73-11e9-aefd-da1a35f02de1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 22 09:22:39.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3195" for this suite.
May 22 09:22:45.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 22 09:22:45.953: INFO: namespace configmap-3195 deletion completed in 6.118729379s

• [SLOW TEST:48.477 seconds]
[sig-storage] ConfigMap
/root/workspace/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /root/workspace/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
May 22 09:22:45.953: INFO: Running AfterSuite actions on all nodes
May 22 09:22:45.956: INFO: Running AfterSuite actions on node 1
May 22 09:22:45.956: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 8238.249 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 2h17m20.805729062s
Test Suite Passed
