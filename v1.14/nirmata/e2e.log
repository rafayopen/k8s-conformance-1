I0513 23:59:21.440765      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-353874590
I0513 23:59:21.441275      16 e2e.go:240] Starting e2e run "1f3dbeca-75db-11e9-8961-767a82fad75b" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1557791959 - Will randomize all specs
Will run 204 of 3584 specs

May 13 23:59:21.773: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 13 23:59:21.779: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
May 13 23:59:22.631: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
May 13 23:59:22.677: INFO: 3 / 3 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
May 13 23:59:22.677: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
May 13 23:59:22.677: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
May 13 23:59:22.689: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
May 13 23:59:22.689: INFO: e2e test version: v1.14.0
May 13 23:59:22.692: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 23:59:22.694: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename watch
May 13 23:59:23.334: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
May 13 23:59:23.386: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6376,SelfLink:/api/v1/namespaces/watch-6376/configmaps/e2e-watch-test-watch-closed,UID:219835d8-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134518,Generation:0,CreationTimestamp:2019-05-13 23:59:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 13 23:59:23.387: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6376,SelfLink:/api/v1/namespaces/watch-6376/configmaps/e2e-watch-test-watch-closed,UID:219835d8-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134519,Generation:0,CreationTimestamp:2019-05-13 23:59:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
May 13 23:59:23.498: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6376,SelfLink:/api/v1/namespaces/watch-6376/configmaps/e2e-watch-test-watch-closed,UID:219835d8-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134520,Generation:0,CreationTimestamp:2019-05-13 23:59:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 13 23:59:23.498: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-6376,SelfLink:/api/v1/namespaces/watch-6376/configmaps/e2e-watch-test-watch-closed,UID:219835d8-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134521,Generation:0,CreationTimestamp:2019-05-13 23:59:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 23:59:23.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6376" for this suite.
May 13 23:59:29.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 23:59:29.735: INFO: namespace watch-6376 deletion completed in 6.199993337s

• [SLOW TEST:7.041 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 23:59:29.736: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-25712476-75db-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 13 23:59:29.820: INFO: Waiting up to 5m0s for pod "pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b" in namespace "secrets-600" to be "success or failure"
May 13 23:59:29.837: INFO: Pod "pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.954262ms
May 13 23:59:31.842: INFO: Pod "pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022367653s
May 13 23:59:33.847: INFO: Pod "pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026631261s
STEP: Saw pod success
May 13 23:59:33.847: INFO: Pod "pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 13 23:59:33.851: INFO: Trying to get logs from node conformance0 pod pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b container secret-env-test: <nil>
STEP: delete the pod
May 13 23:59:33.913: INFO: Waiting for pod pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b to disappear
May 13 23:59:33.925: INFO: Pod pod-secrets-25721b6e-75db-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 23:59:33.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-600" for this suite.
May 13 23:59:39.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 23:59:40.137: INFO: namespace secrets-600 deletion completed in 6.207235002s

• [SLOW TEST:10.402 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 23:59:40.138: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 23:59:49.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2070" for this suite.
May 13 23:59:55.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 13 23:59:55.389: INFO: namespace kubelet-test-2070 deletion completed in 6.205303936s

• [SLOW TEST:15.251 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 13 23:59:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 13 23:59:55.474: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b" in namespace "projected-6992" to be "success or failure"
May 13 23:59:55.481: INFO: Pod "downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.69063ms
May 13 23:59:57.486: INFO: Pod "downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012184382s
May 13 23:59:59.492: INFO: Pod "downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018198445s
STEP: Saw pod success
May 13 23:59:59.492: INFO: Pod "downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 13 23:59:59.497: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 13 23:59:59.542: INFO: Waiting for pod downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b to disappear
May 13 23:59:59.548: INFO: Pod downwardapi-volume-34bcc660-75db-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 13 23:59:59.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6992" for this suite.
May 14 00:00:05.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:00:05.735: INFO: namespace projected-6992 deletion completed in 6.181085732s

• [SLOW TEST:10.346 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:00:05.738: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:00:05.868: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b" in namespace "downward-api-3945" to be "success or failure"
May 14 00:00:05.878: INFO: Pod "downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.348871ms
May 14 00:00:07.884: INFO: Pod "downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016340111s
May 14 00:00:09.890: INFO: Pod "downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022096106s
STEP: Saw pod success
May 14 00:00:09.890: INFO: Pod "downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:00:09.895: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:00:09.944: INFO: Waiting for pod downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b to disappear
May 14 00:00:09.950: INFO: Pod downwardapi-volume-3aeefadf-75db-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:00:09.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3945" for this suite.
May 14 00:00:15.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:00:16.247: INFO: namespace downward-api-3945 deletion completed in 6.291268727s

• [SLOW TEST:10.510 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:00:16.247: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:00:16.301: INFO: Creating deployment "test-recreate-deployment"
May 14 00:00:16.313: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
May 14 00:00:16.335: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
May 14 00:00:18.344: INFO: Waiting deployment "test-recreate-deployment" to complete
May 14 00:00:18.347: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388816, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388816, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388816, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388816, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:00:20.353: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
May 14 00:00:20.365: INFO: Updating deployment test-recreate-deployment
May 14 00:00:20.365: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 14 00:00:20.689: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5534,SelfLink:/apis/apps/v1/namespaces/deployment-5534/deployments/test-recreate-deployment,UID:4129964a-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134681,Generation:2,CreationTimestamp:2019-05-14 00:00:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-05-14 00:00:20 +0000 UTC 2019-05-14 00:00:20 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-05-14 00:00:20 +0000 UTC 2019-05-14 00:00:16 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

May 14 00:00:20.697: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-5534,SelfLink:/apis/apps/v1/namespaces/deployment-5534/replicasets/test-recreate-deployment-c9cbd8684,UID:43ac87a9-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134679,Generation:1,CreationTimestamp:2019-05-14 00:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 4129964a-75db-11e9-9f2c-ceb99be2323d 0xc000addfb0 0xc000addfb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 00:00:20.698: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
May 14 00:00:20.698: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-5534,SelfLink:/apis/apps/v1/namespaces/deployment-5534/replicasets/test-recreate-deployment-7d57d5ff7c,UID:412bbd54-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134669,Generation:2,CreationTimestamp:2019-05-14 00:00:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 4129964a-75db-11e9-9f2c-ceb99be2323d 0xc000adde47 0xc000adde48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 00:00:20.709: INFO: Pod "test-recreate-deployment-c9cbd8684-9cr6c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-9cr6c,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-5534,SelfLink:/api/v1/namespaces/deployment-5534/pods/test-recreate-deployment-c9cbd8684-9cr6c,UID:43af8712-75db-11e9-9f2c-ceb99be2323d,ResourceVersion:134682,Generation:0,CreationTimestamp:2019-05-14 00:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 43ac87a9-75db-11e9-9f2c-ceb99be2323d 0xc0010275b0 0xc0010275b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-g4kd7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-g4kd7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-g4kd7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001027620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001027640}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:00:20 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:00:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:00:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:00:20 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:00:20 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:00:20.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5534" for this suite.
May 14 00:00:26.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:00:27.038: INFO: namespace deployment-5534 deletion completed in 6.319994795s

• [SLOW TEST:10.791 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:00:27.040: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2651
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 00:00:27.105: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 00:00:49.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2651 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:00:49.269: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:00:50.963: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:00:50.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2651" for this suite.
May 14 00:01:14.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:01:15.163: INFO: namespace pod-network-test-2651 deletion completed in 24.195375204s

• [SLOW TEST:48.123 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:01:15.163: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6447b4c0-75db-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6447b4c0-75db-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:01:23.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2611" for this suite.
May 14 00:01:45.330: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:01:45.503: INFO: namespace projected-2611 deletion completed in 22.188153151s

• [SLOW TEST:30.340 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:01:45.503: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:01:45.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1529" for this suite.
May 14 00:02:07.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:02:07.884: INFO: namespace kubelet-test-1529 deletion completed in 22.253891359s

• [SLOW TEST:22.381 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:02:07.885: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:02:07.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 version'
May 14 00:02:08.118: INFO: stderr: ""
May 14 00:02:08.118: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:02:08.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8082" for this suite.
May 14 00:02:14.141: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:02:14.340: INFO: namespace kubectl-8082 deletion completed in 6.216352929s

• [SLOW TEST:6.455 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:02:14.342: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
May 14 00:02:15.969: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:17.973: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:19.978: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:21.990: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:23.977: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:25.979: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693388935, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 00:02:29.421: INFO: Waited 1.435665665s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:02:29.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8713" for this suite.
May 14 00:02:36.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:02:36.207: INFO: namespace aggregator-8713 deletion completed in 6.308180766s

• [SLOW TEST:21.865 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:02:36.208: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:02:36.567: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"94af33d6-75db-11e9-9f2c-ceb99be2323d", Controller:(*bool)(0xc001d78f5a), BlockOwnerDeletion:(*bool)(0xc001d78f5b)}}
May 14 00:02:36.913: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"94a98ac0-75db-11e9-9f2c-ceb99be2323d", Controller:(*bool)(0xc001d7910a), BlockOwnerDeletion:(*bool)(0xc001d7910b)}}
May 14 00:02:36.975: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"94ab5ebe-75db-11e9-9f2c-ceb99be2323d", Controller:(*bool)(0xc002535042), BlockOwnerDeletion:(*bool)(0xc002535043)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:02:42.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8163" for this suite.
May 14 00:02:48.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:02:48.183: INFO: namespace gc-8163 deletion completed in 6.171182008s

• [SLOW TEST:11.976 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:02:48.184: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-2842
May 14 00:02:52.302: INFO: Started pod liveness-exec in namespace container-probe-2842
STEP: checking the pod's current state and verifying that restartCount is present
May 14 00:02:52.307: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:06:52.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2842" for this suite.
May 14 00:06:59.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:06:59.282: INFO: namespace container-probe-2842 deletion completed in 6.265757675s

• [SLOW TEST:251.098 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:06:59.283: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-31640266-75dc-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:06:59.359: INFO: Waiting up to 5m0s for pod "pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b" in namespace "secrets-2158" to be "success or failure"
May 14 00:06:59.364: INFO: Pod "pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493594ms
May 14 00:07:01.369: INFO: Pod "pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010225715s
May 14 00:07:03.374: INFO: Pod "pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015391188s
STEP: Saw pod success
May 14 00:07:03.375: INFO: Pod "pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:07:03.380: INFO: Trying to get logs from node conformance0 pod pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 00:07:03.414: INFO: Waiting for pod pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b to disappear
May 14 00:07:03.420: INFO: Pod pod-secrets-3164efbb-75dc-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:07:03.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2158" for this suite.
May 14 00:07:09.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:07:09.637: INFO: namespace secrets-2158 deletion completed in 6.211246661s

• [SLOW TEST:10.355 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:07:09.638: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1516
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1516 to expose endpoints map[]
May 14 00:07:09.728: INFO: successfully validated that service multi-endpoint-test in namespace services-1516 exposes endpoints map[] (13.392967ms elapsed)
STEP: Creating pod pod1 in namespace services-1516
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1516 to expose endpoints map[pod1:[100]]
May 14 00:07:13.818: INFO: successfully validated that service multi-endpoint-test in namespace services-1516 exposes endpoints map[pod1:[100]] (4.065275041s elapsed)
STEP: Creating pod pod2 in namespace services-1516
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1516 to expose endpoints map[pod1:[100] pod2:[101]]
May 14 00:07:16.882: INFO: successfully validated that service multi-endpoint-test in namespace services-1516 exposes endpoints map[pod1:[100] pod2:[101]] (3.057064792s elapsed)
STEP: Deleting pod pod1 in namespace services-1516
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1516 to expose endpoints map[pod2:[101]]
May 14 00:07:16.967: INFO: successfully validated that service multi-endpoint-test in namespace services-1516 exposes endpoints map[pod2:[101]] (61.601378ms elapsed)
STEP: Deleting pod pod2 in namespace services-1516
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1516 to expose endpoints map[]
May 14 00:07:17.052: INFO: successfully validated that service multi-endpoint-test in namespace services-1516 exposes endpoints map[] (20.71409ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:07:17.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1516" for this suite.
May 14 00:07:39.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:07:39.300: INFO: namespace services-1516 deletion completed in 22.191692671s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.662 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:07:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:07:39.353: INFO: Creating ReplicaSet my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b
May 14 00:07:39.372: INFO: Pod name my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b: Found 0 pods out of 1
May 14 00:07:44.378: INFO: Pod name my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b: Found 1 pods out of 1
May 14 00:07:44.378: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b" is running
May 14 00:07:44.382: INFO: Pod "my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b-pjrjq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:07:39 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:07:43 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:07:43 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:07:39 +0000 UTC Reason: Message:}])
May 14 00:07:44.382: INFO: Trying to dial the pod
May 14 00:07:49.401: INFO: Controller my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b: Got expected result from replica 1 [my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b-pjrjq]: "my-hostname-basic-493d8ec0-75dc-11e9-8961-767a82fad75b-pjrjq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:07:49.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-8819" for this suite.
May 14 00:07:55.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:07:55.596: INFO: namespace replicaset-8819 deletion completed in 6.189776665s

• [SLOW TEST:16.295 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:07:55.597: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 14 00:07:55.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-2950'
May 14 00:07:56.377: INFO: stderr: ""
May 14 00:07:56.377: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 00:07:57.383: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:07:57.384: INFO: Found 0 / 1
May 14 00:07:58.383: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:07:58.383: INFO: Found 0 / 1
May 14 00:07:59.383: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:07:59.383: INFO: Found 1 / 1
May 14 00:07:59.383: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
May 14 00:07:59.387: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:07:59.387: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 00:07:59.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 patch pod redis-master-657ck --namespace=kubectl-2950 -p {"metadata":{"annotations":{"x":"y"}}}'
May 14 00:07:59.544: INFO: stderr: ""
May 14 00:07:59.544: INFO: stdout: "pod/redis-master-657ck patched\n"
STEP: checking annotations
May 14 00:07:59.549: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:07:59.549: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:07:59.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2950" for this suite.
May 14 00:08:21.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:08:21.771: INFO: namespace kubectl-2950 deletion completed in 22.216716812s

• [SLOW TEST:26.174 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:08:21.772: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 14 00:08:21.830: INFO: PodSpec: initContainers in spec.initContainers
May 14 00:09:07.974: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-628f0b4b-75dc-11e9-8961-767a82fad75b", GenerateName:"", Namespace:"init-container-6382", SelfLink:"/api/v1/namespaces/init-container-6382/pods/pod-init-628f0b4b-75dc-11e9-8961-767a82fad75b", UID:"629030e6-75dc-11e9-9f2c-ceb99be2323d", ResourceVersion:"135273", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63693389301, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"830751320"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-vbtj5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0020f0ac0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vbtj5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vbtj5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-vbtj5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000aaa968), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"conformance0", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc00247cd80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000aaaa10)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000aaaa30)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000aaaa38), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000aaaa3c)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693389301, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693389301, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693389301, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693389301, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.10.1.212", PodIP:"10.244.0.25", StartTime:(*v1.Time)(0xc00172b340), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00261e4d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00261e540)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f269e2e65d7b840bc5cca4803555516fa2e57b08c3481d1094f65b1d24342de6"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00172b3c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00172b380), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:09:07.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6382" for this suite.
May 14 00:09:30.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:09:30.202: INFO: namespace init-container-6382 deletion completed in 22.210292992s

• [SLOW TEST:68.430 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:09:30.202: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
May 14 00:09:34.299: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-8b588398-75dc-11e9-8961-767a82fad75b,GenerateName:,Namespace:events-7036,SelfLink:/api/v1/namespaces/events-7036/pods/send-events-8b588398-75dc-11e9-8961-767a82fad75b,UID:8b598fa8-75dc-11e9-9f2c-ceb99be2323d,ResourceVersion:135305,Generation:0,CreationTimestamp:2019-05-14 00:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 259967994,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fqzqb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fqzqb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-fqzqb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d792d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d792f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:09:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:09:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:09:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:09:30 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.26,StartTime:2019-05-14 00:09:30 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-05-14 00:09:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://fde5ff5a202d2263b2944e459ee90adc426fe700d63cba23952db8f04d278185}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
May 14 00:09:36.304: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
May 14 00:09:38.309: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:09:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-7036" for this suite.
May 14 00:10:18.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:10:18.608: INFO: namespace events-7036 deletion completed in 40.259169143s

• [SLOW TEST:48.406 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:10:18.610: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 14 00:10:23.348: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a8410962-75dc-11e9-8961-767a82fad75b"
May 14 00:10:23.348: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a8410962-75dc-11e9-8961-767a82fad75b" in namespace "pods-7076" to be "terminated due to deadline exceeded"
May 14 00:10:23.354: INFO: Pod "pod-update-activedeadlineseconds-a8410962-75dc-11e9-8961-767a82fad75b": Phase="Running", Reason="", readiness=true. Elapsed: 6.566656ms
May 14 00:10:25.358: INFO: Pod "pod-update-activedeadlineseconds-a8410962-75dc-11e9-8961-767a82fad75b": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010577209s
May 14 00:10:25.359: INFO: Pod "pod-update-activedeadlineseconds-a8410962-75dc-11e9-8961-767a82fad75b" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:10:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7076" for this suite.
May 14 00:10:31.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:10:31.567: INFO: namespace pods-7076 deletion completed in 6.201893485s

• [SLOW TEST:12.957 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:10:31.569: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
May 14 00:10:31.654: INFO: Waiting up to 5m0s for pod "pod-afec546b-75dc-11e9-8961-767a82fad75b" in namespace "emptydir-6282" to be "success or failure"
May 14 00:10:31.742: INFO: Pod "pod-afec546b-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 87.826713ms
May 14 00:10:33.746: INFO: Pod "pod-afec546b-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.092085928s
May 14 00:10:35.751: INFO: Pod "pod-afec546b-75dc-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.097423776s
STEP: Saw pod success
May 14 00:10:35.751: INFO: Pod "pod-afec546b-75dc-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:10:35.757: INFO: Trying to get logs from node conformance0 pod pod-afec546b-75dc-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:10:35.790: INFO: Waiting for pod pod-afec546b-75dc-11e9-8961-767a82fad75b to disappear
May 14 00:10:35.801: INFO: Pod pod-afec546b-75dc-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:10:35.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6282" for this suite.
May 14 00:10:41.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:10:42.038: INFO: namespace emptydir-6282 deletion completed in 6.230796572s

• [SLOW TEST:10.470 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:10:42.039: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-b62ae6f0-75dc-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:10:42.143: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b" in namespace "projected-9531" to be "success or failure"
May 14 00:10:42.152: INFO: Pod "pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.070051ms
May 14 00:10:44.157: INFO: Pod "pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013775326s
May 14 00:10:46.164: INFO: Pod "pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020848207s
STEP: Saw pod success
May 14 00:10:46.164: INFO: Pod "pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:10:46.170: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:10:46.210: INFO: Waiting for pod pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b to disappear
May 14 00:10:46.219: INFO: Pod pod-projected-configmaps-b62bdb10-75dc-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:10:46.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9531" for this suite.
May 14 00:10:52.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:10:52.425: INFO: namespace projected-9531 deletion completed in 6.197856121s

• [SLOW TEST:10.386 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:10:52.426: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 14 00:10:52.496: INFO: Waiting up to 5m0s for pod "pod-bc5b404c-75dc-11e9-8961-767a82fad75b" in namespace "emptydir-3110" to be "success or failure"
May 14 00:10:52.524: INFO: Pod "pod-bc5b404c-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 27.482667ms
May 14 00:10:54.668: INFO: Pod "pod-bc5b404c-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.171770141s
May 14 00:10:56.674: INFO: Pod "pod-bc5b404c-75dc-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.17749015s
STEP: Saw pod success
May 14 00:10:56.674: INFO: Pod "pod-bc5b404c-75dc-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:10:56.689: INFO: Trying to get logs from node conformance0 pod pod-bc5b404c-75dc-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:10:56.738: INFO: Waiting for pod pod-bc5b404c-75dc-11e9-8961-767a82fad75b to disappear
May 14 00:10:56.743: INFO: Pod pod-bc5b404c-75dc-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:10:56.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3110" for this suite.
May 14 00:11:02.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:11:02.968: INFO: namespace emptydir-3110 deletion completed in 6.199799342s

• [SLOW TEST:10.543 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:11:02.969: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 14 00:11:11.168: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:11.174: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:13.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:13.178: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:15.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:15.180: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:17.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:17.178: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:19.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:19.179: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:21.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:21.179: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:23.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:23.179: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:25.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:25.178: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:27.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:27.179: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:29.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:29.179: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:31.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:31.180: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:33.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:33.180: INFO: Pod pod-with-prestop-exec-hook still exists
May 14 00:11:35.174: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
May 14 00:11:35.178: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:11:35.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5146" for this suite.
May 14 00:11:57.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:11:57.392: INFO: namespace container-lifecycle-hook-5146 deletion completed in 22.194704213s

• [SLOW TEST:54.423 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:11:57.393: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e314ce06-75dc-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:12:01.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8400" for this suite.
May 14 00:12:25.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:12:25.767: INFO: namespace configmap-8400 deletion completed in 24.210767298s

• [SLOW TEST:28.374 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:12:25.767: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 14 00:12:25.825: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:12:30.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4621" for this suite.
May 14 00:12:36.203: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:12:36.454: INFO: namespace init-container-4621 deletion completed in 6.270465095s

• [SLOW TEST:10.687 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:12:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-fa5cb0fb-75dc-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:12:36.532: INFO: Waiting up to 5m0s for pod "pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b" in namespace "secrets-9530" to be "success or failure"
May 14 00:12:36.539: INFO: Pod "pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.766245ms
May 14 00:12:38.549: INFO: Pod "pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017387285s
May 14 00:12:40.553: INFO: Pod "pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021373256s
STEP: Saw pod success
May 14 00:12:40.553: INFO: Pod "pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:12:40.558: INFO: Trying to get logs from node conformance0 pod pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 00:12:40.583: INFO: Waiting for pod pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b to disappear
May 14 00:12:40.594: INFO: Pod pod-secrets-fa5da14d-75dc-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:12:40.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9530" for this suite.
May 14 00:12:46.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:12:46.918: INFO: namespace secrets-9530 deletion completed in 6.298714395s

• [SLOW TEST:10.462 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:12:46.919: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-009c1b44-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:12:47.031: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b" in namespace "projected-7717" to be "success or failure"
May 14 00:12:47.083: INFO: Pod "pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 51.250431ms
May 14 00:12:49.089: INFO: Pod "pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057408999s
May 14 00:12:51.095: INFO: Pod "pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063573375s
STEP: Saw pod success
May 14 00:12:51.095: INFO: Pod "pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:12:51.102: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:12:51.132: INFO: Waiting for pod pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:12:51.143: INFO: Pod pod-projected-configmaps-009dd428-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:12:51.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7717" for this suite.
May 14 00:12:57.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:12:57.313: INFO: namespace projected-7717 deletion completed in 6.165913274s

• [SLOW TEST:10.395 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:12:57.314: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-06d3ea1a-75dd-11e9-8961-767a82fad75b
STEP: Creating configMap with name cm-test-opt-upd-06d3eab4-75dd-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-06d3ea1a-75dd-11e9-8961-767a82fad75b
STEP: Updating configmap cm-test-opt-upd-06d3eab4-75dd-11e9-8961-767a82fad75b
STEP: Creating configMap with name cm-test-opt-create-06d3eb02-75dd-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:13:05.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7441" for this suite.
May 14 00:13:27.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:13:27.778: INFO: namespace projected-7441 deletion completed in 22.1847944s

• [SLOW TEST:30.465 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:13:27.779: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
May 14 00:13:32.392: INFO: Successfully updated pod "pod-update-18f35eb3-75dd-11e9-8961-767a82fad75b"
STEP: verifying the updated pod is in kubernetes
May 14 00:13:32.403: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:13:32.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-380" for this suite.
May 14 00:13:52.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:13:52.587: INFO: namespace pods-380 deletion completed in 20.179484952s

• [SLOW TEST:24.808 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:13:52.588: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-27c18baf-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:13:52.694: INFO: Waiting up to 5m0s for pod "pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b" in namespace "configmap-7247" to be "success or failure"
May 14 00:13:52.698: INFO: Pod "pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.725655ms
May 14 00:13:54.702: INFO: Pod "pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007990712s
May 14 00:13:56.706: INFO: Pod "pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012651147s
STEP: Saw pod success
May 14 00:13:56.707: INFO: Pod "pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:13:56.711: INFO: Trying to get logs from node conformance0 pod pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:13:56.749: INFO: Waiting for pod pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:13:56.757: INFO: Pod pod-configmaps-27c2b66e-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:13:56.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7247" for this suite.
May 14 00:14:02.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:14:02.978: INFO: namespace configmap-7247 deletion completed in 6.212259915s

• [SLOW TEST:10.390 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:14:02.979: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
May 14 00:14:11.123: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:11.123: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:11.467: INFO: Exec stderr: ""
May 14 00:14:11.467: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:11.467: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:11.864: INFO: Exec stderr: ""
May 14 00:14:11.864: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:11.864: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:12.204: INFO: Exec stderr: ""
May 14 00:14:12.204: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:12.204: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:12.567: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
May 14 00:14:12.567: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:12.567: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:12.972: INFO: Exec stderr: ""
May 14 00:14:12.972: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:12.972: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:13.366: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
May 14 00:14:13.366: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:13.366: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:13.740: INFO: Exec stderr: ""
May 14 00:14:13.740: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:14.158: INFO: Exec stderr: ""
May 14 00:14:14.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:14.159: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:14.497: INFO: Exec stderr: ""
May 14 00:14:14.498: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-912 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:14:14.498: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:14:14.891: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:14:14.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-912" for this suite.
May 14 00:15:08.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:15:09.097: INFO: namespace e2e-kubelet-etc-hosts-912 deletion completed in 54.195090779s

• [SLOW TEST:66.118 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:15:09.102: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:15:09.211: INFO: Creating deployment "nginx-deployment"
May 14 00:15:09.220: INFO: Waiting for observed generation 1
May 14 00:15:11.229: INFO: Waiting for all required pods to come up
May 14 00:15:11.236: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
May 14 00:15:23.246: INFO: Waiting for deployment "nginx-deployment" to complete
May 14 00:15:23.257: INFO: Updating deployment "nginx-deployment" with a non-existent image
May 14 00:15:23.271: INFO: Updating deployment nginx-deployment
May 14 00:15:23.271: INFO: Waiting for observed generation 2
May 14 00:15:25.312: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
May 14 00:15:25.321: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
May 14 00:15:25.329: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 14 00:15:25.341: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
May 14 00:15:25.342: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
May 14 00:15:25.345: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
May 14 00:15:25.361: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
May 14 00:15:25.361: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
May 14 00:15:25.378: INFO: Updating deployment nginx-deployment
May 14 00:15:25.379: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
May 14 00:15:25.393: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
May 14 00:15:25.400: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 14 00:15:27.654: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-6171,SelfLink:/apis/apps/v1/namespaces/deployment-6171/deployments/nginx-deployment,UID:5560e1a1-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136062,Generation:3,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:25,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-05-14 00:15:24 +0000 UTC 2019-05-14 00:15:09 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-05-14 00:15:25 +0000 UTC 2019-05-14 00:15:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

May 14 00:15:27.734: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-6171,SelfLink:/apis/apps/v1/namespaces/deployment-6171/replicasets/nginx-deployment-5f9595f595,UID:5dc210ff-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136060,Generation:3,CreationTimestamp:2019-05-14 00:15:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5560e1a1-75dd-11e9-9f2c-ceb99be2323d 0xc0021c5b97 0xc0021c5b98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 00:15:27.734: INFO: All old ReplicaSets of Deployment "nginx-deployment":
May 14 00:15:27.734: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-6171,SelfLink:/apis/apps/v1/namespaces/deployment-6171/replicasets/nginx-deployment-6f478d8d8,UID:5562b2a1-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136058,Generation:3,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 5560e1a1-75dd-11e9-9f2c-ceb99be2323d 0xc0021c5c67 0xc0021c5c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
May 14 00:15:27.873: INFO: Pod "nginx-deployment-5f9595f595-44r5m" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-44r5m,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-44r5m,UID:5dc67ac5-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135962,Generation:0,CreationTimestamp:2019-05-14 00:15:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efa557 0xc002efa558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efa5d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efa5f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.873: INFO: Pod "nginx-deployment-5f9595f595-5t5b8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-5t5b8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-5t5b8,UID:5f7a7493-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136050,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efa6c0 0xc002efa6c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efa740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efa760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.873: INFO: Pod "nginx-deployment-5f9595f595-cq4tk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cq4tk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-cq4tk,UID:5e3f47c8-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136001,Generation:0,CreationTimestamp:2019-05-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efa7e0 0xc002efa7e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efa860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efa880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.873: INFO: Pod "nginx-deployment-5f9595f595-g25rj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-g25rj,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-g25rj,UID:5fe4df36-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136066,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efa950 0xc002efa951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efa9d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efa9f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.874: INFO: Pod "nginx-deployment-5f9595f595-h9nbm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-h9nbm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-h9nbm,UID:5dcbaf4f-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135999,Generation:0,CreationTimestamp:2019-05-14 00:15:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efaa70 0xc002efaa71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efaaf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efab10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.874: INFO: Pod "nginx-deployment-5f9595f595-jx4zz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-jx4zz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-jx4zz,UID:5f411027-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136035,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efabf0 0xc002efabf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efac70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efac90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.874: INFO: Pod "nginx-deployment-5f9595f595-k4zxv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-k4zxv,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-k4zxv,UID:5f79da1e-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136049,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efad10 0xc002efad11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efad90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efadb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.874: INFO: Pod "nginx-deployment-5f9595f595-kpgz5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kpgz5,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-kpgz5,UID:5f114bcd-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136061,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efae30 0xc002efae31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efaeb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efaed0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.875: INFO: Pod "nginx-deployment-5f9595f595-nmqrz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-nmqrz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-nmqrz,UID:5f6ef237-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136048,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efafa0 0xc002efafa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb020} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb040}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.875: INFO: Pod "nginx-deployment-5f9595f595-qptrb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-qptrb,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-qptrb,UID:5f3b5c17-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136030,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efb0d0 0xc002efb0d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.875: INFO: Pod "nginx-deployment-5f9595f595-tlqkt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tlqkt,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-tlqkt,UID:5dcb0ae5-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135976,Generation:0,CreationTimestamp:2019-05-14 00:15:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efb1f0 0xc002efb1f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:23 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.875: INFO: Pod "nginx-deployment-5f9595f595-w66jm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-w66jm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-w66jm,UID:5e4e8af5-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136002,Generation:0,CreationTimestamp:2019-05-14 00:15:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efb360 0xc002efb361}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:24 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.876: INFO: Pod "nginx-deployment-5f9595f595-zpn8v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zpn8v,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-5f9595f595-zpn8v,UID:5f7a8ee2-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136047,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 5dc210ff-75dd-11e9-9f2c-ceb99be2323d 0xc002efb4d0 0xc002efb4d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb550} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.877: INFO: Pod "nginx-deployment-6f478d8d8-4jp9t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-4jp9t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-4jp9t,UID:5565843c-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135930,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efb5f0 0xc002efb5f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb660} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb680}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.41,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:15 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://e11bf24d6f64c6401c7ae36d9d4c0eb191854713d21b51b965c372c7e922923a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.877: INFO: Pod "nginx-deployment-6f478d8d8-6xdrf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-6xdrf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-6xdrf,UID:557787b8-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135925,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efb750 0xc002efb751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb7c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb7e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.45,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://46682b9901dbf48b098247887965a60e2c196ba802417decae573c80af36d615}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.877: INFO: Pod "nginx-deployment-6f478d8d8-7qr7l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7qr7l,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-7qr7l,UID:5f078bc8-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136026,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efb8b0 0xc002efb8b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efb920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efb940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:,StartTime:2019-05-14 00:15:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.877: INFO: Pod "nginx-deployment-6f478d8d8-7thsm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7thsm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-7thsm,UID:5f564177-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136046,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efba00 0xc002efba01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efba70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efba90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.877: INFO: Pod "nginx-deployment-6f478d8d8-95dhv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-95dhv,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-95dhv,UID:5fa3f01c-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136054,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efbb10 0xc002efbb11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efbb80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efbba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.878: INFO: Pod "nginx-deployment-6f478d8d8-bmnbc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bmnbc,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-bmnbc,UID:55765d3f-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135934,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efbc30 0xc002efbc31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efbca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efbcc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.46,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://cecf0b6151c5cd06819a447a9895600bf631c6bcbf81618e7fe0dba27cfce5fc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.878: INFO: Pod "nginx-deployment-6f478d8d8-bn78g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bn78g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-bn78g,UID:5f138601-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136018,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efbd90 0xc002efbd91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efbe10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efbe30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.878: INFO: Pod "nginx-deployment-6f478d8d8-ddsgl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ddsgl,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-ddsgl,UID:558852e4-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135919,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc002efbeb0 0xc002efbeb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002efbf30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002efbf50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.47,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://32baa6c9bdb07dca2b041a471b88fb223be6feb4fb5643208370b3b283fcc69b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.878: INFO: Pod "nginx-deployment-6f478d8d8-fcmqm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fcmqm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-fcmqm,UID:5578d928-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135928,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac020 0xc0024ac021}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac0b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.44,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://f2a81e9cd27c934bd1a41e58020d660d86a2922de7acde3febe90d39dc5870cf}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.879: INFO: Pod "nginx-deployment-6f478d8d8-fk8vs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fk8vs,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-fk8vs,UID:5f527bf0-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136037,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac180 0xc0024ac181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac1f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.879: INFO: Pod "nginx-deployment-6f478d8d8-j5pzb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-j5pzb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-j5pzb,UID:5f440a92-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136031,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac290 0xc0024ac291}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.879: INFO: Pod "nginx-deployment-6f478d8d8-l7nwk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-l7nwk,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-l7nwk,UID:5f5549c8-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136040,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac3a0 0xc0024ac3a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac410} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac430}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.879: INFO: Pod "nginx-deployment-6f478d8d8-lg9d4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-lg9d4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-lg9d4,UID:5f167154-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136019,Generation:0,CreationTimestamp:2019-05-14 00:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac4b0 0xc0024ac4b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:25 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.880: INFO: Pod "nginx-deployment-6f478d8d8-pc752" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-pc752,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-pc752,UID:558fe802-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135940,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac5c0 0xc0024ac5c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac650}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.49,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://8a7c04aafd4a132eff49ea0359870ccc7a569ffe5de8f46d749597740af4f997}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.880: INFO: Pod "nginx-deployment-6f478d8d8-rgnr5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rgnr5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-rgnr5,UID:558f7e9e-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135922,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac720 0xc0024ac721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac7b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:19 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.50,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:19 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://aa3a74ad24e16dc4664cc773780e21520984dae2725d47b1934d6820c768b253}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.880: INFO: Pod "nginx-deployment-6f478d8d8-rl6h4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rl6h4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-rl6h4,UID:5569ff12-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:135943,Generation:0,CreationTimestamp:2019-05-14 00:15:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac880 0xc0024ac881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ac8f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ac910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:09 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.43,StartTime:2019-05-14 00:15:09 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-05-14 00:15:18 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632 docker://6774d53d3a7e4cc701aac43f9d0cb032186013331b2a90bf07d90f466cbdc6b6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.880: INFO: Pod "nginx-deployment-6f478d8d8-rnklb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rnklb,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-rnklb,UID:5fabed71-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136065,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024ac9e0 0xc0024ac9e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024aca50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024aca70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.881: INFO: Pod "nginx-deployment-6f478d8d8-t4w97" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-t4w97,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-t4w97,UID:5fad1090-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136064,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024acaf0 0xc0024acaf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024acb60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024acb80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.881: INFO: Pod "nginx-deployment-6f478d8d8-z565g" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-z565g,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-z565g,UID:5face884-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136068,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024acc00 0xc0024acc01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024acc70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024acc90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:27 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
May 14 00:15:27.881: INFO: Pod "nginx-deployment-6f478d8d8-zjz2t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zjz2t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-6171,SelfLink:/api/v1/namespaces/deployment-6171/pods/nginx-deployment-6f478d8d8-zjz2t,UID:5faeb7b8-75dd-11e9-9f2c-ceb99be2323d,ResourceVersion:136052,Generation:0,CreationTimestamp:2019-05-14 00:15:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 5562b2a1-75dd-11e9-9f2c-ceb99be2323d 0xc0024acd10 0xc0024acd11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-jp5nv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-jp5nv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-jp5nv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024acd80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024acda0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:15:26 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:15:27.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6171" for this suite.
May 14 00:15:40.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:15:40.480: INFO: namespace deployment-6171 deletion completed in 12.486893527s

• [SLOW TEST:31.378 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:15:40.481: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-681a449b-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:15:40.672: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b" in namespace "projected-3265" to be "success or failure"
May 14 00:15:40.688: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.234593ms
May 14 00:15:42.696: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02393443s
May 14 00:15:44.708: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.035818716s
May 14 00:15:46.716: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.043785296s
May 14 00:15:48.721: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048754853s
May 14 00:15:50.727: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.055367192s
May 14 00:15:52.732: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.059876149s
STEP: Saw pod success
May 14 00:15:52.732: INFO: Pod "pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:15:52.737: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 00:15:52.767: INFO: Waiting for pod pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:15:52.781: INFO: Pod pod-projected-secrets-681ccc10-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:15:52.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3265" for this suite.
May 14 00:15:58.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:15:58.976: INFO: namespace projected-3265 deletion completed in 6.187388379s

• [SLOW TEST:18.496 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:15:58.977: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-7314ddc9-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:15:59.071: INFO: Waiting up to 5m0s for pod "pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b" in namespace "secrets-6611" to be "success or failure"
May 14 00:15:59.077: INFO: Pod "pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.964018ms
May 14 00:16:01.082: INFO: Pod "pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011376094s
May 14 00:16:03.088: INFO: Pod "pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01714004s
STEP: Saw pod success
May 14 00:16:03.088: INFO: Pod "pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:16:03.094: INFO: Trying to get logs from node conformance0 pod pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 00:16:03.122: INFO: Waiting for pod pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:16:03.133: INFO: Pod pod-secrets-7316372a-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:03.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6611" for this suite.
May 14 00:16:09.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:16:09.361: INFO: namespace secrets-6611 deletion completed in 6.223673929s

• [SLOW TEST:10.384 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:16:09.363: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-795a2367-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:16:09.605: INFO: Waiting up to 5m0s for pod "pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b" in namespace "configmap-7006" to be "success or failure"
May 14 00:16:09.615: INFO: Pod "pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.802936ms
May 14 00:16:11.621: INFO: Pod "pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015803269s
May 14 00:16:13.656: INFO: Pod "pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05095221s
STEP: Saw pod success
May 14 00:16:13.656: INFO: Pod "pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:16:13.660: INFO: Trying to get logs from node conformance0 pod pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:16:13.698: INFO: Waiting for pod pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:16:13.704: INFO: Pod pod-configmaps-795c14e9-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:13.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7006" for this suite.
May 14 00:16:19.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:16:19.899: INFO: namespace configmap-7006 deletion completed in 6.188000897s

• [SLOW TEST:10.536 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:16:19.900: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-7f93d69a-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:16:20.061: INFO: Waiting up to 5m0s for pod "pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b" in namespace "secrets-803" to be "success or failure"
May 14 00:16:20.069: INFO: Pod "pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.210238ms
May 14 00:16:22.077: INFO: Pod "pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015470619s
May 14 00:16:24.081: INFO: Pod "pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019828451s
STEP: Saw pod success
May 14 00:16:24.081: INFO: Pod "pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:16:24.085: INFO: Trying to get logs from node conformance0 pod pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 00:16:24.115: INFO: Waiting for pod pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:16:24.122: INFO: Pod pod-secrets-7f9645ab-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:24.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-803" for this suite.
May 14 00:16:30.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:16:30.378: INFO: namespace secrets-803 deletion completed in 6.248945942s

• [SLOW TEST:10.478 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:16:30.379: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-85cbae50-75dd-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:16:30.479: INFO: Waiting up to 5m0s for pod "pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b" in namespace "configmap-5595" to be "success or failure"
May 14 00:16:30.488: INFO: Pod "pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.434971ms
May 14 00:16:32.493: INFO: Pod "pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013793738s
May 14 00:16:34.498: INFO: Pod "pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019300785s
STEP: Saw pod success
May 14 00:16:34.498: INFO: Pod "pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:16:34.504: INFO: Trying to get logs from node conformance0 pod pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:16:34.557: INFO: Waiting for pod pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:16:34.561: INFO: Pod pod-configmaps-85cef907-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:34.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5595" for this suite.
May 14 00:16:40.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:16:40.780: INFO: namespace configmap-5595 deletion completed in 6.213500973s

• [SLOW TEST:10.401 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:16:40.780: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
May 14 00:16:40.878: INFO: Waiting up to 5m0s for pod "client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b" in namespace "containers-312" to be "success or failure"
May 14 00:16:40.886: INFO: Pod "client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.553114ms
May 14 00:16:42.894: INFO: Pod "client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015917684s
May 14 00:16:44.919: INFO: Pod "client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040895582s
STEP: Saw pod success
May 14 00:16:44.919: INFO: Pod "client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:16:44.925: INFO: Trying to get logs from node conformance0 pod client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:16:44.966: INFO: Waiting for pod client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b to disappear
May 14 00:16:44.972: INFO: Pod client-containers-8c00de4d-75dd-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:44.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-312" for this suite.
May 14 00:16:51.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:16:51.206: INFO: namespace containers-312 deletion completed in 6.221187467s

• [SLOW TEST:10.426 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:16:51.207: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
May 14 00:16:56.348: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:16:57.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2685" for this suite.
May 14 00:17:21.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:17:21.619: INFO: namespace replicaset-2685 deletion completed in 24.212730042s

• [SLOW TEST:30.412 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:17:21.620: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:17:25.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4784" for this suite.
May 14 00:18:19.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:18:19.942: INFO: namespace kubelet-test-4784 deletion completed in 54.196795703s

• [SLOW TEST:58.322 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:18:19.943: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
May 14 00:18:20.029: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-353874590 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:18:20.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7969" for this suite.
May 14 00:18:26.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:18:26.356: INFO: namespace kubectl-7969 deletion completed in 6.198960958s

• [SLOW TEST:6.413 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:18:26.363: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
May 14 00:18:26.763: INFO: Pod name wrapped-volume-race-cb1d6274-75dd-11e9-8961-767a82fad75b: Found 0 pods out of 5
May 14 00:18:31.772: INFO: Pod name wrapped-volume-race-cb1d6274-75dd-11e9-8961-767a82fad75b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-cb1d6274-75dd-11e9-8961-767a82fad75b in namespace emptydir-wrapper-9015, will wait for the garbage collector to delete the pods
May 14 00:18:41.998: INFO: Deleting ReplicationController wrapped-volume-race-cb1d6274-75dd-11e9-8961-767a82fad75b took: 29.241517ms
May 14 00:18:42.399: INFO: Terminating ReplicationController wrapped-volume-race-cb1d6274-75dd-11e9-8961-767a82fad75b pods took: 400.359098ms
STEP: Creating RC which spawns configmap-volume pods
May 14 00:19:24.224: INFO: Pod name wrapped-volume-race-ed5c7af3-75dd-11e9-8961-767a82fad75b: Found 0 pods out of 5
May 14 00:19:29.234: INFO: Pod name wrapped-volume-race-ed5c7af3-75dd-11e9-8961-767a82fad75b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ed5c7af3-75dd-11e9-8961-767a82fad75b in namespace emptydir-wrapper-9015, will wait for the garbage collector to delete the pods
May 14 00:19:43.381: INFO: Deleting ReplicationController wrapped-volume-race-ed5c7af3-75dd-11e9-8961-767a82fad75b took: 25.290942ms
May 14 00:19:43.781: INFO: Terminating ReplicationController wrapped-volume-race-ed5c7af3-75dd-11e9-8961-767a82fad75b pods took: 400.284141ms
STEP: Creating RC which spawns configmap-volume pods
May 14 00:20:25.115: INFO: Pod name wrapped-volume-race-11a662f5-75de-11e9-8961-767a82fad75b: Found 0 pods out of 5
May 14 00:20:30.124: INFO: Pod name wrapped-volume-race-11a662f5-75de-11e9-8961-767a82fad75b: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-11a662f5-75de-11e9-8961-767a82fad75b in namespace emptydir-wrapper-9015, will wait for the garbage collector to delete the pods
May 14 00:20:44.246: INFO: Deleting ReplicationController wrapped-volume-race-11a662f5-75de-11e9-8961-767a82fad75b took: 11.969965ms
May 14 00:20:44.646: INFO: Terminating ReplicationController wrapped-volume-race-11a662f5-75de-11e9-8961-767a82fad75b pods took: 400.445645ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:21:25.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9015" for this suite.
May 14 00:21:33.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:21:33.807: INFO: namespace emptydir-wrapper-9015 deletion completed in 8.227097648s

• [SLOW TEST:187.445 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:21:33.808: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:21:33.903: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b" in namespace "projected-66" to be "success or failure"
May 14 00:21:33.915: INFO: Pod "downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.862188ms
May 14 00:21:35.920: INFO: Pod "downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016803866s
May 14 00:21:37.925: INFO: Pod "downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021828538s
STEP: Saw pod success
May 14 00:21:37.926: INFO: Pod "downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:21:37.931: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:21:38.051: INFO: Waiting for pod downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b to disappear
May 14 00:21:38.082: INFO: Pod downwardapi-volume-3aa9b007-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:21:38.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-66" for this suite.
May 14 00:21:44.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:21:44.309: INFO: namespace projected-66 deletion completed in 6.192259297s

• [SLOW TEST:10.502 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:21:44.310: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
May 14 00:21:44.368: INFO: Waiting up to 5m0s for pod "client-containers-40e73276-75de-11e9-8961-767a82fad75b" in namespace "containers-3821" to be "success or failure"
May 14 00:21:44.375: INFO: Pod "client-containers-40e73276-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.284547ms
May 14 00:21:46.381: INFO: Pod "client-containers-40e73276-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013054173s
May 14 00:21:48.386: INFO: Pod "client-containers-40e73276-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017946467s
STEP: Saw pod success
May 14 00:21:48.386: INFO: Pod "client-containers-40e73276-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:21:48.391: INFO: Trying to get logs from node conformance0 pod client-containers-40e73276-75de-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:21:48.428: INFO: Waiting for pod client-containers-40e73276-75de-11e9-8961-767a82fad75b to disappear
May 14 00:21:48.431: INFO: Pod client-containers-40e73276-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:21:48.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3821" for this suite.
May 14 00:21:54.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:21:54.604: INFO: namespace containers-3821 deletion completed in 6.166566899s

• [SLOW TEST:10.294 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:21:54.604: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
May 14 00:21:54.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-8020'
May 14 00:21:55.300: INFO: stderr: ""
May 14 00:21:55.300: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
May 14 00:21:56.306: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:21:56.306: INFO: Found 0 / 1
May 14 00:21:57.305: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:21:57.305: INFO: Found 0 / 1
May 14 00:21:58.306: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:21:58.306: INFO: Found 0 / 1
May 14 00:21:59.305: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:21:59.305: INFO: Found 1 / 1
May 14 00:21:59.305: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 14 00:21:59.309: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:21:59.309: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
May 14 00:21:59.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 logs redis-master-cvcvf redis-master --namespace=kubectl-8020'
May 14 00:21:59.490: INFO: stderr: ""
May 14 00:21:59.490: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 May 00:21:58.119 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 May 00:21:58.119 # Server started, Redis version 3.2.12\n1:M 14 May 00:21:58.119 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 May 00:21:58.119 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
May 14 00:21:59.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 log redis-master-cvcvf redis-master --namespace=kubectl-8020 --tail=1'
May 14 00:21:59.671: INFO: stderr: ""
May 14 00:21:59.671: INFO: stdout: "1:M 14 May 00:21:58.119 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
May 14 00:21:59.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 log redis-master-cvcvf redis-master --namespace=kubectl-8020 --limit-bytes=1'
May 14 00:21:59.837: INFO: stderr: ""
May 14 00:21:59.837: INFO: stdout: " "
STEP: exposing timestamps
May 14 00:21:59.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 log redis-master-cvcvf redis-master --namespace=kubectl-8020 --tail=1 --timestamps'
May 14 00:22:00.027: INFO: stderr: ""
May 14 00:22:00.027: INFO: stdout: "2019-05-14T00:21:58.121076818Z 1:M 14 May 00:21:58.119 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
May 14 00:22:02.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 log redis-master-cvcvf redis-master --namespace=kubectl-8020 --since=1s'
May 14 00:22:02.691: INFO: stderr: ""
May 14 00:22:02.691: INFO: stdout: ""
May 14 00:22:02.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 log redis-master-cvcvf redis-master --namespace=kubectl-8020 --since=24h'
May 14 00:22:02.845: INFO: stderr: ""
May 14 00:22:02.845: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 May 00:21:58.119 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 May 00:21:58.119 # Server started, Redis version 3.2.12\n1:M 14 May 00:21:58.119 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 May 00:21:58.119 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
May 14 00:22:02.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-8020'
May 14 00:22:03.004: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:22:03.004: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
May 14 00:22:03.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get rc,svc -l name=nginx --no-headers --namespace=kubectl-8020'
May 14 00:22:03.201: INFO: stderr: "No resources found.\n"
May 14 00:22:03.201: INFO: stdout: ""
May 14 00:22:03.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -l name=nginx --namespace=kubectl-8020 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 00:22:03.400: INFO: stderr: ""
May 14 00:22:03.400: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:22:03.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8020" for this suite.
May 14 00:22:25.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:22:25.612: INFO: namespace kubectl-8020 deletion completed in 22.205400337s

• [SLOW TEST:31.008 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:22:25.616: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:22:25.695: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
May 14 00:22:25.732: INFO: Number of nodes with available pods: 0
May 14 00:22:25.732: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
May 14 00:22:25.785: INFO: Number of nodes with available pods: 0
May 14 00:22:25.785: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:26.789: INFO: Number of nodes with available pods: 0
May 14 00:22:26.790: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:27.790: INFO: Number of nodes with available pods: 0
May 14 00:22:27.790: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:28.790: INFO: Number of nodes with available pods: 1
May 14 00:22:28.790: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
May 14 00:22:28.835: INFO: Number of nodes with available pods: 1
May 14 00:22:28.835: INFO: Number of running nodes: 0, number of available pods: 1
May 14 00:22:29.840: INFO: Number of nodes with available pods: 0
May 14 00:22:29.840: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
May 14 00:22:29.860: INFO: Number of nodes with available pods: 0
May 14 00:22:29.860: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:30.866: INFO: Number of nodes with available pods: 0
May 14 00:22:30.866: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:31.868: INFO: Number of nodes with available pods: 0
May 14 00:22:31.868: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:32.865: INFO: Number of nodes with available pods: 0
May 14 00:22:32.865: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:33.865: INFO: Number of nodes with available pods: 0
May 14 00:22:33.865: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:34.866: INFO: Number of nodes with available pods: 0
May 14 00:22:34.866: INFO: Node conformance0 is running more than one daemon pod
May 14 00:22:35.865: INFO: Number of nodes with available pods: 1
May 14 00:22:35.865: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3224, will wait for the garbage collector to delete the pods
May 14 00:22:35.939: INFO: Deleting DaemonSet.extensions daemon-set took: 11.495953ms
May 14 00:22:36.239: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.41223ms
May 14 00:22:44.045: INFO: Number of nodes with available pods: 0
May 14 00:22:44.045: INFO: Number of running nodes: 0, number of available pods: 0
May 14 00:22:44.053: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3224/daemonsets","resourceVersion":"137568"},"items":null}

May 14 00:22:44.058: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3224/pods","resourceVersion":"137568"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:22:44.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3224" for this suite.
May 14 00:22:50.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:22:50.276: INFO: namespace daemonsets-3224 deletion completed in 6.187311516s

• [SLOW TEST:24.660 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:22:50.277: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 14 00:22:54.967: INFO: Successfully updated pod "labelsupdate6841768f-75de-11e9-8961-767a82fad75b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:22:56.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-609" for this suite.
May 14 00:23:21.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:23:21.195: INFO: namespace projected-609 deletion completed in 24.185147327s

• [SLOW TEST:30.918 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:23:21.196: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:23:21.271: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b" in namespace "downward-api-8626" to be "success or failure"
May 14 00:23:21.278: INFO: Pod "downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.34413ms
May 14 00:23:23.284: INFO: Pod "downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013144546s
May 14 00:23:25.292: INFO: Pod "downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020676221s
STEP: Saw pod success
May 14 00:23:25.292: INFO: Pod "downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:23:25.306: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:23:25.351: INFO: Waiting for pod downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b to disappear
May 14 00:23:25.378: INFO: Pod downwardapi-volume-7aa81e2c-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:23:25.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8626" for this suite.
May 14 00:23:31.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:23:31.599: INFO: namespace downward-api-8626 deletion completed in 6.216664834s

• [SLOW TEST:10.403 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:23:31.600: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:23:37.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6652" for this suite.
May 14 00:23:43.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:23:44.139: INFO: namespace namespaces-6652 deletion completed in 6.230743445s
STEP: Destroying namespace "nsdeletetest-1414" for this suite.
May 14 00:23:44.143: INFO: Namespace nsdeletetest-1414 was already deleted
STEP: Destroying namespace "nsdeletetest-1865" for this suite.
May 14 00:23:50.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:23:50.328: INFO: namespace nsdeletetest-1865 deletion completed in 6.184870107s

• [SLOW TEST:18.729 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:23:50.329: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
May 14 00:23:50.395: INFO: namespace kubectl-870
May 14 00:23:50.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-870'
May 14 00:23:50.901: INFO: stderr: ""
May 14 00:23:50.901: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 00:23:51.910: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:23:51.910: INFO: Found 0 / 1
May 14 00:23:52.906: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:23:52.907: INFO: Found 0 / 1
May 14 00:23:53.910: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:23:53.910: INFO: Found 1 / 1
May 14 00:23:53.910: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 14 00:23:53.916: INFO: Selector matched 1 pods for map[app:redis]
May 14 00:23:53.916: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 00:23:53.916: INFO: wait on redis-master startup in kubectl-870 
May 14 00:23:53.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 logs redis-master-jl4zg redis-master --namespace=kubectl-870'
May 14 00:23:54.169: INFO: stderr: ""
May 14 00:23:54.169: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 May 00:23:52.650 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 May 00:23:52.650 # Server started, Redis version 3.2.12\n1:M 14 May 00:23:52.650 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 May 00:23:52.650 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
May 14 00:23:54.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-870'
May 14 00:23:54.356: INFO: stderr: ""
May 14 00:23:54.356: INFO: stdout: "service/rm2 exposed\n"
May 14 00:23:54.364: INFO: Service rm2 in namespace kubectl-870 found.
STEP: exposing service
May 14 00:23:56.374: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-870'
May 14 00:23:56.563: INFO: stderr: ""
May 14 00:23:56.563: INFO: stdout: "service/rm3 exposed\n"
May 14 00:23:56.574: INFO: Service rm3 in namespace kubectl-870 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:23:58.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-870" for this suite.
May 14 00:24:20.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:24:20.767: INFO: namespace kubectl-870 deletion completed in 22.179896058s

• [SLOW TEST:30.438 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:24:20.768: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-9e2a166d-75de-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:24:20.848: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b" in namespace "projected-6726" to be "success or failure"
May 14 00:24:20.869: INFO: Pod "pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.242831ms
May 14 00:24:22.875: INFO: Pod "pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027006047s
May 14 00:24:24.881: INFO: Pod "pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032676554s
STEP: Saw pod success
May 14 00:24:24.881: INFO: Pod "pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:24:24.887: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:24:24.936: INFO: Waiting for pod pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b to disappear
May 14 00:24:24.945: INFO: Pod pod-projected-configmaps-9e2b3de2-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:24:24.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6726" for this suite.
May 14 00:24:30.964: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:24:31.257: INFO: namespace projected-6726 deletion completed in 6.307888276s

• [SLOW TEST:10.489 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:24:31.258: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:24:31.394: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b" in namespace "downward-api-3682" to be "success or failure"
May 14 00:24:31.447: INFO: Pod "downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 52.864556ms
May 14 00:24:33.453: INFO: Pod "downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058564906s
May 14 00:24:35.458: INFO: Pod "downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.063625058s
STEP: Saw pod success
May 14 00:24:35.458: INFO: Pod "downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:24:35.463: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:24:35.491: INFO: Waiting for pod downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b to disappear
May 14 00:24:35.514: INFO: Pod downwardapi-volume-a473fc45-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:24:35.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3682" for this suite.
May 14 00:24:41.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:24:41.751: INFO: namespace downward-api-3682 deletion completed in 6.231931575s

• [SLOW TEST:10.493 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:24:41.751: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:24:41.852: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b" in namespace "downward-api-9507" to be "success or failure"
May 14 00:24:41.865: INFO: Pod "downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.150506ms
May 14 00:24:43.872: INFO: Pod "downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019948241s
May 14 00:24:45.877: INFO: Pod "downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024910859s
STEP: Saw pod success
May 14 00:24:45.877: INFO: Pod "downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:24:45.881: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:24:45.914: INFO: Waiting for pod downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b to disappear
May 14 00:24:45.945: INFO: Pod downwardapi-volume-aaadcc94-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:24:45.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9507" for this suite.
May 14 00:24:51.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:24:52.209: INFO: namespace downward-api-9507 deletion completed in 6.258264607s

• [SLOW TEST:10.458 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:24:52.210: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-b0e7e4c7-75de-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:24:52.295: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b" in namespace "projected-8892" to be "success or failure"
May 14 00:24:52.307: INFO: Pod "pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.414516ms
May 14 00:24:54.312: INFO: Pod "pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016205054s
May 14 00:24:56.318: INFO: Pod "pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02208468s
STEP: Saw pod success
May 14 00:24:56.318: INFO: Pod "pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:24:56.323: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 00:24:56.371: INFO: Waiting for pod pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b to disappear
May 14 00:24:56.378: INFO: Pod pod-projected-secrets-b0e90487-75de-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:24:56.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8892" for this suite.
May 14 00:25:02.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:25:02.581: INFO: namespace projected-8892 deletion completed in 6.188553025s

• [SLOW TEST:10.371 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:25:02.582: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0514 00:25:12.687116      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 00:25:12.687: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:25:12.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6632" for this suite.
May 14 00:25:18.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:25:19.213: INFO: namespace gc-6632 deletion completed in 6.519302383s

• [SLOW TEST:16.631 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:25:19.213: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-c10cc8aa-75de-11e9-8961-767a82fad75b
STEP: Creating configMap with name cm-test-opt-upd-c10cc932-75de-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-c10cc8aa-75de-11e9-8961-767a82fad75b
STEP: Updating configmap cm-test-opt-upd-c10cc932-75de-11e9-8961-767a82fad75b
STEP: Creating configMap with name cm-test-opt-create-c10cc97e-75de-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:25:29.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1822" for this suite.
May 14 00:25:53.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:25:53.762: INFO: namespace configmap-1822 deletion completed in 24.176366996s

• [SLOW TEST:34.549 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:25:53.762: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-229p
STEP: Creating a pod to test atomic-volume-subpath
May 14 00:25:53.851: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-229p" in namespace "subpath-4535" to be "success or failure"
May 14 00:25:53.863: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Pending", Reason="", readiness=false. Elapsed: 12.254713ms
May 14 00:25:55.868: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017012969s
May 14 00:25:57.874: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 4.022705293s
May 14 00:25:59.879: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 6.028311888s
May 14 00:26:01.884: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 8.032393476s
May 14 00:26:03.888: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 10.036554363s
May 14 00:26:05.893: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 12.0423244s
May 14 00:26:07.898: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 14.046657238s
May 14 00:26:09.903: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 16.05224684s
May 14 00:26:11.909: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 18.057742229s
May 14 00:26:13.914: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 20.06266598s
May 14 00:26:15.938: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Running", Reason="", readiness=true. Elapsed: 22.086683612s
May 14 00:26:17.948: INFO: Pod "pod-subpath-test-configmap-229p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.096497795s
STEP: Saw pod success
May 14 00:26:17.948: INFO: Pod "pod-subpath-test-configmap-229p" satisfied condition "success or failure"
May 14 00:26:17.952: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-configmap-229p container test-container-subpath-configmap-229p: <nil>
STEP: delete the pod
May 14 00:26:17.979: INFO: Waiting for pod pod-subpath-test-configmap-229p to disappear
May 14 00:26:17.986: INFO: Pod pod-subpath-test-configmap-229p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-229p
May 14 00:26:17.986: INFO: Deleting pod "pod-subpath-test-configmap-229p" in namespace "subpath-4535"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:26:17.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4535" for this suite.
May 14 00:26:24.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:26:24.174: INFO: namespace subpath-4535 deletion completed in 6.1783489s

• [SLOW TEST:30.412 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:26:24.174: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:26:28.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3438" for this suite.
May 14 00:26:34.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:26:34.595: INFO: namespace emptydir-wrapper-3438 deletion completed in 6.233581824s

• [SLOW TEST:10.421 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:26:34.596: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-sswh
STEP: Creating a pod to test atomic-volume-subpath
May 14 00:26:34.722: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-sswh" in namespace "subpath-3923" to be "success or failure"
May 14 00:26:34.748: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Pending", Reason="", readiness=false. Elapsed: 26.028399ms
May 14 00:26:36.761: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039255307s
May 14 00:26:38.766: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 4.044523999s
May 14 00:26:40.773: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 6.05107985s
May 14 00:26:42.778: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 8.055914357s
May 14 00:26:44.787: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 10.065555637s
May 14 00:26:46.792: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 12.070402183s
May 14 00:26:48.799: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 14.077145295s
May 14 00:26:50.804: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 16.082556778s
May 14 00:26:52.809: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 18.087230399s
May 14 00:26:54.814: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 20.092418782s
May 14 00:26:56.819: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Running", Reason="", readiness=true. Elapsed: 22.097617176s
May 14 00:26:58.823: INFO: Pod "pod-subpath-test-configmap-sswh": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.101329017s
STEP: Saw pod success
May 14 00:26:58.823: INFO: Pod "pod-subpath-test-configmap-sswh" satisfied condition "success or failure"
May 14 00:26:58.827: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-configmap-sswh container test-container-subpath-configmap-sswh: <nil>
STEP: delete the pod
May 14 00:26:58.854: INFO: Waiting for pod pod-subpath-test-configmap-sswh to disappear
May 14 00:26:58.861: INFO: Pod pod-subpath-test-configmap-sswh no longer exists
STEP: Deleting pod pod-subpath-test-configmap-sswh
May 14 00:26:58.861: INFO: Deleting pod "pod-subpath-test-configmap-sswh" in namespace "subpath-3923"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:26:58.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3923" for this suite.
May 14 00:27:04.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:27:05.052: INFO: namespace subpath-3923 deletion completed in 6.178424111s

• [SLOW TEST:30.456 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:27:05.053: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-5780
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 00:27:05.115: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 00:27:23.299: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.100:8080/dial?request=hostName&protocol=udp&host=10.244.0.99&port=8081&tries=1'] Namespace:pod-network-test-5780 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:27:23.299: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:27:23.656: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:27:23.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5780" for this suite.
May 14 00:27:45.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:27:45.853: INFO: namespace pod-network-test-5780 deletion completed in 22.191478209s

• [SLOW TEST:40.801 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:27:45.854: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-187c2f2f-75df-11e9-8961-767a82fad75b
STEP: Creating secret with name s-test-opt-upd-187c2fce-75df-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-187c2f2f-75df-11e9-8961-767a82fad75b
STEP: Updating secret s-test-opt-upd-187c2fce-75df-11e9-8961-767a82fad75b
STEP: Creating secret with name s-test-opt-create-187c301f-75df-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:29:16.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8614" for this suite.
May 14 00:29:39.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:29:39.179: INFO: namespace secrets-8614 deletion completed in 22.191995708s

• [SLOW TEST:113.326 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:29:39.182: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
May 14 00:29:47.309: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 00:29:47.320: INFO: Pod pod-with-prestop-http-hook still exists
May 14 00:29:49.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 00:29:49.326: INFO: Pod pod-with-prestop-http-hook still exists
May 14 00:29:51.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 00:29:51.325: INFO: Pod pod-with-prestop-http-hook still exists
May 14 00:29:53.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 00:29:53.325: INFO: Pod pod-with-prestop-http-hook still exists
May 14 00:29:55.320: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
May 14 00:29:55.326: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:29:55.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4628" for this suite.
May 14 00:30:17.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:30:17.552: INFO: namespace container-lifecycle-hook-4628 deletion completed in 22.207556809s

• [SLOW TEST:38.371 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:30:17.553: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b
May 14 00:30:17.628: INFO: Pod name my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b: Found 0 pods out of 1
May 14 00:30:22.633: INFO: Pod name my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b: Found 1 pods out of 1
May 14 00:30:22.633: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b" are running
May 14 00:30:22.637: INFO: Pod "my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b-jh569" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:30:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:30:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:30:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-05-14 00:30:17 +0000 UTC Reason: Message:}])
May 14 00:30:22.638: INFO: Trying to dial the pod
May 14 00:30:27.653: INFO: Controller my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b: Got expected result from replica 1 [my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b-jh569]: "my-hostname-basic-72d38cda-75df-11e9-8961-767a82fad75b-jh569", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:30:27.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1473" for this suite.
May 14 00:30:33.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:30:33.840: INFO: namespace replication-controller-1473 deletion completed in 6.181373587s

• [SLOW TEST:16.287 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:30:33.841: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-7c8b617a-75df-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:30:33.997: INFO: Waiting up to 5m0s for pod "pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b" in namespace "configmap-7022" to be "success or failure"
May 14 00:30:34.012: INFO: Pod "pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.394293ms
May 14 00:30:36.020: INFO: Pod "pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022689764s
May 14 00:30:38.025: INFO: Pod "pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028086503s
STEP: Saw pod success
May 14 00:30:38.025: INFO: Pod "pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:30:38.030: INFO: Trying to get logs from node conformance0 pod pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:30:38.064: INFO: Waiting for pod pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b to disappear
May 14 00:30:38.074: INFO: Pod pod-configmaps-7c8d5bae-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:30:38.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7022" for this suite.
May 14 00:30:44.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:30:44.252: INFO: namespace configmap-7022 deletion completed in 6.172175886s

• [SLOW TEST:10.412 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:30:44.253: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 00:30:44.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1744'
May 14 00:30:44.482: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 00:30:44.482: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
May 14 00:30:46.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete deployment e2e-test-nginx-deployment --namespace=kubectl-1744'
May 14 00:30:46.787: INFO: stderr: ""
May 14 00:30:46.787: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:30:46.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1744" for this suite.
May 14 00:31:08.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:31:09.082: INFO: namespace kubectl-1744 deletion completed in 22.283871871s

• [SLOW TEST:24.829 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:31:09.083: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:31:09.187: INFO: Waiting up to 5m0s for pod "downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b" in namespace "projected-3865" to be "success or failure"
May 14 00:31:09.199: INFO: Pod "downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.730274ms
May 14 00:31:11.205: INFO: Pod "downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017783506s
May 14 00:31:13.210: INFO: Pod "downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022348604s
STEP: Saw pod success
May 14 00:31:13.210: INFO: Pod "downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:31:13.214: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:31:13.259: INFO: Waiting for pod downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b to disappear
May 14 00:31:13.274: INFO: Pod downwardapi-volume-918ea757-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:31:13.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3865" for this suite.
May 14 00:31:19.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:31:19.475: INFO: namespace projected-3865 deletion completed in 6.195730101s

• [SLOW TEST:10.392 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:31:19.476: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-97bf9c42-75df-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:31:19.574: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b" in namespace "projected-1018" to be "success or failure"
May 14 00:31:19.594: INFO: Pod "pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.006205ms
May 14 00:31:21.598: INFO: Pod "pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02406581s
May 14 00:31:23.605: INFO: Pod "pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030721914s
STEP: Saw pod success
May 14 00:31:23.605: INFO: Pod "pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:31:23.612: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:31:23.645: INFO: Waiting for pod pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b to disappear
May 14 00:31:23.654: INFO: Pod pod-projected-configmaps-97c0b1fa-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:31:23.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1018" for this suite.
May 14 00:31:29.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:31:29.861: INFO: namespace projected-1018 deletion completed in 6.200950351s

• [SLOW TEST:10.386 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:31:29.862: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 14 00:31:34.485: INFO: Successfully updated pod "annotationupdate9debe425-75df-11e9-8961-767a82fad75b"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:31:36.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-676" for this suite.
May 14 00:31:58.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:31:58.777: INFO: namespace projected-676 deletion completed in 22.262320528s

• [SLOW TEST:28.916 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:31:58.778: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 14 00:31:58.860: INFO: Waiting up to 5m0s for pod "pod-af2ac989-75df-11e9-8961-767a82fad75b" in namespace "emptydir-3495" to be "success or failure"
May 14 00:31:58.895: INFO: Pod "pod-af2ac989-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.021877ms
May 14 00:32:00.900: INFO: Pod "pod-af2ac989-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039849898s
May 14 00:32:02.906: INFO: Pod "pod-af2ac989-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045489045s
STEP: Saw pod success
May 14 00:32:02.906: INFO: Pod "pod-af2ac989-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:32:02.911: INFO: Trying to get logs from node conformance0 pod pod-af2ac989-75df-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:32:02.956: INFO: Waiting for pod pod-af2ac989-75df-11e9-8961-767a82fad75b to disappear
May 14 00:32:02.960: INFO: Pod pod-af2ac989-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:32:02.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3495" for this suite.
May 14 00:32:08.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:32:09.187: INFO: namespace emptydir-3495 deletion completed in 6.222974477s

• [SLOW TEST:10.409 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:32:09.189: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:32:09.267: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b" in namespace "projected-8474" to be "success or failure"
May 14 00:32:09.292: INFO: Pod "downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.834657ms
May 14 00:32:11.297: INFO: Pod "downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029979076s
May 14 00:32:13.303: INFO: Pod "downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036501509s
STEP: Saw pod success
May 14 00:32:13.303: INFO: Pod "downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:32:13.308: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:32:13.340: INFO: Waiting for pod downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b to disappear
May 14 00:32:13.344: INFO: Pod downwardapi-volume-b55e0ad7-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:32:13.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8474" for this suite.
May 14 00:32:19.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:32:19.594: INFO: namespace projected-8474 deletion completed in 6.245073738s

• [SLOW TEST:10.406 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:32:19.597: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:32:19.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b" in namespace "downward-api-9846" to be "success or failure"
May 14 00:32:19.708: INFO: Pod "downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.508119ms
May 14 00:32:21.741: INFO: Pod "downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.061256535s
May 14 00:32:23.746: INFO: Pod "downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.065764388s
STEP: Saw pod success
May 14 00:32:23.746: INFO: Pod "downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:32:23.750: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:32:23.783: INFO: Waiting for pod downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b to disappear
May 14 00:32:23.792: INFO: Pod downwardapi-volume-bb92ab38-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:32:23.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9846" for this suite.
May 14 00:32:29.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:32:30.061: INFO: namespace downward-api-9846 deletion completed in 6.263924516s

• [SLOW TEST:10.464 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:32:30.064: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-gjw9
STEP: Creating a pod to test atomic-volume-subpath
May 14 00:32:30.157: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-gjw9" in namespace "subpath-8793" to be "success or failure"
May 14 00:32:30.175: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Pending", Reason="", readiness=false. Elapsed: 17.665305ms
May 14 00:32:32.180: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022362169s
May 14 00:32:34.186: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 4.028360388s
May 14 00:32:36.191: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 6.033796199s
May 14 00:32:38.197: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 8.039615983s
May 14 00:32:40.204: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 10.046223201s
May 14 00:32:42.210: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 12.052535488s
May 14 00:32:44.216: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 14.058361787s
May 14 00:32:46.222: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 16.064873577s
May 14 00:32:48.228: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 18.070440338s
May 14 00:32:50.233: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 20.075998729s
May 14 00:32:52.238: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Running", Reason="", readiness=true. Elapsed: 22.081039172s
May 14 00:32:54.243: INFO: Pod "pod-subpath-test-projected-gjw9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.085480778s
STEP: Saw pod success
May 14 00:32:54.243: INFO: Pod "pod-subpath-test-projected-gjw9" satisfied condition "success or failure"
May 14 00:32:54.247: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-projected-gjw9 container test-container-subpath-projected-gjw9: <nil>
STEP: delete the pod
May 14 00:32:54.279: INFO: Waiting for pod pod-subpath-test-projected-gjw9 to disappear
May 14 00:32:54.301: INFO: Pod pod-subpath-test-projected-gjw9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-gjw9
May 14 00:32:54.301: INFO: Deleting pod "pod-subpath-test-projected-gjw9" in namespace "subpath-8793"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:32:54.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8793" for this suite.
May 14 00:33:00.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:33:00.478: INFO: namespace subpath-8793 deletion completed in 6.162833247s

• [SLOW TEST:30.415 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:33:00.478: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-6440
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 00:33:00.530: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 00:33:22.730: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.115:8080/dial?request=hostName&protocol=http&host=10.244.0.114&port=8080&tries=1'] Namespace:pod-network-test-6440 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 00:33:22.730: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 00:33:23.150: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:33:23.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6440" for this suite.
May 14 00:33:47.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:33:47.330: INFO: namespace pod-network-test-6440 deletion completed in 24.175258046s

• [SLOW TEST:46.852 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:33:47.332: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-efe2b972-75df-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:33:47.447: INFO: Waiting up to 5m0s for pod "pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b" in namespace "configmap-8515" to be "success or failure"
May 14 00:33:47.466: INFO: Pod "pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 18.528368ms
May 14 00:33:49.471: INFO: Pod "pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023682127s
May 14 00:33:51.479: INFO: Pod "pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.032050803s
STEP: Saw pod success
May 14 00:33:51.479: INFO: Pod "pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:33:51.490: INFO: Trying to get logs from node conformance0 pod pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:33:51.529: INFO: Waiting for pod pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b to disappear
May 14 00:33:51.533: INFO: Pod pod-configmaps-efe3e59f-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:33:51.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8515" for this suite.
May 14 00:33:57.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:33:57.739: INFO: namespace configmap-8515 deletion completed in 6.201365579s

• [SLOW TEST:10.407 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:33:57.740: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
May 14 00:33:57.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 api-versions'
May 14 00:33:57.952: INFO: stderr: ""
May 14 00:33:57.952: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:33:57.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-703" for this suite.
May 14 00:34:03.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:34:04.191: INFO: namespace kubectl-703 deletion completed in 6.230101533s

• [SLOW TEST:6.451 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:34:04.192: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
May 14 00:34:04.293: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5924,SelfLink:/api/v1/namespaces/watch-5924/configmaps/e2e-watch-test-resource-version,UID:f9e9eb68-75df-11e9-9f2c-ceb99be2323d,ResourceVersion:138651,Generation:0,CreationTimestamp:2019-05-14 00:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 00:34:04.293: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5924,SelfLink:/api/v1/namespaces/watch-5924/configmaps/e2e-watch-test-resource-version,UID:f9e9eb68-75df-11e9-9f2c-ceb99be2323d,ResourceVersion:138652,Generation:0,CreationTimestamp:2019-05-14 00:34:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:34:04.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5924" for this suite.
May 14 00:34:10.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:34:10.477: INFO: namespace watch-5924 deletion completed in 6.179265268s

• [SLOW TEST:6.286 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:34:10.477: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
May 14 00:34:10.544: INFO: Waiting up to 5m0s for pod "pod-fda80348-75df-11e9-8961-767a82fad75b" in namespace "emptydir-5545" to be "success or failure"
May 14 00:34:10.552: INFO: Pod "pod-fda80348-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.67228ms
May 14 00:34:12.556: INFO: Pod "pod-fda80348-75df-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011786331s
May 14 00:34:14.561: INFO: Pod "pod-fda80348-75df-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01678681s
STEP: Saw pod success
May 14 00:34:14.561: INFO: Pod "pod-fda80348-75df-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:34:14.564: INFO: Trying to get logs from node conformance0 pod pod-fda80348-75df-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:34:14.597: INFO: Waiting for pod pod-fda80348-75df-11e9-8961-767a82fad75b to disappear
May 14 00:34:14.608: INFO: Pod pod-fda80348-75df-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:34:14.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5545" for this suite.
May 14 00:34:20.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:34:20.795: INFO: namespace emptydir-5545 deletion completed in 6.17881545s

• [SLOW TEST:10.318 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:34:20.796: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 14 00:34:20.893: INFO: Waiting up to 5m0s for pod "downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b" in namespace "downward-api-3730" to be "success or failure"
May 14 00:34:20.900: INFO: Pod "downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.758526ms
May 14 00:34:22.904: INFO: Pod "downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011305818s
May 14 00:34:24.910: INFO: Pod "downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016502501s
STEP: Saw pod success
May 14 00:34:24.910: INFO: Pod "downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:34:24.918: INFO: Trying to get logs from node conformance0 pod downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 00:34:24.964: INFO: Waiting for pod downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b to disappear
May 14 00:34:24.969: INFO: Pod downward-api-03d2ac46-75e0-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:34:24.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3730" for this suite.
May 14 00:34:30.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:34:31.156: INFO: namespace downward-api-3730 deletion completed in 6.182644586s

• [SLOW TEST:10.360 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:34:31.157: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:34:35.298: INFO: Waiting up to 5m0s for pod "client-envvars-0c677193-75e0-11e9-8961-767a82fad75b" in namespace "pods-2191" to be "success or failure"
May 14 00:34:35.333: INFO: Pod "client-envvars-0c677193-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 35.682112ms
May 14 00:34:37.342: INFO: Pod "client-envvars-0c677193-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044194335s
May 14 00:34:39.346: INFO: Pod "client-envvars-0c677193-75e0-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.048661748s
STEP: Saw pod success
May 14 00:34:39.346: INFO: Pod "client-envvars-0c677193-75e0-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:34:39.351: INFO: Trying to get logs from node conformance0 pod client-envvars-0c677193-75e0-11e9-8961-767a82fad75b container env3cont: <nil>
STEP: delete the pod
May 14 00:34:39.385: INFO: Waiting for pod client-envvars-0c677193-75e0-11e9-8961-767a82fad75b to disappear
May 14 00:34:39.399: INFO: Pod client-envvars-0c677193-75e0-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:34:39.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2191" for this suite.
May 14 00:35:27.425: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:35:27.576: INFO: namespace pods-2191 deletion completed in 48.17096556s

• [SLOW TEST:56.420 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:35:27.577: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-429z
STEP: Creating a pod to test atomic-volume-subpath
May 14 00:35:27.720: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-429z" in namespace "subpath-7020" to be "success or failure"
May 14 00:35:27.782: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Pending", Reason="", readiness=false. Elapsed: 61.892662ms
May 14 00:35:29.786: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Pending", Reason="", readiness=false. Elapsed: 2.066419131s
May 14 00:35:31.797: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 4.076652518s
May 14 00:35:33.802: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 6.082200122s
May 14 00:35:35.807: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 8.086504372s
May 14 00:35:37.812: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 10.092291242s
May 14 00:35:39.817: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 12.097319944s
May 14 00:35:41.825: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 14.105204369s
May 14 00:35:43.832: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 16.111860114s
May 14 00:35:45.837: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 18.11674529s
May 14 00:35:47.842: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 20.122024055s
May 14 00:35:49.848: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Running", Reason="", readiness=true. Elapsed: 22.127957525s
May 14 00:35:51.854: INFO: Pod "pod-subpath-test-downwardapi-429z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.134203155s
STEP: Saw pod success
May 14 00:35:51.854: INFO: Pod "pod-subpath-test-downwardapi-429z" satisfied condition "success or failure"
May 14 00:35:51.859: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-downwardapi-429z container test-container-subpath-downwardapi-429z: <nil>
STEP: delete the pod
May 14 00:35:51.895: INFO: Waiting for pod pod-subpath-test-downwardapi-429z to disappear
May 14 00:35:51.900: INFO: Pod pod-subpath-test-downwardapi-429z no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-429z
May 14 00:35:51.900: INFO: Deleting pod "pod-subpath-test-downwardapi-429z" in namespace "subpath-7020"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:35:51.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7020" for this suite.
May 14 00:35:57.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:35:58.137: INFO: namespace subpath-7020 deletion completed in 6.228099038s

• [SLOW TEST:30.561 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:35:58.138: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-4769/configmap-test-3dd3398e-75e0-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:35:58.213: INFO: Waiting up to 5m0s for pod "pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b" in namespace "configmap-4769" to be "success or failure"
May 14 00:35:58.225: INFO: Pod "pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.427912ms
May 14 00:36:00.241: INFO: Pod "pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027845798s
May 14 00:36:02.246: INFO: Pod "pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03270076s
STEP: Saw pod success
May 14 00:36:02.246: INFO: Pod "pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:36:02.250: INFO: Trying to get logs from node conformance0 pod pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b container env-test: <nil>
STEP: delete the pod
May 14 00:36:02.280: INFO: Waiting for pod pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b to disappear
May 14 00:36:02.293: INFO: Pod pod-configmaps-3dd47aa5-75e0-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:36:02.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4769" for this suite.
May 14 00:36:08.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:36:08.499: INFO: namespace configmap-4769 deletion completed in 6.191522621s

• [SLOW TEST:10.361 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:36:08.500: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-9200
I0514 00:36:08.569385      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9200, replica count: 1
I0514 00:36:09.620038      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 00:36:10.620384      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 00:36:11.620696      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 00:36:11.745: INFO: Created: latency-svc-b8ddx
May 14 00:36:11.757: INFO: Got endpoints: latency-svc-b8ddx [36.574447ms]
May 14 00:36:11.800: INFO: Created: latency-svc-lnr8w
May 14 00:36:11.806: INFO: Created: latency-svc-w2k2l
May 14 00:36:11.810: INFO: Got endpoints: latency-svc-lnr8w [52.362922ms]
May 14 00:36:11.839: INFO: Got endpoints: latency-svc-w2k2l [81.18393ms]
May 14 00:36:11.861: INFO: Created: latency-svc-v8866
May 14 00:36:11.867: INFO: Got endpoints: latency-svc-v8866 [108.770815ms]
May 14 00:36:11.897: INFO: Created: latency-svc-7zxdw
May 14 00:36:11.899: INFO: Got endpoints: latency-svc-7zxdw [141.474406ms]
May 14 00:36:11.919: INFO: Created: latency-svc-vr6sh
May 14 00:36:11.934: INFO: Got endpoints: latency-svc-vr6sh [94.810257ms]
May 14 00:36:11.944: INFO: Created: latency-svc-4w5b8
May 14 00:36:12.000: INFO: Got endpoints: latency-svc-4w5b8 [241.795098ms]
May 14 00:36:12.000: INFO: Created: latency-svc-nlwq8
May 14 00:36:12.001: INFO: Created: latency-svc-ntff4
May 14 00:36:12.013: INFO: Created: latency-svc-fvd4v
May 14 00:36:12.026: INFO: Got endpoints: latency-svc-ntff4 [268.131821ms]
May 14 00:36:12.027: INFO: Got endpoints: latency-svc-nlwq8 [268.499354ms]
May 14 00:36:12.062: INFO: Got endpoints: latency-svc-fvd4v [303.706599ms]
May 14 00:36:12.068: INFO: Created: latency-svc-ptr55
May 14 00:36:12.098: INFO: Created: latency-svc-58czd
May 14 00:36:12.107: INFO: Created: latency-svc-zd6s9
May 14 00:36:12.115: INFO: Got endpoints: latency-svc-ptr55 [357.524801ms]
May 14 00:36:12.119: INFO: Got endpoints: latency-svc-58czd [361.458092ms]
May 14 00:36:12.131: INFO: Got endpoints: latency-svc-zd6s9 [373.117059ms]
May 14 00:36:12.140: INFO: Created: latency-svc-q6jn7
May 14 00:36:12.148: INFO: Created: latency-svc-cft9c
May 14 00:36:12.148: INFO: Got endpoints: latency-svc-q6jn7 [390.224203ms]
May 14 00:36:12.164: INFO: Got endpoints: latency-svc-cft9c [405.532228ms]
May 14 00:36:12.185: INFO: Created: latency-svc-qlh8b
May 14 00:36:12.196: INFO: Created: latency-svc-dwjkr
May 14 00:36:12.198: INFO: Got endpoints: latency-svc-qlh8b [439.302787ms]
May 14 00:36:12.207: INFO: Got endpoints: latency-svc-dwjkr [449.219788ms]
May 14 00:36:12.225: INFO: Created: latency-svc-k7qsv
May 14 00:36:12.225: INFO: Created: latency-svc-hr5cf
May 14 00:36:12.230: INFO: Got endpoints: latency-svc-k7qsv [420.553965ms]
May 14 00:36:12.252: INFO: Created: latency-svc-fkc67
May 14 00:36:12.254: INFO: Got endpoints: latency-svc-hr5cf [386.71826ms]
May 14 00:36:12.283: INFO: Created: latency-svc-lf288
May 14 00:36:12.283: INFO: Created: latency-svc-sbxx7
May 14 00:36:12.283: INFO: Got endpoints: latency-svc-fkc67 [383.934217ms]
May 14 00:36:12.294: INFO: Created: latency-svc-57w76
May 14 00:36:12.296: INFO: Got endpoints: latency-svc-sbxx7 [361.976203ms]
May 14 00:36:12.312: INFO: Got endpoints: latency-svc-lf288 [312.730396ms]
May 14 00:36:12.315: INFO: Created: latency-svc-cdh96
May 14 00:36:12.324: INFO: Got endpoints: latency-svc-57w76 [297.349064ms]
May 14 00:36:12.330: INFO: Created: latency-svc-xjph6
May 14 00:36:12.341: INFO: Got endpoints: latency-svc-cdh96 [315.246357ms]
May 14 00:36:12.358: INFO: Created: latency-svc-szdhx
May 14 00:36:12.360: INFO: Got endpoints: latency-svc-xjph6 [297.980382ms]
May 14 00:36:12.379: INFO: Created: latency-svc-kcvbf
May 14 00:36:12.386: INFO: Got endpoints: latency-svc-szdhx [270.136339ms]
May 14 00:36:12.398: INFO: Got endpoints: latency-svc-kcvbf [278.267225ms]
May 14 00:36:12.399: INFO: Created: latency-svc-lpvpt
May 14 00:36:12.416: INFO: Created: latency-svc-npjpr
May 14 00:36:12.420: INFO: Got endpoints: latency-svc-lpvpt [288.5597ms]
May 14 00:36:12.456: INFO: Created: latency-svc-7vvr9
May 14 00:36:12.458: INFO: Created: latency-svc-kr6wh
May 14 00:36:12.458: INFO: Created: latency-svc-jsgvg
May 14 00:36:12.465: INFO: Got endpoints: latency-svc-npjpr [316.650453ms]
May 14 00:36:12.474: INFO: Created: latency-svc-ddvwg
May 14 00:36:12.480: INFO: Got endpoints: latency-svc-7vvr9 [272.892808ms]
May 14 00:36:12.481: INFO: Got endpoints: latency-svc-kr6wh [282.876576ms]
May 14 00:36:12.494: INFO: Got endpoints: latency-svc-ddvwg [263.865787ms]
May 14 00:36:12.495: INFO: Got endpoints: latency-svc-jsgvg [330.765241ms]
May 14 00:36:12.525: INFO: Created: latency-svc-mfv9q
May 14 00:36:12.525: INFO: Created: latency-svc-xkx52
May 14 00:36:12.528: INFO: Got endpoints: latency-svc-mfv9q [274.290769ms]
May 14 00:36:12.543: INFO: Got endpoints: latency-svc-xkx52 [260.304994ms]
May 14 00:36:12.550: INFO: Created: latency-svc-tqx8s
May 14 00:36:12.559: INFO: Created: latency-svc-9mpnn
May 14 00:36:12.574: INFO: Got endpoints: latency-svc-tqx8s [277.168477ms]
May 14 00:36:12.579: INFO: Got endpoints: latency-svc-9mpnn [266.612385ms]
May 14 00:36:12.588: INFO: Created: latency-svc-587bs
May 14 00:36:12.607: INFO: Created: latency-svc-79jhz
May 14 00:36:12.608: INFO: Got endpoints: latency-svc-587bs [284.217489ms]
May 14 00:36:12.632: INFO: Created: latency-svc-77mhs
May 14 00:36:12.643: INFO: Got endpoints: latency-svc-77mhs [283.01749ms]
May 14 00:36:12.649: INFO: Got endpoints: latency-svc-79jhz [307.230593ms]
May 14 00:36:12.671: INFO: Created: latency-svc-xhq24
May 14 00:36:12.674: INFO: Got endpoints: latency-svc-xhq24 [288.245207ms]
May 14 00:36:12.689: INFO: Created: latency-svc-cxwqz
May 14 00:36:12.700: INFO: Created: latency-svc-kv965
May 14 00:36:12.712: INFO: Got endpoints: latency-svc-kv965 [292.254959ms]
May 14 00:36:12.723: INFO: Created: latency-svc-srf2d
May 14 00:36:12.729: INFO: Got endpoints: latency-svc-cxwqz [331.058378ms]
May 14 00:36:12.733: INFO: Got endpoints: latency-svc-srf2d [267.981294ms]
May 14 00:36:12.747: INFO: Created: latency-svc-sh9mv
May 14 00:36:12.772: INFO: Created: latency-svc-tt8z5
May 14 00:36:12.789: INFO: Got endpoints: latency-svc-tt8z5 [308.308603ms]
May 14 00:36:12.798: INFO: Got endpoints: latency-svc-sh9mv [317.401792ms]
May 14 00:36:12.801: INFO: Created: latency-svc-qsrch
May 14 00:36:12.801: INFO: Created: latency-svc-np2tj
May 14 00:36:12.820: INFO: Got endpoints: latency-svc-qsrch [325.73443ms]
May 14 00:36:12.820: INFO: Got endpoints: latency-svc-np2tj [325.711062ms]
May 14 00:36:12.824: INFO: Created: latency-svc-vmxp5
May 14 00:36:12.835: INFO: Got endpoints: latency-svc-vmxp5 [306.977786ms]
May 14 00:36:12.849: INFO: Created: latency-svc-6rfrx
May 14 00:36:12.866: INFO: Created: latency-svc-hjsjc
May 14 00:36:12.867: INFO: Got endpoints: latency-svc-6rfrx [323.545117ms]
May 14 00:36:12.887: INFO: Got endpoints: latency-svc-hjsjc [313.158208ms]
May 14 00:36:12.899: INFO: Created: latency-svc-2prrf
May 14 00:36:12.918: INFO: Got endpoints: latency-svc-2prrf [339.210645ms]
May 14 00:36:12.923: INFO: Created: latency-svc-f6xks
May 14 00:36:12.935: INFO: Created: latency-svc-957nz
May 14 00:36:12.938: INFO: Created: latency-svc-hwh4r
May 14 00:36:12.944: INFO: Got endpoints: latency-svc-f6xks [335.217957ms]
May 14 00:36:12.954: INFO: Got endpoints: latency-svc-957nz [310.501086ms]
May 14 00:36:12.967: INFO: Created: latency-svc-cmtrp
May 14 00:36:12.971: INFO: Created: latency-svc-zrkvr
May 14 00:36:12.987: INFO: Created: latency-svc-qsck4
May 14 00:36:13.004: INFO: Created: latency-svc-vh425
May 14 00:36:13.008: INFO: Got endpoints: latency-svc-hwh4r [359.317941ms]
May 14 00:36:13.079: INFO: Created: latency-svc-62vs9
May 14 00:36:13.093: INFO: Got endpoints: latency-svc-zrkvr [380.322057ms]
May 14 00:36:13.116: INFO: Created: latency-svc-ckgf4
May 14 00:36:13.134: INFO: Created: latency-svc-9dqbx
May 14 00:36:13.138: INFO: Got endpoints: latency-svc-cmtrp [464.359899ms]
May 14 00:36:13.171: INFO: Got endpoints: latency-svc-qsck4 [441.623169ms]
May 14 00:36:13.233: INFO: Created: latency-svc-db4xt
May 14 00:36:13.249: INFO: Got endpoints: latency-svc-vh425 [515.888346ms]
May 14 00:36:13.273: INFO: Created: latency-svc-cvm6w
May 14 00:36:13.278: INFO: Created: latency-svc-kt687
May 14 00:36:13.280: INFO: Got endpoints: latency-svc-62vs9 [490.424605ms]
May 14 00:36:13.317: INFO: Created: latency-svc-2clff
May 14 00:36:13.335: INFO: Got endpoints: latency-svc-ckgf4 [536.710793ms]
May 14 00:36:13.360: INFO: Got endpoints: latency-svc-9dqbx [539.943804ms]
May 14 00:36:13.371: INFO: Created: latency-svc-hsx8j
May 14 00:36:13.381: INFO: Created: latency-svc-4r6jl
May 14 00:36:13.397: INFO: Created: latency-svc-xb2nm
May 14 00:36:13.417: INFO: Created: latency-svc-xhdll
May 14 00:36:13.419: INFO: Got endpoints: latency-svc-db4xt [598.743123ms]
May 14 00:36:13.438: INFO: Created: latency-svc-52jb9
May 14 00:36:13.454: INFO: Got endpoints: latency-svc-kt687 [618.688124ms]
May 14 00:36:13.474: INFO: Created: latency-svc-2sprh
May 14 00:36:13.485: INFO: Created: latency-svc-tr9j8
May 14 00:36:13.496: INFO: Created: latency-svc-nkgz4
May 14 00:36:13.513: INFO: Got endpoints: latency-svc-cvm6w [645.922492ms]
May 14 00:36:13.539: INFO: Created: latency-svc-g2md7
May 14 00:36:13.574: INFO: Created: latency-svc-9sx72
May 14 00:36:13.582: INFO: Created: latency-svc-bgsnv
May 14 00:36:13.594: INFO: Created: latency-svc-wlbxj
May 14 00:36:13.603: INFO: Got endpoints: latency-svc-2clff [715.535758ms]
May 14 00:36:13.648: INFO: Got endpoints: latency-svc-hsx8j [729.873521ms]
May 14 00:36:13.683: INFO: Created: latency-svc-hb55t
May 14 00:36:13.744: INFO: Created: latency-svc-gvrxc
May 14 00:36:13.752: INFO: Got endpoints: latency-svc-xb2nm [797.92649ms]
May 14 00:36:13.752: INFO: Got endpoints: latency-svc-4r6jl [808.468949ms]
May 14 00:36:13.756: INFO: Created: latency-svc-gpfg8
May 14 00:36:13.769: INFO: Created: latency-svc-9hgzf
May 14 00:36:13.784: INFO: Got endpoints: latency-svc-xhdll [775.950158ms]
May 14 00:36:13.851: INFO: Got endpoints: latency-svc-52jb9 [758.694919ms]
May 14 00:36:13.872: INFO: Created: latency-svc-9sbcp
May 14 00:36:13.960: INFO: Got endpoints: latency-svc-2sprh [821.332565ms]
May 14 00:36:13.992: INFO: Got endpoints: latency-svc-nkgz4 [742.586796ms]
May 14 00:36:14.024: INFO: Got endpoints: latency-svc-tr9j8 [853.204181ms]
May 14 00:36:14.072: INFO: Got endpoints: latency-svc-g2md7 [791.995011ms]
May 14 00:36:14.075: INFO: Created: latency-svc-l42dn
May 14 00:36:14.077: INFO: Created: latency-svc-xz9xq
May 14 00:36:14.084: INFO: Created: latency-svc-2zl99
May 14 00:36:14.089: INFO: Got endpoints: latency-svc-bgsnv [753.952759ms]
May 14 00:36:14.113: INFO: Got endpoints: latency-svc-9sx72 [752.584699ms]
May 14 00:36:14.120: INFO: Created: latency-svc-tbnvj
May 14 00:36:14.164: INFO: Created: latency-svc-tz2zk
May 14 00:36:14.199: INFO: Created: latency-svc-zft6f
May 14 00:36:14.205: INFO: Got endpoints: latency-svc-wlbxj [785.368031ms]
May 14 00:36:14.232: INFO: Got endpoints: latency-svc-hb55t [777.832269ms]
May 14 00:36:14.240: INFO: Created: latency-svc-jrznp
May 14 00:36:14.245: INFO: Created: latency-svc-t7shb
May 14 00:36:14.254: INFO: Got endpoints: latency-svc-gpfg8 [740.993245ms]
May 14 00:36:14.262: INFO: Created: latency-svc-29lnj
May 14 00:36:14.282: INFO: Created: latency-svc-l8psc
May 14 00:36:14.300: INFO: Created: latency-svc-vt225
May 14 00:36:14.305: INFO: Created: latency-svc-zmv98
May 14 00:36:14.310: INFO: Got endpoints: latency-svc-gvrxc [706.894597ms]
May 14 00:36:14.330: INFO: Created: latency-svc-6pwb2
May 14 00:36:14.351: INFO: Got endpoints: latency-svc-9hgzf [702.684702ms]
May 14 00:36:14.368: INFO: Created: latency-svc-5xmg2
May 14 00:36:14.406: INFO: Got endpoints: latency-svc-9sbcp [654.217038ms]
May 14 00:36:14.421: INFO: Created: latency-svc-7mgqd
May 14 00:36:14.459: INFO: Got endpoints: latency-svc-l42dn [706.3797ms]
May 14 00:36:14.472: INFO: Created: latency-svc-rwhgx
May 14 00:36:14.503: INFO: Got endpoints: latency-svc-xz9xq [718.477327ms]
May 14 00:36:14.516: INFO: Created: latency-svc-b2wcm
May 14 00:36:14.553: INFO: Got endpoints: latency-svc-2zl99 [701.421659ms]
May 14 00:36:14.566: INFO: Created: latency-svc-qp4hk
May 14 00:36:14.601: INFO: Got endpoints: latency-svc-tbnvj [641.164093ms]
May 14 00:36:14.622: INFO: Created: latency-svc-f9hhh
May 14 00:36:14.653: INFO: Got endpoints: latency-svc-tz2zk [660.546755ms]
May 14 00:36:14.676: INFO: Created: latency-svc-h6m8z
May 14 00:36:14.703: INFO: Got endpoints: latency-svc-zft6f [678.772773ms]
May 14 00:36:14.724: INFO: Created: latency-svc-6xwl4
May 14 00:36:14.751: INFO: Got endpoints: latency-svc-jrznp [661.739264ms]
May 14 00:36:14.770: INFO: Created: latency-svc-t922d
May 14 00:36:14.803: INFO: Got endpoints: latency-svc-t7shb [730.858166ms]
May 14 00:36:14.822: INFO: Created: latency-svc-qgvxb
May 14 00:36:14.856: INFO: Got endpoints: latency-svc-29lnj [743.224743ms]
May 14 00:36:14.880: INFO: Created: latency-svc-zf8tk
May 14 00:36:14.903: INFO: Got endpoints: latency-svc-l8psc [698.142512ms]
May 14 00:36:14.934: INFO: Created: latency-svc-5p7g4
May 14 00:36:14.951: INFO: Got endpoints: latency-svc-vt225 [719.294077ms]
May 14 00:36:14.974: INFO: Created: latency-svc-g6r9r
May 14 00:36:15.054: INFO: Got endpoints: latency-svc-zmv98 [799.18506ms]
May 14 00:36:15.078: INFO: Got endpoints: latency-svc-6pwb2 [767.973203ms]
May 14 00:36:15.111: INFO: Created: latency-svc-df76l
May 14 00:36:15.129: INFO: Created: latency-svc-qww2j
May 14 00:36:15.140: INFO: Got endpoints: latency-svc-5xmg2 [788.65539ms]
May 14 00:36:15.181: INFO: Got endpoints: latency-svc-7mgqd [774.214413ms]
May 14 00:36:15.198: INFO: Created: latency-svc-9w8g8
May 14 00:36:15.225: INFO: Got endpoints: latency-svc-rwhgx [766.8605ms]
May 14 00:36:15.234: INFO: Created: latency-svc-kzhnq
May 14 00:36:15.268: INFO: Created: latency-svc-wzrtl
May 14 00:36:15.273: INFO: Got endpoints: latency-svc-b2wcm [770.138977ms]
May 14 00:36:15.331: INFO: Created: latency-svc-grmj2
May 14 00:36:15.332: INFO: Got endpoints: latency-svc-qp4hk [779.152072ms]
May 14 00:36:15.355: INFO: Created: latency-svc-5vt9h
May 14 00:36:15.357: INFO: Got endpoints: latency-svc-f9hhh [756.008388ms]
May 14 00:36:15.375: INFO: Created: latency-svc-n5vgt
May 14 00:36:15.402: INFO: Got endpoints: latency-svc-h6m8z [749.233397ms]
May 14 00:36:15.421: INFO: Created: latency-svc-t7tw7
May 14 00:36:15.458: INFO: Got endpoints: latency-svc-6xwl4 [754.507611ms]
May 14 00:36:15.478: INFO: Created: latency-svc-kfqq5
May 14 00:36:15.506: INFO: Got endpoints: latency-svc-t922d [755.795667ms]
May 14 00:36:15.520: INFO: Created: latency-svc-7xwn5
May 14 00:36:15.553: INFO: Got endpoints: latency-svc-qgvxb [750.126052ms]
May 14 00:36:15.568: INFO: Created: latency-svc-wd58n
May 14 00:36:15.602: INFO: Got endpoints: latency-svc-zf8tk [745.170607ms]
May 14 00:36:15.623: INFO: Created: latency-svc-ppvlr
May 14 00:36:15.654: INFO: Got endpoints: latency-svc-5p7g4 [750.249263ms]
May 14 00:36:15.715: INFO: Created: latency-svc-pptjs
May 14 00:36:15.721: INFO: Got endpoints: latency-svc-g6r9r [769.721153ms]
May 14 00:36:15.756: INFO: Got endpoints: latency-svc-df76l [702.776025ms]
May 14 00:36:15.771: INFO: Created: latency-svc-6crdr
May 14 00:36:15.779: INFO: Created: latency-svc-tkr4w
May 14 00:36:15.803: INFO: Got endpoints: latency-svc-qww2j [724.765709ms]
May 14 00:36:15.825: INFO: Created: latency-svc-jtfzw
May 14 00:36:15.852: INFO: Got endpoints: latency-svc-9w8g8 [712.175955ms]
May 14 00:36:15.882: INFO: Created: latency-svc-htxlg
May 14 00:36:15.903: INFO: Got endpoints: latency-svc-kzhnq [722.507719ms]
May 14 00:36:15.922: INFO: Created: latency-svc-hhwdj
May 14 00:36:15.961: INFO: Got endpoints: latency-svc-wzrtl [734.924487ms]
May 14 00:36:15.982: INFO: Created: latency-svc-ttqf4
May 14 00:36:16.005: INFO: Got endpoints: latency-svc-grmj2 [731.950491ms]
May 14 00:36:16.021: INFO: Created: latency-svc-8lc8m
May 14 00:36:16.057: INFO: Got endpoints: latency-svc-5vt9h [724.381348ms]
May 14 00:36:16.078: INFO: Created: latency-svc-kgdxf
May 14 00:36:16.101: INFO: Got endpoints: latency-svc-n5vgt [743.254971ms]
May 14 00:36:16.127: INFO: Created: latency-svc-bjcbk
May 14 00:36:16.151: INFO: Got endpoints: latency-svc-t7tw7 [748.896343ms]
May 14 00:36:16.172: INFO: Created: latency-svc-ldww6
May 14 00:36:16.201: INFO: Got endpoints: latency-svc-kfqq5 [742.723216ms]
May 14 00:36:16.228: INFO: Created: latency-svc-hsk6g
May 14 00:36:16.261: INFO: Got endpoints: latency-svc-7xwn5 [754.744329ms]
May 14 00:36:16.281: INFO: Created: latency-svc-lfn98
May 14 00:36:16.311: INFO: Got endpoints: latency-svc-wd58n [757.930493ms]
May 14 00:36:16.334: INFO: Created: latency-svc-t9m7s
May 14 00:36:16.355: INFO: Got endpoints: latency-svc-ppvlr [752.74155ms]
May 14 00:36:16.385: INFO: Created: latency-svc-6mkn9
May 14 00:36:16.409: INFO: Got endpoints: latency-svc-pptjs [755.021039ms]
May 14 00:36:16.436: INFO: Created: latency-svc-qqzx6
May 14 00:36:16.453: INFO: Got endpoints: latency-svc-6crdr [731.509973ms]
May 14 00:36:16.474: INFO: Created: latency-svc-f2k8k
May 14 00:36:16.505: INFO: Got endpoints: latency-svc-tkr4w [748.210722ms]
May 14 00:36:16.519: INFO: Created: latency-svc-hjvjt
May 14 00:36:16.553: INFO: Got endpoints: latency-svc-jtfzw [750.022521ms]
May 14 00:36:16.575: INFO: Created: latency-svc-hcmft
May 14 00:36:16.603: INFO: Got endpoints: latency-svc-htxlg [750.558032ms]
May 14 00:36:16.624: INFO: Created: latency-svc-9cmws
May 14 00:36:16.658: INFO: Got endpoints: latency-svc-hhwdj [754.888004ms]
May 14 00:36:16.677: INFO: Created: latency-svc-m4rgh
May 14 00:36:16.701: INFO: Got endpoints: latency-svc-ttqf4 [740.675645ms]
May 14 00:36:16.719: INFO: Created: latency-svc-8mwm5
May 14 00:36:16.756: INFO: Got endpoints: latency-svc-8lc8m [750.572479ms]
May 14 00:36:16.774: INFO: Created: latency-svc-g62xx
May 14 00:36:16.805: INFO: Got endpoints: latency-svc-kgdxf [747.975679ms]
May 14 00:36:16.823: INFO: Created: latency-svc-5kmsz
May 14 00:36:16.853: INFO: Got endpoints: latency-svc-bjcbk [751.979458ms]
May 14 00:36:16.873: INFO: Created: latency-svc-zzkws
May 14 00:36:16.902: INFO: Got endpoints: latency-svc-ldww6 [750.42896ms]
May 14 00:36:16.922: INFO: Created: latency-svc-kgxqv
May 14 00:36:16.955: INFO: Got endpoints: latency-svc-hsk6g [754.117031ms]
May 14 00:36:16.981: INFO: Created: latency-svc-dr6pw
May 14 00:36:17.006: INFO: Got endpoints: latency-svc-lfn98 [744.473349ms]
May 14 00:36:17.053: INFO: Created: latency-svc-zfc58
May 14 00:36:17.057: INFO: Got endpoints: latency-svc-t9m7s [746.226779ms]
May 14 00:36:17.085: INFO: Created: latency-svc-765wt
May 14 00:36:17.105: INFO: Got endpoints: latency-svc-6mkn9 [749.960578ms]
May 14 00:36:17.130: INFO: Created: latency-svc-qzdl4
May 14 00:36:17.152: INFO: Got endpoints: latency-svc-qqzx6 [743.338959ms]
May 14 00:36:17.176: INFO: Created: latency-svc-c5t8x
May 14 00:36:17.204: INFO: Got endpoints: latency-svc-f2k8k [751.156602ms]
May 14 00:36:17.226: INFO: Created: latency-svc-s62vj
May 14 00:36:17.253: INFO: Got endpoints: latency-svc-hjvjt [748.343797ms]
May 14 00:36:17.274: INFO: Created: latency-svc-l8th2
May 14 00:36:17.304: INFO: Got endpoints: latency-svc-hcmft [751.454128ms]
May 14 00:36:17.319: INFO: Created: latency-svc-vr9dr
May 14 00:36:17.352: INFO: Got endpoints: latency-svc-9cmws [749.027614ms]
May 14 00:36:17.371: INFO: Created: latency-svc-p8rd8
May 14 00:36:17.402: INFO: Got endpoints: latency-svc-m4rgh [743.244353ms]
May 14 00:36:17.429: INFO: Created: latency-svc-pbndw
May 14 00:36:17.453: INFO: Got endpoints: latency-svc-8mwm5 [751.400517ms]
May 14 00:36:17.469: INFO: Created: latency-svc-k4rww
May 14 00:36:17.501: INFO: Got endpoints: latency-svc-g62xx [744.842801ms]
May 14 00:36:17.517: INFO: Created: latency-svc-7972w
May 14 00:36:17.553: INFO: Got endpoints: latency-svc-5kmsz [747.639295ms]
May 14 00:36:17.578: INFO: Created: latency-svc-65mt2
May 14 00:36:17.602: INFO: Got endpoints: latency-svc-zzkws [748.732333ms]
May 14 00:36:17.618: INFO: Created: latency-svc-hb2b4
May 14 00:36:17.651: INFO: Got endpoints: latency-svc-kgxqv [748.798003ms]
May 14 00:36:17.668: INFO: Created: latency-svc-74xcz
May 14 00:36:17.704: INFO: Got endpoints: latency-svc-dr6pw [748.594593ms]
May 14 00:36:17.720: INFO: Created: latency-svc-9zrt5
May 14 00:36:17.752: INFO: Got endpoints: latency-svc-zfc58 [745.734788ms]
May 14 00:36:17.764: INFO: Created: latency-svc-fv68n
May 14 00:36:17.807: INFO: Got endpoints: latency-svc-765wt [749.372116ms]
May 14 00:36:17.823: INFO: Created: latency-svc-8h4t8
May 14 00:36:17.850: INFO: Got endpoints: latency-svc-qzdl4 [744.678209ms]
May 14 00:36:17.877: INFO: Created: latency-svc-6b2mj
May 14 00:36:17.905: INFO: Got endpoints: latency-svc-c5t8x [752.252527ms]
May 14 00:36:17.925: INFO: Created: latency-svc-5c9pn
May 14 00:36:17.954: INFO: Got endpoints: latency-svc-s62vj [750.003839ms]
May 14 00:36:17.982: INFO: Created: latency-svc-smws8
May 14 00:36:18.010: INFO: Got endpoints: latency-svc-l8th2 [756.619821ms]
May 14 00:36:18.040: INFO: Created: latency-svc-hscwm
May 14 00:36:18.056: INFO: Got endpoints: latency-svc-vr9dr [751.076922ms]
May 14 00:36:18.083: INFO: Created: latency-svc-vs2w9
May 14 00:36:18.104: INFO: Got endpoints: latency-svc-p8rd8 [751.461931ms]
May 14 00:36:18.141: INFO: Created: latency-svc-2htt6
May 14 00:36:18.152: INFO: Got endpoints: latency-svc-pbndw [750.643573ms]
May 14 00:36:18.174: INFO: Created: latency-svc-pttz9
May 14 00:36:18.204: INFO: Got endpoints: latency-svc-k4rww [750.686778ms]
May 14 00:36:18.220: INFO: Created: latency-svc-hsphk
May 14 00:36:18.251: INFO: Got endpoints: latency-svc-7972w [750.459596ms]
May 14 00:36:18.269: INFO: Created: latency-svc-7lv7p
May 14 00:36:18.300: INFO: Got endpoints: latency-svc-65mt2 [746.875483ms]
May 14 00:36:18.317: INFO: Created: latency-svc-d7l72
May 14 00:36:18.350: INFO: Got endpoints: latency-svc-hb2b4 [748.489882ms]
May 14 00:36:18.369: INFO: Created: latency-svc-w8r4m
May 14 00:36:18.407: INFO: Got endpoints: latency-svc-74xcz [755.84171ms]
May 14 00:36:18.431: INFO: Created: latency-svc-h5587
May 14 00:36:18.451: INFO: Got endpoints: latency-svc-9zrt5 [746.822486ms]
May 14 00:36:18.497: INFO: Created: latency-svc-flplr
May 14 00:36:18.504: INFO: Got endpoints: latency-svc-fv68n [751.930927ms]
May 14 00:36:18.542: INFO: Created: latency-svc-nrfxm
May 14 00:36:18.586: INFO: Got endpoints: latency-svc-8h4t8 [779.163427ms]
May 14 00:36:18.608: INFO: Got endpoints: latency-svc-6b2mj [757.804582ms]
May 14 00:36:18.620: INFO: Created: latency-svc-pr5v8
May 14 00:36:18.644: INFO: Created: latency-svc-569pd
May 14 00:36:18.674: INFO: Got endpoints: latency-svc-5c9pn [768.963494ms]
May 14 00:36:18.744: INFO: Got endpoints: latency-svc-smws8 [789.133332ms]
May 14 00:36:18.756: INFO: Created: latency-svc-mtrwk
May 14 00:36:18.775: INFO: Created: latency-svc-bv88w
May 14 00:36:18.777: INFO: Got endpoints: latency-svc-hscwm [767.106885ms]
May 14 00:36:18.803: INFO: Created: latency-svc-gjfx8
May 14 00:36:18.808: INFO: Got endpoints: latency-svc-vs2w9 [752.482423ms]
May 14 00:36:18.827: INFO: Created: latency-svc-7c28l
May 14 00:36:18.871: INFO: Got endpoints: latency-svc-2htt6 [767.314793ms]
May 14 00:36:18.884: INFO: Created: latency-svc-r64r7
May 14 00:36:18.907: INFO: Got endpoints: latency-svc-pttz9 [754.891373ms]
May 14 00:36:18.927: INFO: Created: latency-svc-52knz
May 14 00:36:18.955: INFO: Got endpoints: latency-svc-hsphk [751.225541ms]
May 14 00:36:18.971: INFO: Created: latency-svc-g745g
May 14 00:36:19.010: INFO: Got endpoints: latency-svc-7lv7p [758.578118ms]
May 14 00:36:19.033: INFO: Created: latency-svc-9sv65
May 14 00:36:19.052: INFO: Got endpoints: latency-svc-d7l72 [752.120579ms]
May 14 00:36:19.070: INFO: Created: latency-svc-rc8mb
May 14 00:36:19.106: INFO: Got endpoints: latency-svc-w8r4m [756.077048ms]
May 14 00:36:19.124: INFO: Created: latency-svc-tr6r4
May 14 00:36:19.155: INFO: Got endpoints: latency-svc-h5587 [747.655698ms]
May 14 00:36:19.174: INFO: Created: latency-svc-bww4p
May 14 00:36:19.204: INFO: Got endpoints: latency-svc-flplr [753.28366ms]
May 14 00:36:19.220: INFO: Created: latency-svc-7tnvn
May 14 00:36:19.252: INFO: Got endpoints: latency-svc-nrfxm [747.623899ms]
May 14 00:36:19.268: INFO: Created: latency-svc-255fg
May 14 00:36:19.303: INFO: Got endpoints: latency-svc-pr5v8 [716.711539ms]
May 14 00:36:19.323: INFO: Created: latency-svc-8jd5c
May 14 00:36:19.356: INFO: Got endpoints: latency-svc-569pd [748.04354ms]
May 14 00:36:19.383: INFO: Created: latency-svc-bltdl
May 14 00:36:19.401: INFO: Got endpoints: latency-svc-mtrwk [726.744433ms]
May 14 00:36:19.418: INFO: Created: latency-svc-fb6pv
May 14 00:36:19.450: INFO: Got endpoints: latency-svc-bv88w [705.999823ms]
May 14 00:36:19.467: INFO: Created: latency-svc-znrbj
May 14 00:36:19.503: INFO: Got endpoints: latency-svc-gjfx8 [725.311796ms]
May 14 00:36:19.521: INFO: Created: latency-svc-2bjr6
May 14 00:36:19.554: INFO: Got endpoints: latency-svc-7c28l [745.749126ms]
May 14 00:36:19.569: INFO: Created: latency-svc-2mwz8
May 14 00:36:19.617: INFO: Got endpoints: latency-svc-r64r7 [745.680307ms]
May 14 00:36:19.653: INFO: Got endpoints: latency-svc-52knz [745.900225ms]
May 14 00:36:19.707: INFO: Got endpoints: latency-svc-g745g [751.491999ms]
May 14 00:36:19.755: INFO: Got endpoints: latency-svc-9sv65 [745.189352ms]
May 14 00:36:19.801: INFO: Got endpoints: latency-svc-rc8mb [749.54719ms]
May 14 00:36:19.851: INFO: Got endpoints: latency-svc-tr6r4 [744.63888ms]
May 14 00:36:19.902: INFO: Got endpoints: latency-svc-bww4p [746.998807ms]
May 14 00:36:19.957: INFO: Got endpoints: latency-svc-7tnvn [753.315063ms]
May 14 00:36:20.003: INFO: Got endpoints: latency-svc-255fg [751.09082ms]
May 14 00:36:20.053: INFO: Got endpoints: latency-svc-8jd5c [749.780094ms]
May 14 00:36:20.101: INFO: Got endpoints: latency-svc-bltdl [744.75772ms]
May 14 00:36:20.150: INFO: Got endpoints: latency-svc-fb6pv [748.679797ms]
May 14 00:36:20.200: INFO: Got endpoints: latency-svc-znrbj [750.195547ms]
May 14 00:36:20.253: INFO: Got endpoints: latency-svc-2bjr6 [750.133121ms]
May 14 00:36:20.301: INFO: Got endpoints: latency-svc-2mwz8 [746.489175ms]
May 14 00:36:20.301: INFO: Latencies: [52.362922ms 81.18393ms 94.810257ms 108.770815ms 141.474406ms 241.795098ms 260.304994ms 263.865787ms 266.612385ms 267.981294ms 268.131821ms 268.499354ms 270.136339ms 272.892808ms 274.290769ms 277.168477ms 278.267225ms 282.876576ms 283.01749ms 284.217489ms 288.245207ms 288.5597ms 292.254959ms 297.349064ms 297.980382ms 303.706599ms 306.977786ms 307.230593ms 308.308603ms 310.501086ms 312.730396ms 313.158208ms 315.246357ms 316.650453ms 317.401792ms 323.545117ms 325.711062ms 325.73443ms 330.765241ms 331.058378ms 335.217957ms 339.210645ms 357.524801ms 359.317941ms 361.458092ms 361.976203ms 373.117059ms 380.322057ms 383.934217ms 386.71826ms 390.224203ms 405.532228ms 420.553965ms 439.302787ms 441.623169ms 449.219788ms 464.359899ms 490.424605ms 515.888346ms 536.710793ms 539.943804ms 598.743123ms 618.688124ms 641.164093ms 645.922492ms 654.217038ms 660.546755ms 661.739264ms 678.772773ms 698.142512ms 701.421659ms 702.684702ms 702.776025ms 705.999823ms 706.3797ms 706.894597ms 712.175955ms 715.535758ms 716.711539ms 718.477327ms 719.294077ms 722.507719ms 724.381348ms 724.765709ms 725.311796ms 726.744433ms 729.873521ms 730.858166ms 731.509973ms 731.950491ms 734.924487ms 740.675645ms 740.993245ms 742.586796ms 742.723216ms 743.224743ms 743.244353ms 743.254971ms 743.338959ms 744.473349ms 744.63888ms 744.678209ms 744.75772ms 744.842801ms 745.170607ms 745.189352ms 745.680307ms 745.734788ms 745.749126ms 745.900225ms 746.226779ms 746.489175ms 746.822486ms 746.875483ms 746.998807ms 747.623899ms 747.639295ms 747.655698ms 747.975679ms 748.04354ms 748.210722ms 748.343797ms 748.489882ms 748.594593ms 748.679797ms 748.732333ms 748.798003ms 748.896343ms 749.027614ms 749.233397ms 749.372116ms 749.54719ms 749.780094ms 749.960578ms 750.003839ms 750.022521ms 750.126052ms 750.133121ms 750.195547ms 750.249263ms 750.42896ms 750.459596ms 750.558032ms 750.572479ms 750.643573ms 750.686778ms 751.076922ms 751.09082ms 751.156602ms 751.225541ms 751.400517ms 751.454128ms 751.461931ms 751.491999ms 751.930927ms 751.979458ms 752.120579ms 752.252527ms 752.482423ms 752.584699ms 752.74155ms 753.28366ms 753.315063ms 753.952759ms 754.117031ms 754.507611ms 754.744329ms 754.888004ms 754.891373ms 755.021039ms 755.795667ms 755.84171ms 756.008388ms 756.077048ms 756.619821ms 757.804582ms 757.930493ms 758.578118ms 758.694919ms 766.8605ms 767.106885ms 767.314793ms 767.973203ms 768.963494ms 769.721153ms 770.138977ms 774.214413ms 775.950158ms 777.832269ms 779.152072ms 779.163427ms 785.368031ms 788.65539ms 789.133332ms 791.995011ms 797.92649ms 799.18506ms 808.468949ms 821.332565ms 853.204181ms]
May 14 00:36:20.301: INFO: 50 %ile: 744.63888ms
May 14 00:36:20.301: INFO: 90 %ile: 767.106885ms
May 14 00:36:20.301: INFO: 99 %ile: 821.332565ms
May 14 00:36:20.301: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:36:20.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-9200" for this suite.
May 14 00:36:46.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:36:46.526: INFO: namespace svc-latency-9200 deletion completed in 26.210466463s

• [SLOW TEST:38.027 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:36:46.528: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:36:46.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b" in namespace "downward-api-1433" to be "success or failure"
May 14 00:36:46.696: INFO: Pod "downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.164141ms
May 14 00:36:48.702: INFO: Pod "downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021952209s
May 14 00:36:50.709: INFO: Pod "downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028284982s
STEP: Saw pod success
May 14 00:36:50.709: INFO: Pod "downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:36:50.718: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:36:50.761: INFO: Waiting for pod downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b to disappear
May 14 00:36:50.769: INFO: Pod downwardapi-volume-5ab76f8d-75e0-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:36:50.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1433" for this suite.
May 14 00:36:56.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:36:56.996: INFO: namespace downward-api-1433 deletion completed in 6.217801421s

• [SLOW TEST:10.468 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:36:56.997: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-60eda3c7-75e0-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-60eda3c7-75e0-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:38:25.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1453" for this suite.
May 14 00:38:47.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:38:48.128: INFO: namespace configmap-1453 deletion completed in 22.212023489s

• [SLOW TEST:111.131 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:38:48.130: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-xn5k
STEP: Creating a pod to test atomic-volume-subpath
May 14 00:38:48.217: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-xn5k" in namespace "subpath-8298" to be "success or failure"
May 14 00:38:48.225: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Pending", Reason="", readiness=false. Elapsed: 7.706067ms
May 14 00:38:50.234: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016116749s
May 14 00:38:52.240: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 4.022496023s
May 14 00:38:54.245: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 6.028050177s
May 14 00:38:56.251: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 8.033309735s
May 14 00:38:58.256: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 10.038697872s
May 14 00:39:00.261: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 12.043861508s
May 14 00:39:02.266: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 14.048777105s
May 14 00:39:04.271: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 16.053817777s
May 14 00:39:06.278: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 18.060483714s
May 14 00:39:08.283: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 20.065283494s
May 14 00:39:10.288: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Running", Reason="", readiness=true. Elapsed: 22.070183523s
May 14 00:39:12.292: INFO: Pod "pod-subpath-test-secret-xn5k": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.075019977s
STEP: Saw pod success
May 14 00:39:12.293: INFO: Pod "pod-subpath-test-secret-xn5k" satisfied condition "success or failure"
May 14 00:39:12.297: INFO: Trying to get logs from node conformance0 pod pod-subpath-test-secret-xn5k container test-container-subpath-secret-xn5k: <nil>
STEP: delete the pod
May 14 00:39:12.324: INFO: Waiting for pod pod-subpath-test-secret-xn5k to disappear
May 14 00:39:12.335: INFO: Pod pod-subpath-test-secret-xn5k no longer exists
STEP: Deleting pod pod-subpath-test-secret-xn5k
May 14 00:39:12.335: INFO: Deleting pod "pod-subpath-test-secret-xn5k" in namespace "subpath-8298"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:39:12.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8298" for this suite.
May 14 00:39:18.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:39:18.539: INFO: namespace subpath-8298 deletion completed in 6.182589421s

• [SLOW TEST:30.410 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:39:18.540: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:39:18.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1340" for this suite.
May 14 00:39:24.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:39:24.839: INFO: namespace services-1340 deletion completed in 6.218259159s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:6.300 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:39:24.840: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
May 14 00:39:24.919: INFO: Waiting up to 5m0s for pod "pod-b90941fd-75e0-11e9-8961-767a82fad75b" in namespace "emptydir-1997" to be "success or failure"
May 14 00:39:24.929: INFO: Pod "pod-b90941fd-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.275091ms
May 14 00:39:26.936: INFO: Pod "pod-b90941fd-75e0-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017194535s
May 14 00:39:28.942: INFO: Pod "pod-b90941fd-75e0-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02308437s
STEP: Saw pod success
May 14 00:39:28.942: INFO: Pod "pod-b90941fd-75e0-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:39:28.958: INFO: Trying to get logs from node conformance0 pod pod-b90941fd-75e0-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:39:29.028: INFO: Waiting for pod pod-b90941fd-75e0-11e9-8961-767a82fad75b to disappear
May 14 00:39:29.034: INFO: Pod pod-b90941fd-75e0-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:39:29.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1997" for this suite.
May 14 00:39:35.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:39:35.245: INFO: namespace emptydir-1997 deletion completed in 6.202403554s

• [SLOW TEST:10.405 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:39:35.247: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1560
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-1560
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1560
May 14 00:39:35.362: INFO: Found 0 stateful pods, waiting for 1
May 14 00:39:45.368: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
May 14 00:39:45.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 00:39:45.941: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 00:39:45.941: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 00:39:45.941: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 00:39:45.945: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 14 00:39:55.950: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 00:39:55.950: INFO: Waiting for statefulset status.replicas updated to 0
May 14 00:39:56.012: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:39:56.012: INFO: ss-0  conformance0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  }]
May 14 00:39:56.012: INFO: 
May 14 00:39:56.012: INFO: StatefulSet ss has not reached scale 3, at 1
May 14 00:39:57.030: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.973911458s
May 14 00:39:58.035: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.956025981s
May 14 00:39:59.040: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.951547524s
May 14 00:40:00.045: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.945827171s
May 14 00:40:01.051: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.940779297s
May 14 00:40:02.059: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.934878993s
May 14 00:40:03.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.927490243s
May 14 00:40:04.070: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.921594085s
May 14 00:40:05.074: INFO: Verifying statefulset ss doesn't scale past 3 for another 916.558994ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1560
May 14 00:40:06.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:06.614: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 00:40:06.614: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 00:40:06.614: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 00:40:06.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:07.138: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 14 00:40:07.138: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 00:40:07.138: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 00:40:07.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:07.661: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
May 14 00:40:07.661: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 00:40:07.661: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 00:40:07.666: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 00:40:07.666: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 00:40:07.666: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
May 14 00:40:07.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 00:40:08.162: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 00:40:08.162: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 00:40:08.162: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 00:40:08.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 00:40:08.771: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 00:40:08.771: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 00:40:08.771: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 00:40:08.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 00:40:09.328: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 00:40:09.328: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 00:40:09.328: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 00:40:09.328: INFO: Waiting for statefulset status.replicas updated to 0
May 14 00:40:09.357: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 14 00:40:19.390: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 00:40:19.390: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 14 00:40:19.390: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 14 00:40:19.439: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:19.439: INFO: ss-0  conformance0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  }]
May 14 00:40:19.439: INFO: ss-1  conformance0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:19.439: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:19.439: INFO: 
May 14 00:40:19.439: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 00:40:20.444: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:20.445: INFO: ss-0  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  }]
May 14 00:40:20.445: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:20.445: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:20.445: INFO: 
May 14 00:40:20.445: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 00:40:21.451: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:21.451: INFO: ss-0  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:35 +0000 UTC  }]
May 14 00:40:21.451: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:21.452: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:21.452: INFO: 
May 14 00:40:21.452: INFO: StatefulSet ss has not reached scale 0, at 3
May 14 00:40:22.459: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:22.459: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:22.459: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:22.459: INFO: 
May 14 00:40:22.459: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:23.465: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:23.465: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:23.465: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:23.465: INFO: 
May 14 00:40:23.465: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:24.471: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:24.471: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:24.471: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:24.471: INFO: 
May 14 00:40:24.471: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:25.475: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:25.475: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:25.476: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:25.476: INFO: 
May 14 00:40:25.476: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:26.482: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:26.483: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:26.483: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:26.483: INFO: 
May 14 00:40:26.483: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:27.488: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:27.488: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:27.489: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:27.489: INFO: 
May 14 00:40:27.489: INFO: StatefulSet ss has not reached scale 0, at 2
May 14 00:40:28.494: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
May 14 00:40:28.494: INFO: ss-1  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:28.494: INFO: ss-2  conformance0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:40:09 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 00:39:56 +0000 UTC  }]
May 14 00:40:28.494: INFO: 
May 14 00:40:28.494: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1560
May 14 00:40:29.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:29.694: INFO: rc: 1
May 14 00:40:29.694: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0022454a0 exit status 1 <nil> <nil> true [0xc002316128 0xc002316140 0xc002316158] [0xc002316128 0xc002316140 0xc002316158] [0xc002316138 0xc002316150] [0x9bf9f0 0x9bf9f0] 0xc000a311a0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

May 14 00:40:39.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:39.849: INFO: rc: 1
May 14 00:40:39.849: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002c4be90 exit status 1 <nil> <nil> true [0xc0021f4178 0xc0021f4190 0xc0021f41a8] [0xc0021f4178 0xc0021f4190 0xc0021f41a8] [0xc0021f4188 0xc0021f41a0] [0x9bf9f0 0x9bf9f0] 0xc002646f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:40:49.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:40:49.988: INFO: rc: 1
May 14 00:40:49.989: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f44240 exit status 1 <nil> <nil> true [0xc0021f41b0 0xc0021f41c8 0xc0021f41e0] [0xc0021f41b0 0xc0021f41c8 0xc0021f41e0] [0xc0021f41c0 0xc0021f41d8] [0x9bf9f0 0x9bf9f0] 0xc0026472c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:40:59.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:00.143: INFO: rc: 1
May 14 00:41:00.144: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002245830 exit status 1 <nil> <nil> true [0xc002316160 0xc002316178 0xc002316190] [0xc002316160 0xc002316178 0xc002316190] [0xc002316170 0xc002316188] [0x9bf9f0 0x9bf9f0] 0xc000a31a40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:41:10.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:10.287: INFO: rc: 1
May 14 00:41:10.287: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002245bc0 exit status 1 <nil> <nil> true [0xc002316198 0xc0023161b0 0xc0023161c8] [0xc002316198 0xc0023161b0 0xc0023161c8] [0xc0023161a8 0xc0023161c0] [0x9bf9f0 0x9bf9f0] 0xc000a31ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:41:20.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:20.431: INFO: rc: 1
May 14 00:41:20.431: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f445d0 exit status 1 <nil> <nil> true [0xc0021f41e8 0xc0021f4200 0xc0021f4218] [0xc0021f41e8 0xc0021f4200 0xc0021f4218] [0xc0021f41f8 0xc0021f4210] [0x9bf9f0 0x9bf9f0] 0xc002647620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:41:30.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:30.576: INFO: rc: 1
May 14 00:41:30.576: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f44960 exit status 1 <nil> <nil> true [0xc0021f4220 0xc0021f4238 0xc0021f4250] [0xc0021f4220 0xc0021f4238 0xc0021f4250] [0xc0021f4230 0xc0021f4248] [0x9bf9f0 0x9bf9f0] 0xc002647980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:41:40.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:40.771: INFO: rc: 1
May 14 00:41:40.771: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f44e40 exit status 1 <nil> <nil> true [0xc0021f4258 0xc0021f4270 0xc0021f4288] [0xc0021f4258 0xc0021f4270 0xc0021f4288] [0xc0021f4268 0xc0021f4280] [0x9bf9f0 0x9bf9f0] 0xc002647ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:41:50.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:41:50.915: INFO: rc: 1
May 14 00:41:50.915: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f45200 exit status 1 <nil> <nil> true [0xc0021f4290 0xc0021f42a8 0xc0021f42c0] [0xc0021f4290 0xc0021f42a8 0xc0021f42c0] [0xc0021f42a0 0xc0021f42b8] [0x9bf9f0 0x9bf9f0] 0xc001b3c600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:00.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:01.046: INFO: rc: 1
May 14 00:42:01.046: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002f45590 exit status 1 <nil> <nil> true [0xc0021f42c8 0xc0021f42e0 0xc0021f42f8] [0xc0021f42c8 0xc0021f42e0 0xc0021f42f8] [0xc0021f42d8 0xc0021f42f0] [0x9bf9f0 0x9bf9f0] 0xc001b3ce40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:11.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:11.196: INFO: rc: 1
May 14 00:42:11.196: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc002245f50 exit status 1 <nil> <nil> true [0xc0023161d0 0xc0023161e8 0xc002316200] [0xc0023161d0 0xc0023161e8 0xc002316200] [0xc0023161e0 0xc0023161f8] [0x9bf9f0 0x9bf9f0] 0xc002483500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:21.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:21.345: INFO: rc: 1
May 14 00:42:21.345: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001160330 exit status 1 <nil> <nil> true [0xc002316210 0xc002316228 0xc002316240] [0xc002316210 0xc002316228 0xc002316240] [0xc002316220 0xc002316238] [0x9bf9f0 0x9bf9f0] 0xc001904540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:31.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:31.490: INFO: rc: 1
May 14 00:42:31.490: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bca4b0 exit status 1 <nil> <nil> true [0xc0021f4010 0xc0021f4038 0xc0021f4060] [0xc0021f4010 0xc0021f4038 0xc0021f4060] [0xc0021f4030 0xc0021f4048] [0x9bf9f0 0x9bf9f0] 0xc0026462a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:41.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:41.620: INFO: rc: 1
May 14 00:42:41.620: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bca840 exit status 1 <nil> <nil> true [0xc0021f4078 0xc0021f4090 0xc0021f40a8] [0xc0021f4078 0xc0021f4090 0xc0021f40a8] [0xc0021f4088 0xc0021f40a0] [0x9bf9f0 0x9bf9f0] 0xc002646600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:42:51.620: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:42:51.766: INFO: rc: 1
May 14 00:42:51.766: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ec360 exit status 1 <nil> <nil> true [0xc002316000 0xc002316018 0xc002316030] [0xc002316000 0xc002316018 0xc002316030] [0xc002316010 0xc002316028] [0x9bf9f0 0x9bf9f0] 0xc002483740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:01.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:01.937: INFO: rc: 1
May 14 00:43:01.937: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ec6f0 exit status 1 <nil> <nil> true [0xc002316038 0xc002316058 0xc002316070] [0xc002316038 0xc002316058 0xc002316070] [0xc002316048 0xc002316068] [0x9bf9f0 0x9bf9f0] 0xc000a30300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:11.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:12.077: INFO: rc: 1
May 14 00:43:12.077: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bcabd0 exit status 1 <nil> <nil> true [0xc0021f40b8 0xc0021f40e0 0xc0021f40f8] [0xc0021f40b8 0xc0021f40e0 0xc0021f40f8] [0xc0021f40d8 0xc0021f40f0] [0x9bf9f0 0x9bf9f0] 0xc002646960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:22.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:22.211: INFO: rc: 1
May 14 00:43:22.211: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bcaf60 exit status 1 <nil> <nil> true [0xc0021f4108 0xc0021f4120 0xc0021f4138] [0xc0021f4108 0xc0021f4120 0xc0021f4138] [0xc0021f4118 0xc0021f4130] [0x9bf9f0 0x9bf9f0] 0xc002646d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:32.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:32.363: INFO: rc: 1
May 14 00:43:32.363: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030eca50 exit status 1 <nil> <nil> true [0xc002316078 0xc002316090 0xc0023160a8] [0xc002316078 0xc002316090 0xc0023160a8] [0xc002316088 0xc0023160a0] [0x9bf9f0 0x9bf9f0] 0xc000a30660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:42.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:42.499: INFO: rc: 1
May 14 00:43:42.499: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bcb2f0 exit status 1 <nil> <nil> true [0xc0021f4140 0xc0021f4158 0xc0021f4170] [0xc0021f4140 0xc0021f4158 0xc0021f4170] [0xc0021f4150 0xc0021f4168] [0x9bf9f0 0x9bf9f0] 0xc002647080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:43:52.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:43:52.661: INFO: rc: 1
May 14 00:43:52.661: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bcb680 exit status 1 <nil> <nil> true [0xc0021f4178 0xc0021f4190 0xc0021f41a8] [0xc0021f4178 0xc0021f4190 0xc0021f41a8] [0xc0021f4188 0xc0021f41a0] [0x9bf9f0 0x9bf9f0] 0xc0026473e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:02.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:02.796: INFO: rc: 1
May 14 00:44:02.796: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ecde0 exit status 1 <nil> <nil> true [0xc0023160b0 0xc0023160c8 0xc0023160e8] [0xc0023160b0 0xc0023160c8 0xc0023160e8] [0xc0023160c0 0xc0023160e0] [0x9bf9f0 0x9bf9f0] 0xc000a30d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:12.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:12.931: INFO: rc: 1
May 14 00:44:12.931: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ed170 exit status 1 <nil> <nil> true [0xc0023160f0 0xc002316108 0xc002316120] [0xc0023160f0 0xc002316108 0xc002316120] [0xc002316100 0xc002316118] [0x9bf9f0 0x9bf9f0] 0xc000a314a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:22.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:23.126: INFO: rc: 1
May 14 00:44:23.127: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ec330 exit status 1 <nil> <nil> true [0xc002316008 0xc002316020 0xc002316038] [0xc002316008 0xc002316020 0xc002316038] [0xc002316018 0xc002316030] [0x9bf9f0 0x9bf9f0] 0xc002483740 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:33.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:33.297: INFO: rc: 1
May 14 00:44:33.297: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bca4e0 exit status 1 <nil> <nil> true [0xc0021f4010 0xc0021f4038 0xc0021f4060] [0xc0021f4010 0xc0021f4038 0xc0021f4060] [0xc0021f4030 0xc0021f4048] [0x9bf9f0 0x9bf9f0] 0xc000a302a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:43.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:43.471: INFO: rc: 1
May 14 00:44:43.471: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ec6c0 exit status 1 <nil> <nil> true [0xc002316040 0xc002316060 0xc002316078] [0xc002316040 0xc002316060 0xc002316078] [0xc002316058 0xc002316070] [0x9bf9f0 0x9bf9f0] 0xc002646180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:44:53.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:44:53.631: INFO: rc: 1
May 14 00:44:53.631: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bca8a0 exit status 1 <nil> <nil> true [0xc0021f4078 0xc0021f4090 0xc0021f40a8] [0xc0021f4078 0xc0021f4090 0xc0021f40a8] [0xc0021f4088 0xc0021f40a0] [0x9bf9f0 0x9bf9f0] 0xc000a30600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:45:03.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:45:03.773: INFO: rc: 1
May 14 00:45:03.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc001bcac60 exit status 1 <nil> <nil> true [0xc0021f40b8 0xc0021f40e0 0xc0021f40f8] [0xc0021f40b8 0xc0021f40e0 0xc0021f40f8] [0xc0021f40d8 0xc0021f40f0] [0x9bf9f0 0x9bf9f0] 0xc000a30c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:45:13.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:45:13.948: INFO: rc: 1
May 14 00:45:13.948: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ecab0 exit status 1 <nil> <nil> true [0xc002316080 0xc002316098 0xc0023160b0] [0xc002316080 0xc002316098 0xc0023160b0] [0xc002316090 0xc0023160a8] [0x9bf9f0 0x9bf9f0] 0xc0026464e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:45:23.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:45:24.108: INFO: rc: 1
May 14 00:45:24.108: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-1" not found
 [] <nil> 0xc0030ece70 exit status 1 <nil> <nil> true [0xc0023160b8 0xc0023160d0 0xc0023160f0] [0xc0023160b8 0xc0023160d0 0xc0023160f0] [0xc0023160c8 0xc0023160e8] [0x9bf9f0 0x9bf9f0] 0xc002646840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-1" not found

error:
exit status 1

May 14 00:45:34.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-1560 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:45:34.239: INFO: rc: 1
May 14 00:45:34.239: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: 
May 14 00:45:34.239: INFO: Scaling statefulset ss to 0
May 14 00:45:34.252: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 14 00:45:34.256: INFO: Deleting all statefulset in ns statefulset-1560
May 14 00:45:34.260: INFO: Scaling statefulset ss to 0
May 14 00:45:34.278: INFO: Waiting for statefulset status.replicas updated to 0
May 14 00:45:34.284: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:45:34.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1560" for this suite.
May 14 00:45:40.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:45:40.571: INFO: namespace statefulset-1560 deletion completed in 6.235878206s

• [SLOW TEST:365.325 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:45:40.572: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:46:10.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6578" for this suite.
May 14 00:46:16.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:46:16.405: INFO: namespace container-runtime-6578 deletion completed in 6.239895762s

• [SLOW TEST:35.833 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:46:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
May 14 00:46:16.469: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

May 14 00:46:16.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:17.271: INFO: stderr: ""
May 14 00:46:17.271: INFO: stdout: "service/redis-slave created\n"
May 14 00:46:17.271: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

May 14 00:46:17.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:17.923: INFO: stderr: ""
May 14 00:46:17.923: INFO: stdout: "service/redis-master created\n"
May 14 00:46:17.923: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

May 14 00:46:17.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:18.482: INFO: stderr: ""
May 14 00:46:18.482: INFO: stdout: "service/frontend created\n"
May 14 00:46:18.483: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

May 14 00:46:18.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:18.992: INFO: stderr: ""
May 14 00:46:18.992: INFO: stdout: "deployment.apps/frontend created\n"
May 14 00:46:18.992: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

May 14 00:46:18.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:19.578: INFO: stderr: ""
May 14 00:46:19.578: INFO: stdout: "deployment.apps/redis-master created\n"
May 14 00:46:19.578: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

May 14 00:46:19.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-3968'
May 14 00:46:20.111: INFO: stderr: ""
May 14 00:46:20.111: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
May 14 00:46:20.111: INFO: Waiting for all frontend pods to be Running.
May 14 00:46:30.171: INFO: Waiting for frontend to serve content.
May 14 00:46:30.901: INFO: Trying to add a new entry to the guestbook.
May 14 00:46:30.928: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
May 14 00:46:30.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:31.158: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:31.158: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
May 14 00:46:31.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:31.383: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:31.383: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 14 00:46:31.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:31.623: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:31.623: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 14 00:46:31.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:31.856: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:31.857: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
May 14 00:46:31.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:32.222: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:32.222: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
May 14 00:46:32.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-3968'
May 14 00:46:32.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:46:32.985: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:46:32.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3968" for this suite.
May 14 00:47:15.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:47:15.279: INFO: namespace kubectl-3968 deletion completed in 42.280574994s

• [SLOW TEST:58.873 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:47:15.281: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-d1728344-75e1-11e9-8961-767a82fad75b
STEP: Creating secret with name s-test-opt-upd-d1728491-75e1-11e9-8961-767a82fad75b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-d1728344-75e1-11e9-8961-767a82fad75b
STEP: Updating secret s-test-opt-upd-d1728491-75e1-11e9-8961-767a82fad75b
STEP: Creating secret with name s-test-opt-create-d17284e6-75e1-11e9-8961-767a82fad75b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:48:48.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7816" for this suite.
May 14 00:49:12.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:49:12.729: INFO: namespace projected-7816 deletion completed in 24.26564859s

• [SLOW TEST:117.449 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:49:12.731: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 14 00:49:12.808: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:49:18.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2797" for this suite.
May 14 00:49:24.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:49:24.970: INFO: namespace init-container-2797 deletion completed in 6.244967019s

• [SLOW TEST:12.239 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:49:24.971: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
May 14 00:49:25.600: INFO: created pod pod-service-account-defaultsa
May 14 00:49:25.600: INFO: pod pod-service-account-defaultsa service account token volume mount: true
May 14 00:49:25.614: INFO: created pod pod-service-account-mountsa
May 14 00:49:25.614: INFO: pod pod-service-account-mountsa service account token volume mount: true
May 14 00:49:25.646: INFO: created pod pod-service-account-nomountsa
May 14 00:49:25.647: INFO: pod pod-service-account-nomountsa service account token volume mount: false
May 14 00:49:25.676: INFO: created pod pod-service-account-defaultsa-mountspec
May 14 00:49:25.676: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
May 14 00:49:25.737: INFO: created pod pod-service-account-mountsa-mountspec
May 14 00:49:25.737: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
May 14 00:49:25.776: INFO: created pod pod-service-account-nomountsa-mountspec
May 14 00:49:25.776: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
May 14 00:49:25.801: INFO: created pod pod-service-account-defaultsa-nomountspec
May 14 00:49:25.801: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
May 14 00:49:25.846: INFO: created pod pod-service-account-mountsa-nomountspec
May 14 00:49:25.846: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
May 14 00:49:25.883: INFO: created pod pod-service-account-nomountsa-nomountspec
May 14 00:49:25.884: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:49:25.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2684" for this suite.
May 14 00:49:49.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:49:50.239: INFO: namespace svcaccounts-2684 deletion completed in 24.314458315s

• [SLOW TEST:25.269 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:49:50.240: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:49:50.347: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b" in namespace "projected-5025" to be "success or failure"
May 14 00:49:50.352: INFO: Pod "downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.555492ms
May 14 00:49:52.361: INFO: Pod "downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013887852s
May 14 00:49:54.365: INFO: Pod "downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018629098s
STEP: Saw pod success
May 14 00:49:54.366: INFO: Pod "downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:49:54.375: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:49:54.410: INFO: Waiting for pod downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:49:54.427: INFO: Pod downwardapi-volume-2dd1f4cd-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:49:54.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5025" for this suite.
May 14 00:50:00.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:50:00.666: INFO: namespace projected-5025 deletion completed in 6.229044307s

• [SLOW TEST:10.426 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:50:00.668: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:50:04.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6802" for this suite.
May 14 00:50:58.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:50:59.051: INFO: namespace kubelet-test-6802 deletion completed in 54.215789313s

• [SLOW TEST:58.383 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:50:59.052: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0514 00:51:09.432728      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 00:51:09.432: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:51:09.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3246" for this suite.
May 14 00:51:17.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:51:17.922: INFO: namespace gc-3246 deletion completed in 8.474075924s

• [SLOW TEST:18.870 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:51:17.924: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:51:18.163: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b" in namespace "projected-5689" to be "success or failure"
May 14 00:51:18.170: INFO: Pod "downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.472308ms
May 14 00:51:20.206: INFO: Pod "downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04325577s
May 14 00:51:22.215: INFO: Pod "downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051980276s
STEP: Saw pod success
May 14 00:51:22.215: INFO: Pod "downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:51:22.222: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:51:22.286: INFO: Waiting for pod downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:51:22.295: INFO: Pod downwardapi-volume-6228f371-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:51:22.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5689" for this suite.
May 14 00:51:28.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:51:28.541: INFO: namespace projected-5689 deletion completed in 6.236676394s

• [SLOW TEST:10.617 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:51:28.541: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 14 00:51:28.679: INFO: Waiting up to 5m0s for pod "pod-68685027-75e2-11e9-8961-767a82fad75b" in namespace "emptydir-7543" to be "success or failure"
May 14 00:51:28.766: INFO: Pod "pod-68685027-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 87.000584ms
May 14 00:51:30.775: INFO: Pod "pod-68685027-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.095314446s
May 14 00:51:32.780: INFO: Pod "pod-68685027-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.100354891s
STEP: Saw pod success
May 14 00:51:32.780: INFO: Pod "pod-68685027-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:51:32.787: INFO: Trying to get logs from node conformance0 pod pod-68685027-75e2-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:51:32.830: INFO: Waiting for pod pod-68685027-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:51:32.836: INFO: Pod pod-68685027-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:51:32.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7543" for this suite.
May 14 00:51:38.872: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:51:39.135: INFO: namespace emptydir-7543 deletion completed in 6.28828947s

• [SLOW TEST:10.594 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:51:39.136: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5204.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5204.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5204.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5204.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.196.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.196.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.196.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.196.239_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5204.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5204.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5204.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5204.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5204.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5204.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 239.196.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.196.239_udp@PTR;check="$$(dig +tcp +noall +answer +search 239.196.10.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.10.196.239_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 00:51:45.517: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5204.svc.cluster.local from pod dns-5204/dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b: the server could not find the requested resource (get pods dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b)
May 14 00:51:45.532: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local from pod dns-5204/dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b: the server could not find the requested resource (get pods dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b)
May 14 00:51:45.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local from pod dns-5204/dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b: the server could not find the requested resource (get pods dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b)
May 14 00:51:45.724: INFO: Lookups using dns-5204/dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b failed for: [wheezy_tcp@dns-test-service.dns-5204.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5204.svc.cluster.local]

May 14 00:51:50.865: INFO: DNS probes using dns-5204/dns-test-6ec1cc17-75e2-11e9-8961-767a82fad75b succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:51:51.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5204" for this suite.
May 14 00:51:57.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:51:57.607: INFO: namespace dns-5204 deletion completed in 6.225536346s

• [SLOW TEST:18.472 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:51:57.608: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
May 14 00:51:57.685: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-8074" to be "success or failure"
May 14 00:51:57.715: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 29.63125ms
May 14 00:51:59.722: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036960613s
May 14 00:52:01.729: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043747842s
STEP: Saw pod success
May 14 00:52:01.729: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
May 14 00:52:01.735: INFO: Trying to get logs from node conformance0 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
May 14 00:52:01.779: INFO: Waiting for pod pod-host-path-test to disappear
May 14 00:52:01.789: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:52:01.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-8074" for this suite.
May 14 00:52:07.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:52:08.052: INFO: namespace hostpath-8074 deletion completed in 6.259005775s

• [SLOW TEST:10.445 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:52:08.053: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 14 00:52:08.154: INFO: Number of nodes with available pods: 0
May 14 00:52:08.154: INFO: Node conformance0 is running more than one daemon pod
May 14 00:52:09.204: INFO: Number of nodes with available pods: 0
May 14 00:52:09.204: INFO: Node conformance0 is running more than one daemon pod
May 14 00:52:10.164: INFO: Number of nodes with available pods: 0
May 14 00:52:10.164: INFO: Node conformance0 is running more than one daemon pod
May 14 00:52:11.169: INFO: Number of nodes with available pods: 0
May 14 00:52:11.170: INFO: Node conformance0 is running more than one daemon pod
May 14 00:52:12.171: INFO: Number of nodes with available pods: 1
May 14 00:52:12.171: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
May 14 00:52:12.273: INFO: Number of nodes with available pods: 1
May 14 00:52:12.274: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2514, will wait for the garbage collector to delete the pods
May 14 00:52:13.394: INFO: Deleting DaemonSet.extensions daemon-set took: 17.551937ms
May 14 00:52:13.794: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.497438ms
May 14 00:52:24.101: INFO: Number of nodes with available pods: 0
May 14 00:52:24.101: INFO: Number of running nodes: 0, number of available pods: 0
May 14 00:52:24.104: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2514/daemonsets","resourceVersion":"141386"},"items":null}

May 14 00:52:24.109: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2514/pods","resourceVersion":"141386"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:52:24.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2514" for this suite.
May 14 00:52:30.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:52:30.363: INFO: namespace daemonsets-2514 deletion completed in 6.240232324s

• [SLOW TEST:22.310 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:52:30.364: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 14 00:52:30.453: INFO: Waiting up to 5m0s for pod "downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b" in namespace "downward-api-2251" to be "success or failure"
May 14 00:52:30.462: INFO: Pod "downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.418641ms
May 14 00:52:32.469: INFO: Pod "downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015555272s
May 14 00:52:34.475: INFO: Pod "downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021719303s
STEP: Saw pod success
May 14 00:52:34.475: INFO: Pod "downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:52:34.487: INFO: Trying to get logs from node conformance0 pod downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 00:52:34.542: INFO: Waiting for pod downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:52:34.549: INFO: Pod downward-api-8d40df5c-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:52:34.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2251" for this suite.
May 14 00:52:40.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:52:40.846: INFO: namespace downward-api-2251 deletion completed in 6.290498686s

• [SLOW TEST:10.482 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:52:40.853: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
May 14 00:52:41.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-507'
May 14 00:52:42.205: INFO: stderr: ""
May 14 00:52:42.205: INFO: stdout: "pod/pause created\n"
May 14 00:52:42.205: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
May 14 00:52:42.205: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-507" to be "running and ready"
May 14 00:52:42.235: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 29.92446ms
May 14 00:52:44.240: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03479146s
May 14 00:52:46.246: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.040586133s
May 14 00:52:46.246: INFO: Pod "pause" satisfied condition "running and ready"
May 14 00:52:46.246: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
May 14 00:52:46.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 label pods pause testing-label=testing-label-value --namespace=kubectl-507'
May 14 00:52:46.428: INFO: stderr: ""
May 14 00:52:46.428: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
May 14 00:52:46.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pod pause -L testing-label --namespace=kubectl-507'
May 14 00:52:46.585: INFO: stderr: ""
May 14 00:52:46.585: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod
May 14 00:52:46.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 label pods pause testing-label- --namespace=kubectl-507'
May 14 00:52:46.781: INFO: stderr: ""
May 14 00:52:46.781: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
May 14 00:52:46.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pod pause -L testing-label --namespace=kubectl-507'
May 14 00:52:46.937: INFO: stderr: ""
May 14 00:52:46.938: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
May 14 00:52:46.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-507'
May 14 00:52:47.137: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 00:52:47.137: INFO: stdout: "pod \"pause\" force deleted\n"
May 14 00:52:47.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get rc,svc -l name=pause --no-headers --namespace=kubectl-507'
May 14 00:52:47.381: INFO: stderr: "No resources found.\n"
May 14 00:52:47.381: INFO: stdout: ""
May 14 00:52:47.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -l name=pause --namespace=kubectl-507 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 00:52:47.608: INFO: stderr: ""
May 14 00:52:47.608: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:52:47.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-507" for this suite.
May 14 00:52:53.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:52:53.902: INFO: namespace kubectl-507 deletion completed in 6.284871687s

• [SLOW TEST:13.053 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:52:53.905: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-8077
May 14 00:53:00.084: INFO: Started pod liveness-http in namespace container-probe-8077
STEP: checking the pod's current state and verifying that restartCount is present
May 14 00:53:00.089: INFO: Initial restart count of pod liveness-http is 0
May 14 00:53:24.156: INFO: Restart count of pod container-probe-8077/liveness-http is now 1 (24.067070197s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:53:24.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8077" for this suite.
May 14 00:53:30.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:53:30.430: INFO: namespace container-probe-8077 deletion completed in 6.232601935s

• [SLOW TEST:36.525 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:53:30.430: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 14 00:53:38.668: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 00:53:38.674: INFO: Pod pod-with-poststart-http-hook still exists
May 14 00:53:40.675: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 00:53:40.681: INFO: Pod pod-with-poststart-http-hook still exists
May 14 00:53:42.675: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
May 14 00:53:42.680: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:53:42.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2734" for this suite.
May 14 00:54:06.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:54:06.881: INFO: namespace container-lifecycle-hook-2734 deletion completed in 24.194855211s

• [SLOW TEST:36.451 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:54:06.883: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
May 14 00:54:06.940: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-353874590 proxy --unix-socket=/tmp/kubectl-proxy-unix734008425/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:54:07.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9408" for this suite.
May 14 00:54:13.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:54:13.308: INFO: namespace kubectl-9408 deletion completed in 6.236694361s

• [SLOW TEST:6.425 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:54:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:54:13.388: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b" in namespace "downward-api-1724" to be "success or failure"
May 14 00:54:13.401: INFO: Pod "downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.618244ms
May 14 00:54:15.405: INFO: Pod "downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016696567s
May 14 00:54:17.411: INFO: Pod "downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022372128s
STEP: Saw pod success
May 14 00:54:17.411: INFO: Pod "downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:54:17.417: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:54:17.454: INFO: Waiting for pod downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:54:17.488: INFO: Pod downwardapi-volume-ca9b9221-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:54:17.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1724" for this suite.
May 14 00:54:23.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:54:23.716: INFO: namespace downward-api-1724 deletion completed in 6.220096282s

• [SLOW TEST:10.406 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:54:23.717: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 14 00:54:23.818: INFO: Waiting up to 5m0s for pod "downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b" in namespace "downward-api-1756" to be "success or failure"
May 14 00:54:23.826: INFO: Pod "downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.106011ms
May 14 00:54:25.832: INFO: Pod "downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014567062s
May 14 00:54:27.843: INFO: Pod "downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025140142s
STEP: Saw pod success
May 14 00:54:27.843: INFO: Pod "downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:54:27.849: INFO: Trying to get logs from node conformance0 pod downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 00:54:27.931: INFO: Waiting for pod downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b to disappear
May 14 00:54:27.938: INFO: Pod downward-api-d0d2d368-75e2-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:54:27.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1756" for this suite.
May 14 00:54:33.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:54:34.204: INFO: namespace downward-api-1756 deletion completed in 6.257439522s

• [SLOW TEST:10.487 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:54:34.206: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7092
May 14 00:54:38.308: INFO: Started pod liveness-http in namespace container-probe-7092
STEP: checking the pod's current state and verifying that restartCount is present
May 14 00:54:38.315: INFO: Initial restart count of pod liveness-http is 0
May 14 00:54:56.375: INFO: Restart count of pod container-probe-7092/liveness-http is now 1 (18.059365717s elapsed)
May 14 00:55:16.453: INFO: Restart count of pod container-probe-7092/liveness-http is now 2 (38.137551242s elapsed)
May 14 00:55:34.506: INFO: Restart count of pod container-probe-7092/liveness-http is now 3 (56.19073761s elapsed)
May 14 00:55:56.566: INFO: Restart count of pod container-probe-7092/liveness-http is now 4 (1m18.25073608s elapsed)
May 14 00:57:04.909: INFO: Restart count of pod container-probe-7092/liveness-http is now 5 (2m26.594095994s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:57:04.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7092" for this suite.
May 14 00:57:10.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:57:11.147: INFO: namespace container-probe-7092 deletion completed in 6.199756043s

• [SLOW TEST:156.940 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:57:11.147: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:57:11.227: INFO: Pod name cleanup-pod: Found 0 pods out of 1
May 14 00:57:16.232: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 00:57:16.232: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 14 00:57:16.258: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-2323,SelfLink:/apis/apps/v1/namespaces/deployment-2323/deployments/test-cleanup-deployment,UID:379a264c-75e3-11e9-9f2c-ceb99be2323d,ResourceVersion:141731,Generation:1,CreationTimestamp:2019-05-14 00:57:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

May 14 00:57:16.263: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:57:16.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2323" for this suite.
May 14 00:57:24.296: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:57:24.470: INFO: namespace deployment-2323 deletion completed in 8.197812352s

• [SLOW TEST:13.323 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:57:24.471: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-3c8c6e2f-75e3-11e9-8961-767a82fad75b
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:57:24.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2270" for this suite.
May 14 00:57:30.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:57:30.741: INFO: namespace configmap-2270 deletion completed in 6.193735972s

• [SLOW TEST:6.270 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:57:30.742: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:57:30.826: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
May 14 00:57:30.844: INFO: Number of nodes with available pods: 0
May 14 00:57:30.844: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:31.863: INFO: Number of nodes with available pods: 0
May 14 00:57:31.863: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:32.859: INFO: Number of nodes with available pods: 0
May 14 00:57:32.859: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:33.854: INFO: Number of nodes with available pods: 0
May 14 00:57:33.854: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:34.855: INFO: Number of nodes with available pods: 1
May 14 00:57:34.855: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
May 14 00:57:34.905: INFO: Wrong image for pod: daemon-set-w78lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 14 00:57:35.919: INFO: Wrong image for pod: daemon-set-w78lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 14 00:57:36.921: INFO: Wrong image for pod: daemon-set-w78lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 14 00:57:37.919: INFO: Wrong image for pod: daemon-set-w78lc. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
May 14 00:57:37.919: INFO: Pod daemon-set-w78lc is not available
May 14 00:57:38.923: INFO: Pod daemon-set-kxzfb is not available
STEP: Check that daemon pods are still running on every node of the cluster.
May 14 00:57:38.949: INFO: Number of nodes with available pods: 0
May 14 00:57:38.949: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:39.961: INFO: Number of nodes with available pods: 0
May 14 00:57:39.961: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:40.958: INFO: Number of nodes with available pods: 0
May 14 00:57:40.958: INFO: Node conformance0 is running more than one daemon pod
May 14 00:57:41.998: INFO: Number of nodes with available pods: 1
May 14 00:57:41.998: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1636, will wait for the garbage collector to delete the pods
May 14 00:57:42.099: INFO: Deleting DaemonSet.extensions daemon-set took: 13.773095ms
May 14 00:57:42.400: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.381603ms
May 14 00:57:54.144: INFO: Number of nodes with available pods: 0
May 14 00:57:54.144: INFO: Number of running nodes: 0, number of available pods: 0
May 14 00:57:54.150: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1636/daemonsets","resourceVersion":"141814"},"items":null}

May 14 00:57:54.155: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1636/pods","resourceVersion":"141814"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:57:54.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1636" for this suite.
May 14 00:58:00.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:00.374: INFO: namespace daemonsets-1636 deletion completed in 6.187704577s

• [SLOW TEST:29.632 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:58:00.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b" in namespace "downward-api-4858" to be "success or failure"
May 14 00:58:00.488: INFO: Pod "downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.466095ms
May 14 00:58:02.504: INFO: Pod "downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028143646s
May 14 00:58:04.509: INFO: Pod "downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03282552s
STEP: Saw pod success
May 14 00:58:04.509: INFO: Pod "downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:58:04.514: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:58:04.543: INFO: Waiting for pod downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b to disappear
May 14 00:58:04.562: INFO: Pod downwardapi-volume-51f2fe3d-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:04.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4858" for this suite.
May 14 00:58:10.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:10.760: INFO: namespace downward-api-4858 deletion completed in 6.188690631s

• [SLOW TEST:10.385 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:10.761: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 14 00:58:10.895: INFO: Waiting up to 5m0s for pod "pod-582ada46-75e3-11e9-8961-767a82fad75b" in namespace "emptydir-9080" to be "success or failure"
May 14 00:58:10.911: INFO: Pod "pod-582ada46-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.329125ms
May 14 00:58:12.918: INFO: Pod "pod-582ada46-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02257862s
May 14 00:58:14.923: INFO: Pod "pod-582ada46-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027386405s
STEP: Saw pod success
May 14 00:58:14.923: INFO: Pod "pod-582ada46-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:58:14.927: INFO: Trying to get logs from node conformance0 pod pod-582ada46-75e3-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 00:58:14.957: INFO: Waiting for pod pod-582ada46-75e3-11e9-8961-767a82fad75b to disappear
May 14 00:58:14.964: INFO: Pod pod-582ada46-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:14.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9080" for this suite.
May 14 00:58:20.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:21.164: INFO: namespace emptydir-9080 deletion completed in 6.190034563s

• [SLOW TEST:10.403 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:21.165: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 00:58:21.305: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b" in namespace "projected-7636" to be "success or failure"
May 14 00:58:21.319: INFO: Pod "downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.357952ms
May 14 00:58:23.326: INFO: Pod "downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020364544s
May 14 00:58:25.331: INFO: Pod "downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025821102s
STEP: Saw pod success
May 14 00:58:25.331: INFO: Pod "downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:58:25.336: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 00:58:25.388: INFO: Waiting for pod downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b to disappear
May 14 00:58:25.396: INFO: Pod downwardapi-volume-5e5f793d-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:25.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7636" for this suite.
May 14 00:58:31.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:31.695: INFO: namespace projected-7636 deletion completed in 6.295387063s

• [SLOW TEST:10.530 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:31.696: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-649ce971-75e3-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 00:58:31.790: INFO: Waiting up to 5m0s for pod "pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b" in namespace "configmap-3971" to be "success or failure"
May 14 00:58:31.840: INFO: Pod "pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 49.017674ms
May 14 00:58:33.845: INFO: Pod "pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.05462443s
May 14 00:58:35.849: INFO: Pod "pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.058689978s
STEP: Saw pod success
May 14 00:58:35.849: INFO: Pod "pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:58:35.852: INFO: Trying to get logs from node conformance0 pod pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 00:58:35.904: INFO: Waiting for pod pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b to disappear
May 14 00:58:35.909: INFO: Pod pod-configmaps-649dd7dd-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:35.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3971" for this suite.
May 14 00:58:41.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:42.134: INFO: namespace configmap-3971 deletion completed in 6.216332367s

• [SLOW TEST:10.438 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:42.135: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 00:58:42.204: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:43.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7076" for this suite.
May 14 00:58:49.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:49.491: INFO: namespace custom-resource-definition-7076 deletion completed in 6.192958708s

• [SLOW TEST:7.357 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:49.492: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-5942/secret-test-6f38e444-75e3-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 00:58:49.593: INFO: Waiting up to 5m0s for pod "pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b" in namespace "secrets-5942" to be "success or failure"
May 14 00:58:49.609: INFO: Pod "pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.125376ms
May 14 00:58:51.614: INFO: Pod "pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020765295s
May 14 00:58:53.620: INFO: Pod "pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026501583s
STEP: Saw pod success
May 14 00:58:53.620: INFO: Pod "pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 00:58:53.624: INFO: Trying to get logs from node conformance0 pod pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b container env-test: <nil>
STEP: delete the pod
May 14 00:58:53.655: INFO: Waiting for pod pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b to disappear
May 14 00:58:53.660: INFO: Pod pod-configmaps-6f39fb6f-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 00:58:53.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5942" for this suite.
May 14 00:58:59.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 00:58:59.893: INFO: namespace secrets-5942 deletion completed in 6.227129344s

• [SLOW TEST:10.402 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 00:58:59.894: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-674
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 14 00:59:00.163: INFO: Found 0 stateful pods, waiting for 3
May 14 00:59:10.170: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 00:59:10.170: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 00:59:10.171: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
May 14 00:59:10.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-674 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 00:59:10.667: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 00:59:10.667: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 00:59:10.667: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 14 00:59:20.716: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
May 14 00:59:30.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-674 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 00:59:31.290: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 00:59:31.290: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 00:59:31.290: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 00:59:41.320: INFO: Waiting for StatefulSet statefulset-674/ss2 to complete update
May 14 00:59:41.320: INFO: Waiting for Pod statefulset-674/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 00:59:41.320: INFO: Waiting for Pod statefulset-674/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 00:59:41.320: INFO: Waiting for Pod statefulset-674/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 00:59:51.330: INFO: Waiting for StatefulSet statefulset-674/ss2 to complete update
May 14 00:59:51.330: INFO: Waiting for Pod statefulset-674/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
May 14 01:00:01.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-674 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 01:00:01.926: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 01:00:01.926: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 01:00:01.926: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 01:00:02.072: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
May 14 01:00:12.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-674 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 01:00:12.854: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 01:00:12.854: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 01:00:12.854: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 01:00:22.890: INFO: Waiting for StatefulSet statefulset-674/ss2 to complete update
May 14 01:00:22.890: INFO: Waiting for Pod statefulset-674/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 14 01:00:22.890: INFO: Waiting for Pod statefulset-674/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
May 14 01:00:22.890: INFO: Waiting for Pod statefulset-674/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
May 14 01:00:32.900: INFO: Waiting for StatefulSet statefulset-674/ss2 to complete update
May 14 01:00:32.900: INFO: Waiting for Pod statefulset-674/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
May 14 01:00:32.900: INFO: Waiting for Pod statefulset-674/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 14 01:00:42.903: INFO: Deleting all statefulset in ns statefulset-674
May 14 01:00:42.908: INFO: Scaling statefulset ss2 to 0
May 14 01:01:12.940: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:01:12.944: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:01:12.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-674" for this suite.
May 14 01:01:20.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:01:21.184: INFO: namespace statefulset-674 deletion completed in 8.211699087s

• [SLOW TEST:141.290 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:01:21.186: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
May 14 01:01:21.267: INFO: Waiting up to 5m0s for pod "var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b" in namespace "var-expansion-5249" to be "success or failure"
May 14 01:01:21.274: INFO: Pod "var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.98869ms
May 14 01:01:23.280: INFO: Pod "var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012560525s
May 14 01:01:25.286: INFO: Pod "var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018125788s
STEP: Saw pod success
May 14 01:01:25.286: INFO: Pod "var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:01:25.290: INFO: Trying to get logs from node conformance0 pod var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 01:01:25.319: INFO: Waiting for pod var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b to disappear
May 14 01:01:25.330: INFO: Pod var-expansion-c9a48be5-75e3-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:01:25.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5249" for this suite.
May 14 01:01:31.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:01:31.534: INFO: namespace var-expansion-5249 deletion completed in 6.199585466s

• [SLOW TEST:10.348 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:01:31.537: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:01:31.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1140'
May 14 01:01:32.097: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 01:01:32.097: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
May 14 01:01:32.117: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 0 spec.replicas 1 status.replicas 0
May 14 01:01:32.130: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
May 14 01:01:32.151: INFO: scanned /root for discovery docs: <nil>
May 14 01:01:32.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1140'
May 14 01:01:48.167: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 14 01:01:48.167: INFO: stdout: "Created e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3\nScaling up e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
May 14 01:01:48.167: INFO: stdout: "Created e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3\nScaling up e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
May 14 01:01:48.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1140'
May 14 01:01:48.321: INFO: stderr: ""
May 14 01:01:48.321: INFO: stdout: "e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3-wkgn4 "
May 14 01:01:48.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3-wkgn4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1140'
May 14 01:01:48.473: INFO: stderr: ""
May 14 01:01:48.473: INFO: stdout: "true"
May 14 01:01:48.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3-wkgn4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1140'
May 14 01:01:48.619: INFO: stderr: ""
May 14 01:01:48.619: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
May 14 01:01:48.619: INFO: e2e-test-nginx-rc-78dcfe20fb0a615877351dccd44229c3-wkgn4 is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
May 14 01:01:48.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete rc e2e-test-nginx-rc --namespace=kubectl-1140'
May 14 01:01:48.795: INFO: stderr: ""
May 14 01:01:48.795: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:01:48.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1140" for this suite.
May 14 01:01:54.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:01:55.014: INFO: namespace kubectl-1140 deletion completed in 6.212746125s

• [SLOW TEST:23.477 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:01:55.015: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0514 01:02:25.642943      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 01:02:25.643: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:02:25.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-777" for this suite.
May 14 01:02:31.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:02:31.943: INFO: namespace gc-777 deletion completed in 6.296196359s

• [SLOW TEST:36.928 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:02:31.944: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-7908
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-7908
STEP: Deleting pre-stop pod
May 14 01:02:45.166: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:02:45.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7908" for this suite.
May 14 01:03:25.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:03:25.425: INFO: namespace prestop-7908 deletion completed in 40.228859383s

• [SLOW TEST:53.481 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:03:25.427: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:03:25.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-2329'
May 14 01:03:25.717: INFO: stderr: ""
May 14 01:03:25.718: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
May 14 01:03:30.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pod e2e-test-nginx-pod --namespace=kubectl-2329 -o json'
May 14 01:03:30.926: INFO: stderr: ""
May 14 01:03:30.926: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-05-14T01:03:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-2329\",\n        \"resourceVersion\": \"142619\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-2329/pods/e2e-test-nginx-pod\",\n        \"uid\": \"13d10b75-75e4-11e9-9f2c-ceb99be2323d\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-tsc4x\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"conformance0\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-tsc4x\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-tsc4x\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-14T01:03:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-14T01:03:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-14T01:03:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-05-14T01:03:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://0c409b1c61c14c6c2b6afb76031046f55e2737e9f70e1fb2f7f5474d4b907ae1\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b96aeeb1687703c49096f4969358d44f8520b671da94848309a3ba5be5b4c632\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-05-14T01:03:28Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.10.1.212\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.0.193\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-05-14T01:03:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
May 14 01:03:30.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 replace -f - --namespace=kubectl-2329'
May 14 01:03:31.364: INFO: stderr: ""
May 14 01:03:31.364: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
May 14 01:03:31.375: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete pods e2e-test-nginx-pod --namespace=kubectl-2329'
May 14 01:03:34.554: INFO: stderr: ""
May 14 01:03:34.554: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:03:34.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2329" for this suite.
May 14 01:03:40.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:03:40.790: INFO: namespace kubectl-2329 deletion completed in 6.222727798s

• [SLOW TEST:15.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:03:40.790: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
May 14 01:03:40.912: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:03:42.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3190" for this suite.
May 14 01:03:48.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:03:48.721: INFO: namespace replication-controller-3190 deletion completed in 6.620250387s

• [SLOW TEST:7.931 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:03:48.722: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-1390
STEP: creating a selector
STEP: Creating the service pods in kubernetes
May 14 01:03:48.861: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
May 14 01:04:15.060: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.196 8081 | grep -v '^\s*$'] Namespace:pod-network-test-1390 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
May 14 01:04:15.060: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
May 14 01:04:16.406: INFO: Found all expected endpoints: [netserver-0]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:04:16.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1390" for this suite.
May 14 01:04:40.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:04:40.597: INFO: namespace pod-network-test-1390 deletion completed in 24.186259793s

• [SLOW TEST:51.876 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:04:40.598: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-40843f66-75e4-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:04:40.712: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b" in namespace "projected-8575" to be "success or failure"
May 14 01:04:40.730: INFO: Pod "pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 17.445194ms
May 14 01:04:42.735: INFO: Pod "pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02259017s
May 14 01:04:44.741: INFO: Pod "pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028906736s
STEP: Saw pod success
May 14 01:04:44.741: INFO: Pod "pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:04:44.745: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 01:04:44.777: INFO: Waiting for pod pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:04:44.787: INFO: Pod pod-projected-secrets-40853ad6-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:04:44.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8575" for this suite.
May 14 01:04:50.810: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:04:50.993: INFO: namespace projected-8575 deletion completed in 6.199438599s

• [SLOW TEST:10.396 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:04:50.994: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-46b9ea02-75e4-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:04:51.203: INFO: Waiting up to 5m0s for pod "pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b" in namespace "secrets-6332" to be "success or failure"
May 14 01:04:51.214: INFO: Pod "pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.621985ms
May 14 01:04:53.219: INFO: Pod "pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015519877s
May 14 01:04:55.224: INFO: Pod "pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020409524s
STEP: Saw pod success
May 14 01:04:55.224: INFO: Pod "pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:04:55.228: INFO: Trying to get logs from node conformance0 pod pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 01:04:55.259: INFO: Waiting for pod pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:04:55.263: INFO: Pod pod-secrets-46c584bf-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:04:55.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6332" for this suite.
May 14 01:05:01.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:05:01.451: INFO: namespace secrets-6332 deletion completed in 6.183168564s
STEP: Destroying namespace "secret-namespace-2681" for this suite.
May 14 01:05:07.468: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:05:07.614: INFO: namespace secret-namespace-2681 deletion completed in 6.163311671s

• [SLOW TEST:16.620 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:05:07.615: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
May 14 01:05:07.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-7248'
May 14 01:05:08.043: INFO: stderr: ""
May 14 01:05:08.043: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:05:08.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7248'
May 14 01:05:08.273: INFO: stderr: ""
May 14 01:05:08.273: INFO: stdout: "update-demo-nautilus-9rnvh update-demo-nautilus-czhqc "
May 14 01:05:08.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9rnvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:08.444: INFO: stderr: ""
May 14 01:05:08.444: INFO: stdout: ""
May 14 01:05:08.444: INFO: update-demo-nautilus-9rnvh is created but not running
May 14 01:05:13.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7248'
May 14 01:05:13.599: INFO: stderr: ""
May 14 01:05:13.599: INFO: stdout: "update-demo-nautilus-9rnvh update-demo-nautilus-czhqc "
May 14 01:05:13.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9rnvh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:13.754: INFO: stderr: ""
May 14 01:05:13.754: INFO: stdout: "true"
May 14 01:05:13.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9rnvh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:13.956: INFO: stderr: ""
May 14 01:05:13.956: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:05:13.956: INFO: validating pod update-demo-nautilus-9rnvh
May 14 01:05:13.980: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:05:13.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:05:13.980: INFO: update-demo-nautilus-9rnvh is verified up and running
May 14 01:05:13.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-czhqc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:14.141: INFO: stderr: ""
May 14 01:05:14.141: INFO: stdout: "true"
May 14 01:05:14.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-czhqc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:14.291: INFO: stderr: ""
May 14 01:05:14.291: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:05:14.291: INFO: validating pod update-demo-nautilus-czhqc
May 14 01:05:14.299: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:05:14.299: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:05:14.299: INFO: update-demo-nautilus-czhqc is verified up and running
STEP: rolling-update to new replication controller
May 14 01:05:14.304: INFO: scanned /root for discovery docs: <nil>
May 14 01:05:14.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7248'
May 14 01:05:37.137: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
May 14 01:05:37.138: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:05:37.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7248'
May 14 01:05:37.307: INFO: stderr: ""
May 14 01:05:37.307: INFO: stdout: "update-demo-kitten-cvpln update-demo-kitten-d82wc "
May 14 01:05:37.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-kitten-cvpln -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:37.447: INFO: stderr: ""
May 14 01:05:37.447: INFO: stdout: "true"
May 14 01:05:37.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-kitten-cvpln -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:37.587: INFO: stderr: ""
May 14 01:05:37.587: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 14 01:05:37.587: INFO: validating pod update-demo-kitten-cvpln
May 14 01:05:37.609: INFO: got data: {
  "image": "kitten.jpg"
}

May 14 01:05:37.609: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 14 01:05:37.609: INFO: update-demo-kitten-cvpln is verified up and running
May 14 01:05:37.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-kitten-d82wc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:37.751: INFO: stderr: ""
May 14 01:05:37.751: INFO: stdout: "true"
May 14 01:05:37.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-kitten-d82wc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7248'
May 14 01:05:37.889: INFO: stderr: ""
May 14 01:05:37.889: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
May 14 01:05:37.889: INFO: validating pod update-demo-kitten-d82wc
May 14 01:05:37.897: INFO: got data: {
  "image": "kitten.jpg"
}

May 14 01:05:37.897: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
May 14 01:05:37.897: INFO: update-demo-kitten-d82wc is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:05:37.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7248" for this suite.
May 14 01:06:01.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:06:02.118: INFO: namespace kubectl-7248 deletion completed in 24.215319733s

• [SLOW TEST:54.503 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:06:02.119: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5428.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5428.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5428.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5428.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5428.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5428.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 01:06:06.322: INFO: DNS probes using dns-5428/dns-test-712049e0-75e4-11e9-8961-767a82fad75b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:06:06.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5428" for this suite.
May 14 01:06:12.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:06:12.617: INFO: namespace dns-5428 deletion completed in 6.233488176s

• [SLOW TEST:10.499 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:06:12.619: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-7757fbdc-75e4-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:06:12.696: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b" in namespace "projected-6978" to be "success or failure"
May 14 01:06:12.712: INFO: Pod "pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.662264ms
May 14 01:06:14.717: INFO: Pod "pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021238531s
May 14 01:06:16.722: INFO: Pod "pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026386595s
STEP: Saw pod success
May 14 01:06:16.722: INFO: Pod "pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:06:16.727: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 01:06:16.767: INFO: Waiting for pod pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:06:16.773: INFO: Pod pod-projected-secrets-7758f07e-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:06:16.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6978" for this suite.
May 14 01:06:22.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:06:22.974: INFO: namespace projected-6978 deletion completed in 6.194949348s

• [SLOW TEST:10.355 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:06:22.976: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-smfw9 in namespace proxy-3687
I0514 01:06:23.084375      16 runners.go:184] Created replication controller with name: proxy-service-smfw9, namespace: proxy-3687, replica count: 1
I0514 01:06:24.134845      16 runners.go:184] proxy-service-smfw9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 01:06:25.135221      16 runners.go:184] proxy-service-smfw9 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0514 01:06:26.135501      16 runners.go:184] proxy-service-smfw9 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0514 01:06:27.135754      16 runners.go:184] proxy-service-smfw9 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
May 14 01:06:27.159: INFO: setup took 4.110274608s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
May 14 01:06:27.238: INFO: (0) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 76.314521ms)
May 14 01:06:27.238: INFO: (0) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 77.26457ms)
May 14 01:06:27.238: INFO: (0) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 77.025201ms)
May 14 01:06:27.238: INFO: (0) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 78.755563ms)
May 14 01:06:27.238: INFO: (0) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 78.472084ms)
May 14 01:06:27.241: INFO: (0) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 79.917232ms)
May 14 01:06:27.251: INFO: (0) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 91.24923ms)
May 14 01:06:27.255: INFO: (0) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 94.797132ms)
May 14 01:06:27.255: INFO: (0) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 95.088117ms)
May 14 01:06:27.255: INFO: (0) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 96.364715ms)
May 14 01:06:27.255: INFO: (0) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 95.72811ms)
May 14 01:06:27.261: INFO: (0) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 99.699938ms)
May 14 01:06:27.264: INFO: (0) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 104.453279ms)
May 14 01:06:27.272: INFO: (0) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 112.247342ms)
May 14 01:06:27.281: INFO: (0) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 122.065648ms)
May 14 01:06:27.284: INFO: (0) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 122.441806ms)
May 14 01:06:27.294: INFO: (1) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 10.579531ms)
May 14 01:06:27.311: INFO: (1) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 25.654681ms)
May 14 01:06:27.317: INFO: (1) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 32.920272ms)
May 14 01:06:27.319: INFO: (1) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 35.525546ms)
May 14 01:06:27.319: INFO: (1) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 34.360439ms)
May 14 01:06:27.320: INFO: (1) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 35.734913ms)
May 14 01:06:27.342: INFO: (1) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 57.175656ms)
May 14 01:06:27.343: INFO: (1) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 57.952225ms)
May 14 01:06:27.343: INFO: (1) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 58.355855ms)
May 14 01:06:27.343: INFO: (1) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 58.061278ms)
May 14 01:06:27.343: INFO: (1) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 58.942437ms)
May 14 01:06:27.344: INFO: (1) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 58.624701ms)
May 14 01:06:27.354: INFO: (1) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 69.664589ms)
May 14 01:06:27.354: INFO: (1) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 69.05435ms)
May 14 01:06:27.362: INFO: (1) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 76.862832ms)
May 14 01:06:27.362: INFO: (1) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 77.146908ms)
May 14 01:06:27.390: INFO: (2) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 27.427221ms)
May 14 01:06:27.405: INFO: (2) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 43.240219ms)
May 14 01:06:27.409: INFO: (2) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 46.604748ms)
May 14 01:06:27.409: INFO: (2) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 47.198511ms)
May 14 01:06:27.409: INFO: (2) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 47.658252ms)
May 14 01:06:27.409: INFO: (2) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 46.589342ms)
May 14 01:06:27.410: INFO: (2) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 46.735881ms)
May 14 01:06:27.410: INFO: (2) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 46.42298ms)
May 14 01:06:27.410: INFO: (2) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 47.266213ms)
May 14 01:06:27.410: INFO: (2) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 46.667315ms)
May 14 01:06:27.410: INFO: (2) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 47.518111ms)
May 14 01:06:27.414: INFO: (2) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 52.02221ms)
May 14 01:06:27.414: INFO: (2) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 51.523593ms)
May 14 01:06:27.415: INFO: (2) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 52.65931ms)
May 14 01:06:27.415: INFO: (2) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 52.978539ms)
May 14 01:06:27.416: INFO: (2) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 53.433012ms)
May 14 01:06:27.426: INFO: (3) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 9.977642ms)
May 14 01:06:27.431: INFO: (3) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 15.066255ms)
May 14 01:06:27.435: INFO: (3) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 19.287043ms)
May 14 01:06:27.438: INFO: (3) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 21.87051ms)
May 14 01:06:27.438: INFO: (3) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 22.241081ms)
May 14 01:06:27.441: INFO: (3) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 24.640764ms)
May 14 01:06:27.446: INFO: (3) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 28.985185ms)
May 14 01:06:27.450: INFO: (3) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 33.102954ms)
May 14 01:06:27.457: INFO: (3) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 39.575027ms)
May 14 01:06:27.459: INFO: (3) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 42.234848ms)
May 14 01:06:27.460: INFO: (3) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 42.797785ms)
May 14 01:06:27.461: INFO: (3) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 43.871098ms)
May 14 01:06:27.468: INFO: (3) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 49.405064ms)
May 14 01:06:27.468: INFO: (3) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 49.777653ms)
May 14 01:06:27.469: INFO: (3) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 50.791066ms)
May 14 01:06:27.469: INFO: (3) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 51.633698ms)
May 14 01:06:27.477: INFO: (4) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 7.496969ms)
May 14 01:06:27.499: INFO: (4) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 27.81634ms)
May 14 01:06:27.501: INFO: (4) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 30.219633ms)
May 14 01:06:27.501: INFO: (4) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 31.140792ms)
May 14 01:06:27.502: INFO: (4) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 30.665487ms)
May 14 01:06:27.504: INFO: (4) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 32.792844ms)
May 14 01:06:27.504: INFO: (4) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 33.935351ms)
May 14 01:06:27.507: INFO: (4) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 36.468082ms)
May 14 01:06:27.507: INFO: (4) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 37.093595ms)
May 14 01:06:27.507: INFO: (4) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 37.034425ms)
May 14 01:06:27.507: INFO: (4) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 36.813972ms)
May 14 01:06:27.508: INFO: (4) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 38.671988ms)
May 14 01:06:27.509: INFO: (4) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 38.640915ms)
May 14 01:06:27.509: INFO: (4) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 37.801758ms)
May 14 01:06:27.509: INFO: (4) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 38.206293ms)
May 14 01:06:27.510: INFO: (4) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 40.314479ms)
May 14 01:06:27.525: INFO: (5) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 14.93234ms)
May 14 01:06:27.533: INFO: (5) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 22.977896ms)
May 14 01:06:27.535: INFO: (5) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 23.936271ms)
May 14 01:06:27.535: INFO: (5) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 24.20294ms)
May 14 01:06:27.535: INFO: (5) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 25.036053ms)
May 14 01:06:27.536: INFO: (5) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 23.793693ms)
May 14 01:06:27.538: INFO: (5) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 26.602693ms)
May 14 01:06:27.544: INFO: (5) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 32.326719ms)
May 14 01:06:27.544: INFO: (5) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 33.139038ms)
May 14 01:06:27.544: INFO: (5) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 32.275717ms)
May 14 01:06:27.545: INFO: (5) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 32.995254ms)
May 14 01:06:27.545: INFO: (5) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 34.250611ms)
May 14 01:06:27.552: INFO: (5) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 40.682169ms)
May 14 01:06:27.553: INFO: (5) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 41.380266ms)
May 14 01:06:27.554: INFO: (5) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 41.556749ms)
May 14 01:06:27.556: INFO: (5) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 43.956358ms)
May 14 01:06:27.565: INFO: (6) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 8.794475ms)
May 14 01:06:27.573: INFO: (6) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 15.7645ms)
May 14 01:06:27.577: INFO: (6) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 20.101428ms)
May 14 01:06:27.577: INFO: (6) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 19.861254ms)
May 14 01:06:27.578: INFO: (6) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 20.426079ms)
May 14 01:06:27.579: INFO: (6) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 21.637706ms)
May 14 01:06:27.583: INFO: (6) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 26.56922ms)
May 14 01:06:27.588: INFO: (6) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 29.550872ms)
May 14 01:06:27.589: INFO: (6) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 30.8876ms)
May 14 01:06:27.593: INFO: (6) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 35.693284ms)
May 14 01:06:27.593: INFO: (6) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 35.170695ms)
May 14 01:06:27.595: INFO: (6) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 38.877089ms)
May 14 01:06:27.595: INFO: (6) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 37.268754ms)
May 14 01:06:27.595: INFO: (6) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 38.860864ms)
May 14 01:06:27.596: INFO: (6) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 38.746301ms)
May 14 01:06:27.599: INFO: (6) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 41.259291ms)
May 14 01:06:27.617: INFO: (7) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 16.911421ms)
May 14 01:06:27.621: INFO: (7) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 21.158328ms)
May 14 01:06:27.621: INFO: (7) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 21.651588ms)
May 14 01:06:27.622: INFO: (7) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 22.808017ms)
May 14 01:06:27.622: INFO: (7) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 22.085662ms)
May 14 01:06:27.622: INFO: (7) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 21.534444ms)
May 14 01:06:27.625: INFO: (7) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 26.041963ms)
May 14 01:06:27.625: INFO: (7) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 24.869335ms)
May 14 01:06:27.631: INFO: (7) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 31.889298ms)
May 14 01:06:27.632: INFO: (7) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 30.706428ms)
May 14 01:06:27.632: INFO: (7) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 31.532831ms)
May 14 01:06:27.633: INFO: (7) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 32.709136ms)
May 14 01:06:27.633: INFO: (7) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 34.040791ms)
May 14 01:06:27.634: INFO: (7) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 33.436758ms)
May 14 01:06:27.634: INFO: (7) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 33.429732ms)
May 14 01:06:27.634: INFO: (7) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 33.754624ms)
May 14 01:06:27.640: INFO: (8) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 5.911862ms)
May 14 01:06:27.653: INFO: (8) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 18.103106ms)
May 14 01:06:27.653: INFO: (8) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 17.96683ms)
May 14 01:06:27.656: INFO: (8) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 20.399985ms)
May 14 01:06:27.656: INFO: (8) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 21.085165ms)
May 14 01:06:27.658: INFO: (8) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 22.234032ms)
May 14 01:06:27.662: INFO: (8) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 25.904015ms)
May 14 01:06:27.665: INFO: (8) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 29.621707ms)
May 14 01:06:27.665: INFO: (8) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 30.136918ms)
May 14 01:06:27.665: INFO: (8) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 30.033546ms)
May 14 01:06:27.667: INFO: (8) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 31.3263ms)
May 14 01:06:27.671: INFO: (8) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 34.807724ms)
May 14 01:06:27.672: INFO: (8) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 35.525855ms)
May 14 01:06:27.672: INFO: (8) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 36.079907ms)
May 14 01:06:27.673: INFO: (8) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 36.27246ms)
May 14 01:06:27.674: INFO: (8) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 37.329696ms)
May 14 01:06:27.685: INFO: (9) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 10.51205ms)
May 14 01:06:27.688: INFO: (9) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 13.526425ms)
May 14 01:06:27.689: INFO: (9) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 15.076241ms)
May 14 01:06:27.701: INFO: (9) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 25.951973ms)
May 14 01:06:27.703: INFO: (9) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 28.136808ms)
May 14 01:06:27.703: INFO: (9) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 27.930543ms)
May 14 01:06:27.705: INFO: (9) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 30.005036ms)
May 14 01:06:27.708: INFO: (9) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 32.887221ms)
May 14 01:06:27.708: INFO: (9) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 33.537996ms)
May 14 01:06:27.711: INFO: (9) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 36.399718ms)
May 14 01:06:27.714: INFO: (9) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 39.218258ms)
May 14 01:06:27.716: INFO: (9) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 40.388794ms)
May 14 01:06:27.717: INFO: (9) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 41.641239ms)
May 14 01:06:27.720: INFO: (9) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 44.750137ms)
May 14 01:06:27.721: INFO: (9) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 46.275142ms)
May 14 01:06:27.724: INFO: (9) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 49.829634ms)
May 14 01:06:27.732: INFO: (10) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 7.305125ms)
May 14 01:06:27.733: INFO: (10) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 8.679444ms)
May 14 01:06:27.747: INFO: (10) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 21.681914ms)
May 14 01:06:27.748: INFO: (10) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 22.472633ms)
May 14 01:06:27.751: INFO: (10) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 25.285267ms)
May 14 01:06:27.751: INFO: (10) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 25.356898ms)
May 14 01:06:27.753: INFO: (10) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 28.641552ms)
May 14 01:06:27.755: INFO: (10) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 29.947402ms)
May 14 01:06:27.756: INFO: (10) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 30.533319ms)
May 14 01:06:27.757: INFO: (10) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 31.106718ms)
May 14 01:06:27.762: INFO: (10) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 36.222514ms)
May 14 01:06:27.762: INFO: (10) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 37.116746ms)
May 14 01:06:27.763: INFO: (10) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 37.767465ms)
May 14 01:06:27.765: INFO: (10) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 39.72509ms)
May 14 01:06:27.765: INFO: (10) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 39.080877ms)
May 14 01:06:27.765: INFO: (10) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 39.704308ms)
May 14 01:06:27.774: INFO: (11) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 8.968338ms)
May 14 01:06:27.783: INFO: (11) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 18.336918ms)
May 14 01:06:27.791: INFO: (11) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 26.00982ms)
May 14 01:06:27.793: INFO: (11) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 26.794553ms)
May 14 01:06:27.797: INFO: (11) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 30.488935ms)
May 14 01:06:27.797: INFO: (11) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 30.947396ms)
May 14 01:06:27.798: INFO: (11) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 32.831888ms)
May 14 01:06:27.799: INFO: (11) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 33.548356ms)
May 14 01:06:27.803: INFO: (11) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 36.63142ms)
May 14 01:06:27.807: INFO: (11) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 39.725944ms)
May 14 01:06:27.807: INFO: (11) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 40.309496ms)
May 14 01:06:27.808: INFO: (11) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 41.585436ms)
May 14 01:06:27.809: INFO: (11) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 43.725997ms)
May 14 01:06:27.811: INFO: (11) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 43.860757ms)
May 14 01:06:27.811: INFO: (11) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 44.202246ms)
May 14 01:06:27.811: INFO: (11) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 45.281166ms)
May 14 01:06:27.832: INFO: (12) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 21.118524ms)
May 14 01:06:27.833: INFO: (12) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 20.391718ms)
May 14 01:06:27.840: INFO: (12) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 28.158284ms)
May 14 01:06:27.841: INFO: (12) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 28.532026ms)
May 14 01:06:27.841: INFO: (12) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 28.031555ms)
May 14 01:06:27.842: INFO: (12) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 29.759621ms)
May 14 01:06:27.842: INFO: (12) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 30.722408ms)
May 14 01:06:27.843: INFO: (12) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 31.412822ms)
May 14 01:06:27.843: INFO: (12) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 29.62662ms)
May 14 01:06:27.844: INFO: (12) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 32.270294ms)
May 14 01:06:27.845: INFO: (12) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 33.124174ms)
May 14 01:06:27.847: INFO: (12) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 33.558491ms)
May 14 01:06:27.848: INFO: (12) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 35.150802ms)
May 14 01:06:27.848: INFO: (12) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 35.309344ms)
May 14 01:06:27.850: INFO: (12) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 36.266575ms)
May 14 01:06:27.850: INFO: (12) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 37.46545ms)
May 14 01:06:27.870: INFO: (13) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 18.933349ms)
May 14 01:06:27.870: INFO: (13) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 19.46958ms)
May 14 01:06:27.870: INFO: (13) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 19.427149ms)
May 14 01:06:27.871: INFO: (13) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 19.745956ms)
May 14 01:06:27.880: INFO: (13) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 27.801928ms)
May 14 01:06:27.887: INFO: (13) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 36.063414ms)
May 14 01:06:27.887: INFO: (13) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 34.921137ms)
May 14 01:06:27.888: INFO: (13) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 36.457515ms)
May 14 01:06:27.888: INFO: (13) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 36.965182ms)
May 14 01:06:27.889: INFO: (13) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 36.895895ms)
May 14 01:06:27.889: INFO: (13) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 37.350775ms)
May 14 01:06:27.890: INFO: (13) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 39.998858ms)
May 14 01:06:27.891: INFO: (13) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 38.992719ms)
May 14 01:06:27.894: INFO: (13) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 41.850739ms)
May 14 01:06:27.894: INFO: (13) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 41.788665ms)
May 14 01:06:27.894: INFO: (13) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 41.872628ms)
May 14 01:06:27.916: INFO: (14) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 20.911816ms)
May 14 01:06:27.919: INFO: (14) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 22.571307ms)
May 14 01:06:27.921: INFO: (14) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 26.057933ms)
May 14 01:06:27.922: INFO: (14) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 26.848705ms)
May 14 01:06:27.928: INFO: (14) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 31.199593ms)
May 14 01:06:27.934: INFO: (14) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 38.642657ms)
May 14 01:06:27.935: INFO: (14) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 39.16634ms)
May 14 01:06:27.935: INFO: (14) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 39.333019ms)
May 14 01:06:27.936: INFO: (14) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 41.764276ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 41.249514ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 42.317497ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 41.318862ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 40.5637ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 40.924022ms)
May 14 01:06:27.937: INFO: (14) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 40.837786ms)
May 14 01:06:27.938: INFO: (14) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 42.52402ms)
May 14 01:06:27.953: INFO: (15) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 14.066317ms)
May 14 01:06:27.953: INFO: (15) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 14.691309ms)
May 14 01:06:27.957: INFO: (15) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 18.221943ms)
May 14 01:06:27.959: INFO: (15) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 20.700549ms)
May 14 01:06:27.959: INFO: (15) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 20.893098ms)
May 14 01:06:27.959: INFO: (15) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 20.939844ms)
May 14 01:06:27.960: INFO: (15) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 22.079887ms)
May 14 01:06:27.960: INFO: (15) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 21.637353ms)
May 14 01:06:27.961: INFO: (15) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 22.357586ms)
May 14 01:06:27.964: INFO: (15) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 25.32656ms)
May 14 01:06:27.989: INFO: (15) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 50.480106ms)
May 14 01:06:27.990: INFO: (15) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 51.039899ms)
May 14 01:06:27.990: INFO: (15) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 52.251058ms)
May 14 01:06:27.991: INFO: (15) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 52.045863ms)
May 14 01:06:27.991: INFO: (15) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 52.397417ms)
May 14 01:06:27.992: INFO: (15) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 53.501453ms)
May 14 01:06:28.012: INFO: (16) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 19.529135ms)
May 14 01:06:28.029: INFO: (16) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 35.25278ms)
May 14 01:06:28.032: INFO: (16) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 39.501318ms)
May 14 01:06:28.032: INFO: (16) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 38.645539ms)
May 14 01:06:28.034: INFO: (16) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 40.859959ms)
May 14 01:06:28.039: INFO: (16) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 44.386614ms)
May 14 01:06:28.044: INFO: (16) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 50.321816ms)
May 14 01:06:28.044: INFO: (16) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 49.992073ms)
May 14 01:06:28.044: INFO: (16) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 50.84743ms)
May 14 01:06:28.044: INFO: (16) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 50.850188ms)
May 14 01:06:28.044: INFO: (16) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 50.081676ms)
May 14 01:06:28.045: INFO: (16) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 51.500405ms)
May 14 01:06:28.045: INFO: (16) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 50.591969ms)
May 14 01:06:28.045: INFO: (16) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 50.120743ms)
May 14 01:06:28.045: INFO: (16) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 52.41549ms)
May 14 01:06:28.046: INFO: (16) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 51.001106ms)
May 14 01:06:28.061: INFO: (17) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 15.068308ms)
May 14 01:06:28.067: INFO: (17) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 20.326655ms)
May 14 01:06:28.067: INFO: (17) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 21.319585ms)
May 14 01:06:28.068: INFO: (17) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 21.754283ms)
May 14 01:06:28.068: INFO: (17) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 21.808987ms)
May 14 01:06:28.069: INFO: (17) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 22.750208ms)
May 14 01:06:28.069: INFO: (17) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 23.259768ms)
May 14 01:06:28.070: INFO: (17) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 24.828345ms)
May 14 01:06:28.073: INFO: (17) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 26.884016ms)
May 14 01:06:28.073: INFO: (17) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 26.760984ms)
May 14 01:06:28.074: INFO: (17) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 27.660383ms)
May 14 01:06:28.077: INFO: (17) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 30.57029ms)
May 14 01:06:28.078: INFO: (17) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 31.443007ms)
May 14 01:06:28.079: INFO: (17) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 33.01418ms)
May 14 01:06:28.083: INFO: (17) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 36.962335ms)
May 14 01:06:28.083: INFO: (17) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 37.241825ms)
May 14 01:06:28.105: INFO: (18) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 20.745342ms)
May 14 01:06:28.107: INFO: (18) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 23.283065ms)
May 14 01:06:28.108: INFO: (18) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 23.738245ms)
May 14 01:06:28.110: INFO: (18) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 25.289973ms)
May 14 01:06:28.112: INFO: (18) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 28.359092ms)
May 14 01:06:28.114: INFO: (18) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 30.431899ms)
May 14 01:06:28.118: INFO: (18) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 33.467415ms)
May 14 01:06:28.118: INFO: (18) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 33.141164ms)
May 14 01:06:28.119: INFO: (18) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 34.475035ms)
May 14 01:06:28.120: INFO: (18) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 34.824169ms)
May 14 01:06:28.120: INFO: (18) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 35.545814ms)
May 14 01:06:28.121: INFO: (18) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 35.626388ms)
May 14 01:06:28.121: INFO: (18) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 35.568616ms)
May 14 01:06:28.121: INFO: (18) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 36.449277ms)
May 14 01:06:28.121: INFO: (18) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 36.045421ms)
May 14 01:06:28.121: INFO: (18) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 36.378142ms)
May 14 01:06:28.137: INFO: (19) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:1080/proxy/rewriteme">... (200; 14.88394ms)
May 14 01:06:28.143: INFO: (19) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:462/proxy/: tls qux (200; 21.128938ms)
May 14 01:06:28.144: INFO: (19) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw/proxy/rewriteme">test</a> (200; 21.634158ms)
May 14 01:06:28.146: INFO: (19) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:1080/proxy/rewriteme">test<... (200; 23.540111ms)
May 14 01:06:28.147: INFO: (19) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:162/proxy/: bar (200; 24.182851ms)
May 14 01:06:28.149: INFO: (19) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:160/proxy/: foo (200; 26.221949ms)
May 14 01:06:28.152: INFO: (19) /api/v1/namespaces/proxy-3687/pods/proxy-service-smfw9-f8clw:162/proxy/: bar (200; 29.188918ms)
May 14 01:06:28.153: INFO: (19) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:460/proxy/: tls baz (200; 30.012618ms)
May 14 01:06:28.156: INFO: (19) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname1/proxy/: tls baz (200; 33.521278ms)
May 14 01:06:28.157: INFO: (19) /api/v1/namespaces/proxy-3687/services/https:proxy-service-smfw9:tlsportname2/proxy/: tls qux (200; 34.559903ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname2/proxy/: bar (200; 35.642113ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/services/proxy-service-smfw9:portname1/proxy/: foo (200; 35.509458ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/: <a href="/api/v1/namespaces/proxy-3687/pods/https:proxy-service-smfw9-f8clw:443/proxy/tlsrewritem... (200; 34.904738ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname2/proxy/: bar (200; 35.236066ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/services/http:proxy-service-smfw9:portname1/proxy/: foo (200; 35.858556ms)
May 14 01:06:28.158: INFO: (19) /api/v1/namespaces/proxy-3687/pods/http:proxy-service-smfw9-f8clw:160/proxy/: foo (200; 35.296195ms)
STEP: deleting ReplicationController proxy-service-smfw9 in namespace proxy-3687, will wait for the garbage collector to delete the pods
May 14 01:06:28.224: INFO: Deleting ReplicationController proxy-service-smfw9 took: 11.372867ms
May 14 01:06:28.525: INFO: Terminating ReplicationController proxy-service-smfw9 pods took: 300.478013ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:06:30.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3687" for this suite.
May 14 01:06:36.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:06:36.759: INFO: namespace proxy-3687 deletion completed in 6.227654286s

• [SLOW TEST:13.782 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:06:36.759: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
May 14 01:06:36.882: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143079,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 01:06:36.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143080,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 14 01:06:36.882: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143081,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
May 14 01:06:46.927: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143085,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 01:06:46.927: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143086,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
May 14 01:06:46.927: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7293,SelfLink:/api/v1/namespaces/watch-7293/configmaps/e2e-watch-test-label-changed,UID:85c04a2d-75e4-11e9-9f2c-ceb99be2323d,ResourceVersion:143087,Generation:0,CreationTimestamp:2019-05-14 01:06:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:06:46.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7293" for this suite.
May 14 01:06:52.950: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:06:53.172: INFO: namespace watch-7293 deletion completed in 6.240059152s

• [SLOW TEST:16.414 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:06:53.185: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
May 14 01:06:53.268: INFO: Waiting up to 5m0s for pod "pod-8f87157e-75e4-11e9-8961-767a82fad75b" in namespace "emptydir-5471" to be "success or failure"
May 14 01:06:53.308: INFO: Pod "pod-8f87157e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 39.77548ms
May 14 01:06:55.312: INFO: Pod "pod-8f87157e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044272913s
May 14 01:06:57.319: INFO: Pod "pod-8f87157e-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.05141199s
STEP: Saw pod success
May 14 01:06:57.319: INFO: Pod "pod-8f87157e-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:06:57.325: INFO: Trying to get logs from node conformance0 pod pod-8f87157e-75e4-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:06:57.360: INFO: Waiting for pod pod-8f87157e-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:06:57.364: INFO: Pod pod-8f87157e-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:06:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5471" for this suite.
May 14 01:07:03.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:07:03.573: INFO: namespace emptydir-5471 deletion completed in 6.202386832s

• [SLOW TEST:10.389 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:07:03.575: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
May 14 01:07:03.643: INFO: Waiting up to 5m0s for pod "client-containers-95b7937e-75e4-11e9-8961-767a82fad75b" in namespace "containers-4255" to be "success or failure"
May 14 01:07:03.668: INFO: Pod "client-containers-95b7937e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 25.666504ms
May 14 01:07:05.677: INFO: Pod "client-containers-95b7937e-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033952635s
May 14 01:07:07.682: INFO: Pod "client-containers-95b7937e-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039660556s
STEP: Saw pod success
May 14 01:07:07.683: INFO: Pod "client-containers-95b7937e-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:07:07.687: INFO: Trying to get logs from node conformance0 pod client-containers-95b7937e-75e4-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:07:07.723: INFO: Waiting for pod client-containers-95b7937e-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:07:07.731: INFO: Pod client-containers-95b7937e-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:07:07.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4255" for this suite.
May 14 01:07:13.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:07:13.923: INFO: namespace containers-4255 deletion completed in 6.183205811s

• [SLOW TEST:10.348 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:07:13.924: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
May 14 01:07:14.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 --namespace=kubectl-9011 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
May 14 01:07:16.802: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
May 14 01:07:16.802: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:07:18.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9011" for this suite.
May 14 01:07:24.845: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:07:25.012: INFO: namespace kubectl-9011 deletion completed in 6.191824161s

• [SLOW TEST:11.089 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:07:25.014: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 14 01:07:29.637: INFO: Successfully updated pod "annotationupdatea27f12a9-75e4-11e9-8961-767a82fad75b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:07:33.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1290" for this suite.
May 14 01:07:57.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:07:57.866: INFO: namespace downward-api-1290 deletion completed in 24.183035591s

• [SLOW TEST:32.852 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:07:57.867: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-b6135374-75e4-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:07:57.944: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b" in namespace "projected-7254" to be "success or failure"
May 14 01:07:57.978: INFO: Pod "pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 33.548189ms
May 14 01:07:59.986: INFO: Pod "pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041485062s
May 14 01:08:01.991: INFO: Pod "pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047065511s
STEP: Saw pod success
May 14 01:08:01.991: INFO: Pod "pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:08:01.998: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 01:08:02.043: INFO: Waiting for pod pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:08:02.052: INFO: Pod pod-projected-secrets-b6142696-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:08:02.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7254" for this suite.
May 14 01:08:08.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:08:08.397: INFO: namespace projected-7254 deletion completed in 6.339414561s

• [SLOW TEST:10.531 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:08:08.398: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
May 14 01:08:08.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 cluster-info'
May 14 01:08:08.716: INFO: stderr: ""
May 14 01:08:08.716: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.10.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:08:08.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9578" for this suite.
May 14 01:08:14.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:08:14.893: INFO: namespace kubectl-9578 deletion completed in 6.170081516s

• [SLOW TEST:6.495 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:08:14.894: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-3450/configmap-test-c03a6ca9-75e4-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 01:08:14.979: INFO: Waiting up to 5m0s for pod "pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b" in namespace "configmap-3450" to be "success or failure"
May 14 01:08:14.984: INFO: Pod "pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.113611ms
May 14 01:08:16.988: INFO: Pod "pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009346143s
May 14 01:08:18.997: INFO: Pod "pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018213603s
STEP: Saw pod success
May 14 01:08:18.997: INFO: Pod "pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:08:19.005: INFO: Trying to get logs from node conformance0 pod pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b container env-test: <nil>
STEP: delete the pod
May 14 01:08:19.056: INFO: Waiting for pod pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:08:19.067: INFO: Pod pod-configmaps-c03b87fc-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:08:19.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3450" for this suite.
May 14 01:08:25.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:08:25.258: INFO: namespace configmap-3450 deletion completed in 6.183222467s

• [SLOW TEST:10.364 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:08:25.260: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 14 01:08:25.343: INFO: Waiting up to 5m0s for pod "downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b" in namespace "downward-api-872" to be "success or failure"
May 14 01:08:25.374: INFO: Pod "downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.322454ms
May 14 01:08:27.379: INFO: Pod "downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035787579s
May 14 01:08:29.384: INFO: Pod "downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.040591425s
STEP: Saw pod success
May 14 01:08:29.384: INFO: Pod "downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:08:29.389: INFO: Trying to get logs from node conformance0 pod downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 01:08:29.421: INFO: Waiting for pod downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b to disappear
May 14 01:08:29.435: INFO: Pod downward-api-c6672fe9-75e4-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:08:29.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-872" for this suite.
May 14 01:08:35.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:08:35.620: INFO: namespace downward-api-872 deletion completed in 6.178457477s

• [SLOW TEST:10.360 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:08:35.622: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3437
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3437
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3437
May 14 01:08:35.765: INFO: Found 0 stateful pods, waiting for 1
May 14 01:08:45.770: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
May 14 01:08:45.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 01:08:46.235: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 01:08:46.235: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 01:08:46.235: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 01:08:46.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
May 14 01:08:56.249: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:08:56.249: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:08:56.273: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996528s
May 14 01:08:57.279: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.992737688s
May 14 01:08:58.284: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986534777s
May 14 01:08:59.289: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981104104s
May 14 01:09:00.296: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.976042308s
May 14 01:09:01.302: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.968957351s
May 14 01:09:02.307: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.963729066s
May 14 01:09:03.312: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.958872791s
May 14 01:09:04.336: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953568606s
May 14 01:09:05.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 928.995505ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3437
May 14 01:09:06.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 01:09:06.902: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 01:09:06.902: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 01:09:06.902: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 01:09:06.911: INFO: Found 1 stateful pods, waiting for 3
May 14 01:09:16.919: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:09:16.919: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:09:16.919: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
May 14 01:09:16.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 01:09:17.421: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 01:09:17.421: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 01:09:17.421: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 01:09:17.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 01:09:18.135: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 01:09:18.135: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 01:09:18.135: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 01:09:18.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
May 14 01:09:18.730: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
May 14 01:09:18.730: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
May 14 01:09:18.730: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

May 14 01:09:18.730: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:09:18.734: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
May 14 01:09:28.746: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:09:28.746: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:09:28.746: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
May 14 01:09:28.776: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996615s
May 14 01:09:29.784: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989037178s
May 14 01:09:30.804: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980652824s
May 14 01:09:31.813: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96044796s
May 14 01:09:32.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.951771821s
May 14 01:09:33.826: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.944564483s
May 14 01:09:34.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938653479s
May 14 01:09:35.837: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933070539s
May 14 01:09:36.844: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927161124s
May 14 01:09:37.853: INFO: Verifying statefulset ss doesn't scale past 3 for another 920.789228ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3437
May 14 01:09:38.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 01:09:39.309: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 01:09:39.309: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 01:09:39.309: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 01:09:39.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 01:09:40.004: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 01:09:40.004: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 01:09:40.004: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 01:09:40.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 exec --namespace=statefulset-3437 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
May 14 01:09:40.508: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
May 14 01:09:40.508: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
May 14 01:09:40.508: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

May 14 01:09:40.509: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 14 01:10:00.533: INFO: Deleting all statefulset in ns statefulset-3437
May 14 01:10:00.537: INFO: Scaling statefulset ss to 0
May 14 01:10:00.552: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:10:00.556: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:10:00.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3437" for this suite.
May 14 01:10:06.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:10:06.813: INFO: namespace statefulset-3437 deletion completed in 6.215923581s

• [SLOW TEST:91.191 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:10:06.813: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:10:06.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6934'
May 14 01:10:07.048: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 01:10:07.048: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
May 14 01:10:07.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete jobs e2e-test-nginx-job --namespace=kubectl-6934'
May 14 01:10:07.255: INFO: stderr: ""
May 14 01:10:07.255: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:10:07.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6934" for this suite.
May 14 01:10:29.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:10:29.449: INFO: namespace kubectl-6934 deletion completed in 22.187577916s

• [SLOW TEST:22.636 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:10:29.450: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7970.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7970.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
May 14 01:10:35.592: INFO: DNS probes using dns-7970/dns-test-106d818f-75e5-11e9-8961-767a82fad75b succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:10:35.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7970" for this suite.
May 14 01:10:41.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:10:41.850: INFO: namespace dns-7970 deletion completed in 6.221189101s

• [SLOW TEST:12.400 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:10:41.851: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-17dc3a66-75e5-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:10:42.023: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b" in namespace "projected-2638" to be "success or failure"
May 14 01:10:42.037: INFO: Pod "pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 13.581914ms
May 14 01:10:44.045: INFO: Pod "pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021967339s
May 14 01:10:46.050: INFO: Pod "pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026950725s
STEP: Saw pod success
May 14 01:10:46.050: INFO: Pod "pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:10:46.055: INFO: Trying to get logs from node conformance0 pod pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b container projected-secret-volume-test: <nil>
STEP: delete the pod
May 14 01:10:46.089: INFO: Waiting for pod pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:10:46.098: INFO: Pod pod-projected-secrets-17ded6b9-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:10:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2638" for this suite.
May 14 01:10:52.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:10:52.312: INFO: namespace projected-2638 deletion completed in 6.197999698s

• [SLOW TEST:10.461 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:10:52.314: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 14 01:10:52.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-8701'
May 14 01:10:52.893: INFO: stderr: ""
May 14 01:10:52.893: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:10:52.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8701'
May 14 01:10:53.133: INFO: stderr: ""
May 14 01:10:53.133: INFO: stdout: "update-demo-nautilus-jb2qd update-demo-nautilus-mqpzz "
May 14 01:10:53.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-jb2qd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8701'
May 14 01:10:53.312: INFO: stderr: ""
May 14 01:10:53.312: INFO: stdout: ""
May 14 01:10:53.312: INFO: update-demo-nautilus-jb2qd is created but not running
May 14 01:10:58.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8701'
May 14 01:10:58.465: INFO: stderr: ""
May 14 01:10:58.465: INFO: stdout: "update-demo-nautilus-jb2qd update-demo-nautilus-mqpzz "
May 14 01:10:58.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-jb2qd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8701'
May 14 01:10:58.687: INFO: stderr: ""
May 14 01:10:58.687: INFO: stdout: "true"
May 14 01:10:58.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-jb2qd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8701'
May 14 01:10:58.842: INFO: stderr: ""
May 14 01:10:58.843: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:10:58.843: INFO: validating pod update-demo-nautilus-jb2qd
May 14 01:10:58.850: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:10:58.850: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:10:58.850: INFO: update-demo-nautilus-jb2qd is verified up and running
May 14 01:10:58.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-mqpzz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8701'
May 14 01:10:58.996: INFO: stderr: ""
May 14 01:10:58.997: INFO: stdout: "true"
May 14 01:10:58.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-mqpzz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8701'
May 14 01:10:59.153: INFO: stderr: ""
May 14 01:10:59.154: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:10:59.154: INFO: validating pod update-demo-nautilus-mqpzz
May 14 01:10:59.161: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:10:59.161: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:10:59.162: INFO: update-demo-nautilus-mqpzz is verified up and running
STEP: using delete to clean up resources
May 14 01:10:59.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-8701'
May 14 01:10:59.319: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 01:10:59.319: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 14 01:10:59.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8701'
May 14 01:10:59.500: INFO: stderr: "No resources found.\n"
May 14 01:10:59.501: INFO: stdout: ""
May 14 01:10:59.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -l name=update-demo --namespace=kubectl-8701 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 01:10:59.786: INFO: stderr: ""
May 14 01:10:59.786: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:10:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8701" for this suite.
May 14 01:11:05.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:11:06.049: INFO: namespace kubectl-8701 deletion completed in 6.25020473s

• [SLOW TEST:13.736 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:11:06.050: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
May 14 01:11:06.132: INFO: Waiting up to 5m0s for pod "pod-263f17ac-75e5-11e9-8961-767a82fad75b" in namespace "emptydir-6903" to be "success or failure"
May 14 01:11:06.152: INFO: Pod "pod-263f17ac-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 19.907496ms
May 14 01:11:08.158: INFO: Pod "pod-263f17ac-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026156578s
May 14 01:11:10.163: INFO: Pod "pod-263f17ac-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030561377s
STEP: Saw pod success
May 14 01:11:10.163: INFO: Pod "pod-263f17ac-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:11:10.169: INFO: Trying to get logs from node conformance0 pod pod-263f17ac-75e5-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:11:10.243: INFO: Waiting for pod pod-263f17ac-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:11:10.270: INFO: Pod pod-263f17ac-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:11:10.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6903" for this suite.
May 14 01:11:16.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:11:16.560: INFO: namespace emptydir-6903 deletion completed in 6.273273037s

• [SLOW TEST:10.510 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:11:16.561: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 14 01:11:16.639: INFO: Waiting up to 5m0s for pod "pod-2c835dbd-75e5-11e9-8961-767a82fad75b" in namespace "emptydir-5008" to be "success or failure"
May 14 01:11:16.651: INFO: Pod "pod-2c835dbd-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.693988ms
May 14 01:11:18.674: INFO: Pod "pod-2c835dbd-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035024598s
May 14 01:11:20.679: INFO: Pod "pod-2c835dbd-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.03959017s
STEP: Saw pod success
May 14 01:11:20.679: INFO: Pod "pod-2c835dbd-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:11:20.683: INFO: Trying to get logs from node conformance0 pod pod-2c835dbd-75e5-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:11:20.719: INFO: Waiting for pod pod-2c835dbd-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:11:20.732: INFO: Pod pod-2c835dbd-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:11:20.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5008" for this suite.
May 14 01:11:26.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:11:26.968: INFO: namespace emptydir-5008 deletion completed in 6.199174634s

• [SLOW TEST:10.408 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:11:26.970: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:11:27.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9474'
May 14 01:11:27.223: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 01:11:27.223: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
May 14 01:11:27.265: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-54b82]
May 14 01:11:27.265: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-54b82" in namespace "kubectl-9474" to be "running and ready"
May 14 01:11:27.287: INFO: Pod "e2e-test-nginx-rc-54b82": Phase="Pending", Reason="", readiness=false. Elapsed: 21.640608ms
May 14 01:11:29.295: INFO: Pod "e2e-test-nginx-rc-54b82": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029644859s
May 14 01:11:31.300: INFO: Pod "e2e-test-nginx-rc-54b82": Phase="Running", Reason="", readiness=true. Elapsed: 4.034564724s
May 14 01:11:31.300: INFO: Pod "e2e-test-nginx-rc-54b82" satisfied condition "running and ready"
May 14 01:11:31.300: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-54b82]
May 14 01:11:31.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 logs rc/e2e-test-nginx-rc --namespace=kubectl-9474'
May 14 01:11:31.477: INFO: stderr: ""
May 14 01:11:31.477: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
May 14 01:11:31.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete rc e2e-test-nginx-rc --namespace=kubectl-9474'
May 14 01:11:31.667: INFO: stderr: ""
May 14 01:11:31.667: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:11:31.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9474" for this suite.
May 14 01:11:53.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:11:53.881: INFO: namespace kubectl-9474 deletion completed in 22.205253314s

• [SLOW TEST:26.912 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:11:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:11:53.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6121'
May 14 01:11:54.516: INFO: stderr: ""
May 14 01:11:54.516: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
May 14 01:11:54.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete pods e2e-test-nginx-pod --namespace=kubectl-6121'
May 14 01:12:04.008: INFO: stderr: ""
May 14 01:12:04.008: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:12:04.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6121" for this suite.
May 14 01:12:10.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:12:10.227: INFO: namespace kubectl-6121 deletion completed in 6.210906905s

• [SLOW TEST:16.345 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:12:10.227: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
May 14 01:12:10.302: INFO: Waiting up to 5m0s for pod "pod-4c7ee105-75e5-11e9-8961-767a82fad75b" in namespace "emptydir-7963" to be "success or failure"
May 14 01:12:10.333: INFO: Pod "pod-4c7ee105-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 30.668467ms
May 14 01:12:12.340: INFO: Pod "pod-4c7ee105-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03732464s
May 14 01:12:14.345: INFO: Pod "pod-4c7ee105-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042956851s
STEP: Saw pod success
May 14 01:12:14.346: INFO: Pod "pod-4c7ee105-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:12:14.350: INFO: Trying to get logs from node conformance0 pod pod-4c7ee105-75e5-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:12:14.398: INFO: Waiting for pod pod-4c7ee105-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:12:14.404: INFO: Pod pod-4c7ee105-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:12:14.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7963" for this suite.
May 14 01:12:20.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:12:20.602: INFO: namespace emptydir-7963 deletion completed in 6.193681081s

• [SLOW TEST:10.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:12:20.605: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4410
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4410
STEP: Creating statefulset with conflicting port in namespace statefulset-4410
STEP: Waiting until pod test-pod will start running in namespace statefulset-4410
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4410
May 14 01:12:24.780: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 54dafa69-75e5-11e9-9f2c-ceb99be2323d, status phase: Pending. Waiting for statefulset controller to delete.
May 14 01:12:33.984: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 54dafa69-75e5-11e9-9f2c-ceb99be2323d, status phase: Failed. Waiting for statefulset controller to delete.
May 14 01:12:34.055: INFO: Observed stateful pod in namespace: statefulset-4410, name: ss-0, uid: 54dafa69-75e5-11e9-9f2c-ceb99be2323d, status phase: Failed. Waiting for statefulset controller to delete.
May 14 01:12:34.067: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4410
STEP: Removing pod with conflicting port in namespace statefulset-4410
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4410 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 14 01:12:38.174: INFO: Deleting all statefulset in ns statefulset-4410
May 14 01:12:38.179: INFO: Scaling statefulset ss to 0
May 14 01:12:48.203: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:12:48.207: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:12:48.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4410" for this suite.
May 14 01:12:54.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:12:54.442: INFO: namespace statefulset-4410 deletion completed in 6.205006614s

• [SLOW TEST:33.837 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:12:54.443: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
May 14 01:12:54.516: INFO: Waiting up to 5m0s for pod "var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b" in namespace "var-expansion-1168" to be "success or failure"
May 14 01:12:54.524: INFO: Pod "var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.487579ms
May 14 01:12:56.530: INFO: Pod "var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013576581s
May 14 01:12:58.535: INFO: Pod "var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018044054s
STEP: Saw pod success
May 14 01:12:58.535: INFO: Pod "var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:12:58.540: INFO: Trying to get logs from node conformance0 pod var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 01:12:58.584: INFO: Waiting for pod var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:12:58.602: INFO: Pod var-expansion-66da16c2-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:12:58.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1168" for this suite.
May 14 01:13:04.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:13:04.811: INFO: namespace var-expansion-1168 deletion completed in 6.200647957s

• [SLOW TEST:10.368 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:13:04.811: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
May 14 01:13:04.870: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:13:11.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9227" for this suite.
May 14 01:13:34.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:13:34.146: INFO: namespace init-container-9227 deletion completed in 22.168340963s

• [SLOW TEST:29.335 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:13:34.147: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 14 01:13:34.212: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 01:13:34.220: INFO: Waiting for terminating namespaces to be deleted...
May 14 01:13:34.224: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
May 14 01:13:34.243: INFO: kube-flannel-ds-7rgsb from kube-system started at 2019-05-13 18:58:52 +0000 UTC (1 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container kube-flannel ready: true, restart count 0
May 14 01:13:34.243: INFO: sonobuoy-systemd-logs-daemon-set-56580dbecea949d3-c489c from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 14 01:13:34.243: INFO: 	Container systemd-logs ready: true, restart count 1
May 14 01:13:34.243: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 23:58:47 +0000 UTC (1 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 01:13:34.243: INFO: metrics-server-7658bc7d99-v55kv from kube-system started at 2019-05-13 18:59:37 +0000 UTC (1 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container metrics-server ready: true, restart count 0
May 14 01:13:34.243: INFO: sonobuoy-e2e-job-802ffc2c840c4d59 from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container e2e ready: true, restart count 0
May 14 01:13:34.243: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 01:13:34.243: INFO: nirmata-cni-installer-5hwgr from nirmata started at 2019-05-13 18:58:59 +0000 UTC (1 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container install-cni ready: true, restart count 0
May 14 01:13:34.243: INFO: kube-dns-59b9f565f4-ftlkq from kube-system started at 2019-05-13 18:59:04 +0000 UTC (3 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container dnsmasq ready: true, restart count 0
May 14 01:13:34.243: INFO: 	Container kubedns ready: true, restart count 0
May 14 01:13:34.243: INFO: 	Container sidecar ready: true, restart count 0
May 14 01:13:34.243: INFO: nirmata-kube-controller-5b4b6fc6-v5lhm from nirmata started at 2019-05-13 18:59:02 +0000 UTC (1 container statuses recorded)
May 14 01:13:34.243: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.159e6795d26b474a], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:13:35.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8434" for this suite.
May 14 01:13:41.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:13:41.495: INFO: namespace sched-pred-8434 deletion completed in 6.193794502s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:7.348 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:13:41.498: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
May 14 01:13:41.577: INFO: Waiting up to 5m0s for pod "var-expansion-82e66967-75e5-11e9-8961-767a82fad75b" in namespace "var-expansion-3694" to be "success or failure"
May 14 01:13:41.599: INFO: Pod "var-expansion-82e66967-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 22.734126ms
May 14 01:13:43.605: INFO: Pod "var-expansion-82e66967-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028170917s
May 14 01:13:45.610: INFO: Pod "var-expansion-82e66967-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033291943s
STEP: Saw pod success
May 14 01:13:45.610: INFO: Pod "var-expansion-82e66967-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:13:45.620: INFO: Trying to get logs from node conformance0 pod var-expansion-82e66967-75e5-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 01:13:45.652: INFO: Waiting for pod var-expansion-82e66967-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:13:45.666: INFO: Pod var-expansion-82e66967-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:13:45.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3694" for this suite.
May 14 01:13:51.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:13:51.847: INFO: namespace var-expansion-3694 deletion completed in 6.172865496s

• [SLOW TEST:10.350 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:13:51.848: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
May 14 01:13:56.544: INFO: Successfully updated pod "labelsupdate8911dc30-75e5-11e9-8961-767a82fad75b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:14:00.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6611" for this suite.
May 14 01:14:22.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:14:22.899: INFO: namespace downward-api-6611 deletion completed in 22.310642781s

• [SLOW TEST:31.051 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:14:22.900: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-9b9412e1-75e5-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:14:22.984: INFO: Waiting up to 5m0s for pod "pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b" in namespace "secrets-1222" to be "success or failure"
May 14 01:14:22.988: INFO: Pod "pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650535ms
May 14 01:14:24.993: INFO: Pod "pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009454881s
May 14 01:14:26.998: INFO: Pod "pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014307807s
STEP: Saw pod success
May 14 01:14:26.998: INFO: Pod "pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:14:27.003: INFO: Trying to get logs from node conformance0 pod pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 01:14:27.061: INFO: Waiting for pod pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:14:27.068: INFO: Pod pod-secrets-9b9539d5-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:14:27.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1222" for this suite.
May 14 01:14:33.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:14:33.284: INFO: namespace secrets-1222 deletion completed in 6.205922167s

• [SLOW TEST:10.384 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:14:33.286: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
May 14 01:14:33.355: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:14:44.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9349" for this suite.
May 14 01:14:50.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:14:50.220: INFO: namespace pods-9349 deletion completed in 6.199142404s

• [SLOW TEST:16.934 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:14:50.221: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
May 14 01:14:50.342: INFO: Number of nodes with available pods: 0
May 14 01:14:50.343: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:51.355: INFO: Number of nodes with available pods: 0
May 14 01:14:51.355: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:52.353: INFO: Number of nodes with available pods: 0
May 14 01:14:52.353: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:53.356: INFO: Number of nodes with available pods: 1
May 14 01:14:53.356: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Stop a daemon pod, check that the daemon pod is revived.
May 14 01:14:53.395: INFO: Number of nodes with available pods: 0
May 14 01:14:53.395: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:54.403: INFO: Number of nodes with available pods: 0
May 14 01:14:54.403: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:55.404: INFO: Number of nodes with available pods: 0
May 14 01:14:55.404: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:56.405: INFO: Number of nodes with available pods: 0
May 14 01:14:56.405: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:57.406: INFO: Number of nodes with available pods: 0
May 14 01:14:57.406: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:58.407: INFO: Number of nodes with available pods: 0
May 14 01:14:58.407: INFO: Node conformance0 is running more than one daemon pod
May 14 01:14:59.405: INFO: Number of nodes with available pods: 1
May 14 01:14:59.405: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8829, will wait for the garbage collector to delete the pods
May 14 01:14:59.476: INFO: Deleting DaemonSet.extensions daemon-set took: 12.45513ms
May 14 01:14:59.777: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.46343ms
May 14 01:15:14.085: INFO: Number of nodes with available pods: 0
May 14 01:15:14.085: INFO: Number of running nodes: 0, number of available pods: 0
May 14 01:15:14.091: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8829/daemonsets","resourceVersion":"144207"},"items":null}

May 14 01:15:14.096: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8829/pods","resourceVersion":"144207"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:15:14.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8829" for this suite.
May 14 01:15:20.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:15:20.358: INFO: namespace daemonsets-8829 deletion completed in 6.240546323s

• [SLOW TEST:30.137 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:15:20.358: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:15:20.420: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
May 14 01:15:20.449: INFO: Pod name sample-pod: Found 0 pods out of 1
May 14 01:15:25.455: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 01:15:25.455: INFO: Creating deployment "test-rolling-update-deployment"
May 14 01:15:25.463: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
May 14 01:15:25.472: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
May 14 01:15:27.482: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
May 14 01:15:27.487: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693393325, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693393325, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693393325, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693393325, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:15:29.491: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 14 01:15:29.505: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2404,SelfLink:/apis/apps/v1/namespaces/deployment-2404/deployments/test-rolling-update-deployment,UID:c0d3bffa-75e5-11e9-9f2c-ceb99be2323d,ResourceVersion:144275,Generation:1,CreationTimestamp:2019-05-14 01:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-14 01:15:25 +0000 UTC 2019-05-14 01:15:25 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-14 01:15:28 +0000 UTC 2019-05-14 01:15:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 14 01:15:29.511: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2404,SelfLink:/apis/apps/v1/namespaces/deployment-2404/replicasets/test-rolling-update-deployment-67599b4d9,UID:c0d7a435-75e5-11e9-9f2c-ceb99be2323d,ResourceVersion:144264,Generation:1,CreationTimestamp:2019-05-14 01:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c0d3bffa-75e5-11e9-9f2c-ceb99be2323d 0xc002e49e40 0xc002e49e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 14 01:15:29.511: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
May 14 01:15:29.511: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2404,SelfLink:/apis/apps/v1/namespaces/deployment-2404/replicasets/test-rolling-update-controller,UID:bdd404ac-75e5-11e9-9f2c-ceb99be2323d,ResourceVersion:144273,Generation:2,CreationTimestamp:2019-05-14 01:15:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment c0d3bffa-75e5-11e9-9f2c-ceb99be2323d 0xc002e49d77 0xc002e49d78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 01:15:29.517: INFO: Pod "test-rolling-update-deployment-67599b4d9-j45gw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-j45gw,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2404,SelfLink:/api/v1/namespaces/deployment-2404/pods/test-rolling-update-deployment-67599b4d9-j45gw,UID:c0d9aa4f-75e5-11e9-9f2c-ceb99be2323d,ResourceVersion:144263,Generation:0,CreationTimestamp:2019-05-14 01:15:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 c0d7a435-75e5-11e9-9f2c-ceb99be2323d 0xc002f20950 0xc002f20951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pvqj8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pvqj8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pvqj8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f209c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f209e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:15:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:15:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:15:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:15:25 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.236,StartTime:2019-05-14 01:15:25 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-14 01:15:27 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://b2cbeb0e63a68bbe469632f69ca3a9a6b8d50b6d5114bac5670183fe79d8c849}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:15:29.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2404" for this suite.
May 14 01:15:35.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:15:35.758: INFO: namespace deployment-2404 deletion completed in 6.235199987s

• [SLOW TEST:15.400 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:15:35.760: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
May 14 01:15:35.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-3871'
May 14 01:15:36.086: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
May 14 01:15:36.086: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
May 14 01:15:40.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3871'
May 14 01:15:40.369: INFO: stderr: ""
May 14 01:15:40.369: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:15:40.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3871" for this suite.
May 14 01:15:46.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:15:46.643: INFO: namespace kubectl-3871 deletion completed in 6.26105443s

• [SLOW TEST:10.883 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:15:46.643: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 14 01:15:46.755: INFO: Waiting up to 5m0s for pod "pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b" in namespace "emptydir-9579" to be "success or failure"
May 14 01:15:46.764: INFO: Pod "pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.49883ms
May 14 01:15:48.768: INFO: Pod "pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012964951s
May 14 01:15:50.774: INFO: Pod "pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018965985s
STEP: Saw pod success
May 14 01:15:50.775: INFO: Pod "pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:15:50.779: INFO: Trying to get logs from node conformance0 pod pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:15:50.817: INFO: Waiting for pod pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:15:50.843: INFO: Pod pod-cd7f1b4a-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:15:50.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9579" for this suite.
May 14 01:15:56.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:15:57.031: INFO: namespace emptydir-9579 deletion completed in 6.180487195s

• [SLOW TEST:10.388 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:15:57.032: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 01:15:57.121: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b" in namespace "downward-api-6952" to be "success or failure"
May 14 01:15:57.134: INFO: Pod "downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.707827ms
May 14 01:15:59.139: INFO: Pod "downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017486127s
May 14 01:16:01.146: INFO: Pod "downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0243905s
STEP: Saw pod success
May 14 01:16:01.146: INFO: Pod "downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:16:01.151: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 01:16:01.199: INFO: Waiting for pod downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:16:01.206: INFO: Pod downwardapi-volume-d3b0b3eb-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:16:01.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6952" for this suite.
May 14 01:16:07.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:16:07.426: INFO: namespace downward-api-6952 deletion completed in 6.214191971s

• [SLOW TEST:10.395 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:16:07.427: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d9e02d9d-75e5-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume secrets
May 14 01:16:07.502: INFO: Waiting up to 5m0s for pod "pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b" in namespace "secrets-8211" to be "success or failure"
May 14 01:16:07.526: INFO: Pod "pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 24.205779ms
May 14 01:16:09.531: INFO: Pod "pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02879926s
May 14 01:16:11.536: INFO: Pod "pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033770745s
STEP: Saw pod success
May 14 01:16:11.536: INFO: Pod "pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:16:11.540: INFO: Trying to get logs from node conformance0 pod pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b container secret-volume-test: <nil>
STEP: delete the pod
May 14 01:16:11.568: INFO: Waiting for pod pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:16:11.592: INFO: Pod pod-secrets-d9e12148-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:16:11.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8211" for this suite.
May 14 01:16:17.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:16:17.805: INFO: namespace secrets-8211 deletion completed in 6.207657324s

• [SLOW TEST:10.379 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:16:17.806: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:16:44.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4366" for this suite.
May 14 01:16:50.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:16:50.265: INFO: namespace namespaces-4366 deletion completed in 6.205578984s
STEP: Destroying namespace "nsdeletetest-4415" for this suite.
May 14 01:16:50.270: INFO: Namespace nsdeletetest-4415 was already deleted
STEP: Destroying namespace "nsdeletetest-4868" for this suite.
May 14 01:16:56.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:16:56.463: INFO: namespace nsdeletetest-4868 deletion completed in 6.192268979s

• [SLOW TEST:38.657 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:16:56.463: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 01:16:56.546: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b" in namespace "projected-5923" to be "success or failure"
May 14 01:16:56.592: INFO: Pod "downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 46.03365ms
May 14 01:16:58.598: INFO: Pod "downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051949771s
May 14 01:17:00.603: INFO: Pod "downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.056617336s
STEP: Saw pod success
May 14 01:17:00.603: INFO: Pod "downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:17:00.608: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 01:17:00.640: INFO: Waiting for pod downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b to disappear
May 14 01:17:00.647: INFO: Pod downwardapi-volume-f71c0ee4-75e5-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:17:00.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5923" for this suite.
May 14 01:17:06.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:17:06.838: INFO: namespace projected-5923 deletion completed in 6.183565343s

• [SLOW TEST:10.374 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:17:06.838: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
May 14 01:17:06.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-309'
May 14 01:17:07.316: INFO: stderr: ""
May 14 01:17:07.316: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:17:07.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:07.598: INFO: stderr: ""
May 14 01:17:07.598: INFO: stdout: "update-demo-nautilus-9fpkt update-demo-nautilus-vmw2x "
May 14 01:17:07.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:07.819: INFO: stderr: ""
May 14 01:17:07.819: INFO: stdout: ""
May 14 01:17:07.819: INFO: update-demo-nautilus-9fpkt is created but not running
May 14 01:17:12.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:12.958: INFO: stderr: ""
May 14 01:17:12.958: INFO: stdout: "update-demo-nautilus-9fpkt update-demo-nautilus-vmw2x "
May 14 01:17:12.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:13.109: INFO: stderr: ""
May 14 01:17:13.109: INFO: stdout: "true"
May 14 01:17:13.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:13.252: INFO: stderr: ""
May 14 01:17:13.252: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:13.252: INFO: validating pod update-demo-nautilus-9fpkt
May 14 01:17:13.260: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:13.260: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:13.260: INFO: update-demo-nautilus-9fpkt is verified up and running
May 14 01:17:13.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-vmw2x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:13.448: INFO: stderr: ""
May 14 01:17:13.448: INFO: stdout: "true"
May 14 01:17:13.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-vmw2x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:13.697: INFO: stderr: ""
May 14 01:17:13.698: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:13.698: INFO: validating pod update-demo-nautilus-vmw2x
May 14 01:17:13.705: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:13.705: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:13.705: INFO: update-demo-nautilus-vmw2x is verified up and running
STEP: scaling down the replication controller
May 14 01:17:13.711: INFO: scanned /root for discovery docs: <nil>
May 14 01:17:13.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-309'
May 14 01:17:14.992: INFO: stderr: ""
May 14 01:17:14.992: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:17:14.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:15.134: INFO: stderr: ""
May 14 01:17:15.134: INFO: stdout: "update-demo-nautilus-9fpkt update-demo-nautilus-vmw2x "
STEP: Replicas for name=update-demo: expected=1 actual=2
May 14 01:17:20.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:20.289: INFO: stderr: ""
May 14 01:17:20.289: INFO: stdout: "update-demo-nautilus-9fpkt "
May 14 01:17:20.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:20.437: INFO: stderr: ""
May 14 01:17:20.438: INFO: stdout: "true"
May 14 01:17:20.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:20.602: INFO: stderr: ""
May 14 01:17:20.602: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:20.602: INFO: validating pod update-demo-nautilus-9fpkt
May 14 01:17:20.608: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:20.608: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:20.608: INFO: update-demo-nautilus-9fpkt is verified up and running
STEP: scaling up the replication controller
May 14 01:17:20.614: INFO: scanned /root for discovery docs: <nil>
May 14 01:17:20.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-309'
May 14 01:17:21.891: INFO: stderr: ""
May 14 01:17:21.891: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
May 14 01:17:21.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:22.096: INFO: stderr: ""
May 14 01:17:22.096: INFO: stdout: "update-demo-nautilus-9fpkt update-demo-nautilus-mj6sw "
May 14 01:17:22.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:22.290: INFO: stderr: ""
May 14 01:17:22.290: INFO: stdout: "true"
May 14 01:17:22.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:22.512: INFO: stderr: ""
May 14 01:17:22.512: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:22.512: INFO: validating pod update-demo-nautilus-9fpkt
May 14 01:17:22.519: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:22.519: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:22.519: INFO: update-demo-nautilus-9fpkt is verified up and running
May 14 01:17:22.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-mj6sw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:22.693: INFO: stderr: ""
May 14 01:17:22.693: INFO: stdout: ""
May 14 01:17:22.693: INFO: update-demo-nautilus-mj6sw is created but not running
May 14 01:17:27.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-309'
May 14 01:17:27.858: INFO: stderr: ""
May 14 01:17:27.859: INFO: stdout: "update-demo-nautilus-9fpkt update-demo-nautilus-mj6sw "
May 14 01:17:27.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:28.024: INFO: stderr: ""
May 14 01:17:28.024: INFO: stdout: "true"
May 14 01:17:28.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-9fpkt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:28.164: INFO: stderr: ""
May 14 01:17:28.164: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:28.164: INFO: validating pod update-demo-nautilus-9fpkt
May 14 01:17:28.171: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:28.171: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:28.171: INFO: update-demo-nautilus-9fpkt is verified up and running
May 14 01:17:28.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-mj6sw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:28.323: INFO: stderr: ""
May 14 01:17:28.323: INFO: stdout: "true"
May 14 01:17:28.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods update-demo-nautilus-mj6sw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-309'
May 14 01:17:28.464: INFO: stderr: ""
May 14 01:17:28.464: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
May 14 01:17:28.464: INFO: validating pod update-demo-nautilus-mj6sw
May 14 01:17:28.472: INFO: got data: {
  "image": "nautilus.jpg"
}

May 14 01:17:28.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
May 14 01:17:28.472: INFO: update-demo-nautilus-mj6sw is verified up and running
STEP: using delete to clean up resources
May 14 01:17:28.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 delete --grace-period=0 --force -f - --namespace=kubectl-309'
May 14 01:17:28.621: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
May 14 01:17:28.621: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
May 14 01:17:28.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-309'
May 14 01:17:28.786: INFO: stderr: "No resources found.\n"
May 14 01:17:28.786: INFO: stdout: ""
May 14 01:17:28.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 get pods -l name=update-demo --namespace=kubectl-309 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
May 14 01:17:29.016: INFO: stderr: ""
May 14 01:17:29.016: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:17:29.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-309" for this suite.
May 14 01:17:35.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:17:35.214: INFO: namespace kubectl-309 deletion completed in 6.19021965s

• [SLOW TEST:28.376 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:17:35.215: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0514 01:17:36.448208      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 01:17:36.448: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:17:36.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7519" for this suite.
May 14 01:17:42.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:17:42.647: INFO: namespace gc-7519 deletion completed in 6.185676461s

• [SLOW TEST:7.432 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:17:42.647: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:17:42.728: INFO: (0) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.432661ms)
May 14 01:17:42.733: INFO: (1) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 5.571252ms)
May 14 01:17:42.740: INFO: (2) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.098393ms)
May 14 01:17:42.746: INFO: (3) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.383348ms)
May 14 01:17:42.753: INFO: (4) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.55116ms)
May 14 01:17:42.760: INFO: (5) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.201978ms)
May 14 01:17:42.770: INFO: (6) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.286955ms)
May 14 01:17:42.777: INFO: (7) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.792609ms)
May 14 01:17:42.786: INFO: (8) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.915544ms)
May 14 01:17:42.796: INFO: (9) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.988825ms)
May 14 01:17:42.805: INFO: (10) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.884751ms)
May 14 01:17:42.812: INFO: (11) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.513445ms)
May 14 01:17:42.822: INFO: (12) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.46387ms)
May 14 01:17:42.833: INFO: (13) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 11.040432ms)
May 14 01:17:42.840: INFO: (14) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.83569ms)
May 14 01:17:42.847: INFO: (15) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.038984ms)
May 14 01:17:42.858: INFO: (16) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 10.543173ms)
May 14 01:17:42.864: INFO: (17) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.166545ms)
May 14 01:17:42.870: INFO: (18) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.082373ms)
May 14 01:17:42.878: INFO: (19) /api/v1/nodes/conformance0:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.048235ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:17:42.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2112" for this suite.
May 14 01:17:48.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:17:49.090: INFO: namespace proxy-2112 deletion completed in 6.204873188s

• [SLOW TEST:6.443 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:17:49.091: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
May 14 01:17:49.161: INFO: Waiting up to 5m0s for pod "client-containers-1678cec5-75e6-11e9-8961-767a82fad75b" in namespace "containers-3984" to be "success or failure"
May 14 01:17:49.182: INFO: Pod "client-containers-1678cec5-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 20.347091ms
May 14 01:17:51.187: INFO: Pod "client-containers-1678cec5-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025616146s
May 14 01:17:53.192: INFO: Pod "client-containers-1678cec5-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030272093s
STEP: Saw pod success
May 14 01:17:53.192: INFO: Pod "client-containers-1678cec5-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:17:53.196: INFO: Trying to get logs from node conformance0 pod client-containers-1678cec5-75e6-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:17:53.231: INFO: Waiting for pod client-containers-1678cec5-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:17:53.235: INFO: Pod client-containers-1678cec5-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:17:53.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3984" for this suite.
May 14 01:17:59.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:17:59.449: INFO: namespace containers-3984 deletion completed in 6.200880098s

• [SLOW TEST:10.358 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:17:59.450: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0514 01:18:39.561959      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 01:18:39.562: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:18:39.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5596" for this suite.
May 14 01:18:47.586: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:18:47.822: INFO: namespace gc-5596 deletion completed in 8.254821645s

• [SLOW TEST:48.372 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:18:47.823: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-397ff9c8-75e6-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 01:18:47.962: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b" in namespace "projected-7297" to be "success or failure"
May 14 01:18:47.999: INFO: Pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 36.516685ms
May 14 01:18:50.003: INFO: Pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040714237s
May 14 01:18:52.008: INFO: Pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.045554913s
May 14 01:18:54.018: INFO: Pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.055489981s
STEP: Saw pod success
May 14 01:18:54.018: INFO: Pod "pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:18:54.022: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 01:18:54.060: INFO: Waiting for pod pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:18:54.065: INFO: Pod pod-projected-configmaps-39824f55-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:18:54.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7297" for this suite.
May 14 01:19:00.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:19:00.251: INFO: namespace projected-7297 deletion completed in 6.179502857s

• [SLOW TEST:12.428 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:19:00.252: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:19:00.443: INFO: Conformance test suite needs a cluster with at least 2 nodes.
May 14 01:19:00.443: INFO: Create a RollingUpdate DaemonSet
May 14 01:19:00.454: INFO: Check that daemon pods launch on every node of the cluster
May 14 01:19:00.483: INFO: Number of nodes with available pods: 0
May 14 01:19:00.483: INFO: Node conformance0 is running more than one daemon pod
May 14 01:19:01.499: INFO: Number of nodes with available pods: 0
May 14 01:19:01.499: INFO: Node conformance0 is running more than one daemon pod
May 14 01:19:02.493: INFO: Number of nodes with available pods: 0
May 14 01:19:02.493: INFO: Node conformance0 is running more than one daemon pod
May 14 01:19:03.492: INFO: Number of nodes with available pods: 1
May 14 01:19:03.492: INFO: Number of running nodes: 1, number of available pods: 1
May 14 01:19:03.492: INFO: Update the DaemonSet to trigger a rollout
May 14 01:19:03.509: INFO: Updating DaemonSet daemon-set
May 14 01:19:14.529: INFO: Roll back the DaemonSet before rollout is complete
May 14 01:19:14.539: INFO: Updating DaemonSet daemon-set
May 14 01:19:14.539: INFO: Make sure DaemonSet rollback is complete
May 14 01:19:14.545: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:14.545: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:15.562: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:15.562: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:16.562: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:16.562: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:17.563: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:17.563: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:18.562: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:18.562: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:19.563: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:19.563: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:20.563: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:20.563: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:21.562: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:21.563: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:22.562: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:22.562: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:23.563: INFO: Wrong image for pod: daemon-set-jjv9n. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
May 14 01:19:23.563: INFO: Pod daemon-set-jjv9n is not available
May 14 01:19:24.565: INFO: Pod daemon-set-qhstt is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4729, will wait for the garbage collector to delete the pods
May 14 01:19:24.674: INFO: Deleting DaemonSet.extensions daemon-set took: 22.4136ms
May 14 01:19:24.974: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.430596ms
May 14 01:19:26.579: INFO: Number of nodes with available pods: 0
May 14 01:19:26.579: INFO: Number of running nodes: 0, number of available pods: 0
May 14 01:19:26.592: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4729/daemonsets","resourceVersion":"144964"},"items":null}

May 14 01:19:26.609: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4729/pods","resourceVersion":"144964"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:19:26.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4729" for this suite.
May 14 01:19:32.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:19:32.932: INFO: namespace daemonsets-4729 deletion completed in 6.263731552s

• [SLOW TEST:32.681 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:19:32.933: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-545f64ac-75e6-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 01:19:33.030: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b" in namespace "projected-6579" to be "success or failure"
May 14 01:19:33.040: INFO: Pod "pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.599281ms
May 14 01:19:35.045: INFO: Pod "pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011967475s
May 14 01:19:37.050: INFO: Pod "pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016288853s
STEP: Saw pod success
May 14 01:19:37.050: INFO: Pod "pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:19:37.055: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 01:19:37.090: INFO: Waiting for pod pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:19:37.096: INFO: Pod pod-projected-configmaps-5460ef3b-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:19:37.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6579" for this suite.
May 14 01:19:43.122: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:19:43.277: INFO: namespace projected-6579 deletion completed in 6.170699816s

• [SLOW TEST:10.345 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:19:43.279: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-5a889e1a-75e6-11e9-8961-767a82fad75b
STEP: Creating secret with name secret-projected-all-test-volume-5a889dcf-75e6-11e9-8961-767a82fad75b
STEP: Creating a pod to test Check all projections for projected volume plugin
May 14 01:19:43.388: INFO: Waiting up to 5m0s for pod "projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b" in namespace "projected-5406" to be "success or failure"
May 14 01:19:43.417: INFO: Pod "projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 28.899937ms
May 14 01:19:45.423: INFO: Pod "projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035153833s
May 14 01:19:47.429: INFO: Pod "projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.041273651s
STEP: Saw pod success
May 14 01:19:47.429: INFO: Pod "projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:19:47.434: INFO: Trying to get logs from node conformance0 pod projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b container projected-all-volume-test: <nil>
STEP: delete the pod
May 14 01:19:47.472: INFO: Waiting for pod projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:19:47.489: INFO: Pod projected-volume-5a889d4c-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:19:47.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5406" for this suite.
May 14 01:19:53.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:19:53.703: INFO: namespace projected-5406 deletion completed in 6.207257755s

• [SLOW TEST:10.424 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:19:53.704: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-3803
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3803 to expose endpoints map[]
May 14 01:19:53.788: INFO: successfully validated that service endpoint-test2 in namespace services-3803 exposes endpoints map[] (6.822729ms elapsed)
STEP: Creating pod pod1 in namespace services-3803
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3803 to expose endpoints map[pod1:[80]]
May 14 01:19:56.847: INFO: successfully validated that service endpoint-test2 in namespace services-3803 exposes endpoints map[pod1:[80]] (3.043558247s elapsed)
STEP: Creating pod pod2 in namespace services-3803
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3803 to expose endpoints map[pod1:[80] pod2:[80]]
May 14 01:20:00.012: INFO: successfully validated that service endpoint-test2 in namespace services-3803 exposes endpoints map[pod1:[80] pod2:[80]] (3.148904053s elapsed)
STEP: Deleting pod pod1 in namespace services-3803
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3803 to expose endpoints map[pod2:[80]]
May 14 01:20:00.078: INFO: successfully validated that service endpoint-test2 in namespace services-3803 exposes endpoints map[pod2:[80]] (40.931359ms elapsed)
STEP: Deleting pod pod2 in namespace services-3803
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3803 to expose endpoints map[]
May 14 01:20:01.118: INFO: successfully validated that service endpoint-test2 in namespace services-3803 exposes endpoints map[] (1.019162547s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:20:01.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3803" for this suite.
May 14 01:20:23.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:20:23.331: INFO: namespace services-3803 deletion completed in 22.176611328s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

• [SLOW TEST:29.628 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:20:23.332: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
May 14 01:20:28.019: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8170 pod-service-account-72b9c55d-75e6-11e9-8961-767a82fad75b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
May 14 01:20:28.560: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8170 pod-service-account-72b9c55d-75e6-11e9-8961-767a82fad75b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
May 14 01:20:29.029: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8170 pod-service-account-72b9c55d-75e6-11e9-8961-767a82fad75b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:20:29.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8170" for this suite.
May 14 01:20:35.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:20:35.699: INFO: namespace svcaccounts-8170 deletion completed in 6.189781632s

• [SLOW TEST:12.367 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:20:35.699: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-79c8a08c-75e6-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 01:20:35.779: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b" in namespace "projected-7930" to be "success or failure"
May 14 01:20:35.821: INFO: Pod "pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 41.312573ms
May 14 01:20:37.828: INFO: Pod "pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048896193s
May 14 01:20:39.834: INFO: Pod "pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054536176s
STEP: Saw pod success
May 14 01:20:39.834: INFO: Pod "pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:20:39.838: INFO: Trying to get logs from node conformance0 pod pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b container projected-configmap-volume-test: <nil>
STEP: delete the pod
May 14 01:20:39.875: INFO: Waiting for pod pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:20:39.882: INFO: Pod pod-projected-configmaps-79c9a073-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:20:39.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7930" for this suite.
May 14 01:20:45.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:20:46.219: INFO: namespace projected-7930 deletion completed in 6.331471969s

• [SLOW TEST:10.520 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:20:46.219: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 14 01:20:46.273: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 01:20:46.286: INFO: Waiting for terminating namespaces to be deleted...
May 14 01:20:46.291: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
May 14 01:20:46.301: INFO: sonobuoy-systemd-logs-daemon-set-56580dbecea949d3-c489c from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:20:46.301: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 14 01:20:46.301: INFO: 	Container systemd-logs ready: true, restart count 1
May 14 01:20:46.301: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 23:58:47 +0000 UTC (1 container statuses recorded)
May 14 01:20:46.301: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 01:20:46.301: INFO: sonobuoy-e2e-job-802ffc2c840c4d59 from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:20:46.301: INFO: 	Container e2e ready: true, restart count 0
May 14 01:20:46.301: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 01:20:46.302: INFO: nirmata-cni-installer-5hwgr from nirmata started at 2019-05-13 18:58:59 +0000 UTC (1 container statuses recorded)
May 14 01:20:46.302: INFO: 	Container install-cni ready: true, restart count 0
May 14 01:20:46.302: INFO: kube-dns-59b9f565f4-ftlkq from kube-system started at 2019-05-13 18:59:04 +0000 UTC (3 container statuses recorded)
May 14 01:20:46.302: INFO: 	Container dnsmasq ready: true, restart count 0
May 14 01:20:46.302: INFO: 	Container kubedns ready: true, restart count 0
May 14 01:20:46.302: INFO: 	Container sidecar ready: true, restart count 0
May 14 01:20:46.302: INFO: metrics-server-7658bc7d99-v55kv from kube-system started at 2019-05-13 18:59:37 +0000 UTC (1 container statuses recorded)
May 14 01:20:46.302: INFO: 	Container metrics-server ready: true, restart count 0
May 14 01:20:46.302: INFO: nirmata-kube-controller-5b4b6fc6-v5lhm from nirmata started at 2019-05-13 18:59:02 +0000 UTC (1 container statuses recorded)
May 14 01:20:46.302: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
May 14 01:20:46.302: INFO: kube-flannel-ds-7rgsb from kube-system started at 2019-05-13 18:58:52 +0000 UTC (1 container statuses recorded)
May 14 01:20:46.302: INFO: 	Container kube-flannel ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node conformance0
May 14 01:20:46.348: INFO: Pod sonobuoy requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod sonobuoy-e2e-job-802ffc2c840c4d59 requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod sonobuoy-systemd-logs-daemon-set-56580dbecea949d3-c489c requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod kube-dns-59b9f565f4-ftlkq requesting resource cpu=260m on Node conformance0
May 14 01:20:46.348: INFO: Pod kube-flannel-ds-7rgsb requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod metrics-server-7658bc7d99-v55kv requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod nirmata-cni-installer-5hwgr requesting resource cpu=0m on Node conformance0
May 14 01:20:46.348: INFO: Pod nirmata-kube-controller-5b4b6fc6-v5lhm requesting resource cpu=0m on Node conformance0
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b.159e67fa6cfb8178], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4506/filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b to conformance0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b.159e67fab2faaf26], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b.159e67fab8d33282], Reason = [Created], Message = [Created container filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b.159e67fad146f5ef], Reason = [Started], Message = [Started container filler-pod-8017d67e-75e6-11e9-8961-767a82fad75b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.159e67fb5db1508b], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 Insufficient cpu.]
STEP: removing the label node off the node conformance0
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:20:51.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4506" for this suite.
May 14 01:20:57.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:20:57.648: INFO: namespace sched-pred-4506 deletion completed in 6.191155796s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:11.429 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:20:57.651: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
May 14 01:21:05.836: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:05.841: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:07.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:07.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:09.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:09.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:11.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:11.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:13.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:13.845: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:15.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:15.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:17.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:17.847: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:19.842: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:19.847: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:21.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:21.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:23.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:23.845: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:25.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:25.845: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:27.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:27.845: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:29.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:29.848: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:31.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:31.846: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:33.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:33.845: INFO: Pod pod-with-poststart-exec-hook still exists
May 14 01:21:35.841: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
May 14 01:21:35.845: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:21:35.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7366" for this suite.
May 14 01:21:59.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:22:00.096: INFO: namespace container-lifecycle-hook-7366 deletion completed in 24.246673067s

• [SLOW TEST:62.446 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:22:00.097: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-ac169a2f-75e6-11e9-8961-767a82fad75b
STEP: Creating a pod to test consume configMaps
May 14 01:22:00.177: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b" in namespace "configmap-4253" to be "success or failure"
May 14 01:22:00.217: INFO: Pod "pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 40.519267ms
May 14 01:22:02.223: INFO: Pod "pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046065636s
May 14 01:22:04.228: INFO: Pod "pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.051396867s
STEP: Saw pod success
May 14 01:22:04.228: INFO: Pod "pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:22:04.233: INFO: Trying to get logs from node conformance0 pod pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b container configmap-volume-test: <nil>
STEP: delete the pod
May 14 01:22:04.269: INFO: Waiting for pod pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b to disappear
May 14 01:22:04.274: INFO: Pod pod-configmaps-ac178fd2-75e6-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:22:04.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4253" for this suite.
May 14 01:22:10.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:22:10.445: INFO: namespace configmap-4253 deletion completed in 6.165251536s

• [SLOW TEST:10.348 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:22:10.446: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:22:10.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 version --client'
May 14 01:22:10.628: INFO: stderr: ""
May 14 01:22:10.628: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
May 14 01:22:10.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-732'
May 14 01:22:11.248: INFO: stderr: ""
May 14 01:22:11.248: INFO: stdout: "replicationcontroller/redis-master created\n"
May 14 01:22:11.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 create -f - --namespace=kubectl-732'
May 14 01:22:11.845: INFO: stderr: ""
May 14 01:22:11.845: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
May 14 01:22:12.850: INFO: Selector matched 1 pods for map[app:redis]
May 14 01:22:12.850: INFO: Found 0 / 1
May 14 01:22:13.851: INFO: Selector matched 1 pods for map[app:redis]
May 14 01:22:13.851: INFO: Found 0 / 1
May 14 01:22:14.851: INFO: Selector matched 1 pods for map[app:redis]
May 14 01:22:14.851: INFO: Found 0 / 1
May 14 01:22:15.850: INFO: Selector matched 1 pods for map[app:redis]
May 14 01:22:15.850: INFO: Found 1 / 1
May 14 01:22:15.850: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
May 14 01:22:15.854: INFO: Selector matched 1 pods for map[app:redis]
May 14 01:22:15.854: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
May 14 01:22:15.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 describe pod redis-master-sz7gt --namespace=kubectl-732'
May 14 01:22:16.095: INFO: stderr: ""
May 14 01:22:16.095: INFO: stdout: "Name:               redis-master-sz7gt\nNamespace:          kubectl-732\nPriority:           0\nPriorityClassName:  <none>\nNode:               conformance0/10.10.1.212\nStart Time:         Tue, 14 May 2019 01:22:11 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.244.0.24\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://cd6ad81511a734e0ed4ecff7e0ef6c79ab0ea67c3652c478f5b15197541048b3\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 14 May 2019 01:22:14 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-r6rz7 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-r6rz7:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-r6rz7\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                   Message\n  ----    ------     ----  ----                   -------\n  Normal  Scheduled  5s    default-scheduler      Successfully assigned kubectl-732/redis-master-sz7gt to conformance0\n  Normal  Pulled     3s    kubelet, conformance0  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    3s    kubelet, conformance0  Created container redis-master\n  Normal  Started    2s    kubelet, conformance0  Started container redis-master\n"
May 14 01:22:16.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 describe rc redis-master --namespace=kubectl-732'
May 14 01:22:16.321: INFO: stderr: ""
May 14 01:22:16.321: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-732\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  5s    replication-controller  Created pod: redis-master-sz7gt\n"
May 14 01:22:16.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 describe service redis-master --namespace=kubectl-732'
May 14 01:22:16.481: INFO: stderr: ""
May 14 01:22:16.481: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-732\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.10.69.207\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.0.24:6379\nSession Affinity:  None\nEvents:            <none>\n"
May 14 01:22:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 describe node conformance0'
May 14 01:22:16.680: INFO: stderr: ""
May 14 01:22:16.680: INFO: stdout: "Name:               conformance0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=conformance0\n                    kubernetes.io/os=linux\n                    nirmata.io/cluster.name=conformance\n                    nirmata.io/cluster.role=control-plane\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"92:f8:e3:a7:e5:00\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.10.1.212\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 06 May 2019 22:38:18 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 14 May 2019 01:21:28 +0000   Mon, 06 May 2019 22:38:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 14 May 2019 01:21:28 +0000   Mon, 06 May 2019 22:38:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 14 May 2019 01:21:28 +0000   Mon, 06 May 2019 22:38:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 14 May 2019 01:21:28 +0000   Mon, 13 May 2019 18:58:59 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.10.1.212\n  Hostname:    conformance0\nCapacity:\n cpu:                2\n ephemeral-storage:  31166436Ki\n hugepages-2Mi:      0\n memory:             8149868Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  28722987371\n hugepages-2Mi:      0\n memory:             8047468Ki\n pods:               110\nSystem Info:\n Machine ID:                 cf9039846e817bf110c3933d5c3e0c56\n System UUID:                EB0EE6DC-ED5B-CF59-01CC-0B8140179331\n Boot ID:                    f23dca8c-2cbe-455f-a8dc-c114bc1cd008\n Kernel Version:             4.4.0-131-generic\n OS Image:                   Debian GNU/Linux 9 (stretch)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.6\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  heptio-sonobuoy            sonobuoy-e2e-job-802ffc2c840c4d59                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-56580dbecea949d3-c489c    0 (0%)        0 (0%)      0 (0%)           0 (0%)         83m\n  kube-system                kube-dns-59b9f565f4-ftlkq                                  260m (13%)    0 (0%)      110Mi (1%)       170Mi (2%)     6h23m\n  kube-system                kube-flannel-ds-7rgsb                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h23m\n  kube-system                metrics-server-7658bc7d99-v55kv                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h22m\n  kubectl-732                redis-master-sz7gt                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         5s\n  nirmata                    nirmata-cni-installer-5hwgr                                0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h23m\n  nirmata                    nirmata-kube-controller-5b4b6fc6-v5lhm                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         6h23m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                260m (13%)  0 (0%)\n  memory             110Mi (1%)  170Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
May 14 01:22:16.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-353874590 describe namespace kubectl-732'
May 14 01:22:16.841: INFO: stderr: ""
May 14 01:22:16.841: INFO: stdout: "Name:         kubectl-732\nLabels:       e2e-framework=kubectl\n              e2e-run=1f3dbeca-75db-11e9-8961-767a82fad75b\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:22:16.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-732" for this suite.
May 14 01:22:40.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:22:41.052: INFO: namespace kubectl-732 deletion completed in 24.205264499s

• [SLOW TEST:30.606 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:22:41.052: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:22:41.127: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:22:45.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6896" for this suite.
May 14 01:23:39.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:23:39.431: INFO: namespace pods-6896 deletion completed in 54.239019023s

• [SLOW TEST:58.379 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:23:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:24:03.555: INFO: Container started at 2019-05-14 01:23:41 +0000 UTC, pod became ready at 2019-05-14 01:24:02 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:24:03.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3484" for this suite.
May 14 01:24:25.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:24:25.735: INFO: namespace container-probe-3484 deletion completed in 22.176444018s

• [SLOW TEST:46.303 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:24:25.736: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
May 14 01:24:25.825: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145417,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 01:24:25.826: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145417,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
May 14 01:24:35.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145421,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
May 14 01:24:35.834: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145421,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
May 14 01:24:45.844: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145424,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 01:24:45.845: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145424,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
May 14 01:24:55.856: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145427,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
May 14 01:24:55.856: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-a,UID:02e87f82-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145427,Generation:0,CreationTimestamp:2019-05-14 01:24:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
May 14 01:25:05.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-b,UID:1ac60293-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145430,Generation:0,CreationTimestamp:2019-05-14 01:25:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 01:25:05.866: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-b,UID:1ac60293-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145430,Generation:0,CreationTimestamp:2019-05-14 01:25:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
May 14 01:25:15.875: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-b,UID:1ac60293-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145433,Generation:0,CreationTimestamp:2019-05-14 01:25:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
May 14 01:25:15.875: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6967,SelfLink:/api/v1/namespaces/watch-6967/configmaps/e2e-watch-test-configmap-b,UID:1ac60293-75e7-11e9-9f2c-ceb99be2323d,ResourceVersion:145433,Generation:0,CreationTimestamp:2019-05-14 01:25:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:25:25.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6967" for this suite.
May 14 01:25:31.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:25:32.089: INFO: namespace watch-6967 deletion completed in 6.208398578s

• [SLOW TEST:66.353 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:25:32.090: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:25:32.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3913" for this suite.
May 14 01:25:54.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:25:54.408: INFO: namespace pods-3913 deletion completed in 22.232572239s

• [SLOW TEST:22.319 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:25:54.410: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
May 14 01:25:58.546: INFO: Pod pod-hostip-37c34880-75e7-11e9-8961-767a82fad75b has hostIP: 10.10.1.212
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:25:58.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4846" for this suite.
May 14 01:26:20.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:26:20.823: INFO: namespace pods-4846 deletion completed in 22.269378941s

• [SLOW TEST:26.413 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:26:20.830: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-6324
May 14 01:26:24.947: INFO: Started pod liveness-http in namespace container-probe-6324
STEP: checking the pod's current state and verifying that restartCount is present
May 14 01:26:24.952: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:30:25.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6324" for this suite.
May 14 01:30:31.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:30:31.988: INFO: namespace container-probe-6324 deletion completed in 6.279070077s

• [SLOW TEST:251.159 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:30:31.989: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:30:32.077: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:30:36.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1884" for this suite.
May 14 01:31:16.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:31:16.610: INFO: namespace pods-1884 deletion completed in 40.164808494s

• [SLOW TEST:44.622 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:31:16.612: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:31:21.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6669" for this suite.
May 14 01:31:43.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:31:44.008: INFO: namespace replication-controller-6669 deletion completed in 22.215764327s

• [SLOW TEST:27.395 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:31:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
May 14 01:31:44.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b" in namespace "projected-8643" to be "success or failure"
May 14 01:31:44.097: INFO: Pod "downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.684473ms
May 14 01:31:46.103: INFO: Pod "downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012221812s
May 14 01:31:48.109: INFO: Pod "downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017967862s
STEP: Saw pod success
May 14 01:31:48.109: INFO: Pod "downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:31:48.114: INFO: Trying to get logs from node conformance0 pod downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b container client-container: <nil>
STEP: delete the pod
May 14 01:31:48.156: INFO: Waiting for pod downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b to disappear
May 14 01:31:48.172: INFO: Pod downwardapi-volume-08217dc6-75e8-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:31:48.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8643" for this suite.
May 14 01:31:54.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:31:54.435: INFO: namespace projected-8643 deletion completed in 6.256077392s

• [SLOW TEST:10.427 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:31:54.436: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:31:54.515: INFO: Pod name rollover-pod: Found 0 pods out of 1
May 14 01:31:59.519: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
May 14 01:31:59.520: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
May 14 01:32:01.526: INFO: Creating deployment "test-rollover-deployment"
May 14 01:32:01.538: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
May 14 01:32:03.550: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
May 14 01:32:03.560: INFO: Ensure that both replica sets have 1 created replica
May 14 01:32:03.569: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
May 14 01:32:03.590: INFO: Updating deployment test-rollover-deployment
May 14 01:32:03.590: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
May 14 01:32:05.607: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
May 14 01:32:05.645: INFO: Make sure deployment "test-rollover-deployment" is complete
May 14 01:32:05.665: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:05.666: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394323, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:07.677: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:07.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394326, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:09.677: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:09.677: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394326, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:11.676: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:11.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394326, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:13.676: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:13.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394326, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:15.676: INFO: all replica sets need to contain the pod-template-hash label
May 14 01:32:15.676: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394326, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63693394321, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
May 14 01:32:17.677: INFO: 
May 14 01:32:17.677: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
May 14 01:32:17.690: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4460,SelfLink:/apis/apps/v1/namespaces/deployment-4460/deployments/test-rollover-deployment,UID:12880488-75e8-11e9-9f2c-ceb99be2323d,ResourceVersion:145760,Generation:2,CreationTimestamp:2019-05-14 01:32:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-05-14 01:32:01 +0000 UTC 2019-05-14 01:32:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-05-14 01:32:16 +0000 UTC 2019-05-14 01:32:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

May 14 01:32:17.697: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-4460,SelfLink:/apis/apps/v1/namespaces/deployment-4460/replicasets/test-rollover-deployment-766b4d6c9d,UID:13c22d11-75e8-11e9-9f2c-ceb99be2323d,ResourceVersion:145750,Generation:2,CreationTimestamp:2019-05-14 01:32:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 12880488-75e8-11e9-9f2c-ceb99be2323d 0xc00265b1c7 0xc00265b1c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
May 14 01:32:17.697: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
May 14 01:32:17.697: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4460,SelfLink:/apis/apps/v1/namespaces/deployment-4460/replicasets/test-rollover-controller,UID:0e583bae-75e8-11e9-9f2c-ceb99be2323d,ResourceVersion:145759,Generation:2,CreationTimestamp:2019-05-14 01:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 12880488-75e8-11e9-9f2c-ceb99be2323d 0xc00265afd7 0xc00265afd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 01:32:17.697: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-4460,SelfLink:/apis/apps/v1/namespaces/deployment-4460/replicasets/test-rollover-deployment-6455657675,UID:128b8217-75e8-11e9-9f2c-ceb99be2323d,ResourceVersion:145730,Generation:2,CreationTimestamp:2019-05-14 01:32:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 12880488-75e8-11e9-9f2c-ceb99be2323d 0xc00265b0a7 0xc00265b0a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
May 14 01:32:17.704: INFO: Pod "test-rollover-deployment-766b4d6c9d-4s77t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-4s77t,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-4460,SelfLink:/api/v1/namespaces/deployment-4460/pods/test-rollover-deployment-766b4d6c9d-4s77t,UID:13cf9ce5-75e8-11e9-9f2c-ceb99be2323d,ResourceVersion:145745,Generation:0,CreationTimestamp:2019-05-14 01:32:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 13c22d11-75e8-11e9-9f2c-ceb99be2323d 0xc00265bda7 0xc00265bda8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-q2xlc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-q2xlc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-q2xlc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:conformance0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00265be20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00265be40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:32:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:32:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:32:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-05-14 01:32:03 +0000 UTC  }],Message:,Reason:,HostIP:10.10.1.212,PodIP:10.244.0.35,StartTime:2019-05-14 01:32:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-05-14 01:32:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7ee136854bf2ce592c83472c7ff7111281df1e79fca0cce161589fd2156202bb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:32:17.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4460" for this suite.
May 14 01:32:25.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:32:25.907: INFO: namespace deployment-4460 deletion completed in 8.197864967s

• [SLOW TEST:31.471 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:32:25.907: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
May 14 01:32:25.975: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
May 14 01:32:25.987: INFO: Waiting for terminating namespaces to be deleted...
May 14 01:32:25.996: INFO: 
Logging pods the kubelet thinks is on node conformance0 before test
May 14 01:32:26.012: INFO: nirmata-kube-controller-5b4b6fc6-v5lhm from nirmata started at 2019-05-13 18:59:02 +0000 UTC (1 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container nirmata-kube-controller ready: true, restart count 0
May 14 01:32:26.012: INFO: kube-flannel-ds-7rgsb from kube-system started at 2019-05-13 18:58:52 +0000 UTC (1 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container kube-flannel ready: true, restart count 0
May 14 01:32:26.012: INFO: sonobuoy-systemd-logs-daemon-set-56580dbecea949d3-c489c from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container sonobuoy-worker ready: true, restart count 1
May 14 01:32:26.012: INFO: 	Container systemd-logs ready: true, restart count 1
May 14 01:32:26.012: INFO: sonobuoy from heptio-sonobuoy started at 2019-05-13 23:58:47 +0000 UTC (1 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container kube-sonobuoy ready: true, restart count 0
May 14 01:32:26.012: INFO: metrics-server-7658bc7d99-v55kv from kube-system started at 2019-05-13 18:59:37 +0000 UTC (1 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container metrics-server ready: true, restart count 0
May 14 01:32:26.012: INFO: sonobuoy-e2e-job-802ffc2c840c4d59 from heptio-sonobuoy started at 2019-05-13 23:58:55 +0000 UTC (2 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container e2e ready: true, restart count 0
May 14 01:32:26.012: INFO: 	Container sonobuoy-worker ready: true, restart count 0
May 14 01:32:26.012: INFO: nirmata-cni-installer-5hwgr from nirmata started at 2019-05-13 18:58:59 +0000 UTC (1 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container install-cni ready: true, restart count 0
May 14 01:32:26.012: INFO: kube-dns-59b9f565f4-ftlkq from kube-system started at 2019-05-13 18:59:04 +0000 UTC (3 container statuses recorded)
May 14 01:32:26.012: INFO: 	Container dnsmasq ready: true, restart count 0
May 14 01:32:26.012: INFO: 	Container kubedns ready: true, restart count 0
May 14 01:32:26.012: INFO: 	Container sidecar ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-238a529f-75e8-11e9-8961-767a82fad75b 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-238a529f-75e8-11e9-8961-767a82fad75b off the node conformance0
STEP: verifying the node doesn't have the label kubernetes.io/e2e-238a529f-75e8-11e9-8961-767a82fad75b
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:32:34.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9975" for this suite.
May 14 01:32:46.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:32:46.366: INFO: namespace sched-pred-9975 deletion completed in 12.173308699s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

• [SLOW TEST:20.458 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:32:46.369: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:33:46.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9508" for this suite.
May 14 01:34:08.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:34:08.615: INFO: namespace container-probe-9508 deletion completed in 22.168565796s

• [SLOW TEST:82.247 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:34:08.616: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
May 14 01:34:08.684: INFO: (0) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.062409ms)
May 14 01:34:08.691: INFO: (1) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.320275ms)
May 14 01:34:08.700: INFO: (2) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.261021ms)
May 14 01:34:08.708: INFO: (3) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.510273ms)
May 14 01:34:08.716: INFO: (4) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.276103ms)
May 14 01:34:08.724: INFO: (5) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.936247ms)
May 14 01:34:08.732: INFO: (6) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.712063ms)
May 14 01:34:08.740: INFO: (7) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.349851ms)
May 14 01:34:08.747: INFO: (8) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.953279ms)
May 14 01:34:08.753: INFO: (9) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.33534ms)
May 14 01:34:08.761: INFO: (10) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.575236ms)
May 14 01:34:08.768: INFO: (11) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.420593ms)
May 14 01:34:08.774: INFO: (12) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.455245ms)
May 14 01:34:08.783: INFO: (13) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.197523ms)
May 14 01:34:08.792: INFO: (14) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 9.660835ms)
May 14 01:34:08.800: INFO: (15) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.423281ms)
May 14 01:34:08.808: INFO: (16) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.228194ms)
May 14 01:34:08.815: INFO: (17) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 7.161285ms)
May 14 01:34:08.822: INFO: (18) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 6.651768ms)
May 14 01:34:08.831: INFO: (19) /api/v1/nodes/conformance0/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="alternatives.log.1">alternatives.l... (200; 8.238583ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:34:08.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4972" for this suite.
May 14 01:34:14.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:34:15.030: INFO: namespace proxy-4972 deletion completed in 6.194431689s

• [SLOW TEST:6.415 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:34:15.031: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:34:19.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8653" for this suite.
May 14 01:35:07.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:35:07.329: INFO: namespace kubelet-test-8653 deletion completed in 48.198659657s

• [SLOW TEST:52.299 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:35:07.330: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
May 14 01:35:07.407: INFO: Waiting up to 5m0s for pod "pod-8150da30-75e8-11e9-8961-767a82fad75b" in namespace "emptydir-4741" to be "success or failure"
May 14 01:35:07.414: INFO: Pod "pod-8150da30-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.419026ms
May 14 01:35:09.420: INFO: Pod "pod-8150da30-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013270115s
May 14 01:35:11.429: INFO: Pod "pod-8150da30-75e8-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021736035s
STEP: Saw pod success
May 14 01:35:11.429: INFO: Pod "pod-8150da30-75e8-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:35:11.433: INFO: Trying to get logs from node conformance0 pod pod-8150da30-75e8-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:35:11.468: INFO: Waiting for pod pod-8150da30-75e8-11e9-8961-767a82fad75b to disappear
May 14 01:35:11.486: INFO: Pod pod-8150da30-75e8-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:35:11.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4741" for this suite.
May 14 01:35:17.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:35:17.717: INFO: namespace emptydir-4741 deletion completed in 6.223909858s

• [SLOW TEST:10.387 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:35:17.718: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
May 14 01:35:17.784: INFO: Waiting up to 5m0s for pod "downward-api-87807a73-75e8-11e9-8961-767a82fad75b" in namespace "downward-api-9886" to be "success or failure"
May 14 01:35:17.816: INFO: Pod "downward-api-87807a73-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 31.535246ms
May 14 01:35:19.822: INFO: Pod "downward-api-87807a73-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038048451s
May 14 01:35:21.828: INFO: Pod "downward-api-87807a73-75e8-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043356237s
STEP: Saw pod success
May 14 01:35:21.828: INFO: Pod "downward-api-87807a73-75e8-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:35:21.832: INFO: Trying to get logs from node conformance0 pod downward-api-87807a73-75e8-11e9-8961-767a82fad75b container dapi-container: <nil>
STEP: delete the pod
May 14 01:35:21.881: INFO: Waiting for pod downward-api-87807a73-75e8-11e9-8961-767a82fad75b to disappear
May 14 01:35:21.886: INFO: Pod downward-api-87807a73-75e8-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:35:21.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9886" for this suite.
May 14 01:35:27.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:35:28.096: INFO: namespace downward-api-9886 deletion completed in 6.205572137s

• [SLOW TEST:10.378 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:35:28.096: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
May 14 01:35:34.329: INFO: 0 pods remaining
May 14 01:35:34.329: INFO: 0 pods has nil DeletionTimestamp
May 14 01:35:34.329: INFO: 
STEP: Gathering metrics
W0514 01:35:35.238266      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
May 14 01:35:35.238: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:35:35.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4390" for this suite.
May 14 01:35:41.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:35:41.478: INFO: namespace gc-4390 deletion completed in 6.2349026s

• [SLOW TEST:13.382 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:35:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-7626
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
May 14 01:35:41.627: INFO: Found 0 stateful pods, waiting for 3
May 14 01:35:51.633: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:35:51.633: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:35:51.633: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
May 14 01:35:51.669: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
May 14 01:36:01.728: INFO: Updating stateful set ss2
May 14 01:36:01.783: INFO: Waiting for Pod statefulset-7626/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 01:36:11.799: INFO: Waiting for Pod statefulset-7626/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
May 14 01:36:22.173: INFO: Found 2 stateful pods, waiting for 3
May 14 01:36:32.178: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:36:32.178: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
May 14 01:36:32.178: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
May 14 01:36:32.209: INFO: Updating stateful set ss2
May 14 01:36:32.222: INFO: Waiting for Pod statefulset-7626/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 01:36:42.257: INFO: Updating stateful set ss2
May 14 01:36:42.287: INFO: Waiting for StatefulSet statefulset-7626/ss2 to complete update
May 14 01:36:42.287: INFO: Waiting for Pod statefulset-7626/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
May 14 01:36:52.297: INFO: Waiting for StatefulSet statefulset-7626/ss2 to complete update
May 14 01:36:52.297: INFO: Waiting for Pod statefulset-7626/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
May 14 01:37:02.299: INFO: Deleting all statefulset in ns statefulset-7626
May 14 01:37:02.307: INFO: Scaling statefulset ss2 to 0
May 14 01:37:22.340: INFO: Waiting for statefulset status.replicas updated to 0
May 14 01:37:22.344: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:37:22.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7626" for this suite.
May 14 01:37:28.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:37:28.681: INFO: namespace statefulset-7626 deletion completed in 6.293721584s

• [SLOW TEST:107.201 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:37:28.684: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
May 14 01:37:28.780: INFO: Waiting up to 5m0s for pod "pod-d5913bd6-75e8-11e9-8961-767a82fad75b" in namespace "emptydir-600" to be "success or failure"
May 14 01:37:28.796: INFO: Pod "pod-d5913bd6-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.9337ms
May 14 01:37:30.818: INFO: Pod "pod-d5913bd6-75e8-11e9-8961-767a82fad75b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03754871s
May 14 01:37:32.824: INFO: Pod "pod-d5913bd6-75e8-11e9-8961-767a82fad75b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.043594836s
STEP: Saw pod success
May 14 01:37:32.824: INFO: Pod "pod-d5913bd6-75e8-11e9-8961-767a82fad75b" satisfied condition "success or failure"
May 14 01:37:32.828: INFO: Trying to get logs from node conformance0 pod pod-d5913bd6-75e8-11e9-8961-767a82fad75b container test-container: <nil>
STEP: delete the pod
May 14 01:37:32.862: INFO: Waiting for pod pod-d5913bd6-75e8-11e9-8961-767a82fad75b to disappear
May 14 01:37:32.876: INFO: Pod pod-d5913bd6-75e8-11e9-8961-767a82fad75b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:37:32.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-600" for this suite.
May 14 01:37:38.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:37:39.103: INFO: namespace emptydir-600 deletion completed in 6.216519784s

• [SLOW TEST:10.420 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
May 14 01:37:39.104: INFO: >>> kubeConfig: /tmp/kubeconfig-353874590
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-4894
May 14 01:37:43.220: INFO: Started pod liveness-exec in namespace container-probe-4894
STEP: checking the pod's current state and verifying that restartCount is present
May 14 01:37:43.224: INFO: Initial restart count of pod liveness-exec is 0
May 14 01:38:31.361: INFO: Restart count of pod container-probe-4894/liveness-exec is now 1 (48.137203922s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
May 14 01:38:31.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4894" for this suite.
May 14 01:38:37.427: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
May 14 01:38:37.609: INFO: namespace container-probe-4894 deletion completed in 6.208046865s

• [SLOW TEST:58.506 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSMay 14 01:38:37.610: INFO: Running AfterSuite actions on all nodes
May 14 01:38:37.610: INFO: Running AfterSuite actions on node 1
May 14 01:38:37.610: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5955.838 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h39m18.400303518s
Test Suite Passed
