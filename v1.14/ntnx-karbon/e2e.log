I0813 06:05:08.335154      18 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-872026410
I0813 06:05:08.335421      18 e2e.go:240] Starting e2e run "4cc3a5e8-bd90-11e9-8bf8-0a58ac140308" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565676307 - Will randomize all specs
Will run 204 of 3584 specs

Aug 13 06:05:08.529: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 06:05:08.532: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 13 06:05:08.550: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 13 06:05:08.581: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 13 06:05:08.581: INFO: expected 1 pod replicas in namespace 'kube-system', 1 are Running and Ready.
Aug 13 06:05:08.581: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 13 06:05:08.590: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Aug 13 06:05:08.590: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
Aug 13 06:05:08.590: INFO: e2e test version: v1.14.0
Aug 13 06:05:08.592: INFO: kube-apiserver version: v1.14.0
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:05:08.592: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename services
Aug 13 06:05:08.627: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:05:08.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3625" for this suite.
Aug 13 06:05:14.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:05:14.737: INFO: namespace services-3625 deletion completed in 6.102520944s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:6.145 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:05:14.737: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Aug 13 06:05:14.768: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 13 06:05:14.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:15.259: INFO: stderr: ""
Aug 13 06:05:15.259: INFO: stdout: "service/redis-slave created\n"
Aug 13 06:05:15.259: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 13 06:05:15.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:15.499: INFO: stderr: ""
Aug 13 06:05:15.499: INFO: stdout: "service/redis-master created\n"
Aug 13 06:05:15.500: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 13 06:05:15.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:15.816: INFO: stderr: ""
Aug 13 06:05:15.816: INFO: stdout: "service/frontend created\n"
Aug 13 06:05:15.817: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 13 06:05:15.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:16.041: INFO: stderr: ""
Aug 13 06:05:16.041: INFO: stdout: "deployment.apps/frontend created\n"
Aug 13 06:05:16.041: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 13 06:05:16.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:16.332: INFO: stderr: ""
Aug 13 06:05:16.332: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 13 06:05:16.332: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 13 06:05:16.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7868'
Aug 13 06:05:16.548: INFO: stderr: ""
Aug 13 06:05:16.548: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 13 06:05:16.548: INFO: Waiting for all frontend pods to be Running.
Aug 13 06:05:36.599: INFO: Waiting for frontend to serve content.
Aug 13 06:05:37.619: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection refused [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection refu...', 111)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stream in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Aug 13 06:05:42.642: INFO: Trying to add a new entry to the guestbook.
Aug 13 06:05:42.655: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 13 06:05:42.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:42.782: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:42.782: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 06:05:42.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:42.884: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:42.884: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 06:05:42.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:42.992: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:42.992: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 06:05:42.993: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:43.088: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:43.088: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 06:05:43.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:43.175: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:43.175: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 13 06:05:43.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7868'
Aug 13 06:05:43.265: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:05:43.265: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:05:43.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7868" for this suite.
Aug 13 06:06:33.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:06:33.366: INFO: namespace kubectl-7868 deletion completed in 50.09677145s

â€¢ [SLOW TEST:78.629 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:06:33.366: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Aug 13 06:06:33.412: INFO: Waiting up to 5m0s for pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308" in namespace "containers-2470" to be "success or failure"
Aug 13 06:06:33.421: INFO: Pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.898901ms
Aug 13 06:06:35.424: INFO: Pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012232873s
Aug 13 06:06:37.429: INFO: Pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016659973s
Aug 13 06:06:39.432: INFO: Pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020177938s
STEP: Saw pod success
Aug 13 06:06:39.432: INFO: Pod "client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:06:39.434: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:06:39.467: INFO: Waiting for pod client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:06:39.470: INFO: Pod client-containers-8020592b-bd90-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:06:39.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2470" for this suite.
Aug 13 06:06:45.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:06:45.571: INFO: namespace containers-2470 deletion completed in 6.096425592s

â€¢ [SLOW TEST:12.204 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:06:45.571: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:06:52.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8194" for this suite.
Aug 13 06:07:14.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:14.741: INFO: namespace replication-controller-8194 deletion completed in 22.103852783s

â€¢ [SLOW TEST:29.170 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:14.742: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-98c8d9b4-bd90-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:07:14.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308" in namespace "projected-9650" to be "success or failure"
Aug 13 06:07:14.793: INFO: Pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.918395ms
Aug 13 06:07:16.797: INFO: Pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009828992s
Aug 13 06:07:18.800: INFO: Pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013064807s
Aug 13 06:07:20.803: INFO: Pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016242581s
STEP: Saw pod success
Aug 13 06:07:20.803: INFO: Pod "pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:07:20.805: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:07:20.822: INFO: Waiting for pod pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:07:20.826: INFO: Pod pod-projected-configmaps-98c98bee-bd90-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:07:20.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9650" for this suite.
Aug 13 06:07:26.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:26.930: INFO: namespace projected-9650 deletion completed in 6.098356984s

â€¢ [SLOW TEST:12.188 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:26.930: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 06:07:26.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2206'
Aug 13 06:07:27.068: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 06:07:27.068: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Aug 13 06:07:27.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2206'
Aug 13 06:07:27.166: INFO: stderr: ""
Aug 13 06:07:27.166: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:07:27.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2206" for this suite.
Aug 13 06:07:33.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:33.265: INFO: namespace kubectl-2206 deletion completed in 6.094990174s

â€¢ [SLOW TEST:6.335 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:33.265: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 13 06:07:33.314: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1235,SelfLink:/api/v1/namespaces/watch-1235/configmaps/e2e-watch-test-watch-closed,UID:a3d505db-bd90-11e9-868a-506b8de0bf77,ResourceVersion:4852,Generation:0,CreationTimestamp:2019-08-13 06:07:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 06:07:33.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1235,SelfLink:/api/v1/namespaces/watch-1235/configmaps/e2e-watch-test-watch-closed,UID:a3d505db-bd90-11e9-868a-506b8de0bf77,ResourceVersion:4853,Generation:0,CreationTimestamp:2019-08-13 06:07:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 13 06:07:33.331: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1235,SelfLink:/api/v1/namespaces/watch-1235/configmaps/e2e-watch-test-watch-closed,UID:a3d505db-bd90-11e9-868a-506b8de0bf77,ResourceVersion:4854,Generation:0,CreationTimestamp:2019-08-13 06:07:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 06:07:33.332: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1235,SelfLink:/api/v1/namespaces/watch-1235/configmaps/e2e-watch-test-watch-closed,UID:a3d505db-bd90-11e9-868a-506b8de0bf77,ResourceVersion:4855,Generation:0,CreationTimestamp:2019-08-13 06:07:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:07:33.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1235" for this suite.
Aug 13 06:07:39.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:39.446: INFO: namespace watch-1235 deletion completed in 6.11108764s

â€¢ [SLOW TEST:6.181 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:39.447: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Aug 13 06:07:39.493: INFO: Waiting up to 5m0s for pod "client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308" in namespace "containers-8672" to be "success or failure"
Aug 13 06:07:39.497: INFO: Pod "client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.100394ms
Aug 13 06:07:41.501: INFO: Pod "client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007774077s
Aug 13 06:07:43.505: INFO: Pod "client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011881118s
STEP: Saw pod success
Aug 13 06:07:43.505: INFO: Pod "client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:07:43.508: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:07:43.529: INFO: Waiting for pod client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:07:43.532: INFO: Pod client-containers-a7836206-bd90-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:07:43.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8672" for this suite.
Aug 13 06:07:49.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:49.636: INFO: namespace containers-8672 deletion completed in 6.100852367s

â€¢ [SLOW TEST:10.189 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:49.637: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0813 06:07:50.702502      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 06:07:50.702: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:07:50.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1532" for this suite.
Aug 13 06:07:56.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:07:56.801: INFO: namespace gc-1532 deletion completed in 6.095519767s

â€¢ [SLOW TEST:7.164 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:07:56.801: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 13 06:07:56.838: INFO: Waiting up to 5m0s for pod "downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308" in namespace "downward-api-4504" to be "success or failure"
Aug 13 06:07:56.844: INFO: Pod "downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.44349ms
Aug 13 06:07:58.849: INFO: Pod "downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010757884s
Aug 13 06:08:00.853: INFO: Pod "downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015088042s
STEP: Saw pod success
Aug 13 06:08:00.853: INFO: Pod "downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:08:00.856: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:08:00.878: INFO: Waiting for pod downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:08:00.882: INFO: Pod downward-api-b1da428d-bd90-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:08:00.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4504" for this suite.
Aug 13 06:08:06.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:08:06.976: INFO: namespace downward-api-4504 deletion completed in 6.088751356s

â€¢ [SLOW TEST:10.175 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:08:06.976: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:08:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1471" for this suite.
Aug 13 06:08:39.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:08:39.211: INFO: namespace namespaces-1471 deletion completed in 6.103638021s
STEP: Destroying namespace "nsdeletetest-4770" for this suite.
Aug 13 06:08:39.213: INFO: Namespace nsdeletetest-4770 was already deleted
STEP: Destroying namespace "nsdeletetest-5116" for this suite.
Aug 13 06:08:45.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:08:45.313: INFO: namespace nsdeletetest-5116 deletion completed in 6.099669487s

â€¢ [SLOW TEST:38.337 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:08:45.313: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-136
Aug 13 06:08:51.377: INFO: Started pod liveness-http in namespace container-probe-136
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 06:08:51.380: INFO: Initial restart count of pod liveness-http is 0
Aug 13 06:09:09.418: INFO: Restart count of pod container-probe-136/liveness-http is now 1 (18.038176072s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:09:09.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-136" for this suite.
Aug 13 06:09:15.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:09:15.533: INFO: namespace container-probe-136 deletion completed in 6.095301502s

â€¢ [SLOW TEST:30.220 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:09:15.534: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 13 06:09:21.587: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-e0c82f76-bd90-11e9-8bf8-0a58ac140308,GenerateName:,Namespace:events-1002,SelfLink:/api/v1/namespaces/events-1002/pods/send-events-e0c82f76-bd90-11e9-8bf8-0a58ac140308,UID:e0ca284c-bd90-11e9-868a-506b8de0bf77,ResourceVersion:5227,Generation:0,CreationTimestamp:2019-08-13 06:09:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 566687901,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kfnp4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kfnp4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-kfnp4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:09:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:09:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:09:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:09:15 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.19,StartTime:2019-08-13 06:09:15 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-13 06:09:20 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://c7cf62d2cc4f0007439b40105eab78278125a33353b7010e6133777144b9900b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 13 06:09:23.592: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 13 06:09:25.596: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:09:25.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1002" for this suite.
Aug 13 06:10:11.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:10:11.716: INFO: namespace events-1002 deletion completed in 46.10403392s

â€¢ [SLOW TEST:56.182 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:10:11.717: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:10:17.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5819" for this suite.
Aug 13 06:10:23.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:10:23.928: INFO: namespace namespaces-5819 deletion completed in 6.084223326s
STEP: Destroying namespace "nsdeletetest-9161" for this suite.
Aug 13 06:10:23.930: INFO: Namespace nsdeletetest-9161 was already deleted
STEP: Destroying namespace "nsdeletetest-4577" for this suite.
Aug 13 06:10:29.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:10:30.053: INFO: namespace nsdeletetest-4577 deletion completed in 6.122587143s

â€¢ [SLOW TEST:18.336 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:10:30.053: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:10:30.089: INFO: Creating deployment "nginx-deployment"
Aug 13 06:10:30.095: INFO: Waiting for observed generation 1
Aug 13 06:10:32.102: INFO: Waiting for all required pods to come up
Aug 13 06:10:32.105: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 13 06:10:38.114: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 13 06:10:38.119: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 13 06:10:38.130: INFO: Updating deployment nginx-deployment
Aug 13 06:10:38.130: INFO: Waiting for observed generation 2
Aug 13 06:10:40.136: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 13 06:10:40.139: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 13 06:10:40.142: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 13 06:10:40.151: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 13 06:10:40.151: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 13 06:10:40.154: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 13 06:10:40.159: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 13 06:10:40.159: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 13 06:10:40.169: INFO: Updating deployment nginx-deployment
Aug 13 06:10:40.169: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 13 06:10:40.186: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 13 06:10:40.202: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 13 06:10:40.217: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-444,SelfLink:/apis/apps/v1/namespaces/deployment-444/deployments/nginx-deployment,UID:0d354f9a-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5620,Generation:3,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-08-13 06:10:38 +0000 UTC 2019-08-13 06:10:30 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-08-13 06:10:40 +0000 UTC 2019-08-13 06:10:40 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 13 06:10:40.241: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-444,SelfLink:/apis/apps/v1/namespaces/deployment-444/replicasets/nginx-deployment-5f9595f595,UID:1200384c-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5611,Generation:3,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0d354f9a-bd91-11e9-868a-506b8de0bf77 0xc001e0f207 0xc001e0f208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 06:10:40.241: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 13 06:10:40.241: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-444,SelfLink:/apis/apps/v1/namespaces/deployment-444/replicasets/nginx-deployment-6f478d8d8,UID:0d361127-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5608,Generation:3,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0d354f9a-bd91-11e9-868a-506b8de0bf77 0xc001e0f2d7 0xc001e0f2d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 13 06:10:40.262: INFO: Pod "nginx-deployment-5f9595f595-45dfx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-45dfx,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-45dfx,UID:133dbcbf-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5652,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc001e0fbb0 0xc001e0fbb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-9xrj2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9xrj2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-9xrj2,UID:1201e8af-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5605,Generation:0,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc001e0fc80 0xc001e0fc81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.138,PodIP:172.20.3.14,StartTime:2019-08-13 06:10:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = manifest for docker.io/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-bxrpg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-bxrpg,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-bxrpg,UID:12095a52-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5589,Generation:0,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc001e0fdc0 0xc001e0fdc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.134,PodIP:,StartTime:2019-08-13 06:10:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-cbwsk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-cbwsk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-cbwsk,UID:1200e6dc-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5568,Generation:0,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc001e0fee0 0xc001e0fee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:36 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.134,PodIP:,StartTime:2019-08-13 06:10:36 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-l8dg6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-l8dg6,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-l8dg6,UID:1339f29f-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5654,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c58000 0xc002c58001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.138,PodIP:,StartTime:2019-08-13 06:10:40 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-lstmh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lstmh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-lstmh,UID:133da1cd-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5649,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c58120 0xc002c58121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-pls22" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pls22,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-pls22,UID:1201e218-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5570,Generation:0,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c581f0 0xc002c581f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:,StartTime:2019-08-13 06:10:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.263: INFO: Pod "nginx-deployment-5f9595f595-pssww" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pssww,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-pssww,UID:133dda41-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5653,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c58310 0xc002c58311}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-5f9595f595-rhnr9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rhnr9,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-rhnr9,UID:133db410-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5650,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c583e0 0xc002c583e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-5f9595f595-sqfdz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-sqfdz,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-sqfdz,UID:13419687-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5657,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c584d0 0xc002c584d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-5f9595f595-wglb2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-wglb2,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-wglb2,UID:120c46b1-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5592,Generation:0,CreationTimestamp:2019-08-13 06:10:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c58597 0xc002c58598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:,StartTime:2019-08-13 06:10:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-5f9595f595-x8s2k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-x8s2k,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-x8s2k,UID:133c4d4b-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5646,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c586b0 0xc002c586b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-5f9595f595-zpwql" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-zpwql,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-5f9595f595-zpwql,UID:133c31d2-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5641,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 1200384c-bd91-11e9-868a-506b8de0bf77 0xc002c587c0 0xc002c587c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.264: INFO: Pod "nginx-deployment-6f478d8d8-92dmd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-92dmd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-92dmd,UID:133e525f-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5661,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58890 0xc002c58891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.265: INFO: Pod "nginx-deployment-6f478d8d8-9jbv9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-9jbv9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-9jbv9,UID:133e6ab6-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5662,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58950 0xc002c58951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.265: INFO: Pod "nginx-deployment-6f478d8d8-bvfgf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bvfgf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-bvfgf,UID:0d3c79e4-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5513,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58a10 0xc002c58a11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.21,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b92a4fe88502081c78db210939d96e761ba701a676f4a13c5c15f81ae48e796c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.265: INFO: Pod "nginx-deployment-6f478d8d8-cdnqf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-cdnqf,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-cdnqf,UID:0d405203-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5510,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58b20 0xc002c58b21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.23,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9a2552b5933f744e22d77ff09a48a967d062496402447c7680f51d77d8db782d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.278: INFO: Pod "nginx-deployment-6f478d8d8-fzpsz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-fzpsz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-fzpsz,UID:0d3c85be-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5479,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58c30 0xc002c58c31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.22,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:31 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://96ad3eb49010224a4aa9d3e3509924206ab17173e15aa571db16b4a04d520e18}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.278: INFO: Pod "nginx-deployment-6f478d8d8-h6zgr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-h6zgr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-h6zgr,UID:133b3cbc-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5638,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58d40 0xc002c58d41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.278: INFO: Pod "nginx-deployment-6f478d8d8-hvhx7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hvhx7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-hvhx7,UID:0d381f93-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5507,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58e00 0xc002c58e01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.20,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://a00b3e2380e2343ed26aeb8b0dfe6062858ac50a95ab2453fa179eee8b0b1008}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.279: INFO: Pod "nginx-deployment-6f478d8d8-j6pbz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-j6pbz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-j6pbz,UID:133b23a0-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5630,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58f10 0xc002c58f11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.279: INFO: Pod "nginx-deployment-6f478d8d8-kr7dm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-kr7dm,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-kr7dm,UID:0d3c8697-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5504,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c58fd0 0xc002c58fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.138,PodIP:172.20.3.11,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://69fde825d6ffd85484f532f2fab7b002d41f3d31427fdb3ff747ab907dfbc335}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.279: INFO: Pod "nginx-deployment-6f478d8d8-njs8n" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-njs8n,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-njs8n,UID:133b3387-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5637,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c590e0 0xc002c590e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.279: INFO: Pod "nginx-deployment-6f478d8d8-nktwd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nktwd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-nktwd,UID:0d4052c6-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5525,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c591a0 0xc002c591a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.134,PodIP:172.20.2.10,StartTime:2019-08-13 06:10:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c005beef1de5533b7f6def1666fc4a116a22adee682c6b7181cf46ee5f7cb466}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.279: INFO: Pod "nginx-deployment-6f478d8d8-nxrzx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-nxrzx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-nxrzx,UID:1338569f-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5647,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c592b0 0xc002c592b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.134,PodIP:,StartTime:2019-08-13 06:10:38 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-q99zg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-q99zg,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-q99zg,UID:1339c552-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5622,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c593b0 0xc002c593b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-qbq2j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-qbq2j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-qbq2j,UID:0d40653e-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5522,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59470 0xc002c59471}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.138,PodIP:172.20.3.13,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:33 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://6202dba9bfdd0b35cdc3778ac351574cd45c34a3b21b8706512582a759da2812}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-s4cj5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s4cj5,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-s4cj5,UID:133e2c99-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5658,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59580 0xc002c59581}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-skph4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-skph4,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-skph4,UID:133b442d-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5639,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59640 0xc002c59641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-vzqw2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vzqw2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-vzqw2,UID:133e54fd-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5660,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59700 0xc002c59701}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-w28dh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-w28dh,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-w28dh,UID:1339b503-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5619,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c597c0 0xc002c597c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.280: INFO: Pod "nginx-deployment-6f478d8d8-wxw5d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wxw5d,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-wxw5d,UID:133e4846-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5659,Generation:0,CreationTimestamp:2019-08-13 06:10:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59880 0xc002c59881}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:40 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 13 06:10:40.281: INFO: Pod "nginx-deployment-6f478d8d8-zfmwd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-zfmwd,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-444,SelfLink:/api/v1/namespaces/deployment-444/pods/nginx-deployment-6f478d8d8-zfmwd,UID:0d3a02aa-bd91-11e9-868a-506b8de0bf77,ResourceVersion:5500,Generation:0,CreationTimestamp:2019-08-13 06:10:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 0d361127-bd91-11e9-868a-506b8de0bf77 0xc002c59940 0xc002c59941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lk9wx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lk9wx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lk9wx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:10:30 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.138,PodIP:172.20.3.12,StartTime:2019-08-13 06:10:30 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:10:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3c5b25fbe15325401c2ca7b4bd30e879f1b20e9359f2f015d10d79c0a9785b3c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:10:40.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-444" for this suite.
Aug 13 06:10:48.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:10:48.439: INFO: namespace deployment-444 deletion completed in 8.145182819s

â€¢ [SLOW TEST:18.387 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:10:48.440: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 13 06:10:48.497: INFO: Waiting up to 5m0s for pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308" in namespace "emptydir-1318" to be "success or failure"
Aug 13 06:10:48.502: INFO: Pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.776276ms
Aug 13 06:10:50.506: INFO: Pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009935059s
Aug 13 06:10:52.510: INFO: Pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013914962s
Aug 13 06:10:54.514: INFO: Pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.017869616s
STEP: Saw pod success
Aug 13 06:10:54.514: INFO: Pod "pod-182b389c-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:10:54.517: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-182b389c-bd91-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:10:54.542: INFO: Waiting for pod pod-182b389c-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:10:54.545: INFO: Pod pod-182b389c-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:10:54.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1318" for this suite.
Aug 13 06:11:00.562: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:11:00.644: INFO: namespace emptydir-1318 deletion completed in 6.094383854s

â€¢ [SLOW TEST:12.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:11:00.644: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-1f6f0a58-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:11:00.695: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308" in namespace "projected-9480" to be "success or failure"
Aug 13 06:11:00.700: INFO: Pod "pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.378618ms
Aug 13 06:11:02.704: INFO: Pod "pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009002082s
Aug 13 06:11:04.709: INFO: Pod "pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013632383s
STEP: Saw pod success
Aug 13 06:11:04.709: INFO: Pod "pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:11:04.714: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:11:04.740: INFO: Waiting for pod pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:11:04.743: INFO: Pod pod-projected-secrets-1f702296-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:11:04.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9480" for this suite.
Aug 13 06:11:10.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:11:10.837: INFO: namespace projected-9480 deletion completed in 6.089913302s

â€¢ [SLOW TEST:10.193 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:11:10.838: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 06:11:10.902: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:10.902: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:10.905: INFO: Number of nodes with available pods: 0
Aug 13 06:11:10.905: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:11.909: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:11.909: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:11.915: INFO: Number of nodes with available pods: 0
Aug 13 06:11:11.915: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:12.910: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:12.910: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:12.918: INFO: Number of nodes with available pods: 0
Aug 13 06:11:12.918: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:13.909: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:13.909: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:13.912: INFO: Number of nodes with available pods: 3
Aug 13 06:11:13.912: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 13 06:11:13.927: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:13.927: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:13.930: INFO: Number of nodes with available pods: 2
Aug 13 06:11:13.930: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:14.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:14.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:14.938: INFO: Number of nodes with available pods: 2
Aug 13 06:11:14.938: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:15.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:15.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:15.937: INFO: Number of nodes with available pods: 2
Aug 13 06:11:15.938: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:16.934: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:16.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:16.937: INFO: Number of nodes with available pods: 2
Aug 13 06:11:16.937: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:17.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:17.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:17.939: INFO: Number of nodes with available pods: 2
Aug 13 06:11:17.939: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:18.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:18.935: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:18.941: INFO: Number of nodes with available pods: 2
Aug 13 06:11:18.941: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:19.934: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:19.934: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:19.938: INFO: Number of nodes with available pods: 2
Aug 13 06:11:19.938: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:11:20.937: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:20.937: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:11:20.941: INFO: Number of nodes with available pods: 3
Aug 13 06:11:20.941: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1116, will wait for the garbage collector to delete the pods
Aug 13 06:11:21.006: INFO: Deleting DaemonSet.extensions daemon-set took: 8.887584ms
Aug 13 06:11:21.406: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.330651ms
Aug 13 06:11:27.910: INFO: Number of nodes with available pods: 0
Aug 13 06:11:27.910: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 06:11:27.914: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-1116/daemonsets","resourceVersion":"6119"},"items":null}

Aug 13 06:11:27.917: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-1116/pods","resourceVersion":"6119"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:11:27.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1116" for this suite.
Aug 13 06:11:33.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:11:34.071: INFO: namespace daemonsets-1116 deletion completed in 6.135631515s

â€¢ [SLOW TEST:23.233 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:11:34.071: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 13 06:11:38.645: INFO: Successfully updated pod "labelsupdate335b21e3-bd91-11e9-8bf8-0a58ac140308"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:11:40.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6941" for this suite.
Aug 13 06:12:02.680: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:12:02.765: INFO: namespace projected-6941 deletion completed in 22.098507607s

â€¢ [SLOW TEST:28.694 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:12:02.765: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0813 06:12:12.875198      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 06:12:12.875: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:12:12.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2650" for this suite.
Aug 13 06:12:18.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:12:18.983: INFO: namespace gc-2650 deletion completed in 6.104206793s

â€¢ [SLOW TEST:16.218 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:12:18.984: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 06:12:19.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1816'
Aug 13 06:12:19.143: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 06:12:19.143: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Aug 13 06:12:19.159: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Aug 13 06:12:19.167: INFO: scanned /root for discovery docs: <nil>
Aug 13 06:12:19.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-1816'
Aug 13 06:12:34.998: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 13 06:12:34.998: INFO: stdout: "Created e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f\nScaling up e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 13 06:12:34.998: INFO: stdout: "Created e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f\nScaling up e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 13 06:12:34.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-1816'
Aug 13 06:12:35.085: INFO: stderr: ""
Aug 13 06:12:35.085: INFO: stdout: "e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f-v92xw "
Aug 13 06:12:35.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f-v92xw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1816'
Aug 13 06:12:35.167: INFO: stderr: ""
Aug 13 06:12:35.167: INFO: stdout: "true"
Aug 13 06:12:35.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f-v92xw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1816'
Aug 13 06:12:35.250: INFO: stderr: ""
Aug 13 06:12:35.250: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 13 06:12:35.250: INFO: e2e-test-nginx-rc-09dc1700a992c3a006c4662af38f6b0f-v92xw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Aug 13 06:12:35.251: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete rc e2e-test-nginx-rc --namespace=kubectl-1816'
Aug 13 06:12:35.339: INFO: stderr: ""
Aug 13 06:12:35.339: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:12:35.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1816" for this suite.
Aug 13 06:12:41.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:12:41.464: INFO: namespace kubectl-1816 deletion completed in 6.119393338s

â€¢ [SLOW TEST:22.480 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:12:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Aug 13 06:12:41.494: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-872026410 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:12:41.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-397" for this suite.
Aug 13 06:12:47.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:12:47.694: INFO: namespace kubectl-397 deletion completed in 6.114616848s

â€¢ [SLOW TEST:6.230 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:12:47.695: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:12:47.734: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308" in namespace "downward-api-394" to be "success or failure"
Aug 13 06:12:47.737: INFO: Pod "downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.832803ms
Aug 13 06:12:49.742: INFO: Pod "downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008487724s
Aug 13 06:12:51.746: INFO: Pod "downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012409772s
STEP: Saw pod success
Aug 13 06:12:51.746: INFO: Pod "downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:12:51.749: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:12:51.769: INFO: Waiting for pod downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:12:51.773: INFO: Pod downwardapi-volume-5f3d0f53-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:12:51.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-394" for this suite.
Aug 13 06:12:57.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:12:57.888: INFO: namespace downward-api-394 deletion completed in 6.108224439s

â€¢ [SLOW TEST:10.193 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:12:57.888: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-6225/secret-test-65500175-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:12:57.933: INFO: Waiting up to 5m0s for pod "pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308" in namespace "secrets-6225" to be "success or failure"
Aug 13 06:12:57.945: INFO: Pod "pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 11.378571ms
Aug 13 06:12:59.949: INFO: Pod "pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015377021s
Aug 13 06:13:01.954: INFO: Pod "pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020004742s
STEP: Saw pod success
Aug 13 06:13:01.954: INFO: Pod "pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:13:01.959: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308 container env-test: <nil>
STEP: delete the pod
Aug 13 06:13:01.993: INFO: Waiting for pod pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:13:01.997: INFO: Pod pod-configmaps-655147f7-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:13:01.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6225" for this suite.
Aug 13 06:13:08.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:13:08.103: INFO: namespace secrets-6225 deletion completed in 6.099694228s

â€¢ [SLOW TEST:10.215 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:13:08.103: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:13:08.136: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:13:12.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4022" for this suite.
Aug 13 06:13:50.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:13:50.274: INFO: namespace pods-4022 deletion completed in 38.093259013s

â€¢ [SLOW TEST:42.171 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:13:50.275: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:13:50.322: INFO: (0) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 11.672063ms)
Aug 13 06:13:50.326: INFO: (1) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.615048ms)
Aug 13 06:13:50.328: INFO: (2) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.575357ms)
Aug 13 06:13:50.331: INFO: (3) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.022727ms)
Aug 13 06:13:50.334: INFO: (4) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.135027ms)
Aug 13 06:13:50.337: INFO: (5) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.684024ms)
Aug 13 06:13:50.340: INFO: (6) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.566274ms)
Aug 13 06:13:50.344: INFO: (7) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.702705ms)
Aug 13 06:13:50.347: INFO: (8) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.936968ms)
Aug 13 06:13:50.351: INFO: (9) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.872406ms)
Aug 13 06:13:50.355: INFO: (10) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.020896ms)
Aug 13 06:13:50.359: INFO: (11) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.492571ms)
Aug 13 06:13:50.365: INFO: (12) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 6.223304ms)
Aug 13 06:13:50.370: INFO: (13) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.675362ms)
Aug 13 06:13:50.374: INFO: (14) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.560422ms)
Aug 13 06:13:50.378: INFO: (15) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.418935ms)
Aug 13 06:13:50.381: INFO: (16) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.286894ms)
Aug 13 06:13:50.386: INFO: (17) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.938462ms)
Aug 13 06:13:50.390: INFO: (18) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.323442ms)
Aug 13 06:13:50.393: INFO: (19) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 2.96294ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:13:50.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4379" for this suite.
Aug 13 06:13:56.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:13:56.496: INFO: namespace proxy-4379 deletion completed in 6.099979437s

â€¢ [SLOW TEST:6.222 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:13:56.497: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:13:56.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308" in namespace "projected-9372" to be "success or failure"
Aug 13 06:13:56.591: INFO: Pod "downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572855ms
Aug 13 06:13:58.595: INFO: Pod "downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006343627s
Aug 13 06:14:00.599: INFO: Pod "downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009991301s
STEP: Saw pod success
Aug 13 06:14:00.599: INFO: Pod "downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:14:00.602: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:14:00.622: INFO: Waiting for pod downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:14:00.625: INFO: Pod downwardapi-volume-8847a8f1-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:00.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9372" for this suite.
Aug 13 06:14:06.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:14:06.737: INFO: namespace projected-9372 deletion completed in 6.107326377s

â€¢ [SLOW TEST:10.240 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:14:06.737: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-8e5f4597-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:14:06.818: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308" in namespace "projected-6706" to be "success or failure"
Aug 13 06:14:06.824: INFO: Pod "pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.261941ms
Aug 13 06:14:08.828: INFO: Pod "pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009755257s
Aug 13 06:14:10.833: INFO: Pod "pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014486512s
STEP: Saw pod success
Aug 13 06:14:10.833: INFO: Pod "pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:14:10.836: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:14:10.866: INFO: Waiting for pod pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:14:10.870: INFO: Pod pod-projected-secrets-8e602637-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:10.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6706" for this suite.
Aug 13 06:14:16.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:14:16.973: INFO: namespace projected-6706 deletion completed in 6.099610053s

â€¢ [SLOW TEST:10.236 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:14:16.973: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-94748f4c-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:14:17.020: INFO: Waiting up to 5m0s for pod "pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308" in namespace "configmap-4763" to be "success or failure"
Aug 13 06:14:17.024: INFO: Pod "pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.932128ms
Aug 13 06:14:19.029: INFO: Pod "pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009254325s
Aug 13 06:14:21.032: INFO: Pod "pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012595424s
STEP: Saw pod success
Aug 13 06:14:21.033: INFO: Pod "pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:14:21.035: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:14:21.056: INFO: Waiting for pod pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:14:21.060: INFO: Pod pod-configmaps-94755678-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:21.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4763" for this suite.
Aug 13 06:14:27.074: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:14:27.157: INFO: namespace configmap-4763 deletion completed in 6.094132119s

â€¢ [SLOW TEST:10.184 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:14:27.158: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 13 06:14:27.193: INFO: Waiting up to 5m0s for pod "downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308" in namespace "downward-api-4759" to be "success or failure"
Aug 13 06:14:27.195: INFO: Pod "downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205265ms
Aug 13 06:14:29.199: INFO: Pod "downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006300914s
Aug 13 06:14:31.204: INFO: Pod "downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010559123s
STEP: Saw pod success
Aug 13 06:14:31.204: INFO: Pod "downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:14:31.207: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:14:31.226: INFO: Waiting for pod downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:14:31.231: INFO: Pod downward-api-9a85ba6a-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:31.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4759" for this suite.
Aug 13 06:14:37.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:14:37.357: INFO: namespace downward-api-4759 deletion completed in 6.122074107s

â€¢ [SLOW TEST:10.199 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:14:37.357: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-a09aac0d-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:14:37.401: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308" in namespace "projected-2558" to be "success or failure"
Aug 13 06:14:37.405: INFO: Pod "pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.205695ms
Aug 13 06:14:39.408: INFO: Pod "pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006749959s
Aug 13 06:14:41.412: INFO: Pod "pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010224353s
STEP: Saw pod success
Aug 13 06:14:41.412: INFO: Pod "pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:14:41.414: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:14:41.433: INFO: Waiting for pod pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:14:41.436: INFO: Pod pod-projected-configmaps-a09b7166-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:41.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2558" for this suite.
Aug 13 06:14:47.450: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:14:47.531: INFO: namespace projected-2558 deletion completed in 6.091981276s

â€¢ [SLOW TEST:10.175 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:14:47.532: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:14:51.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3749" for this suite.
Aug 13 06:15:41.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:15:41.699: INFO: namespace kubelet-test-3749 deletion completed in 50.096386507s

â€¢ [SLOW TEST:54.168 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:15:41.700: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Aug 13 06:15:41.739: INFO: Waiting up to 5m0s for pod "var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308" in namespace "var-expansion-3349" to be "success or failure"
Aug 13 06:15:41.747: INFO: Pod "var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.390564ms
Aug 13 06:15:43.751: INFO: Pod "var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012508884s
Aug 13 06:15:45.754: INFO: Pod "var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015578306s
STEP: Saw pod success
Aug 13 06:15:45.754: INFO: Pod "var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:15:45.757: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:15:45.775: INFO: Waiting for pod var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:15:45.778: INFO: Pod var-expansion-c6f4716c-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:15:45.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3349" for this suite.
Aug 13 06:15:51.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:15:51.877: INFO: namespace var-expansion-3349 deletion completed in 6.09592815s

â€¢ [SLOW TEST:10.177 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:15:51.877: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Aug 13 06:15:51.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-4429'
Aug 13 06:15:52.377: INFO: stderr: ""
Aug 13 06:15:52.377: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:15:52.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4429'
Aug 13 06:15:52.481: INFO: stderr: ""
Aug 13 06:15:52.481: INFO: stdout: "update-demo-nautilus-8gpp6 update-demo-nautilus-sntvp "
Aug 13 06:15:52.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-8gpp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:15:52.575: INFO: stderr: ""
Aug 13 06:15:52.575: INFO: stdout: ""
Aug 13 06:15:52.575: INFO: update-demo-nautilus-8gpp6 is created but not running
Aug 13 06:15:57.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4429'
Aug 13 06:15:57.661: INFO: stderr: ""
Aug 13 06:15:57.661: INFO: stdout: "update-demo-nautilus-8gpp6 update-demo-nautilus-sntvp "
Aug 13 06:15:57.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-8gpp6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:15:57.740: INFO: stderr: ""
Aug 13 06:15:57.740: INFO: stdout: "true"
Aug 13 06:15:57.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-8gpp6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:15:57.820: INFO: stderr: ""
Aug 13 06:15:57.820: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:15:57.820: INFO: validating pod update-demo-nautilus-8gpp6
Aug 13 06:15:57.824: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:15:57.824: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:15:57.824: INFO: update-demo-nautilus-8gpp6 is verified up and running
Aug 13 06:15:57.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-sntvp -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:15:57.917: INFO: stderr: ""
Aug 13 06:15:57.917: INFO: stdout: "true"
Aug 13 06:15:57.917: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-sntvp -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:15:58.004: INFO: stderr: ""
Aug 13 06:15:58.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:15:58.004: INFO: validating pod update-demo-nautilus-sntvp
Aug 13 06:15:58.011: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:15:58.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:15:58.011: INFO: update-demo-nautilus-sntvp is verified up and running
STEP: rolling-update to new replication controller
Aug 13 06:15:58.012: INFO: scanned /root for discovery docs: <nil>
Aug 13 06:15:58.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-4429'
Aug 13 06:16:21.505: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 13 06:16:21.505: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:16:21.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4429'
Aug 13 06:16:21.604: INFO: stderr: ""
Aug 13 06:16:21.604: INFO: stdout: "update-demo-kitten-gdt6w update-demo-kitten-x9lb5 "
Aug 13 06:16:21.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-kitten-gdt6w -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:16:21.689: INFO: stderr: ""
Aug 13 06:16:21.689: INFO: stdout: "true"
Aug 13 06:16:21.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-kitten-gdt6w -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:16:21.779: INFO: stderr: ""
Aug 13 06:16:21.779: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 13 06:16:21.779: INFO: validating pod update-demo-kitten-gdt6w
Aug 13 06:16:21.783: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 13 06:16:21.784: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 13 06:16:21.784: INFO: update-demo-kitten-gdt6w is verified up and running
Aug 13 06:16:21.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-kitten-x9lb5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:16:21.869: INFO: stderr: ""
Aug 13 06:16:21.869: INFO: stdout: "true"
Aug 13 06:16:21.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-kitten-x9lb5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4429'
Aug 13 06:16:21.955: INFO: stderr: ""
Aug 13 06:16:21.955: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 13 06:16:21.955: INFO: validating pod update-demo-kitten-x9lb5
Aug 13 06:16:21.958: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 13 06:16:21.958: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 13 06:16:21.958: INFO: update-demo-kitten-x9lb5 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:16:21.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4429" for this suite.
Aug 13 06:16:43.973: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:16:44.060: INFO: namespace kubectl-4429 deletion completed in 22.098455855s

â€¢ [SLOW TEST:52.183 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:16:44.060: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-ec20aea7-bd91-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:16:44.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308" in namespace "configmap-736" to be "success or failure"
Aug 13 06:16:44.115: INFO: Pod "pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.773997ms
Aug 13 06:16:46.119: INFO: Pod "pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007287738s
Aug 13 06:16:48.123: INFO: Pod "pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011271295s
STEP: Saw pod success
Aug 13 06:16:48.123: INFO: Pod "pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:16:48.126: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:16:48.146: INFO: Waiting for pod pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:16:48.149: INFO: Pod pod-configmaps-ec216cc0-bd91-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:16:48.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-736" for this suite.
Aug 13 06:16:54.167: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:16:54.249: INFO: namespace configmap-736 deletion completed in 6.09375151s

â€¢ [SLOW TEST:10.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:16:54.249: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0813 06:17:24.816304      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 06:17:24.816: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:17:24.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4381" for this suite.
Aug 13 06:17:30.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:17:30.912: INFO: namespace gc-4381 deletion completed in 6.092888171s

â€¢ [SLOW TEST:36.663 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:17:30.912: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-080c2e61-bd92-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:17:30.952: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308" in namespace "projected-1982" to be "success or failure"
Aug 13 06:17:30.958: INFO: Pod "pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.019245ms
Aug 13 06:17:32.962: INFO: Pod "pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010569904s
Aug 13 06:17:34.966: INFO: Pod "pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014488854s
STEP: Saw pod success
Aug 13 06:17:34.966: INFO: Pod "pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:17:34.969: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:17:34.989: INFO: Waiting for pod pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:17:34.992: INFO: Pod pod-projected-configmaps-080cc23b-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:17:34.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1982" for this suite.
Aug 13 06:17:41.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:17:41.093: INFO: namespace projected-1982 deletion completed in 6.098465149s

â€¢ [SLOW TEST:10.181 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:17:41.094: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 13 06:17:41.131: INFO: Waiting up to 5m0s for pod "pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308" in namespace "emptydir-532" to be "success or failure"
Aug 13 06:17:41.136: INFO: Pod "pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.083416ms
Aug 13 06:17:43.139: INFO: Pod "pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008471427s
STEP: Saw pod success
Aug 13 06:17:43.139: INFO: Pod "pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:17:43.142: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:17:43.163: INFO: Waiting for pod pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:17:43.166: INFO: Pod pod-0e1e27f5-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:17:43.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-532" for this suite.
Aug 13 06:17:49.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:17:49.268: INFO: namespace emptydir-532 deletion completed in 6.097501989s

â€¢ [SLOW TEST:8.174 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:17:49.268: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 13 06:17:49.305: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7706,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 06:17:49.306: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7706,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 13 06:17:59.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7724,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 13 06:17:59.314: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7724,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 13 06:18:09.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7743,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 06:18:09.324: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7743,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 13 06:18:19.331: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7762,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 06:18:19.331: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-a,UID:12fdfd04-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7762,Generation:0,CreationTimestamp:2019-08-13 06:17:49 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 13 06:18:29.346: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-b,UID:2ada2dca-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7780,Generation:0,CreationTimestamp:2019-08-13 06:18:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 06:18:29.346: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-b,UID:2ada2dca-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7780,Generation:0,CreationTimestamp:2019-08-13 06:18:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 13 06:18:39.354: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-b,UID:2ada2dca-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7801,Generation:0,CreationTimestamp:2019-08-13 06:18:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 06:18:39.354: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6546,SelfLink:/api/v1/namespaces/watch-6546/configmaps/e2e-watch-test-configmap-b,UID:2ada2dca-bd92-11e9-868a-506b8de0bf77,ResourceVersion:7801,Generation:0,CreationTimestamp:2019-08-13 06:18:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:18:49.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6546" for this suite.
Aug 13 06:18:55.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:18:55.457: INFO: namespace watch-6546 deletion completed in 6.097827277s

â€¢ [SLOW TEST:66.189 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:18:55.457: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Aug 13 06:18:55.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 api-versions'
Aug 13 06:18:55.581: INFO: stderr: ""
Aug 13 06:18:55.581: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1alpha1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:18:55.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7675" for this suite.
Aug 13 06:19:01.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:19:01.694: INFO: namespace kubectl-7675 deletion completed in 6.107224292s

â€¢ [SLOW TEST:6.237 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:19:01.694: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-3e2a5199-bd92-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:19:01.752: INFO: Waiting up to 5m0s for pod "pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308" in namespace "secrets-3201" to be "success or failure"
Aug 13 06:19:01.758: INFO: Pod "pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.605233ms
Aug 13 06:19:03.763: INFO: Pod "pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011313312s
Aug 13 06:19:05.767: INFO: Pod "pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015578136s
STEP: Saw pod success
Aug 13 06:19:05.767: INFO: Pod "pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:19:05.771: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308 container secret-env-test: <nil>
STEP: delete the pod
Aug 13 06:19:05.797: INFO: Waiting for pod pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:19:05.801: INFO: Pod pod-secrets-3e2b7a28-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:19:05.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3201" for this suite.
Aug 13 06:19:11.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:19:11.905: INFO: namespace secrets-3201 deletion completed in 6.100130346s

â€¢ [SLOW TEST:10.210 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:19:11.905: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:19:11.939: INFO: Waiting up to 5m0s for pod "downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308" in namespace "projected-6405" to be "success or failure"
Aug 13 06:19:11.953: INFO: Pod "downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 14.033909ms
Aug 13 06:19:13.957: INFO: Pod "downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017679356s
Aug 13 06:19:15.961: INFO: Pod "downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021911439s
STEP: Saw pod success
Aug 13 06:19:15.961: INFO: Pod "downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:19:15.964: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:19:15.983: INFO: Waiting for pod downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:19:15.986: INFO: Pod downwardapi-volume-443e6a80-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:19:15.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6405" for this suite.
Aug 13 06:19:22.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:19:22.091: INFO: namespace projected-6405 deletion completed in 6.101241152s

â€¢ [SLOW TEST:10.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:19:22.092: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 13 06:19:32.163: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 06:19:32.165: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 06:19:34.165: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 06:19:34.169: INFO: Pod pod-with-prestop-http-hook still exists
Aug 13 06:19:36.165: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 13 06:19:36.169: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:19:36.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2247" for this suite.
Aug 13 06:19:58.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:19:58.270: INFO: namespace container-lifecycle-hook-2247 deletion completed in 22.089243461s

â€¢ [SLOW TEST:36.179 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:19:58.271: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:19:58.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308" in namespace "projected-3871" to be "success or failure"
Aug 13 06:19:58.313: INFO: Pod "downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.467533ms
Aug 13 06:20:00.317: INFO: Pod "downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007227196s
Aug 13 06:20:02.321: INFO: Pod "downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011322865s
STEP: Saw pod success
Aug 13 06:20:02.321: INFO: Pod "downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:20:02.324: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:20:02.357: INFO: Waiting for pod downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:20:02.363: INFO: Pod downwardapi-volume-5fe1dc93-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:20:02.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3871" for this suite.
Aug 13 06:20:08.383: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:20:08.462: INFO: namespace projected-3871 deletion completed in 6.093872449s

â€¢ [SLOW TEST:10.191 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:20:08.462: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 13 06:20:13.024: INFO: Successfully updated pod "pod-update-activedeadlineseconds-65f49f5e-bd92-11e9-8bf8-0a58ac140308"
Aug 13 06:20:13.024: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-65f49f5e-bd92-11e9-8bf8-0a58ac140308" in namespace "pods-63" to be "terminated due to deadline exceeded"
Aug 13 06:20:13.041: INFO: Pod "pod-update-activedeadlineseconds-65f49f5e-bd92-11e9-8bf8-0a58ac140308": Phase="Running", Reason="", readiness=true. Elapsed: 17.681723ms
Aug 13 06:20:15.047: INFO: Pod "pod-update-activedeadlineseconds-65f49f5e-bd92-11e9-8bf8-0a58ac140308": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.02294026s
Aug 13 06:20:15.047: INFO: Pod "pod-update-activedeadlineseconds-65f49f5e-bd92-11e9-8bf8-0a58ac140308" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:20:15.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-63" for this suite.
Aug 13 06:20:21.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:20:21.165: INFO: namespace pods-63 deletion completed in 6.114568261s

â€¢ [SLOW TEST:12.703 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:20:21.166: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:20:21.209: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308" in namespace "projected-7751" to be "success or failure"
Aug 13 06:20:21.215: INFO: Pod "downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168512ms
Aug 13 06:20:23.218: INFO: Pod "downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007733085s
Aug 13 06:20:25.225: INFO: Pod "downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014676924s
STEP: Saw pod success
Aug 13 06:20:25.225: INFO: Pod "downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:20:25.228: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:20:25.249: INFO: Waiting for pod downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:20:25.252: INFO: Pod downwardapi-volume-6d882425-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:20:25.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7751" for this suite.
Aug 13 06:20:31.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:20:31.353: INFO: namespace projected-7751 deletion completed in 6.097991916s

â€¢ [SLOW TEST:10.188 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:20:31.354: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:20:31.407: INFO: Waiting up to 5m0s for pod "downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308" in namespace "downward-api-6618" to be "success or failure"
Aug 13 06:20:31.410: INFO: Pod "downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05707ms
Aug 13 06:20:33.414: INFO: Pod "downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007068652s
Aug 13 06:20:35.418: INFO: Pod "downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010967263s
STEP: Saw pod success
Aug 13 06:20:35.418: INFO: Pod "downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:20:35.420: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:20:35.440: INFO: Waiting for pod downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:20:35.442: INFO: Pod downwardapi-volume-739bad4e-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:20:35.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6618" for this suite.
Aug 13 06:20:41.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:20:41.537: INFO: namespace downward-api-6618 deletion completed in 6.089849578s

â€¢ [SLOW TEST:10.184 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:20:41.537: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-79ad6244-bd92-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-79ad6244-bd92-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:20:47.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6699" for this suite.
Aug 13 06:21:09.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:21:09.729: INFO: namespace configmap-6699 deletion completed in 22.093454586s

â€¢ [SLOW TEST:28.192 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:21:09.730: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Aug 13 06:21:09.774: INFO: Waiting up to 5m0s for pod "var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308" in namespace "var-expansion-6507" to be "success or failure"
Aug 13 06:21:09.778: INFO: Pod "var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.776403ms
Aug 13 06:21:11.782: INFO: Pod "var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007641984s
Aug 13 06:21:13.785: INFO: Pod "var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011157183s
STEP: Saw pod success
Aug 13 06:21:13.785: INFO: Pod "var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:21:13.788: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:21:13.807: INFO: Waiting for pod var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:21:13.810: INFO: Pod var-expansion-8a7a28be-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:21:13.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6507" for this suite.
Aug 13 06:21:19.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:21:19.917: INFO: namespace var-expansion-6507 deletion completed in 6.104072803s

â€¢ [SLOW TEST:10.187 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:21:19.917: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Aug 13 06:21:20.535: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 13 06:21:22.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:21:24.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:21:26.584: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:21:28.583: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274080, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:21:31.414: INFO: Waited 823.965916ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:21:31.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7957" for this suite.
Aug 13 06:21:38.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:21:38.147: INFO: namespace aggregator-7957 deletion completed in 6.190698806s

â€¢ [SLOW TEST:18.230 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:21:38.148: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-7795/configmap-test-9b69991a-bd92-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:21:38.189: INFO: Waiting up to 5m0s for pod "pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308" in namespace "configmap-7795" to be "success or failure"
Aug 13 06:21:38.192: INFO: Pod "pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.800966ms
Aug 13 06:21:40.195: INFO: Pod "pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006528518s
Aug 13 06:21:42.199: INFO: Pod "pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009847117s
STEP: Saw pod success
Aug 13 06:21:42.199: INFO: Pod "pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:21:42.201: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308 container env-test: <nil>
STEP: delete the pod
Aug 13 06:21:42.221: INFO: Waiting for pod pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:21:42.223: INFO: Pod pod-configmaps-9b6a330c-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:21:42.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7795" for this suite.
Aug 13 06:21:48.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:21:48.341: INFO: namespace configmap-7795 deletion completed in 6.113386502s

â€¢ [SLOW TEST:10.193 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:21:48.341: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-2660
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 06:21:48.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 06:22:16.473: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.77:8080/dial?request=hostName&protocol=udp&host=172.20.3.30&port=8081&tries=1'] Namespace:pod-network-test-2660 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 06:22:16.473: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 06:22:16.562: INFO: Waiting for endpoints: map[]
Aug 13 06:22:16.566: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.77:8080/dial?request=hostName&protocol=udp&host=172.20.4.76&port=8081&tries=1'] Namespace:pod-network-test-2660 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 06:22:16.566: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 06:22:16.654: INFO: Waiting for endpoints: map[]
Aug 13 06:22:16.657: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.77:8080/dial?request=hostName&protocol=udp&host=172.20.2.34&port=8081&tries=1'] Namespace:pod-network-test-2660 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 06:22:16.657: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 06:22:16.768: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:22:16.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2660" for this suite.
Aug 13 06:22:38.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:22:38.899: INFO: namespace pod-network-test-2660 deletion completed in 22.125538415s

â€¢ [SLOW TEST:50.558 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:22:38.899: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Aug 13 06:22:38.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 --namespace=kubectl-4376 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 13 06:22:41.790: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 13 06:22:41.790: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:22:43.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4376" for this suite.
Aug 13 06:22:49.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:22:49.903: INFO: namespace kubectl-4376 deletion completed in 6.104476319s

â€¢ [SLOW TEST:11.004 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:22:49.903: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 13 06:22:49.943: INFO: Waiting up to 5m0s for pod "pod-c62f5027-bd92-11e9-8bf8-0a58ac140308" in namespace "emptydir-6730" to be "success or failure"
Aug 13 06:22:49.950: INFO: Pod "pod-c62f5027-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 7.439011ms
Aug 13 06:22:51.955: INFO: Pod "pod-c62f5027-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012099848s
Aug 13 06:22:53.959: INFO: Pod "pod-c62f5027-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015911723s
STEP: Saw pod success
Aug 13 06:22:53.959: INFO: Pod "pod-c62f5027-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:22:53.962: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-c62f5027-bd92-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:22:53.982: INFO: Waiting for pod pod-c62f5027-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:22:53.985: INFO: Pod pod-c62f5027-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:22:53.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6730" for this suite.
Aug 13 06:23:00.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:23:00.107: INFO: namespace emptydir-6730 deletion completed in 6.117466234s

â€¢ [SLOW TEST:10.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:23:00.107: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:24:00.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3376" for this suite.
Aug 13 06:24:22.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:24:22.250: INFO: namespace container-probe-3376 deletion completed in 22.100617058s

â€¢ [SLOW TEST:82.143 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:24:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-fd39ac83-bd92-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:24:22.291: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308" in namespace "projected-3460" to be "success or failure"
Aug 13 06:24:22.297: INFO: Pod "pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.055061ms
Aug 13 06:24:24.301: INFO: Pod "pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010112999s
Aug 13 06:24:26.304: INFO: Pod "pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01379887s
STEP: Saw pod success
Aug 13 06:24:26.304: INFO: Pod "pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:24:26.307: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:24:26.327: INFO: Waiting for pod pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:24:26.329: INFO: Pod pod-projected-configmaps-fd3a3aa3-bd92-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:24:26.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3460" for this suite.
Aug 13 06:24:32.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:24:32.430: INFO: namespace projected-3460 deletion completed in 6.097451988s

â€¢ [SLOW TEST:10.179 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:24:32.430: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:24:32.480: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 06:24:32.496: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:32.496: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:32.503: INFO: Number of nodes with available pods: 0
Aug 13 06:24:32.503: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:24:33.508: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:33.508: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:33.512: INFO: Number of nodes with available pods: 0
Aug 13 06:24:33.512: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:24:34.507: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:34.508: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:34.510: INFO: Number of nodes with available pods: 0
Aug 13 06:24:34.511: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:24:35.508: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:35.508: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:35.511: INFO: Number of nodes with available pods: 1
Aug 13 06:24:35.511: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:24:36.507: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:36.507: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:36.510: INFO: Number of nodes with available pods: 3
Aug 13 06:24:36.510: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 13 06:24:36.537: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:36.537: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:36.537: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:36.542: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:36.542: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:37.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:37.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:37.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:37.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:37.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:38.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:38.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:38.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:38.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:38.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:38.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:39.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:39.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:39.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:39.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:39.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:39.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:40.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:40.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:40.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:40.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:40.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:40.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:41.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:41.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:41.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:41.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:41.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:41.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:42.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:42.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:42.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:42.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:42.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:42.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:43.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:43.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:43.545: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:43.545: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:43.548: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:43.548: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:44.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:44.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:44.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:44.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:44.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:44.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:45.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:45.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:45.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:45.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:45.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:45.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:46.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:46.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:46.546: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:46.546: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:46.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:46.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:47.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:47.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:47.545: INFO: Wrong image for pod: daemon-set-zfw9n. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:47.545: INFO: Pod daemon-set-zfw9n is not available
Aug 13 06:24:47.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:47.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:48.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:48.545: INFO: Pod daemon-set-7j5lv is not available
Aug 13 06:24:48.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:48.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:48.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:49.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:49.545: INFO: Pod daemon-set-7j5lv is not available
Aug 13 06:24:49.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:49.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:49.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:50.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:50.546: INFO: Pod daemon-set-7j5lv is not available
Aug 13 06:24:50.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:50.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:50.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:51.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:51.546: INFO: Pod daemon-set-7j5lv is not available
Aug 13 06:24:51.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:51.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:51.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:52.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:52.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:52.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:52.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:53.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:53.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:53.546: INFO: Pod daemon-set-pghj8 is not available
Aug 13 06:24:53.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:53.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:54.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:54.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:54.546: INFO: Pod daemon-set-pghj8 is not available
Aug 13 06:24:54.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:54.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:55.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:55.545: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:55.545: INFO: Pod daemon-set-pghj8 is not available
Aug 13 06:24:55.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:55.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:56.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:56.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:56.546: INFO: Pod daemon-set-pghj8 is not available
Aug 13 06:24:56.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:56.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:57.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:57.546: INFO: Wrong image for pod: daemon-set-pghj8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:57.546: INFO: Pod daemon-set-pghj8 is not available
Aug 13 06:24:57.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:57.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:58.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:58.546: INFO: Pod daemon-set-9s2lh is not available
Aug 13 06:24:58.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:58.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:59.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:24:59.545: INFO: Pod daemon-set-9s2lh is not available
Aug 13 06:24:59.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:24:59.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:00.545: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:00.545: INFO: Pod daemon-set-9s2lh is not available
Aug 13 06:25:00.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:00.549: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:01.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:01.546: INFO: Pod daemon-set-9s2lh is not available
Aug 13 06:25:01.552: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:01.552: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:02.547: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:02.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:02.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:03.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:03.546: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:03.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:03.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:04.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:04.546: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:04.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:04.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:05.547: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:05.547: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:05.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:05.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:06.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:06.546: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:06.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:06.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:07.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:07.546: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:07.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:07.550: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:08.546: INFO: Wrong image for pod: daemon-set-2hvr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 13 06:25:08.546: INFO: Pod daemon-set-2hvr8 is not available
Aug 13 06:25:08.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:08.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:09.546: INFO: Pod daemon-set-fn4wz is not available
Aug 13 06:25:09.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:09.551: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 13 06:25:09.555: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:09.555: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:09.558: INFO: Number of nodes with available pods: 2
Aug 13 06:25:09.558: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-2 is running more than one daemon pod
Aug 13 06:25:10.562: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:10.562: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:10.565: INFO: Number of nodes with available pods: 2
Aug 13 06:25:10.565: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-2 is running more than one daemon pod
Aug 13 06:25:11.563: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:11.563: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:25:11.567: INFO: Number of nodes with available pods: 3
Aug 13 06:25:11.567: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6268, will wait for the garbage collector to delete the pods
Aug 13 06:25:11.640: INFO: Deleting DaemonSet.extensions daemon-set took: 7.54452ms
Aug 13 06:25:12.040: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.276785ms
Aug 13 06:25:17.844: INFO: Number of nodes with available pods: 0
Aug 13 06:25:17.844: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 06:25:17.846: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6268/daemonsets","resourceVersion":"9202"},"items":null}

Aug 13 06:25:17.848: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6268/pods","resourceVersion":"9202"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:25:17.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6268" for this suite.
Aug 13 06:25:23.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:25:23.954: INFO: namespace daemonsets-6268 deletion completed in 6.093084569s

â€¢ [SLOW TEST:51.523 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:25:23.954: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 13 06:25:24.003: INFO: Waiting up to 5m0s for pod "downward-api-22026707-bd93-11e9-8bf8-0a58ac140308" in namespace "downward-api-7711" to be "success or failure"
Aug 13 06:25:24.009: INFO: Pod "downward-api-22026707-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.265772ms
Aug 13 06:25:26.013: INFO: Pod "downward-api-22026707-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010816213s
Aug 13 06:25:28.018: INFO: Pod "downward-api-22026707-bd93-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015433176s
STEP: Saw pod success
Aug 13 06:25:28.018: INFO: Pod "downward-api-22026707-bd93-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:25:28.024: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downward-api-22026707-bd93-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:25:28.049: INFO: Waiting for pod downward-api-22026707-bd93-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:25:28.052: INFO: Pod downward-api-22026707-bd93-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:25:28.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7711" for this suite.
Aug 13 06:25:34.068: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:25:34.163: INFO: namespace downward-api-7711 deletion completed in 6.105697988s

â€¢ [SLOW TEST:10.209 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:25:34.163: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-5494
I0813 06:25:34.207223      18 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5494, replica count: 1
I0813 06:25:35.257748      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 06:25:36.257984      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 06:25:37.258251      18 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 13 06:25:37.370: INFO: Created: latency-svc-6gsnn
Aug 13 06:25:37.382: INFO: Got endpoints: latency-svc-6gsnn [23.62736ms]
Aug 13 06:25:37.402: INFO: Created: latency-svc-csjzz
Aug 13 06:25:37.415: INFO: Got endpoints: latency-svc-csjzz [32.585282ms]
Aug 13 06:25:37.418: INFO: Created: latency-svc-q48dv
Aug 13 06:25:37.425: INFO: Got endpoints: latency-svc-q48dv [43.632559ms]
Aug 13 06:25:37.429: INFO: Created: latency-svc-6g5cn
Aug 13 06:25:37.435: INFO: Got endpoints: latency-svc-6g5cn [53.093147ms]
Aug 13 06:25:37.442: INFO: Created: latency-svc-mctd5
Aug 13 06:25:37.449: INFO: Got endpoints: latency-svc-mctd5 [66.305771ms]
Aug 13 06:25:37.451: INFO: Created: latency-svc-nxcw2
Aug 13 06:25:37.461: INFO: Got endpoints: latency-svc-nxcw2 [78.360083ms]
Aug 13 06:25:37.462: INFO: Created: latency-svc-6lcdc
Aug 13 06:25:37.472: INFO: Got endpoints: latency-svc-6lcdc [89.620831ms]
Aug 13 06:25:37.478: INFO: Created: latency-svc-dv662
Aug 13 06:25:37.483: INFO: Got endpoints: latency-svc-dv662 [101.069486ms]
Aug 13 06:25:37.486: INFO: Created: latency-svc-85cgl
Aug 13 06:25:37.492: INFO: Got endpoints: latency-svc-85cgl [109.431813ms]
Aug 13 06:25:37.496: INFO: Created: latency-svc-b7stt
Aug 13 06:25:37.501: INFO: Got endpoints: latency-svc-b7stt [118.958496ms]
Aug 13 06:25:37.505: INFO: Created: latency-svc-l6zh9
Aug 13 06:25:37.510: INFO: Got endpoints: latency-svc-l6zh9 [127.949386ms]
Aug 13 06:25:37.514: INFO: Created: latency-svc-wz6ds
Aug 13 06:25:37.525: INFO: Got endpoints: latency-svc-wz6ds [142.632025ms]
Aug 13 06:25:37.533: INFO: Created: latency-svc-989bm
Aug 13 06:25:37.537: INFO: Created: latency-svc-l4k46
Aug 13 06:25:37.537: INFO: Got endpoints: latency-svc-989bm [154.706114ms]
Aug 13 06:25:37.544: INFO: Got endpoints: latency-svc-l4k46 [161.23559ms]
Aug 13 06:25:37.554: INFO: Created: latency-svc-f2z8s
Aug 13 06:25:37.554: INFO: Got endpoints: latency-svc-f2z8s [171.92304ms]
Aug 13 06:25:37.559: INFO: Created: latency-svc-mkcm4
Aug 13 06:25:37.566: INFO: Got endpoints: latency-svc-mkcm4 [183.630066ms]
Aug 13 06:25:37.566: INFO: Created: latency-svc-jd8v9
Aug 13 06:25:37.576: INFO: Got endpoints: latency-svc-jd8v9 [161.758331ms]
Aug 13 06:25:37.580: INFO: Created: latency-svc-nhcx7
Aug 13 06:25:37.592: INFO: Got endpoints: latency-svc-nhcx7 [166.420995ms]
Aug 13 06:25:37.592: INFO: Created: latency-svc-7bpdh
Aug 13 06:25:37.597: INFO: Got endpoints: latency-svc-7bpdh [161.756437ms]
Aug 13 06:25:37.600: INFO: Created: latency-svc-r4ztt
Aug 13 06:25:37.605: INFO: Got endpoints: latency-svc-r4ztt [156.163967ms]
Aug 13 06:25:37.610: INFO: Created: latency-svc-6gtm8
Aug 13 06:25:37.614: INFO: Created: latency-svc-tgv26
Aug 13 06:25:37.619: INFO: Got endpoints: latency-svc-6gtm8 [158.470925ms]
Aug 13 06:25:37.627: INFO: Got endpoints: latency-svc-tgv26 [154.594768ms]
Aug 13 06:25:37.628: INFO: Created: latency-svc-q2kcm
Aug 13 06:25:37.632: INFO: Got endpoints: latency-svc-q2kcm [148.603716ms]
Aug 13 06:25:37.640: INFO: Created: latency-svc-t4lcz
Aug 13 06:25:37.647: INFO: Got endpoints: latency-svc-t4lcz [155.636203ms]
Aug 13 06:25:37.651: INFO: Created: latency-svc-8b24s
Aug 13 06:25:37.654: INFO: Got endpoints: latency-svc-8b24s [152.352879ms]
Aug 13 06:25:37.657: INFO: Created: latency-svc-hlpgp
Aug 13 06:25:37.665: INFO: Got endpoints: latency-svc-hlpgp [154.891257ms]
Aug 13 06:25:37.667: INFO: Created: latency-svc-bjtxq
Aug 13 06:25:37.671: INFO: Got endpoints: latency-svc-bjtxq [145.711531ms]
Aug 13 06:25:37.677: INFO: Created: latency-svc-5qkd8
Aug 13 06:25:37.683: INFO: Got endpoints: latency-svc-5qkd8 [145.878038ms]
Aug 13 06:25:37.689: INFO: Created: latency-svc-6xs54
Aug 13 06:25:37.693: INFO: Got endpoints: latency-svc-6xs54 [149.712391ms]
Aug 13 06:25:37.697: INFO: Created: latency-svc-hzkzq
Aug 13 06:25:37.704: INFO: Got endpoints: latency-svc-hzkzq [149.905885ms]
Aug 13 06:25:37.708: INFO: Created: latency-svc-2k6rn
Aug 13 06:25:37.716: INFO: Got endpoints: latency-svc-2k6rn [150.251893ms]
Aug 13 06:25:37.721: INFO: Created: latency-svc-pz6gl
Aug 13 06:25:37.726: INFO: Got endpoints: latency-svc-pz6gl [149.822718ms]
Aug 13 06:25:37.730: INFO: Created: latency-svc-lvclw
Aug 13 06:25:37.735: INFO: Got endpoints: latency-svc-lvclw [142.591317ms]
Aug 13 06:25:37.738: INFO: Created: latency-svc-fnp9h
Aug 13 06:25:37.747: INFO: Got endpoints: latency-svc-fnp9h [149.735536ms]
Aug 13 06:25:37.750: INFO: Created: latency-svc-2pkdn
Aug 13 06:25:37.759: INFO: Got endpoints: latency-svc-2pkdn [154.370943ms]
Aug 13 06:25:37.762: INFO: Created: latency-svc-56tzn
Aug 13 06:25:37.767: INFO: Got endpoints: latency-svc-56tzn [147.660946ms]
Aug 13 06:25:37.771: INFO: Created: latency-svc-dqq4s
Aug 13 06:25:37.778: INFO: Got endpoints: latency-svc-dqq4s [151.497574ms]
Aug 13 06:25:37.781: INFO: Created: latency-svc-t7bjk
Aug 13 06:25:37.790: INFO: Got endpoints: latency-svc-t7bjk [157.94641ms]
Aug 13 06:25:37.793: INFO: Created: latency-svc-jmssc
Aug 13 06:25:37.800: INFO: Created: latency-svc-kqchr
Aug 13 06:25:37.807: INFO: Created: latency-svc-ptw97
Aug 13 06:25:37.818: INFO: Created: latency-svc-fnm5d
Aug 13 06:25:37.828: INFO: Got endpoints: latency-svc-jmssc [180.507493ms]
Aug 13 06:25:37.830: INFO: Created: latency-svc-n4h26
Aug 13 06:25:37.838: INFO: Created: latency-svc-qfg2n
Aug 13 06:25:37.846: INFO: Created: latency-svc-mwczq
Aug 13 06:25:37.855: INFO: Created: latency-svc-vr88n
Aug 13 06:25:37.862: INFO: Created: latency-svc-kprxz
Aug 13 06:25:37.876: INFO: Got endpoints: latency-svc-kqchr [222.17761ms]
Aug 13 06:25:37.880: INFO: Created: latency-svc-ck2hl
Aug 13 06:25:37.886: INFO: Created: latency-svc-lvd8h
Aug 13 06:25:37.897: INFO: Created: latency-svc-nwkq6
Aug 13 06:25:37.903: INFO: Created: latency-svc-7fwtn
Aug 13 06:25:37.909: INFO: Created: latency-svc-g2cx9
Aug 13 06:25:37.915: INFO: Created: latency-svc-sqbtp
Aug 13 06:25:37.926: INFO: Created: latency-svc-q5l2h
Aug 13 06:25:37.930: INFO: Got endpoints: latency-svc-ptw97 [264.56953ms]
Aug 13 06:25:37.933: INFO: Created: latency-svc-lpjq5
Aug 13 06:25:37.942: INFO: Created: latency-svc-mlxkp
Aug 13 06:25:37.979: INFO: Got endpoints: latency-svc-fnm5d [308.363122ms]
Aug 13 06:25:37.996: INFO: Created: latency-svc-p9tzj
Aug 13 06:25:38.026: INFO: Got endpoints: latency-svc-n4h26 [342.654575ms]
Aug 13 06:25:38.038: INFO: Created: latency-svc-zrd2w
Aug 13 06:25:38.085: INFO: Got endpoints: latency-svc-qfg2n [391.11638ms]
Aug 13 06:25:38.100: INFO: Created: latency-svc-bxxgg
Aug 13 06:25:38.126: INFO: Got endpoints: latency-svc-mwczq [421.478928ms]
Aug 13 06:25:38.137: INFO: Created: latency-svc-xxc6l
Aug 13 06:25:38.177: INFO: Got endpoints: latency-svc-vr88n [460.547001ms]
Aug 13 06:25:38.193: INFO: Created: latency-svc-2k29n
Aug 13 06:25:38.227: INFO: Got endpoints: latency-svc-kprxz [500.541178ms]
Aug 13 06:25:38.239: INFO: Created: latency-svc-p8hrw
Aug 13 06:25:38.276: INFO: Got endpoints: latency-svc-ck2hl [541.07369ms]
Aug 13 06:25:38.288: INFO: Created: latency-svc-qwhmp
Aug 13 06:25:38.329: INFO: Got endpoints: latency-svc-lvd8h [581.650366ms]
Aug 13 06:25:38.341: INFO: Created: latency-svc-kgx9c
Aug 13 06:25:38.377: INFO: Got endpoints: latency-svc-nwkq6 [617.37952ms]
Aug 13 06:25:38.394: INFO: Created: latency-svc-t8t5l
Aug 13 06:25:38.427: INFO: Got endpoints: latency-svc-7fwtn [660.529515ms]
Aug 13 06:25:38.439: INFO: Created: latency-svc-rsjc2
Aug 13 06:25:38.477: INFO: Got endpoints: latency-svc-g2cx9 [699.400191ms]
Aug 13 06:25:38.491: INFO: Created: latency-svc-6nqt4
Aug 13 06:25:38.527: INFO: Got endpoints: latency-svc-sqbtp [736.626404ms]
Aug 13 06:25:38.538: INFO: Created: latency-svc-ctr7v
Aug 13 06:25:38.579: INFO: Got endpoints: latency-svc-q5l2h [750.839427ms]
Aug 13 06:25:38.600: INFO: Created: latency-svc-gld2d
Aug 13 06:25:38.625: INFO: Got endpoints: latency-svc-lpjq5 [749.299096ms]
Aug 13 06:25:38.638: INFO: Created: latency-svc-lj9nl
Aug 13 06:25:38.676: INFO: Got endpoints: latency-svc-mlxkp [745.576611ms]
Aug 13 06:25:38.687: INFO: Created: latency-svc-s7fs9
Aug 13 06:25:38.725: INFO: Got endpoints: latency-svc-p9tzj [746.189904ms]
Aug 13 06:25:38.737: INFO: Created: latency-svc-gx5k5
Aug 13 06:25:38.777: INFO: Got endpoints: latency-svc-zrd2w [751.441966ms]
Aug 13 06:25:38.790: INFO: Created: latency-svc-4pmdh
Aug 13 06:25:38.826: INFO: Got endpoints: latency-svc-bxxgg [741.008314ms]
Aug 13 06:25:38.846: INFO: Created: latency-svc-xvlk5
Aug 13 06:25:38.885: INFO: Got endpoints: latency-svc-xxc6l [759.233258ms]
Aug 13 06:25:38.910: INFO: Created: latency-svc-gk2hs
Aug 13 06:25:38.926: INFO: Got endpoints: latency-svc-2k29n [749.6909ms]
Aug 13 06:25:38.937: INFO: Created: latency-svc-4ng98
Aug 13 06:25:38.983: INFO: Got endpoints: latency-svc-p8hrw [756.297669ms]
Aug 13 06:25:38.996: INFO: Created: latency-svc-wj4z2
Aug 13 06:25:39.041: INFO: Got endpoints: latency-svc-qwhmp [765.48003ms]
Aug 13 06:25:39.076: INFO: Created: latency-svc-f9x7g
Aug 13 06:25:39.086: INFO: Got endpoints: latency-svc-kgx9c [756.898042ms]
Aug 13 06:25:39.105: INFO: Created: latency-svc-2gg5m
Aug 13 06:25:39.128: INFO: Got endpoints: latency-svc-t8t5l [751.099949ms]
Aug 13 06:25:39.139: INFO: Created: latency-svc-ggqbt
Aug 13 06:25:39.184: INFO: Got endpoints: latency-svc-rsjc2 [756.123128ms]
Aug 13 06:25:39.200: INFO: Created: latency-svc-w8zwf
Aug 13 06:25:39.230: INFO: Got endpoints: latency-svc-6nqt4 [752.44827ms]
Aug 13 06:25:39.243: INFO: Created: latency-svc-8vlj5
Aug 13 06:25:39.277: INFO: Got endpoints: latency-svc-ctr7v [750.602214ms]
Aug 13 06:25:39.293: INFO: Created: latency-svc-wrk8x
Aug 13 06:25:39.326: INFO: Got endpoints: latency-svc-gld2d [747.346558ms]
Aug 13 06:25:39.343: INFO: Created: latency-svc-rcczd
Aug 13 06:25:39.380: INFO: Got endpoints: latency-svc-lj9nl [755.131947ms]
Aug 13 06:25:39.395: INFO: Created: latency-svc-7pqpp
Aug 13 06:25:39.426: INFO: Got endpoints: latency-svc-s7fs9 [749.977451ms]
Aug 13 06:25:39.438: INFO: Created: latency-svc-q4kp7
Aug 13 06:25:39.477: INFO: Got endpoints: latency-svc-gx5k5 [751.108199ms]
Aug 13 06:25:39.490: INFO: Created: latency-svc-q4j9z
Aug 13 06:25:39.526: INFO: Got endpoints: latency-svc-4pmdh [749.097247ms]
Aug 13 06:25:39.540: INFO: Created: latency-svc-lpdwb
Aug 13 06:25:39.577: INFO: Got endpoints: latency-svc-xvlk5 [751.538544ms]
Aug 13 06:25:39.589: INFO: Created: latency-svc-9qf7k
Aug 13 06:25:39.628: INFO: Got endpoints: latency-svc-gk2hs [742.654377ms]
Aug 13 06:25:39.640: INFO: Created: latency-svc-xpqd7
Aug 13 06:25:39.678: INFO: Got endpoints: latency-svc-4ng98 [751.618442ms]
Aug 13 06:25:39.691: INFO: Created: latency-svc-hh9wx
Aug 13 06:25:39.725: INFO: Got endpoints: latency-svc-wj4z2 [741.904347ms]
Aug 13 06:25:39.737: INFO: Created: latency-svc-mrrrf
Aug 13 06:25:39.776: INFO: Got endpoints: latency-svc-f9x7g [734.974541ms]
Aug 13 06:25:39.790: INFO: Created: latency-svc-8z499
Aug 13 06:25:39.826: INFO: Got endpoints: latency-svc-2gg5m [740.355304ms]
Aug 13 06:25:39.840: INFO: Created: latency-svc-b4m4r
Aug 13 06:25:39.883: INFO: Got endpoints: latency-svc-ggqbt [755.137733ms]
Aug 13 06:25:39.897: INFO: Created: latency-svc-gvbs4
Aug 13 06:25:39.932: INFO: Got endpoints: latency-svc-w8zwf [748.38159ms]
Aug 13 06:25:39.946: INFO: Created: latency-svc-msg96
Aug 13 06:25:39.977: INFO: Got endpoints: latency-svc-8vlj5 [746.985378ms]
Aug 13 06:25:39.993: INFO: Created: latency-svc-cnwfz
Aug 13 06:25:40.032: INFO: Got endpoints: latency-svc-wrk8x [754.589482ms]
Aug 13 06:25:40.045: INFO: Created: latency-svc-8sgn9
Aug 13 06:25:40.080: INFO: Got endpoints: latency-svc-rcczd [754.065335ms]
Aug 13 06:25:40.095: INFO: Created: latency-svc-d5gdm
Aug 13 06:25:40.128: INFO: Got endpoints: latency-svc-7pqpp [747.022752ms]
Aug 13 06:25:40.145: INFO: Created: latency-svc-9njrh
Aug 13 06:25:40.177: INFO: Got endpoints: latency-svc-q4kp7 [749.860581ms]
Aug 13 06:25:40.190: INFO: Created: latency-svc-sz676
Aug 13 06:25:40.228: INFO: Got endpoints: latency-svc-q4j9z [751.142763ms]
Aug 13 06:25:40.244: INFO: Created: latency-svc-66ts6
Aug 13 06:25:40.276: INFO: Got endpoints: latency-svc-lpdwb [748.502772ms]
Aug 13 06:25:40.291: INFO: Created: latency-svc-594q6
Aug 13 06:25:40.327: INFO: Got endpoints: latency-svc-9qf7k [749.796744ms]
Aug 13 06:25:40.338: INFO: Created: latency-svc-49kct
Aug 13 06:25:40.375: INFO: Got endpoints: latency-svc-xpqd7 [747.284376ms]
Aug 13 06:25:40.387: INFO: Created: latency-svc-f4hpj
Aug 13 06:25:40.429: INFO: Got endpoints: latency-svc-hh9wx [750.734292ms]
Aug 13 06:25:40.442: INFO: Created: latency-svc-dxlwh
Aug 13 06:25:40.477: INFO: Got endpoints: latency-svc-mrrrf [751.258572ms]
Aug 13 06:25:40.491: INFO: Created: latency-svc-5vtpw
Aug 13 06:25:40.526: INFO: Got endpoints: latency-svc-8z499 [750.136296ms]
Aug 13 06:25:40.537: INFO: Created: latency-svc-btnlx
Aug 13 06:25:40.578: INFO: Got endpoints: latency-svc-b4m4r [751.441623ms]
Aug 13 06:25:40.593: INFO: Created: latency-svc-mgx47
Aug 13 06:25:40.626: INFO: Got endpoints: latency-svc-gvbs4 [742.632445ms]
Aug 13 06:25:40.637: INFO: Created: latency-svc-xpwkl
Aug 13 06:25:40.677: INFO: Got endpoints: latency-svc-msg96 [744.772218ms]
Aug 13 06:25:40.690: INFO: Created: latency-svc-cdtqp
Aug 13 06:25:40.728: INFO: Got endpoints: latency-svc-cnwfz [750.584834ms]
Aug 13 06:25:40.742: INFO: Created: latency-svc-vgk5c
Aug 13 06:25:40.776: INFO: Got endpoints: latency-svc-8sgn9 [743.604048ms]
Aug 13 06:25:40.787: INFO: Created: latency-svc-4d777
Aug 13 06:25:40.828: INFO: Got endpoints: latency-svc-d5gdm [747.06273ms]
Aug 13 06:25:40.838: INFO: Created: latency-svc-d46nm
Aug 13 06:25:40.876: INFO: Got endpoints: latency-svc-9njrh [748.547984ms]
Aug 13 06:25:40.889: INFO: Created: latency-svc-zv62w
Aug 13 06:25:40.927: INFO: Got endpoints: latency-svc-sz676 [750.072957ms]
Aug 13 06:25:40.939: INFO: Created: latency-svc-lnqdw
Aug 13 06:25:40.975: INFO: Got endpoints: latency-svc-66ts6 [747.372268ms]
Aug 13 06:25:40.986: INFO: Created: latency-svc-7m6tr
Aug 13 06:25:41.030: INFO: Got endpoints: latency-svc-594q6 [753.427502ms]
Aug 13 06:25:41.051: INFO: Created: latency-svc-k2g9m
Aug 13 06:25:41.077: INFO: Got endpoints: latency-svc-49kct [749.761604ms]
Aug 13 06:25:41.092: INFO: Created: latency-svc-wjrrt
Aug 13 06:25:41.126: INFO: Got endpoints: latency-svc-f4hpj [751.203964ms]
Aug 13 06:25:41.140: INFO: Created: latency-svc-d5cr6
Aug 13 06:25:41.179: INFO: Got endpoints: latency-svc-dxlwh [750.277641ms]
Aug 13 06:25:41.198: INFO: Created: latency-svc-4n9jh
Aug 13 06:25:41.226: INFO: Got endpoints: latency-svc-5vtpw [749.587565ms]
Aug 13 06:25:41.238: INFO: Created: latency-svc-9bb9m
Aug 13 06:25:41.275: INFO: Got endpoints: latency-svc-btnlx [748.900075ms]
Aug 13 06:25:41.288: INFO: Created: latency-svc-gqvx4
Aug 13 06:25:41.330: INFO: Got endpoints: latency-svc-mgx47 [752.834766ms]
Aug 13 06:25:41.343: INFO: Created: latency-svc-pwhfw
Aug 13 06:25:41.376: INFO: Got endpoints: latency-svc-xpwkl [749.956322ms]
Aug 13 06:25:41.387: INFO: Created: latency-svc-prmkx
Aug 13 06:25:41.433: INFO: Got endpoints: latency-svc-cdtqp [755.782624ms]
Aug 13 06:25:41.449: INFO: Created: latency-svc-2wvkx
Aug 13 06:25:41.476: INFO: Got endpoints: latency-svc-vgk5c [748.344672ms]
Aug 13 06:25:41.490: INFO: Created: latency-svc-j5j7x
Aug 13 06:25:41.526: INFO: Got endpoints: latency-svc-4d777 [750.70059ms]
Aug 13 06:25:41.537: INFO: Created: latency-svc-9nf59
Aug 13 06:25:41.580: INFO: Got endpoints: latency-svc-d46nm [752.211409ms]
Aug 13 06:25:41.597: INFO: Created: latency-svc-xs7sd
Aug 13 06:25:41.626: INFO: Got endpoints: latency-svc-zv62w [750.236714ms]
Aug 13 06:25:41.643: INFO: Created: latency-svc-8ts6c
Aug 13 06:25:41.676: INFO: Got endpoints: latency-svc-lnqdw [749.472113ms]
Aug 13 06:25:41.692: INFO: Created: latency-svc-xmclp
Aug 13 06:25:41.726: INFO: Got endpoints: latency-svc-7m6tr [750.533348ms]
Aug 13 06:25:41.738: INFO: Created: latency-svc-vwj2s
Aug 13 06:25:41.775: INFO: Got endpoints: latency-svc-k2g9m [745.313409ms]
Aug 13 06:25:41.791: INFO: Created: latency-svc-j5hns
Aug 13 06:25:41.828: INFO: Got endpoints: latency-svc-wjrrt [750.804014ms]
Aug 13 06:25:41.841: INFO: Created: latency-svc-8lvdk
Aug 13 06:25:41.875: INFO: Got endpoints: latency-svc-d5cr6 [748.448101ms]
Aug 13 06:25:41.889: INFO: Created: latency-svc-dx5jt
Aug 13 06:25:41.925: INFO: Got endpoints: latency-svc-4n9jh [745.973883ms]
Aug 13 06:25:41.941: INFO: Created: latency-svc-fcglx
Aug 13 06:25:41.976: INFO: Got endpoints: latency-svc-9bb9m [749.647168ms]
Aug 13 06:25:41.989: INFO: Created: latency-svc-k4rqn
Aug 13 06:25:42.059: INFO: Got endpoints: latency-svc-gqvx4 [783.259389ms]
Aug 13 06:25:42.078: INFO: Got endpoints: latency-svc-pwhfw [747.11394ms]
Aug 13 06:25:42.083: INFO: Created: latency-svc-tbhlc
Aug 13 06:25:42.095: INFO: Created: latency-svc-shd42
Aug 13 06:25:42.127: INFO: Got endpoints: latency-svc-prmkx [751.273069ms]
Aug 13 06:25:42.137: INFO: Created: latency-svc-2ckzl
Aug 13 06:25:42.177: INFO: Got endpoints: latency-svc-2wvkx [744.247537ms]
Aug 13 06:25:42.196: INFO: Created: latency-svc-vr4jk
Aug 13 06:25:42.231: INFO: Got endpoints: latency-svc-j5j7x [754.464989ms]
Aug 13 06:25:42.244: INFO: Created: latency-svc-wh6hx
Aug 13 06:25:42.276: INFO: Got endpoints: latency-svc-9nf59 [750.008949ms]
Aug 13 06:25:42.291: INFO: Created: latency-svc-fshwq
Aug 13 06:25:42.326: INFO: Got endpoints: latency-svc-xs7sd [745.810555ms]
Aug 13 06:25:42.337: INFO: Created: latency-svc-6nhgc
Aug 13 06:25:42.376: INFO: Got endpoints: latency-svc-8ts6c [749.67924ms]
Aug 13 06:25:42.389: INFO: Created: latency-svc-mlhdg
Aug 13 06:25:42.425: INFO: Got endpoints: latency-svc-xmclp [748.674422ms]
Aug 13 06:25:42.435: INFO: Created: latency-svc-pgzhp
Aug 13 06:25:42.485: INFO: Got endpoints: latency-svc-vwj2s [759.080223ms]
Aug 13 06:25:42.496: INFO: Created: latency-svc-czmzp
Aug 13 06:25:42.526: INFO: Got endpoints: latency-svc-j5hns [750.708935ms]
Aug 13 06:25:42.536: INFO: Created: latency-svc-9xszv
Aug 13 06:25:42.574: INFO: Got endpoints: latency-svc-8lvdk [746.587862ms]
Aug 13 06:25:42.587: INFO: Created: latency-svc-t9ml9
Aug 13 06:25:42.625: INFO: Got endpoints: latency-svc-dx5jt [750.102005ms]
Aug 13 06:25:42.635: INFO: Created: latency-svc-hh82n
Aug 13 06:25:42.675: INFO: Got endpoints: latency-svc-fcglx [749.760937ms]
Aug 13 06:25:42.693: INFO: Created: latency-svc-bzzh5
Aug 13 06:25:42.726: INFO: Got endpoints: latency-svc-k4rqn [749.690776ms]
Aug 13 06:25:42.737: INFO: Created: latency-svc-dk8t7
Aug 13 06:25:42.776: INFO: Got endpoints: latency-svc-tbhlc [717.010343ms]
Aug 13 06:25:42.789: INFO: Created: latency-svc-dsxbv
Aug 13 06:25:42.826: INFO: Got endpoints: latency-svc-shd42 [748.173152ms]
Aug 13 06:25:42.837: INFO: Created: latency-svc-csmrp
Aug 13 06:25:42.878: INFO: Got endpoints: latency-svc-2ckzl [751.01539ms]
Aug 13 06:25:42.891: INFO: Created: latency-svc-5nh26
Aug 13 06:25:42.926: INFO: Got endpoints: latency-svc-vr4jk [748.437505ms]
Aug 13 06:25:42.940: INFO: Created: latency-svc-d9s4t
Aug 13 06:25:42.977: INFO: Got endpoints: latency-svc-wh6hx [745.885473ms]
Aug 13 06:25:42.992: INFO: Created: latency-svc-mp2s6
Aug 13 06:25:43.029: INFO: Got endpoints: latency-svc-fshwq [752.698096ms]
Aug 13 06:25:43.039: INFO: Created: latency-svc-px6xv
Aug 13 06:25:43.085: INFO: Got endpoints: latency-svc-6nhgc [759.335063ms]
Aug 13 06:25:43.098: INFO: Created: latency-svc-5vlt6
Aug 13 06:25:43.127: INFO: Got endpoints: latency-svc-mlhdg [750.878782ms]
Aug 13 06:25:43.140: INFO: Created: latency-svc-f2pd5
Aug 13 06:25:43.176: INFO: Got endpoints: latency-svc-pgzhp [751.314199ms]
Aug 13 06:25:43.195: INFO: Created: latency-svc-778r6
Aug 13 06:25:43.225: INFO: Got endpoints: latency-svc-czmzp [739.788507ms]
Aug 13 06:25:43.243: INFO: Created: latency-svc-5lg44
Aug 13 06:25:43.276: INFO: Got endpoints: latency-svc-9xszv [749.464277ms]
Aug 13 06:25:43.288: INFO: Created: latency-svc-rvg95
Aug 13 06:25:43.325: INFO: Got endpoints: latency-svc-t9ml9 [750.865129ms]
Aug 13 06:25:43.336: INFO: Created: latency-svc-gp7n6
Aug 13 06:25:43.376: INFO: Got endpoints: latency-svc-hh82n [751.028726ms]
Aug 13 06:25:43.388: INFO: Created: latency-svc-95vq4
Aug 13 06:25:43.426: INFO: Got endpoints: latency-svc-bzzh5 [751.280891ms]
Aug 13 06:25:43.439: INFO: Created: latency-svc-r89dj
Aug 13 06:25:43.475: INFO: Got endpoints: latency-svc-dk8t7 [749.193646ms]
Aug 13 06:25:43.494: INFO: Created: latency-svc-r7t9s
Aug 13 06:25:43.526: INFO: Got endpoints: latency-svc-dsxbv [750.525831ms]
Aug 13 06:25:43.538: INFO: Created: latency-svc-bql6m
Aug 13 06:25:43.576: INFO: Got endpoints: latency-svc-csmrp [750.086303ms]
Aug 13 06:25:43.589: INFO: Created: latency-svc-qj9q9
Aug 13 06:25:43.626: INFO: Got endpoints: latency-svc-5nh26 [747.851455ms]
Aug 13 06:25:43.637: INFO: Created: latency-svc-xb2gf
Aug 13 06:25:43.678: INFO: Got endpoints: latency-svc-d9s4t [752.004364ms]
Aug 13 06:25:43.691: INFO: Created: latency-svc-glnmm
Aug 13 06:25:43.725: INFO: Got endpoints: latency-svc-mp2s6 [748.344552ms]
Aug 13 06:25:43.737: INFO: Created: latency-svc-jjqbq
Aug 13 06:25:43.776: INFO: Got endpoints: latency-svc-px6xv [746.916519ms]
Aug 13 06:25:43.794: INFO: Created: latency-svc-9rrxj
Aug 13 06:25:43.825: INFO: Got endpoints: latency-svc-5vlt6 [740.06594ms]
Aug 13 06:25:43.836: INFO: Created: latency-svc-dgm5l
Aug 13 06:25:43.875: INFO: Got endpoints: latency-svc-f2pd5 [747.968194ms]
Aug 13 06:25:43.888: INFO: Created: latency-svc-l9q9l
Aug 13 06:25:43.925: INFO: Got endpoints: latency-svc-778r6 [748.590677ms]
Aug 13 06:25:43.940: INFO: Created: latency-svc-g5hxr
Aug 13 06:25:43.976: INFO: Got endpoints: latency-svc-5lg44 [751.233459ms]
Aug 13 06:25:43.990: INFO: Created: latency-svc-n7nf5
Aug 13 06:25:44.026: INFO: Got endpoints: latency-svc-rvg95 [749.924179ms]
Aug 13 06:25:44.036: INFO: Created: latency-svc-cd59k
Aug 13 06:25:44.078: INFO: Got endpoints: latency-svc-gp7n6 [752.169738ms]
Aug 13 06:25:44.093: INFO: Created: latency-svc-b6g6w
Aug 13 06:25:44.126: INFO: Got endpoints: latency-svc-95vq4 [749.923895ms]
Aug 13 06:25:44.137: INFO: Created: latency-svc-5fsj4
Aug 13 06:25:44.178: INFO: Got endpoints: latency-svc-r89dj [751.593911ms]
Aug 13 06:25:44.189: INFO: Created: latency-svc-ksdwm
Aug 13 06:25:44.227: INFO: Got endpoints: latency-svc-r7t9s [751.796758ms]
Aug 13 06:25:44.238: INFO: Created: latency-svc-qbc59
Aug 13 06:25:44.275: INFO: Got endpoints: latency-svc-bql6m [748.697886ms]
Aug 13 06:25:44.290: INFO: Created: latency-svc-lp8qx
Aug 13 06:25:44.327: INFO: Got endpoints: latency-svc-qj9q9 [751.1268ms]
Aug 13 06:25:44.338: INFO: Created: latency-svc-jct8j
Aug 13 06:25:44.375: INFO: Got endpoints: latency-svc-xb2gf [748.926433ms]
Aug 13 06:25:44.391: INFO: Created: latency-svc-hdp45
Aug 13 06:25:44.425: INFO: Got endpoints: latency-svc-glnmm [747.361675ms]
Aug 13 06:25:44.439: INFO: Created: latency-svc-7j8gt
Aug 13 06:25:44.477: INFO: Got endpoints: latency-svc-jjqbq [751.874526ms]
Aug 13 06:25:44.491: INFO: Created: latency-svc-52m9d
Aug 13 06:25:44.525: INFO: Got endpoints: latency-svc-9rrxj [748.810096ms]
Aug 13 06:25:44.537: INFO: Created: latency-svc-q2szd
Aug 13 06:25:44.579: INFO: Got endpoints: latency-svc-dgm5l [754.070794ms]
Aug 13 06:25:44.596: INFO: Created: latency-svc-bkzvw
Aug 13 06:25:44.626: INFO: Got endpoints: latency-svc-l9q9l [750.328332ms]
Aug 13 06:25:44.637: INFO: Created: latency-svc-hwvkk
Aug 13 06:25:44.675: INFO: Got endpoints: latency-svc-g5hxr [750.103724ms]
Aug 13 06:25:44.688: INFO: Created: latency-svc-64rs4
Aug 13 06:25:44.726: INFO: Got endpoints: latency-svc-n7nf5 [749.555234ms]
Aug 13 06:25:44.738: INFO: Created: latency-svc-5g228
Aug 13 06:25:44.776: INFO: Got endpoints: latency-svc-cd59k [750.046513ms]
Aug 13 06:25:44.788: INFO: Created: latency-svc-bpzg7
Aug 13 06:25:44.825: INFO: Got endpoints: latency-svc-b6g6w [747.6366ms]
Aug 13 06:25:44.837: INFO: Created: latency-svc-ljhbv
Aug 13 06:25:44.877: INFO: Got endpoints: latency-svc-5fsj4 [750.398405ms]
Aug 13 06:25:44.888: INFO: Created: latency-svc-dd225
Aug 13 06:25:44.924: INFO: Got endpoints: latency-svc-ksdwm [746.332954ms]
Aug 13 06:25:44.935: INFO: Created: latency-svc-lwwr6
Aug 13 06:25:44.975: INFO: Got endpoints: latency-svc-qbc59 [747.308307ms]
Aug 13 06:25:44.987: INFO: Created: latency-svc-mmcwl
Aug 13 06:25:45.028: INFO: Got endpoints: latency-svc-lp8qx [752.503155ms]
Aug 13 06:25:45.042: INFO: Created: latency-svc-8twzp
Aug 13 06:25:45.080: INFO: Got endpoints: latency-svc-jct8j [752.801297ms]
Aug 13 06:25:45.097: INFO: Created: latency-svc-2c6td
Aug 13 06:25:45.126: INFO: Got endpoints: latency-svc-hdp45 [751.000221ms]
Aug 13 06:25:45.137: INFO: Created: latency-svc-vfhjv
Aug 13 06:25:45.176: INFO: Got endpoints: latency-svc-7j8gt [750.529955ms]
Aug 13 06:25:45.188: INFO: Created: latency-svc-lpgvp
Aug 13 06:25:45.226: INFO: Got endpoints: latency-svc-52m9d [749.488144ms]
Aug 13 06:25:45.277: INFO: Got endpoints: latency-svc-q2szd [752.108635ms]
Aug 13 06:25:45.327: INFO: Got endpoints: latency-svc-bkzvw [747.193791ms]
Aug 13 06:25:45.375: INFO: Got endpoints: latency-svc-hwvkk [749.805453ms]
Aug 13 06:25:45.427: INFO: Got endpoints: latency-svc-64rs4 [752.09737ms]
Aug 13 06:25:45.475: INFO: Got endpoints: latency-svc-5g228 [749.33595ms]
Aug 13 06:25:45.525: INFO: Got endpoints: latency-svc-bpzg7 [749.41318ms]
Aug 13 06:25:45.576: INFO: Got endpoints: latency-svc-ljhbv [750.661426ms]
Aug 13 06:25:45.627: INFO: Got endpoints: latency-svc-dd225 [750.657294ms]
Aug 13 06:25:45.675: INFO: Got endpoints: latency-svc-lwwr6 [750.789112ms]
Aug 13 06:25:45.725: INFO: Got endpoints: latency-svc-mmcwl [749.137557ms]
Aug 13 06:25:45.777: INFO: Got endpoints: latency-svc-8twzp [748.938431ms]
Aug 13 06:25:45.826: INFO: Got endpoints: latency-svc-2c6td [745.425858ms]
Aug 13 06:25:45.875: INFO: Got endpoints: latency-svc-vfhjv [749.272588ms]
Aug 13 06:25:45.928: INFO: Got endpoints: latency-svc-lpgvp [751.535017ms]
Aug 13 06:25:45.928: INFO: Latencies: [32.585282ms 43.632559ms 53.093147ms 66.305771ms 78.360083ms 89.620831ms 101.069486ms 109.431813ms 118.958496ms 127.949386ms 142.591317ms 142.632025ms 145.711531ms 145.878038ms 147.660946ms 148.603716ms 149.712391ms 149.735536ms 149.822718ms 149.905885ms 150.251893ms 151.497574ms 152.352879ms 154.370943ms 154.594768ms 154.706114ms 154.891257ms 155.636203ms 156.163967ms 157.94641ms 158.470925ms 161.23559ms 161.756437ms 161.758331ms 166.420995ms 171.92304ms 180.507493ms 183.630066ms 222.17761ms 264.56953ms 308.363122ms 342.654575ms 391.11638ms 421.478928ms 460.547001ms 500.541178ms 541.07369ms 581.650366ms 617.37952ms 660.529515ms 699.400191ms 717.010343ms 734.974541ms 736.626404ms 739.788507ms 740.06594ms 740.355304ms 741.008314ms 741.904347ms 742.632445ms 742.654377ms 743.604048ms 744.247537ms 744.772218ms 745.313409ms 745.425858ms 745.576611ms 745.810555ms 745.885473ms 745.973883ms 746.189904ms 746.332954ms 746.587862ms 746.916519ms 746.985378ms 747.022752ms 747.06273ms 747.11394ms 747.193791ms 747.284376ms 747.308307ms 747.346558ms 747.361675ms 747.372268ms 747.6366ms 747.851455ms 747.968194ms 748.173152ms 748.344552ms 748.344672ms 748.38159ms 748.437505ms 748.448101ms 748.502772ms 748.547984ms 748.590677ms 748.674422ms 748.697886ms 748.810096ms 748.900075ms 748.926433ms 748.938431ms 749.097247ms 749.137557ms 749.193646ms 749.272588ms 749.299096ms 749.33595ms 749.41318ms 749.464277ms 749.472113ms 749.488144ms 749.555234ms 749.587565ms 749.647168ms 749.67924ms 749.690776ms 749.6909ms 749.760937ms 749.761604ms 749.796744ms 749.805453ms 749.860581ms 749.923895ms 749.924179ms 749.956322ms 749.977451ms 750.008949ms 750.046513ms 750.072957ms 750.086303ms 750.102005ms 750.103724ms 750.136296ms 750.236714ms 750.277641ms 750.328332ms 750.398405ms 750.525831ms 750.529955ms 750.533348ms 750.584834ms 750.602214ms 750.657294ms 750.661426ms 750.70059ms 750.708935ms 750.734292ms 750.789112ms 750.804014ms 750.839427ms 750.865129ms 750.878782ms 751.000221ms 751.01539ms 751.028726ms 751.099949ms 751.108199ms 751.1268ms 751.142763ms 751.203964ms 751.233459ms 751.258572ms 751.273069ms 751.280891ms 751.314199ms 751.441623ms 751.441966ms 751.535017ms 751.538544ms 751.593911ms 751.618442ms 751.796758ms 751.874526ms 752.004364ms 752.09737ms 752.108635ms 752.169738ms 752.211409ms 752.44827ms 752.503155ms 752.698096ms 752.801297ms 752.834766ms 753.427502ms 754.065335ms 754.070794ms 754.464989ms 754.589482ms 755.131947ms 755.137733ms 755.782624ms 756.123128ms 756.297669ms 756.898042ms 759.080223ms 759.233258ms 759.335063ms 765.48003ms 783.259389ms]
Aug 13 06:25:45.928: INFO: 50 %ile: 748.926433ms
Aug 13 06:25:45.928: INFO: 90 %ile: 752.503155ms
Aug 13 06:25:45.928: INFO: 99 %ile: 765.48003ms
Aug 13 06:25:45.928: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:25:45.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-5494" for this suite.
Aug 13 06:25:59.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:26:00.038: INFO: namespace svc-latency-5494 deletion completed in 14.105148851s

â€¢ [SLOW TEST:25.875 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:26:00.038: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 13 06:26:00.076: INFO: Waiting up to 5m0s for pod "pod-3782f464-bd93-11e9-8bf8-0a58ac140308" in namespace "emptydir-2788" to be "success or failure"
Aug 13 06:26:00.082: INFO: Pod "pod-3782f464-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.293108ms
Aug 13 06:26:02.086: INFO: Pod "pod-3782f464-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010342535s
Aug 13 06:26:04.091: INFO: Pod "pod-3782f464-bd93-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015024149s
STEP: Saw pod success
Aug 13 06:26:04.091: INFO: Pod "pod-3782f464-bd93-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:26:04.095: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-3782f464-bd93-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:26:04.115: INFO: Waiting for pod pod-3782f464-bd93-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:26:04.119: INFO: Pod pod-3782f464-bd93-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:26:04.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2788" for this suite.
Aug 13 06:26:10.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:26:10.218: INFO: namespace emptydir-2788 deletion completed in 6.095704168s

â€¢ [SLOW TEST:10.180 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:26:10.218: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 13 06:26:10.252: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 06:26:10.258: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 06:26:10.260: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-0 before test
Aug 13 06:26:10.270: INFO: kube-proxy-ds-8swrw from kube-system started at 2019-08-13 05:39:38 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 06:26:10.270: INFO: csi-node-ntnx-plugin-6f274 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 06:26:10.270: INFO: kube-flannel-ds-gv2mw from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 06:26:10.270: INFO: node-exporter-g2xn7 from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 06:26:10.270: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-13 05:44:30 +0000 UTC (3 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 06:26:10.270: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 06:26:10.270: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-13 06:04:31 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 13 06:26:10.270: INFO: sonobuoy-e2e-job-5d1df1b5d76149a9 from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container e2e ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 06:26:10.270: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-hmngc from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 13 06:26:10.270: INFO: prometheus-operator-694bbc8678-f5zfp from ntnx-system started at 2019-08-13 05:43:07 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 13 06:26:10.270: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 06:26:10.270: INFO: fluent-bit-78krq from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 06:26:10.270: INFO: kubernetes-events-printer-6494d69c4b-p55sc from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.270: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 13 06:26:10.270: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-1 before test
Aug 13 06:26:10.337: INFO: kube-proxy-ds-2tbd6 from kube-system started at 2019-08-13 05:39:37 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 06:26:10.337: INFO: csi-node-ntnx-plugin-jqc6g from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 06:26:10.337: INFO: fluent-bit-zzqvx from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 06:26:10.337: INFO: kibana-logging-68b6fc7cb6-hc5bk from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 13 06:26:10.337: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-13 05:43:25 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 06:26:10.337: INFO: kube-flannel-ds-cfprh from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 06:26:10.337: INFO: node-exporter-rpqwn from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 06:26:10.337: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-13 05:43:45 +0000 UTC (3 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 06:26:10.337: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 06:26:10.337: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-x8sg2 from heptio-sonobuoy started at 2019-08-13 06:04:37 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 13 06:26:10.337: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-13 05:41:30 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.337: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 13 06:26:10.337: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-2 before test
Aug 13 06:26:10.345: INFO: kube-proxy-ds-thn6s from kube-system started at 2019-08-13 05:39:39 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 06:26:10.345: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-h884r from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 13 06:26:10.345: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 06:26:10.345: INFO: fluent-bit-4bnfm from ntnx-system started at 2019-08-13 05:40:30 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 06:26:10.345: INFO: node-exporter-46562 from ntnx-system started at 2019-08-13 05:43:08 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 06:26:10.345: INFO: csi-node-ntnx-plugin-ncz7w from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 06:26:10.345: INFO: kube-flannel-ds-dt86l from kube-system started at 2019-08-13 05:39:43 +0000 UTC (1 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 06:26:10.345: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-13 05:43:17 +0000 UTC (2 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 06:26:10.345: INFO: kube-state-metrics-6bf48db96d-k6wlz from ntnx-system started at 2019-08-13 05:43:19 +0000 UTC (4 container statuses recorded)
Aug 13 06:26:10.345: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 13 06:26:10.345: INFO: 	Container kube-state-metrics ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ba67747645d74f], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:26:11.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-360" for this suite.
Aug 13 06:26:17.380: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:26:17.465: INFO: namespace sched-pred-360 deletion completed in 6.096329539s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.247 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:26:17.465: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-41e8d685-bd93-11e9-8bf8-0a58ac140308
STEP: Creating secret with name s-test-opt-upd-41e8d746-bd93-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-41e8d685-bd93-11e9-8bf8-0a58ac140308
STEP: Updating secret s-test-opt-upd-41e8d746-bd93-11e9-8bf8-0a58ac140308
STEP: Creating secret with name s-test-opt-create-41e8d787-bd93-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:26:23.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9318" for this suite.
Aug 13 06:26:45.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:26:45.710: INFO: namespace secrets-9318 deletion completed in 22.09264662s

â€¢ [SLOW TEST:28.245 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:26:45.710: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 13 06:26:45.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-9969'
Aug 13 06:26:46.193: INFO: stderr: ""
Aug 13 06:26:46.193: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:26:46.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9969'
Aug 13 06:26:46.316: INFO: stderr: ""
Aug 13 06:26:46.316: INFO: stdout: "update-demo-nautilus-kg64b update-demo-nautilus-xmn7q "
Aug 13 06:26:46.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-kg64b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9969'
Aug 13 06:26:46.395: INFO: stderr: ""
Aug 13 06:26:46.395: INFO: stdout: ""
Aug 13 06:26:46.395: INFO: update-demo-nautilus-kg64b is created but not running
Aug 13 06:26:51.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9969'
Aug 13 06:26:51.480: INFO: stderr: ""
Aug 13 06:26:51.480: INFO: stdout: "update-demo-nautilus-kg64b update-demo-nautilus-xmn7q "
Aug 13 06:26:51.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-kg64b -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9969'
Aug 13 06:26:51.578: INFO: stderr: ""
Aug 13 06:26:51.578: INFO: stdout: "true"
Aug 13 06:26:51.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-kg64b -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9969'
Aug 13 06:26:51.665: INFO: stderr: ""
Aug 13 06:26:51.665: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:26:51.665: INFO: validating pod update-demo-nautilus-kg64b
Aug 13 06:26:51.669: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:26:51.670: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:26:51.670: INFO: update-demo-nautilus-kg64b is verified up and running
Aug 13 06:26:51.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-xmn7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9969'
Aug 13 06:26:51.754: INFO: stderr: ""
Aug 13 06:26:51.754: INFO: stdout: "true"
Aug 13 06:26:51.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-xmn7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9969'
Aug 13 06:26:51.836: INFO: stderr: ""
Aug 13 06:26:51.836: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:26:51.836: INFO: validating pod update-demo-nautilus-xmn7q
Aug 13 06:26:51.840: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:26:51.840: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:26:51.840: INFO: update-demo-nautilus-xmn7q is verified up and running
STEP: using delete to clean up resources
Aug 13 06:26:51.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-9969'
Aug 13 06:26:51.929: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:26:51.929: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 13 06:26:51.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9969'
Aug 13 06:26:52.021: INFO: stderr: "No resources found.\n"
Aug 13 06:26:52.021: INFO: stdout: ""
Aug 13 06:26:52.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=update-demo --namespace=kubectl-9969 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:26:52.103: INFO: stderr: ""
Aug 13 06:26:52.103: INFO: stdout: "update-demo-nautilus-kg64b\nupdate-demo-nautilus-xmn7q\n"
Aug 13 06:26:52.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9969'
Aug 13 06:26:52.687: INFO: stderr: "No resources found.\n"
Aug 13 06:26:52.687: INFO: stdout: ""
Aug 13 06:26:52.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=update-demo --namespace=kubectl-9969 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:26:52.764: INFO: stderr: ""
Aug 13 06:26:52.764: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:26:52.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9969" for this suite.
Aug 13 06:27:14.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:27:14.867: INFO: namespace kubectl-9969 deletion completed in 22.099053894s

â€¢ [SLOW TEST:29.157 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:27:14.867: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:27:40.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-963" for this suite.
Aug 13 06:27:46.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:27:46.233: INFO: namespace container-runtime-963 deletion completed in 6.100148984s

â€¢ [SLOW TEST:31.366 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:27:46.233: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-9263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9263 to expose endpoints map[]
Aug 13 06:27:46.277: INFO: Get endpoints failed (2.747708ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 13 06:27:47.280: INFO: successfully validated that service endpoint-test2 in namespace services-9263 exposes endpoints map[] (1.006482649s elapsed)
STEP: Creating pod pod1 in namespace services-9263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9263 to expose endpoints map[pod1:[80]]
Aug 13 06:27:51.324: INFO: successfully validated that service endpoint-test2 in namespace services-9263 exposes endpoints map[pod1:[80]] (4.035736017s elapsed)
STEP: Creating pod pod2 in namespace services-9263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9263 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 13 06:27:54.371: INFO: successfully validated that service endpoint-test2 in namespace services-9263 exposes endpoints map[pod1:[80] pod2:[80]] (3.041887665s elapsed)
STEP: Deleting pod pod1 in namespace services-9263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9263 to expose endpoints map[pod2:[80]]
Aug 13 06:27:55.407: INFO: successfully validated that service endpoint-test2 in namespace services-9263 exposes endpoints map[pod2:[80]] (1.028314607s elapsed)
STEP: Deleting pod pod2 in namespace services-9263
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9263 to expose endpoints map[]
Aug 13 06:27:56.420: INFO: successfully validated that service endpoint-test2 in namespace services-9263 exposes endpoints map[] (1.007490963s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:27:56.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9263" for this suite.
Aug 13 06:28:18.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:28:18.554: INFO: namespace services-9263 deletion completed in 22.104796806s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:32.321 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:28:18.554: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Aug 13 06:28:18.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-757'
Aug 13 06:28:18.855: INFO: stderr: ""
Aug 13 06:28:18.855: INFO: stdout: "pod/pause created\n"
Aug 13 06:28:18.855: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 13 06:28:18.855: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-757" to be "running and ready"
Aug 13 06:28:18.859: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308701ms
Aug 13 06:28:20.862: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006375968s
Aug 13 06:28:22.866: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.010545352s
Aug 13 06:28:22.866: INFO: Pod "pause" satisfied condition "running and ready"
Aug 13 06:28:22.866: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 13 06:28:22.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 label pods pause testing-label=testing-label-value --namespace=kubectl-757'
Aug 13 06:28:22.966: INFO: stderr: ""
Aug 13 06:28:22.966: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 13 06:28:22.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pod pause -L testing-label --namespace=kubectl-757'
Aug 13 06:28:23.051: INFO: stderr: ""
Aug 13 06:28:23.051: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 13 06:28:23.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 label pods pause testing-label- --namespace=kubectl-757'
Aug 13 06:28:23.146: INFO: stderr: ""
Aug 13 06:28:23.146: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 13 06:28:23.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pod pause -L testing-label --namespace=kubectl-757'
Aug 13 06:28:23.226: INFO: stderr: ""
Aug 13 06:28:23.226: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          5s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Aug 13 06:28:23.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-757'
Aug 13 06:28:23.313: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:28:23.313: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 13 06:28:23.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=pause --no-headers --namespace=kubectl-757'
Aug 13 06:28:23.412: INFO: stderr: "No resources found.\n"
Aug 13 06:28:23.412: INFO: stdout: ""
Aug 13 06:28:23.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=pause --namespace=kubectl-757 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:28:23.509: INFO: stderr: ""
Aug 13 06:28:23.509: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:28:23.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-757" for this suite.
Aug 13 06:28:29.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:28:29.605: INFO: namespace kubectl-757 deletion completed in 6.091437202s

â€¢ [SLOW TEST:11.051 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:28:29.605: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-90a8d9db-bd93-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:28:29.647: INFO: Waiting up to 5m0s for pod "pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308" in namespace "configmap-4479" to be "success or failure"
Aug 13 06:28:29.665: INFO: Pod "pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 17.859615ms
Aug 13 06:28:31.672: INFO: Pod "pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024853547s
Aug 13 06:28:33.676: INFO: Pod "pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02854152s
STEP: Saw pod success
Aug 13 06:28:33.676: INFO: Pod "pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:28:33.679: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:28:33.697: INFO: Waiting for pod pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:28:33.700: INFO: Pod pod-configmaps-90a97570-bd93-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:28:33.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4479" for this suite.
Aug 13 06:28:39.716: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:28:39.806: INFO: namespace configmap-4479 deletion completed in 6.101943985s

â€¢ [SLOW TEST:10.201 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:28:39.806: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Aug 13 06:28:39.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-7162'
Aug 13 06:28:40.103: INFO: stderr: ""
Aug 13 06:28:40.103: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:28:40.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:40.211: INFO: stderr: ""
Aug 13 06:28:40.211: INFO: stdout: "update-demo-nautilus-2mf9d update-demo-nautilus-4c6vm "
Aug 13 06:28:40.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-2mf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:40.296: INFO: stderr: ""
Aug 13 06:28:40.296: INFO: stdout: ""
Aug 13 06:28:40.296: INFO: update-demo-nautilus-2mf9d is created but not running
Aug 13 06:28:45.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:45.377: INFO: stderr: ""
Aug 13 06:28:45.377: INFO: stdout: "update-demo-nautilus-2mf9d update-demo-nautilus-4c6vm "
Aug 13 06:28:45.377: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-2mf9d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:45.451: INFO: stderr: ""
Aug 13 06:28:45.452: INFO: stdout: "true"
Aug 13 06:28:45.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-2mf9d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:45.530: INFO: stderr: ""
Aug 13 06:28:45.530: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:45.530: INFO: validating pod update-demo-nautilus-2mf9d
Aug 13 06:28:45.535: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:45.535: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:45.535: INFO: update-demo-nautilus-2mf9d is verified up and running
Aug 13 06:28:45.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:45.614: INFO: stderr: ""
Aug 13 06:28:45.614: INFO: stdout: "true"
Aug 13 06:28:45.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:45.691: INFO: stderr: ""
Aug 13 06:28:45.691: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:45.691: INFO: validating pod update-demo-nautilus-4c6vm
Aug 13 06:28:45.696: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:45.696: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:45.696: INFO: update-demo-nautilus-4c6vm is verified up and running
STEP: scaling down the replication controller
Aug 13 06:28:45.698: INFO: scanned /root for discovery docs: <nil>
Aug 13 06:28:45.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7162'
Aug 13 06:28:46.810: INFO: stderr: ""
Aug 13 06:28:46.810: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:28:46.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:46.890: INFO: stderr: ""
Aug 13 06:28:46.890: INFO: stdout: "update-demo-nautilus-2mf9d update-demo-nautilus-4c6vm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 13 06:28:51.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:51.974: INFO: stderr: ""
Aug 13 06:28:51.974: INFO: stdout: "update-demo-nautilus-4c6vm "
Aug 13 06:28:51.974: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:52.054: INFO: stderr: ""
Aug 13 06:28:52.054: INFO: stdout: "true"
Aug 13 06:28:52.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:52.132: INFO: stderr: ""
Aug 13 06:28:52.133: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:52.133: INFO: validating pod update-demo-nautilus-4c6vm
Aug 13 06:28:52.137: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:52.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:52.137: INFO: update-demo-nautilus-4c6vm is verified up and running
STEP: scaling up the replication controller
Aug 13 06:28:52.139: INFO: scanned /root for discovery docs: <nil>
Aug 13 06:28:52.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7162'
Aug 13 06:28:53.268: INFO: stderr: ""
Aug 13 06:28:53.268: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 13 06:28:53.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:53.364: INFO: stderr: ""
Aug 13 06:28:53.364: INFO: stdout: "update-demo-nautilus-4c6vm update-demo-nautilus-tqffg "
Aug 13 06:28:53.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:53.446: INFO: stderr: ""
Aug 13 06:28:53.446: INFO: stdout: "true"
Aug 13 06:28:53.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:53.529: INFO: stderr: ""
Aug 13 06:28:53.530: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:53.530: INFO: validating pod update-demo-nautilus-4c6vm
Aug 13 06:28:53.535: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:53.535: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:53.535: INFO: update-demo-nautilus-4c6vm is verified up and running
Aug 13 06:28:53.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-tqffg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:53.628: INFO: stderr: ""
Aug 13 06:28:53.628: INFO: stdout: ""
Aug 13 06:28:53.628: INFO: update-demo-nautilus-tqffg is created but not running
Aug 13 06:28:58.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7162'
Aug 13 06:28:58.722: INFO: stderr: ""
Aug 13 06:28:58.723: INFO: stdout: "update-demo-nautilus-4c6vm update-demo-nautilus-tqffg "
Aug 13 06:28:58.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:58.803: INFO: stderr: ""
Aug 13 06:28:58.803: INFO: stdout: "true"
Aug 13 06:28:58.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-4c6vm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:58.895: INFO: stderr: ""
Aug 13 06:28:58.895: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:58.895: INFO: validating pod update-demo-nautilus-4c6vm
Aug 13 06:28:58.898: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:58.898: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:58.898: INFO: update-demo-nautilus-4c6vm is verified up and running
Aug 13 06:28:58.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-tqffg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:58.975: INFO: stderr: ""
Aug 13 06:28:58.975: INFO: stdout: "true"
Aug 13 06:28:58.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods update-demo-nautilus-tqffg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7162'
Aug 13 06:28:59.061: INFO: stderr: ""
Aug 13 06:28:59.061: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 13 06:28:59.061: INFO: validating pod update-demo-nautilus-tqffg
Aug 13 06:28:59.066: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 13 06:28:59.066: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 13 06:28:59.066: INFO: update-demo-nautilus-tqffg is verified up and running
STEP: using delete to clean up resources
Aug 13 06:28:59.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-7162'
Aug 13 06:28:59.150: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:28:59.150: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 13 06:28:59.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7162'
Aug 13 06:28:59.243: INFO: stderr: "No resources found.\n"
Aug 13 06:28:59.243: INFO: stdout: ""
Aug 13 06:28:59.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=update-demo --namespace=kubectl-7162 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:28:59.338: INFO: stderr: ""
Aug 13 06:28:59.338: INFO: stdout: "update-demo-nautilus-4c6vm\nupdate-demo-nautilus-tqffg\n"
Aug 13 06:28:59.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7162'
Aug 13 06:28:59.922: INFO: stderr: "No resources found.\n"
Aug 13 06:28:59.922: INFO: stdout: ""
Aug 13 06:28:59.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=update-demo --namespace=kubectl-7162 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:29:00.003: INFO: stderr: ""
Aug 13 06:29:00.003: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:29:00.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7162" for this suite.
Aug 13 06:29:22.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:29:22.110: INFO: namespace kubectl-7162 deletion completed in 22.101354576s

â€¢ [SLOW TEST:42.304 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:29:22.110: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-aff4e79c-bd93-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:29:22.151: INFO: Waiting up to 5m0s for pod "pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308" in namespace "secrets-6511" to be "success or failure"
Aug 13 06:29:22.156: INFO: Pod "pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.787089ms
Aug 13 06:29:24.161: INFO: Pod "pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01010385s
Aug 13 06:29:26.165: INFO: Pod "pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014146869s
STEP: Saw pod success
Aug 13 06:29:26.165: INFO: Pod "pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:29:26.168: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:29:26.191: INFO: Waiting for pod pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:29:26.193: INFO: Pod pod-secrets-aff57e4c-bd93-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:29:26.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6511" for this suite.
Aug 13 06:29:32.212: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:29:32.306: INFO: namespace secrets-6511 deletion completed in 6.10871115s

â€¢ [SLOW TEST:10.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:29:32.306: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-1300
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-1300
STEP: Deleting pre-stop pod
Aug 13 06:29:47.395: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:29:47.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-1300" for this suite.
Aug 13 06:30:25.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:30:25.522: INFO: namespace prestop-1300 deletion completed in 38.105260398s

â€¢ [SLOW TEST:53.216 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:30:25.522: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 06:30:25.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-55'
Aug 13 06:30:25.661: INFO: stderr: ""
Aug 13 06:30:25.661: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 13 06:30:30.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pod e2e-test-nginx-pod --namespace=kubectl-55 -o json'
Aug 13 06:30:30.794: INFO: stderr: ""
Aug 13 06:30:30.794: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-08-13T06:30:25Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-55\",\n        \"resourceVersion\": \"11560\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-55/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d5cdec12-bd93-11e9-ae6c-506b8d956efe\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-kwj96\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"karbon-multi-fourteen-aca02f-k8s-worker-2\",\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"volumes\": [\n            {\n                \"name\": \"default-token-kwj96\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-kwj96\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T06:30:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T06:30:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T06:30:28Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-13T06:30:25Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://d54302d072347606bf8e3767fe56744d4e2ca0a8ff1d0c5375872b3ec9da442b\",\n                \"image\": \"docker.io/nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-13T06:30:27Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.40.139\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.4.102\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-13T06:30:25Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 13 06:30:30.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 replace -f - --namespace=kubectl-55'
Aug 13 06:30:31.046: INFO: stderr: ""
Aug 13 06:30:31.046: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Aug 13 06:30:31.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete pods e2e-test-nginx-pod --namespace=kubectl-55'
Aug 13 06:30:33.110: INFO: stderr: ""
Aug 13 06:30:33.110: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:30:33.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-55" for this suite.
Aug 13 06:30:39.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:30:39.206: INFO: namespace kubectl-55 deletion completed in 6.091118178s

â€¢ [SLOW TEST:13.684 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:30:39.207: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 13 06:30:43.761: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2676 pod-service-account-de365b0e-bd93-11e9-8bf8-0a58ac140308 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 13 06:30:43.925: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2676 pod-service-account-de365b0e-bd93-11e9-8bf8-0a58ac140308 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 13 06:30:44.091: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2676 pod-service-account-de365b0e-bd93-11e9-8bf8-0a58ac140308 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:30:44.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2676" for this suite.
Aug 13 06:30:50.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:30:50.348: INFO: namespace svcaccounts-2676 deletion completed in 6.08900162s

â€¢ [SLOW TEST:11.142 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:30:50.349: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 06:30:50.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7125'
Aug 13 06:30:50.481: INFO: stderr: ""
Aug 13 06:30:50.481: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Aug 13 06:30:50.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete pods e2e-test-nginx-pod --namespace=kubectl-7125'
Aug 13 06:30:53.460: INFO: stderr: ""
Aug 13 06:30:53.460: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:30:53.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7125" for this suite.
Aug 13 06:30:59.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:30:59.563: INFO: namespace kubectl-7125 deletion completed in 6.099469082s

â€¢ [SLOW TEST:9.214 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:30:59.563: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 13 06:31:04.142: INFO: Successfully updated pod "annotationupdateea0b74ee-bd93-11e9-8bf8-0a58ac140308"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:31:06.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-591" for this suite.
Aug 13 06:31:28.177: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:31:28.265: INFO: namespace downward-api-591 deletion completed in 22.098631332s

â€¢ [SLOW TEST:28.701 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:31:28.265: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Aug 13 06:31:32.316: INFO: Pod pod-hostip-fb25f172-bd93-11e9-8bf8-0a58ac140308 has hostIP: 10.45.40.139
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:31:32.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2897" for this suite.
Aug 13 06:31:54.331: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:31:54.416: INFO: namespace pods-2897 deletion completed in 22.096943838s

â€¢ [SLOW TEST:26.151 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:31:54.417: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:31:54.454: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 13 06:31:59.459: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 06:31:59.459: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 13 06:32:01.463: INFO: Creating deployment "test-rollover-deployment"
Aug 13 06:32:01.472: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 13 06:32:03.484: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 13 06:32:03.489: INFO: Ensure that both replica sets have 1 created replica
Aug 13 06:32:03.494: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 13 06:32:03.511: INFO: Updating deployment test-rollover-deployment
Aug 13 06:32:03.511: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 13 06:32:05.523: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 13 06:32:05.528: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 13 06:32:05.534: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:05.534: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274723, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:07.540: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:07.540: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274726, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:09.541: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:09.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274726, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:11.542: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:11.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274726, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:13.542: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:13.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274726, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:15.541: INFO: all replica sets need to contain the pod-template-hash label
Aug 13 06:32:15.542: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274726, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701274721, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 06:32:17.540: INFO: 
Aug 13 06:32:17.540: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 13 06:32:17.548: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4634,SelfLink:/apis/apps/v1/namespaces/deployment-4634/deployments/test-rollover-deployment,UID:0eeb963e-bd94-11e9-868a-506b8de0bf77,ResourceVersion:11958,Generation:2,CreationTimestamp:2019-08-13 06:32:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-13 06:32:01 +0000 UTC 2019-08-13 06:32:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-13 06:32:16 +0000 UTC 2019-08-13 06:32:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 13 06:32:17.551: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-4634,SelfLink:/apis/apps/v1/namespaces/deployment-4634/replicasets/test-rollover-deployment-766b4d6c9d,UID:1023ff32-bd94-11e9-868a-506b8de0bf77,ResourceVersion:11947,Generation:2,CreationTimestamp:2019-08-13 06:32:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0eeb963e-bd94-11e9-868a-506b8de0bf77 0xc000a0a127 0xc000a0a128}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 06:32:17.551: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 13 06:32:17.551: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4634,SelfLink:/apis/apps/v1/namespaces/deployment-4634/replicasets/test-rollover-controller,UID:0abcdf3b-bd94-11e9-868a-506b8de0bf77,ResourceVersion:11956,Generation:2,CreationTimestamp:2019-08-13 06:31:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0eeb963e-bd94-11e9-868a-506b8de0bf77 0xc0027f5f77 0xc0027f5f78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 06:32:17.551: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-4634,SelfLink:/apis/apps/v1/namespaces/deployment-4634/replicasets/test-rollover-deployment-6455657675,UID:0eedce3e-bd94-11e9-868a-506b8de0bf77,ResourceVersion:11913,Generation:2,CreationTimestamp:2019-08-13 06:32:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 0eeb963e-bd94-11e9-868a-506b8de0bf77 0xc000a0a047 0xc000a0a048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 06:32:17.554: INFO: Pod "test-rollover-deployment-766b4d6c9d-bpnhw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-bpnhw,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-4634,SelfLink:/api/v1/namespaces/deployment-4634/pods/test-rollover-deployment-766b4d6c9d-bpnhw,UID:1029bb11-bd94-11e9-868a-506b8de0bf77,ResourceVersion:11926,Generation:0,CreationTimestamp:2019-08-13 06:32:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 1023ff32-bd94-11e9-868a-506b8de0bf77 0xc001d94467 0xc001d94468}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-phntr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-phntr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-phntr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:32:03 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:32:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:32:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:32:03 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.109,StartTime:2019-08-13 06:32:03 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-13 06:32:06 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://9652cc9bf1ce7752a92857263febb5675ea81c55a1169d33689671cd85525e7b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:32:17.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4634" for this suite.
Aug 13 06:32:23.567: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:32:23.654: INFO: namespace deployment-4634 deletion completed in 6.097480921s

â€¢ [SLOW TEST:29.238 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:32:23.654: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 13 06:32:23.928: INFO: Pod name wrapped-volume-race-1c4d3440-bd94-11e9-8bf8-0a58ac140308: Found 0 pods out of 5
Aug 13 06:32:28.938: INFO: Pod name wrapped-volume-race-1c4d3440-bd94-11e9-8bf8-0a58ac140308: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-1c4d3440-bd94-11e9-8bf8-0a58ac140308 in namespace emptydir-wrapper-5990, will wait for the garbage collector to delete the pods
Aug 13 06:32:45.030: INFO: Deleting ReplicationController wrapped-volume-race-1c4d3440-bd94-11e9-8bf8-0a58ac140308 took: 16.758594ms
Aug 13 06:32:45.430: INFO: Terminating ReplicationController wrapped-volume-race-1c4d3440-bd94-11e9-8bf8-0a58ac140308 pods took: 400.287508ms
STEP: Creating RC which spawns configmap-volume pods
Aug 13 06:33:28.846: INFO: Pod name wrapped-volume-race-42fec218-bd94-11e9-8bf8-0a58ac140308: Found 0 pods out of 5
Aug 13 06:33:33.852: INFO: Pod name wrapped-volume-race-42fec218-bd94-11e9-8bf8-0a58ac140308: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-42fec218-bd94-11e9-8bf8-0a58ac140308 in namespace emptydir-wrapper-5990, will wait for the garbage collector to delete the pods
Aug 13 06:33:43.938: INFO: Deleting ReplicationController wrapped-volume-race-42fec218-bd94-11e9-8bf8-0a58ac140308 took: 7.840043ms
Aug 13 06:33:44.338: INFO: Terminating ReplicationController wrapped-volume-race-42fec218-bd94-11e9-8bf8-0a58ac140308 pods took: 400.270851ms
STEP: Creating RC which spawns configmap-volume pods
Aug 13 06:34:20.454: INFO: Pod name wrapped-volume-race-61c16efe-bd94-11e9-8bf8-0a58ac140308: Found 0 pods out of 5
Aug 13 06:34:25.459: INFO: Pod name wrapped-volume-race-61c16efe-bd94-11e9-8bf8-0a58ac140308: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-61c16efe-bd94-11e9-8bf8-0a58ac140308 in namespace emptydir-wrapper-5990, will wait for the garbage collector to delete the pods
Aug 13 06:34:37.560: INFO: Deleting ReplicationController wrapped-volume-race-61c16efe-bd94-11e9-8bf8-0a58ac140308 took: 7.41978ms
Aug 13 06:34:37.960: INFO: Terminating ReplicationController wrapped-volume-race-61c16efe-bd94-11e9-8bf8-0a58ac140308 pods took: 400.254815ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:35:18.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5990" for this suite.
Aug 13 06:35:24.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:35:24.273: INFO: namespace emptydir-wrapper-5990 deletion completed in 6.088709244s

â€¢ [SLOW TEST:180.619 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:35:24.274: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-4009
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4009 to expose endpoints map[]
Aug 13 06:35:24.321: INFO: Get endpoints failed (7.547575ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Aug 13 06:35:25.324: INFO: successfully validated that service multi-endpoint-test in namespace services-4009 exposes endpoints map[] (1.010952484s elapsed)
STEP: Creating pod pod1 in namespace services-4009
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4009 to expose endpoints map[pod1:[100]]
Aug 13 06:35:29.375: INFO: successfully validated that service multi-endpoint-test in namespace services-4009 exposes endpoints map[pod1:[100]] (4.041811255s elapsed)
STEP: Creating pod pod2 in namespace services-4009
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4009 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 13 06:35:32.421: INFO: successfully validated that service multi-endpoint-test in namespace services-4009 exposes endpoints map[pod1:[100] pod2:[101]] (3.041271047s elapsed)
STEP: Deleting pod pod1 in namespace services-4009
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4009 to expose endpoints map[pod2:[101]]
Aug 13 06:35:33.442: INFO: successfully validated that service multi-endpoint-test in namespace services-4009 exposes endpoints map[pod2:[101]] (1.016000823s elapsed)
STEP: Deleting pod pod2 in namespace services-4009
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-4009 to expose endpoints map[]
Aug 13 06:35:33.458: INFO: successfully validated that service multi-endpoint-test in namespace services-4009 exposes endpoints map[] (9.299222ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:35:33.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4009" for this suite.
Aug 13 06:35:39.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:35:39.581: INFO: namespace services-4009 deletion completed in 6.090905131s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:15.308 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:35:39.581: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:35:43.658: INFO: Waiting up to 5m0s for pod "client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308" in namespace "pods-2541" to be "success or failure"
Aug 13 06:35:43.663: INFO: Pod "client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.764183ms
Aug 13 06:35:45.667: INFO: Pod "client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009134624s
Aug 13 06:35:47.672: INFO: Pod "client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013766926s
STEP: Saw pod success
Aug 13 06:35:47.672: INFO: Pod "client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:35:47.674: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308 container env3cont: <nil>
STEP: delete the pod
Aug 13 06:35:47.697: INFO: Waiting for pod client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:35:47.703: INFO: Pod client-envvars-9359e1d3-bd94-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:35:47.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2541" for this suite.
Aug 13 06:36:29.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:36:29.814: INFO: namespace pods-2541 deletion completed in 42.107937995s

â€¢ [SLOW TEST:50.233 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:36:29.815: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:36:29.856: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 13 06:36:34.860: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 06:36:34.860: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 13 06:36:34.881: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8351,SelfLink:/apis/apps/v1/namespaces/deployment-8351/deployments/test-cleanup-deployment,UID:b1e153a5-bd94-11e9-868a-506b8de0bf77,ResourceVersion:13412,Generation:1,CreationTimestamp:2019-08-13 06:36:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Aug 13 06:36:34.886: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Aug 13 06:36:34.886: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Aug 13 06:36:34.886: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-8351,SelfLink:/apis/apps/v1/namespaces/deployment-8351/replicasets/test-cleanup-controller,UID:aee3c827-bd94-11e9-868a-506b8de0bf77,ResourceVersion:13413,Generation:1,CreationTimestamp:2019-08-13 06:36:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment b1e153a5-bd94-11e9-868a-506b8de0bf77 0xc002770f37 0xc002770f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 06:36:34.893: INFO: Pod "test-cleanup-controller-vtxt7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-vtxt7,GenerateName:test-cleanup-controller-,Namespace:deployment-8351,SelfLink:/api/v1/namespaces/deployment-8351/pods/test-cleanup-controller-vtxt7,UID:aee4ee04-bd94-11e9-868a-506b8de0bf77,ResourceVersion:13405,Generation:0,CreationTimestamp:2019-08-13 06:36:29 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller aee3c827-bd94-11e9-868a-506b8de0bf77 0xc0027714a7 0xc0027714a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-khxgx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-khxgx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-khxgx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:36:29 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:36:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:36:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:36:29 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.113,StartTime:2019-08-13 06:36:29 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-13 06:36:32 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/nginx:1.14-alpine docker-pullable://docker.io/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://5442ba09f9973c49bf2ffc7536fbfa43431cb53a1c153d43cc4c6ab877417e3f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:36:34.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8351" for this suite.
Aug 13 06:36:40.920: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:36:41.000: INFO: namespace deployment-8351 deletion completed in 6.098316909s

â€¢ [SLOW TEST:11.186 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:36:41.000: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 13 06:36:46.071: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:36:47.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1246" for this suite.
Aug 13 06:37:09.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:37:09.183: INFO: namespace replicaset-1246 deletion completed in 22.093308758s

â€¢ [SLOW TEST:28.183 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:37:09.183: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 13 06:37:09.221: INFO: Waiting up to 5m0s for pod "pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308" in namespace "emptydir-3370" to be "success or failure"
Aug 13 06:37:09.227: INFO: Pod "pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.056169ms
Aug 13 06:37:11.232: INFO: Pod "pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010364781s
Aug 13 06:37:13.236: INFO: Pod "pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014680288s
STEP: Saw pod success
Aug 13 06:37:13.236: INFO: Pod "pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:37:13.239: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:37:13.260: INFO: Waiting for pod pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:37:13.262: INFO: Pod pod-c65a8f29-bd94-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:37:13.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3370" for this suite.
Aug 13 06:37:19.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:37:19.383: INFO: namespace emptydir-3370 deletion completed in 6.116083713s

â€¢ [SLOW TEST:10.200 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:37:19.384: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-92tf
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 06:37:19.435: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-92tf" in namespace "subpath-9573" to be "success or failure"
Aug 13 06:37:19.439: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.220372ms
Aug 13 06:37:21.442: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007580048s
Aug 13 06:37:23.446: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 4.011332776s
Aug 13 06:37:25.450: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 6.015197538s
Aug 13 06:37:27.454: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 8.019042956s
Aug 13 06:37:29.458: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 10.023127575s
Aug 13 06:37:31.461: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 12.02605998s
Aug 13 06:37:33.465: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 14.03045492s
Aug 13 06:37:35.469: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 16.034491329s
Aug 13 06:37:37.473: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 18.038208422s
Aug 13 06:37:39.478: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 20.042648454s
Aug 13 06:37:41.495: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Running", Reason="", readiness=true. Elapsed: 22.060079754s
Aug 13 06:37:43.499: INFO: Pod "pod-subpath-test-projected-92tf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.064196078s
STEP: Saw pod success
Aug 13 06:37:43.499: INFO: Pod "pod-subpath-test-projected-92tf" satisfied condition "success or failure"
Aug 13 06:37:43.502: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-subpath-test-projected-92tf container test-container-subpath-projected-92tf: <nil>
STEP: delete the pod
Aug 13 06:37:43.531: INFO: Waiting for pod pod-subpath-test-projected-92tf to disappear
Aug 13 06:37:43.533: INFO: Pod pod-subpath-test-projected-92tf no longer exists
STEP: Deleting pod pod-subpath-test-projected-92tf
Aug 13 06:37:43.534: INFO: Deleting pod "pod-subpath-test-projected-92tf" in namespace "subpath-9573"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:37:43.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9573" for this suite.
Aug 13 06:37:49.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:37:49.642: INFO: namespace subpath-9573 deletion completed in 6.102507623s

â€¢ [SLOW TEST:30.259 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:37:49.643: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-109
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 13 06:37:49.699: INFO: Found 0 stateful pods, waiting for 3
Aug 13 06:37:59.703: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:37:59.703: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:37:59.703: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=false
Aug 13 06:38:09.703: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:38:09.704: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:38:09.704: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 13 06:38:09.732: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 13 06:38:19.765: INFO: Updating stateful set ss2
Aug 13 06:38:19.771: INFO: Waiting for Pod statefulset-109/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Aug 13 06:38:29.855: INFO: Found 2 stateful pods, waiting for 3
Aug 13 06:38:39.859: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:38:39.859: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:38:39.859: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 13 06:38:39.892: INFO: Updating stateful set ss2
Aug 13 06:38:39.920: INFO: Waiting for Pod statefulset-109/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Aug 13 06:38:49.943: INFO: Updating stateful set ss2
Aug 13 06:38:49.951: INFO: Waiting for StatefulSet statefulset-109/ss2 to complete update
Aug 13 06:38:49.951: INFO: Waiting for Pod statefulset-109/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 13 06:38:59.970: INFO: Deleting all statefulset in ns statefulset-109
Aug 13 06:38:59.974: INFO: Scaling statefulset ss2 to 0
Aug 13 06:39:19.990: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 06:39:19.994: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:39:20.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-109" for this suite.
Aug 13 06:39:26.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:39:26.119: INFO: namespace statefulset-109 deletion completed in 6.100887096s

â€¢ [SLOW TEST:96.477 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:39:26.120: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:39:30.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-851" for this suite.
Aug 13 06:39:36.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:39:36.333: INFO: namespace emptydir-wrapper-851 deletion completed in 6.116091125s

â€¢ [SLOW TEST:10.214 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:39:36.334: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 13 06:39:36.397: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7208,SelfLink:/api/v1/namespaces/watch-7208/configmaps/e2e-watch-test-resource-version,UID:1e104a2d-bd95-11e9-868a-506b8de0bf77,ResourceVersion:14168,Generation:0,CreationTimestamp:2019-08-13 06:39:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 06:39:36.397: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7208,SelfLink:/api/v1/namespaces/watch-7208/configmaps/e2e-watch-test-resource-version,UID:1e104a2d-bd95-11e9-868a-506b8de0bf77,ResourceVersion:14169,Generation:0,CreationTimestamp:2019-08-13 06:39:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:39:36.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7208" for this suite.
Aug 13 06:39:42.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:39:42.511: INFO: namespace watch-7208 deletion completed in 6.110010704s

â€¢ [SLOW TEST:6.178 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:39:42.512: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 13 06:39:42.549: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:39:58.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-738" for this suite.
Aug 13 06:40:04.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:40:04.755: INFO: namespace pods-738 deletion completed in 6.143330385s

â€¢ [SLOW TEST:22.243 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:40:04.755: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-2f0256de-bd95-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:40:04.811: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308" in namespace "projected-6495" to be "success or failure"
Aug 13 06:40:04.820: INFO: Pod "pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.763638ms
Aug 13 06:40:06.826: INFO: Pod "pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014246886s
Aug 13 06:40:08.830: INFO: Pod "pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01815179s
STEP: Saw pod success
Aug 13 06:40:08.830: INFO: Pod "pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:40:08.832: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:40:08.855: INFO: Waiting for pod pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:40:08.859: INFO: Pod pod-projected-configmaps-2f03534c-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:40:08.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6495" for this suite.
Aug 13 06:40:14.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:40:14.955: INFO: namespace projected-6495 deletion completed in 6.092109331s

â€¢ [SLOW TEST:10.200 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:40:14.955: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 13 06:40:15.036: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:15.036: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:15.055: INFO: Number of nodes with available pods: 0
Aug 13 06:40:15.055: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:40:16.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:16.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:16.063: INFO: Number of nodes with available pods: 0
Aug 13 06:40:16.063: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:40:17.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:17.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:17.064: INFO: Number of nodes with available pods: 0
Aug 13 06:40:17.064: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:40:18.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:18.060: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:18.063: INFO: Number of nodes with available pods: 3
Aug 13 06:40:18.063: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 13 06:40:18.087: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:18.087: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:18.091: INFO: Number of nodes with available pods: 2
Aug 13 06:40:18.091: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-1 is running more than one daemon pod
Aug 13 06:40:19.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:19.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:19.098: INFO: Number of nodes with available pods: 2
Aug 13 06:40:19.098: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-1 is running more than one daemon pod
Aug 13 06:40:20.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:20.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:20.099: INFO: Number of nodes with available pods: 2
Aug 13 06:40:20.099: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-1 is running more than one daemon pod
Aug 13 06:40:21.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:21.096: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:40:21.099: INFO: Number of nodes with available pods: 3
Aug 13 06:40:21.099: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8889, will wait for the garbage collector to delete the pods
Aug 13 06:40:21.164: INFO: Deleting DaemonSet.extensions daemon-set took: 7.475206ms
Aug 13 06:40:21.565: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.256437ms
Aug 13 06:40:27.868: INFO: Number of nodes with available pods: 0
Aug 13 06:40:27.868: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 06:40:27.871: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8889/daemonsets","resourceVersion":"14391"},"items":null}

Aug 13 06:40:27.873: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8889/pods","resourceVersion":"14391"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:40:27.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8889" for this suite.
Aug 13 06:40:33.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:40:33.985: INFO: namespace daemonsets-8889 deletion completed in 6.094494897s

â€¢ [SLOW TEST:19.030 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:40:33.985: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 13 06:40:34.030: INFO: Waiting up to 5m0s for pod "pod-406d0f06-bd95-11e9-8bf8-0a58ac140308" in namespace "emptydir-52" to be "success or failure"
Aug 13 06:40:34.037: INFO: Pod "pod-406d0f06-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.976435ms
Aug 13 06:40:36.041: INFO: Pod "pod-406d0f06-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011733328s
Aug 13 06:40:38.045: INFO: Pod "pod-406d0f06-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015658447s
STEP: Saw pod success
Aug 13 06:40:38.045: INFO: Pod "pod-406d0f06-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:40:38.049: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-406d0f06-bd95-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:40:38.071: INFO: Waiting for pod pod-406d0f06-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:40:38.074: INFO: Pod pod-406d0f06-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:40:38.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-52" for this suite.
Aug 13 06:40:44.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:40:44.174: INFO: namespace emptydir-52 deletion completed in 6.096188716s

â€¢ [SLOW TEST:10.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:40:44.174: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-467ff2d3-bd95-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:40:44.219: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308" in namespace "projected-4838" to be "success or failure"
Aug 13 06:40:44.223: INFO: Pod "pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698087ms
Aug 13 06:40:46.227: INFO: Pod "pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007474647s
Aug 13 06:40:48.232: INFO: Pod "pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012184341s
STEP: Saw pod success
Aug 13 06:40:48.232: INFO: Pod "pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:40:48.234: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:40:48.254: INFO: Waiting for pod pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:40:48.256: INFO: Pod pod-projected-secrets-4680ac99-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:40:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4838" for this suite.
Aug 13 06:40:54.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:40:54.372: INFO: namespace projected-4838 deletion completed in 6.111992321s

â€¢ [SLOW TEST:10.198 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:40:54.372: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0813 06:41:00.438161      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 06:41:00.438: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:41:00.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3384" for this suite.
Aug 13 06:41:06.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:41:06.533: INFO: namespace gc-3384 deletion completed in 6.092383279s

â€¢ [SLOW TEST:12.161 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:41:06.533: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-53d38957-bd95-11e9-8bf8-0a58ac140308
STEP: Creating secret with name secret-projected-all-test-volume-53d38940-bd95-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 13 06:41:06.583: INFO: Waiting up to 5m0s for pod "projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308" in namespace "projected-1180" to be "success or failure"
Aug 13 06:41:06.588: INFO: Pod "projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.351049ms
Aug 13 06:41:08.593: INFO: Pod "projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010163135s
Aug 13 06:41:10.597: INFO: Pod "projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014266102s
STEP: Saw pod success
Aug 13 06:41:10.597: INFO: Pod "projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:41:10.600: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308 container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 13 06:41:10.620: INFO: Waiting for pod projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:41:10.623: INFO: Pod projected-volume-53d38906-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:41:10.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1180" for this suite.
Aug 13 06:41:16.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:41:16.728: INFO: namespace projected-1180 deletion completed in 6.100527908s

â€¢ [SLOW TEST:10.195 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:41:16.728: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-59e8375d-bd95-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:41:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2100" for this suite.
Aug 13 06:41:42.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:41:42.915: INFO: namespace configmap-2100 deletion completed in 22.104019857s

â€¢ [SLOW TEST:26.186 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:41:42.915: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Aug 13 06:41:42.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-111'
Aug 13 06:41:43.416: INFO: stderr: ""
Aug 13 06:41:43.416: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Aug 13 06:41:44.421: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 06:41:44.421: INFO: Found 0 / 1
Aug 13 06:41:45.421: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 06:41:45.421: INFO: Found 0 / 1
Aug 13 06:41:46.421: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 06:41:46.421: INFO: Found 1 / 1
Aug 13 06:41:46.421: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 06:41:46.424: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 06:41:46.424: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 13 06:41:46.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 logs redis-master-rmccz redis-master --namespace=kubectl-111'
Aug 13 06:41:46.520: INFO: stderr: ""
Aug 13 06:41:46.520: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 06:41:45.880 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 06:41:45.880 # Server started, Redis version 3.2.12\n1:M 13 Aug 06:41:45.880 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 06:41:45.880 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 13 06:41:46.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 log redis-master-rmccz redis-master --namespace=kubectl-111 --tail=1'
Aug 13 06:41:46.610: INFO: stderr: ""
Aug 13 06:41:46.610: INFO: stdout: "1:M 13 Aug 06:41:45.880 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 13 06:41:46.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 log redis-master-rmccz redis-master --namespace=kubectl-111 --limit-bytes=1'
Aug 13 06:41:46.704: INFO: stderr: ""
Aug 13 06:41:46.704: INFO: stdout: " "
STEP: exposing timestamps
Aug 13 06:41:46.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 log redis-master-rmccz redis-master --namespace=kubectl-111 --tail=1 --timestamps'
Aug 13 06:41:46.797: INFO: stderr: ""
Aug 13 06:41:46.797: INFO: stdout: "2019-08-13T06:41:45.881343217Z 1:M 13 Aug 06:41:45.880 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 13 06:41:49.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 log redis-master-rmccz redis-master --namespace=kubectl-111 --since=1s'
Aug 13 06:41:49.394: INFO: stderr: ""
Aug 13 06:41:49.394: INFO: stdout: ""
Aug 13 06:41:49.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 log redis-master-rmccz redis-master --namespace=kubectl-111 --since=24h'
Aug 13 06:41:49.487: INFO: stderr: ""
Aug 13 06:41:49.487: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 06:41:45.880 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 06:41:45.880 # Server started, Redis version 3.2.12\n1:M 13 Aug 06:41:45.880 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 06:41:45.880 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Aug 13 06:41:49.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete --grace-period=0 --force -f - --namespace=kubectl-111'
Aug 13 06:41:49.572: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 13 06:41:49.572: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 13 06:41:49.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get rc,svc -l name=nginx --no-headers --namespace=kubectl-111'
Aug 13 06:41:49.664: INFO: stderr: "No resources found.\n"
Aug 13 06:41:49.664: INFO: stdout: ""
Aug 13 06:41:49.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 get pods -l name=nginx --namespace=kubectl-111 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 13 06:41:49.748: INFO: stderr: ""
Aug 13 06:41:49.748: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:41:49.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-111" for this suite.
Aug 13 06:42:11.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:42:11.862: INFO: namespace kubectl-111 deletion completed in 22.108693328s

â€¢ [SLOW TEST:28.947 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:42:11.862: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:42:11.898: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308" in namespace "downward-api-7891" to be "success or failure"
Aug 13 06:42:11.904: INFO: Pod "downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.915272ms
Aug 13 06:42:13.908: INFO: Pod "downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009821603s
Aug 13 06:42:15.911: INFO: Pod "downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013396745s
STEP: Saw pod success
Aug 13 06:42:15.911: INFO: Pod "downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:42:15.914: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:42:15.933: INFO: Waiting for pod downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:42:15.939: INFO: Pod downwardapi-volume-7ac37f2f-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:42:15.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7891" for this suite.
Aug 13 06:42:21.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:42:22.051: INFO: namespace downward-api-7891 deletion completed in 6.107450079s

â€¢ [SLOW TEST:10.189 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:42:22.051: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 13 06:42:22.095: INFO: Waiting up to 5m0s for pod "downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308" in namespace "downward-api-9498" to be "success or failure"
Aug 13 06:42:22.104: INFO: Pod "downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.717058ms
Aug 13 06:42:24.108: INFO: Pod "downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012302043s
Aug 13 06:42:26.112: INFO: Pod "downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016275794s
STEP: Saw pod success
Aug 13 06:42:26.112: INFO: Pod "downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:42:26.114: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:42:26.138: INFO: Waiting for pod downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:42:26.141: INFO: Pod downward-api-80d70628-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:42:26.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9498" for this suite.
Aug 13 06:42:32.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:42:32.245: INFO: namespace downward-api-9498 deletion completed in 6.100856756s

â€¢ [SLOW TEST:10.194 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:42:32.246: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1626
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1626
STEP: Creating statefulset with conflicting port in namespace statefulset-1626
STEP: Waiting until pod test-pod will start running in namespace statefulset-1626
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1626
Aug 13 06:42:38.367: INFO: Observed stateful pod in namespace: statefulset-1626, name: ss-0, uid: 8a2e5535-bd95-11e9-868a-506b8de0bf77, status phase: Failed. Waiting for statefulset controller to delete.
Aug 13 06:42:38.368: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1626
STEP: Removing pod with conflicting port in namespace statefulset-1626
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1626 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 13 06:42:42.413: INFO: Deleting all statefulset in ns statefulset-1626
Aug 13 06:42:42.416: INFO: Scaling statefulset ss to 0
Aug 13 06:42:52.432: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 06:42:52.435: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:42:52.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1626" for this suite.
Aug 13 06:42:58.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:42:58.551: INFO: namespace statefulset-1626 deletion completed in 6.09792709s

â€¢ [SLOW TEST:26.305 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:42:58.551: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 13 06:43:06.638: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:06.640: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:08.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:08.645: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:10.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:10.644: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:12.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:12.645: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:14.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:14.644: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:16.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:16.644: INFO: Pod pod-with-poststart-http-hook still exists
Aug 13 06:43:18.640: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 13 06:43:18.644: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:43:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9251" for this suite.
Aug 13 06:43:40.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:43:40.754: INFO: namespace container-lifecycle-hook-9251 deletion completed in 22.106912341s

â€¢ [SLOW TEST:42.203 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:43:40.755: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Aug 13 06:43:40.853: INFO: Waiting up to 5m0s for pod "var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308" in namespace "var-expansion-1933" to be "success or failure"
Aug 13 06:43:40.859: INFO: Pod "var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.803796ms
Aug 13 06:43:42.865: INFO: Pod "var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011222767s
Aug 13 06:43:44.869: INFO: Pod "var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015140153s
STEP: Saw pod success
Aug 13 06:43:44.869: INFO: Pod "var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:43:44.871: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 06:43:44.891: INFO: Waiting for pod var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:43:44.894: INFO: Pod var-expansion-afc8bc81-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:43:44.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1933" for this suite.
Aug 13 06:43:50.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:43:50.984: INFO: namespace var-expansion-1933 deletion completed in 6.086801647s

â€¢ [SLOW TEST:10.230 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:43:50.984: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:43:51.051: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"b5db55da-bd95-11e9-868a-506b8de0bf77", Controller:(*bool)(0xc002ace726), BlockOwnerDeletion:(*bool)(0xc002ace727)}}
Aug 13 06:43:51.061: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b5d93c07-bd95-11e9-868a-506b8de0bf77", Controller:(*bool)(0xc001b146d6), BlockOwnerDeletion:(*bool)(0xc001b146d7)}}
Aug 13 06:43:51.067: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"b5da19cd-bd95-11e9-868a-506b8de0bf77", Controller:(*bool)(0xc002770f16), BlockOwnerDeletion:(*bool)(0xc002770f17)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:43:56.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7877" for this suite.
Aug 13 06:44:02.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:44:02.196: INFO: namespace gc-7877 deletion completed in 6.111421276s

â€¢ [SLOW TEST:11.211 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:44:02.196: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:44:02.239: INFO: (0) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.507821ms)
Aug 13 06:44:02.243: INFO: (1) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.955559ms)
Aug 13 06:44:02.247: INFO: (2) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.86841ms)
Aug 13 06:44:02.250: INFO: (3) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.906408ms)
Aug 13 06:44:02.254: INFO: (4) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.590573ms)
Aug 13 06:44:02.258: INFO: (5) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.72994ms)
Aug 13 06:44:02.261: INFO: (6) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.568944ms)
Aug 13 06:44:02.267: INFO: (7) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 5.004611ms)
Aug 13 06:44:02.271: INFO: (8) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.159663ms)
Aug 13 06:44:02.274: INFO: (9) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.674145ms)
Aug 13 06:44:02.278: INFO: (10) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.786454ms)
Aug 13 06:44:02.283: INFO: (11) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.435927ms)
Aug 13 06:44:02.287: INFO: (12) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.119856ms)
Aug 13 06:44:02.291: INFO: (13) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.084414ms)
Aug 13 06:44:02.295: INFO: (14) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.105367ms)
Aug 13 06:44:02.299: INFO: (15) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.839361ms)
Aug 13 06:44:02.304: INFO: (16) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.697525ms)
Aug 13 06:44:02.308: INFO: (17) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.062374ms)
Aug 13 06:44:02.312: INFO: (18) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 4.427328ms)
Aug 13 06:44:02.316: INFO: (19) /api/v1/nodes/karbon-multi-fourteen-aca02f-k8s-worker-0:10250/proxy/logs/: <pre>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href="chrony/">chrony/</a>
<a hr... (200; 3.210095ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:44:02.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-217" for this suite.
Aug 13 06:44:08.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:44:08.429: INFO: namespace proxy-217 deletion completed in 6.107757264s

â€¢ [SLOW TEST:6.233 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:44:08.429: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:44:08.468: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:44:12.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5730" for this suite.
Aug 13 06:44:50.594: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:44:50.684: INFO: namespace pods-5730 deletion completed in 38.102234235s

â€¢ [SLOW TEST:42.255 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:44:50.685: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:44:50.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7616" for this suite.
Aug 13 06:45:12.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:45:12.833: INFO: namespace pods-7616 deletion completed in 22.099915389s

â€¢ [SLOW TEST:22.149 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:45:12.834: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:45:12.890: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 13 06:45:12.900: INFO: Number of nodes with available pods: 0
Aug 13 06:45:12.900: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 13 06:45:12.920: INFO: Number of nodes with available pods: 0
Aug 13 06:45:12.920: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:13.924: INFO: Number of nodes with available pods: 0
Aug 13 06:45:13.924: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:14.924: INFO: Number of nodes with available pods: 0
Aug 13 06:45:14.924: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:15.925: INFO: Number of nodes with available pods: 1
Aug 13 06:45:15.925: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 13 06:45:15.942: INFO: Number of nodes with available pods: 1
Aug 13 06:45:15.942: INFO: Number of running nodes: 0, number of available pods: 1
Aug 13 06:45:16.945: INFO: Number of nodes with available pods: 0
Aug 13 06:45:16.945: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 13 06:45:16.954: INFO: Number of nodes with available pods: 0
Aug 13 06:45:16.954: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:17.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:17.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:18.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:18.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:19.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:19.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:20.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:20.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:21.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:21.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:22.959: INFO: Number of nodes with available pods: 0
Aug 13 06:45:22.959: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:23.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:23.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:24.959: INFO: Number of nodes with available pods: 0
Aug 13 06:45:24.959: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:25.959: INFO: Number of nodes with available pods: 0
Aug 13 06:45:25.959: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:26.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:26.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:27.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:27.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:28.959: INFO: Number of nodes with available pods: 0
Aug 13 06:45:28.959: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:29.958: INFO: Number of nodes with available pods: 0
Aug 13 06:45:29.958: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:45:30.958: INFO: Number of nodes with available pods: 1
Aug 13 06:45:30.958: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6850, will wait for the garbage collector to delete the pods
Aug 13 06:45:31.037: INFO: Deleting DaemonSet.extensions daemon-set took: 19.89978ms
Aug 13 06:45:31.437: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.299759ms
Aug 13 06:45:34.641: INFO: Number of nodes with available pods: 0
Aug 13 06:45:34.641: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 06:45:34.643: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6850/daemonsets","resourceVersion":"15775"},"items":null}

Aug 13 06:45:34.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6850/pods","resourceVersion":"15775"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:45:34.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6850" for this suite.
Aug 13 06:45:40.679: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:45:40.776: INFO: namespace daemonsets-6850 deletion completed in 6.108286423s

â€¢ [SLOW TEST:27.942 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:45:40.776: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-f749273f-bd95-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:45:40.818: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308" in namespace "projected-2907" to be "success or failure"
Aug 13 06:45:40.823: INFO: Pod "pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658482ms
Aug 13 06:45:42.827: INFO: Pod "pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008601829s
Aug 13 06:45:44.831: INFO: Pod "pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012743525s
STEP: Saw pod success
Aug 13 06:45:44.831: INFO: Pod "pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:45:44.834: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:45:44.857: INFO: Waiting for pod pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:45:44.860: INFO: Pod pod-projected-secrets-f749eac8-bd95-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:45:44.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2907" for this suite.
Aug 13 06:45:50.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:45:50.958: INFO: namespace projected-2907 deletion completed in 6.094805009s

â€¢ [SLOW TEST:10.183 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:45:50.959: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fd5b71d0-bd95-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fd5b71d0-bd95-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:47:21.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4115" for this suite.
Aug 13 06:47:43.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:47:43.576: INFO: namespace projected-4115 deletion completed in 22.099455467s

â€¢ [SLOW TEST:112.618 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:47:43.577: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-407b16a2-bd96-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:47:43.616: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308" in namespace "projected-3113" to be "success or failure"
Aug 13 06:47:43.620: INFO: Pod "pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.379322ms
Aug 13 06:47:45.624: INFO: Pod "pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008096515s
Aug 13 06:47:47.629: INFO: Pod "pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012748241s
STEP: Saw pod success
Aug 13 06:47:47.629: INFO: Pod "pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:47:47.632: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:47:47.651: INFO: Waiting for pod pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:47:47.654: INFO: Pod pod-projected-configmaps-407b9729-bd96-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:47:47.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3113" for this suite.
Aug 13 06:47:53.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:47:53.764: INFO: namespace projected-3113 deletion completed in 6.106095213s

â€¢ [SLOW TEST:10.187 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:47:53.764: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0813 06:48:03.817949      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 06:48:03.817: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:48:03.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2473" for this suite.
Aug 13 06:48:09.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:48:09.918: INFO: namespace gc-2473 deletion completed in 6.096928843s

â€¢ [SLOW TEST:16.154 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:48:09.919: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-502f60bb-bd96-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:48:09.967: INFO: Waiting up to 5m0s for pod "pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308" in namespace "configmap-6254" to be "success or failure"
Aug 13 06:48:09.972: INFO: Pod "pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078247ms
Aug 13 06:48:11.977: INFO: Pod "pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00999985s
Aug 13 06:48:13.982: INFO: Pod "pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01419263s
STEP: Saw pod success
Aug 13 06:48:13.982: INFO: Pod "pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:48:13.985: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:48:14.005: INFO: Waiting for pod pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:48:14.008: INFO: Pod pod-configmaps-50301dad-bd96-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:48:14.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6254" for this suite.
Aug 13 06:48:20.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:48:20.105: INFO: namespace configmap-6254 deletion completed in 6.09346774s

â€¢ [SLOW TEST:10.187 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:48:20.105: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 13 06:48:24.671: INFO: Successfully updated pod "labelsupdate5640d2e1-bd96-11e9-8bf8-0a58ac140308"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:48:26.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6582" for this suite.
Aug 13 06:48:48.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:48:48.800: INFO: namespace downward-api-6582 deletion completed in 22.099602327s

â€¢ [SLOW TEST:28.695 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:48:48.800: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 13 06:48:48.830: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:48:53.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3282" for this suite.
Aug 13 06:48:59.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:48:59.729: INFO: namespace init-container-3282 deletion completed in 6.120953711s

â€¢ [SLOW TEST:10.929 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:48:59.729: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:48:59.775: INFO: Create a RollingUpdate DaemonSet
Aug 13 06:48:59.781: INFO: Check that daemon pods launch on every node of the cluster
Aug 13 06:48:59.788: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:48:59.788: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:48:59.792: INFO: Number of nodes with available pods: 0
Aug 13 06:48:59.792: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:49:00.797: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:00.797: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:00.801: INFO: Number of nodes with available pods: 0
Aug 13 06:49:00.801: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:49:01.798: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:01.798: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:01.803: INFO: Number of nodes with available pods: 0
Aug 13 06:49:01.803: INFO: Node karbon-multi-fourteen-aca02f-k8s-worker-0 is running more than one daemon pod
Aug 13 06:49:02.796: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:02.796: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:02.800: INFO: Number of nodes with available pods: 3
Aug 13 06:49:02.800: INFO: Number of running nodes: 3, number of available pods: 3
Aug 13 06:49:02.800: INFO: Update the DaemonSet to trigger a rollout
Aug 13 06:49:02.808: INFO: Updating DaemonSet daemon-set
Aug 13 06:49:06.823: INFO: Roll back the DaemonSet before rollout is complete
Aug 13 06:49:06.831: INFO: Updating DaemonSet daemon-set
Aug 13 06:49:06.831: INFO: Make sure DaemonSet rollback is complete
Aug 13 06:49:06.837: INFO: Wrong image for pod: daemon-set-zqsmc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 13 06:49:06.837: INFO: Pod daemon-set-zqsmc is not available
Aug 13 06:49:06.842: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:06.842: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:07.846: INFO: Wrong image for pod: daemon-set-zqsmc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 13 06:49:07.846: INFO: Pod daemon-set-zqsmc is not available
Aug 13 06:49:07.851: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:07.851: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:08.846: INFO: Pod daemon-set-4926b is not available
Aug 13 06:49:08.850: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 13 06:49:08.850: INFO: DaemonSet pods can't tolerate node karbon-multi-fourteen-aca02f-k8s-master-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8190, will wait for the garbage collector to delete the pods
Aug 13 06:49:08.921: INFO: Deleting DaemonSet.extensions daemon-set took: 11.462569ms
Aug 13 06:49:09.321: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.229146ms
Aug 13 06:49:17.824: INFO: Number of nodes with available pods: 0
Aug 13 06:49:17.824: INFO: Number of running nodes: 0, number of available pods: 0
Aug 13 06:49:17.826: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8190/daemonsets","resourceVersion":"16506"},"items":null}

Aug 13 06:49:17.829: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8190/pods","resourceVersion":"16506"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:49:17.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8190" for this suite.
Aug 13 06:49:23.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:49:23.944: INFO: namespace daemonsets-8190 deletion completed in 6.095336073s

â€¢ [SLOW TEST:24.215 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:49:23.944: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-7c4e6f27-bd96-11e9-8bf8-0a58ac140308
STEP: Creating secret with name s-test-opt-upd-7c4e7008-bd96-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-7c4e6f27-bd96-11e9-8bf8-0a58ac140308
STEP: Updating secret s-test-opt-upd-7c4e7008-bd96-11e9-8bf8-0a58ac140308
STEP: Creating secret with name s-test-opt-create-7c4e7035-bd96-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:49:30.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7607" for this suite.
Aug 13 06:49:52.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:49:52.186: INFO: namespace projected-7607 deletion completed in 22.094571514s

â€¢ [SLOW TEST:28.242 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:49:52.186: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Aug 13 06:49:52.217: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-872026410 proxy --unix-socket=/tmp/kubectl-proxy-unix284952901/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:49:52.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9251" for this suite.
Aug 13 06:49:58.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:49:58.383: INFO: namespace kubectl-9251 deletion completed in 6.085768088s

â€¢ [SLOW TEST:6.197 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:49:58.383: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-90d48440-bd96-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:49:58.420: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308" in namespace "projected-5731" to be "success or failure"
Aug 13 06:49:58.424: INFO: Pod "pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.579124ms
Aug 13 06:50:00.429: INFO: Pod "pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008045424s
STEP: Saw pod success
Aug 13 06:50:00.429: INFO: Pod "pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:50:00.433: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:50:00.454: INFO: Waiting for pod pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:50:00.456: INFO: Pod pod-projected-secrets-90d520a0-bd96-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:50:00.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5731" for this suite.
Aug 13 06:50:06.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:50:06.563: INFO: namespace projected-5731 deletion completed in 6.099156399s

â€¢ [SLOW TEST:8.180 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:50:06.564: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 06:50:26.611: INFO: Container started at 2019-08-13 06:50:09 +0000 UTC, pod became ready at 2019-08-13 06:50:24 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:50:26.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8421" for this suite.
Aug 13 06:50:48.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:50:48.711: INFO: namespace container-probe-8421 deletion completed in 22.095921282s

â€¢ [SLOW TEST:42.147 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:50:48.711: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308
Aug 13 06:50:48.752: INFO: Pod name my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308: Found 0 pods out of 1
Aug 13 06:50:53.756: INFO: Pod name my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308: Found 1 pods out of 1
Aug 13 06:50:53.756: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308" are running
Aug 13 06:50:53.759: INFO: Pod "my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308-v7zn5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 06:50:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 06:50:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 06:50:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 06:50:48 +0000 UTC Reason: Message:}])
Aug 13 06:50:53.759: INFO: Trying to dial the pod
Aug 13 06:50:58.770: INFO: Controller my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308: Got expected result from replica 1 [my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308-v7zn5]: "my-hostname-basic-aed4f855-bd96-11e9-8bf8-0a58ac140308-v7zn5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:50:58.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-513" for this suite.
Aug 13 06:51:04.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:51:04.888: INFO: namespace replication-controller-513 deletion completed in 6.114576281s

â€¢ [SLOW TEST:16.177 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:51:04.889: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4824
Aug 13 06:51:06.940: INFO: Started pod liveness-http in namespace container-probe-4824
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 06:51:06.943: INFO: Initial restart count of pod liveness-http is 0
Aug 13 06:51:28.990: INFO: Restart count of pod container-probe-4824/liveness-http is now 1 (22.046918575s elapsed)
Aug 13 06:51:47.034: INFO: Restart count of pod container-probe-4824/liveness-http is now 2 (40.091502311s elapsed)
Aug 13 06:52:07.073: INFO: Restart count of pod container-probe-4824/liveness-http is now 3 (1m0.130350418s elapsed)
Aug 13 06:52:27.111: INFO: Restart count of pod container-probe-4824/liveness-http is now 4 (1m20.168293736s elapsed)
Aug 13 06:53:29.238: INFO: Restart count of pod container-probe-4824/liveness-http is now 5 (2m22.29490086s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:53:29.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4824" for this suite.
Aug 13 06:53:35.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:53:35.394: INFO: namespace container-probe-4824 deletion completed in 6.121984699s

â€¢ [SLOW TEST:150.506 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:53:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-1230a0ae-bd97-11e9-8bf8-0a58ac140308
STEP: Creating configMap with name cm-test-opt-upd-1230a0f5-bd97-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1230a0ae-bd97-11e9-8bf8-0a58ac140308
STEP: Updating configmap cm-test-opt-upd-1230a0f5-bd97-11e9-8bf8-0a58ac140308
STEP: Creating configMap with name cm-test-opt-create-1230a110-bd97-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:53:43.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7973" for this suite.
Aug 13 06:54:05.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:54:05.647: INFO: namespace configmap-7973 deletion completed in 22.09029709s

â€¢ [SLOW TEST:30.252 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:54:05.647: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:54:05.689: INFO: Waiting up to 5m0s for pod "downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308" in namespace "downward-api-5724" to be "success or failure"
Aug 13 06:54:05.693: INFO: Pod "downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658362ms
Aug 13 06:54:07.696: INFO: Pod "downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007386387s
Aug 13 06:54:09.701: INFO: Pod "downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011736225s
STEP: Saw pod success
Aug 13 06:54:09.701: INFO: Pod "downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:54:09.704: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:54:09.728: INFO: Waiting for pod downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:54:09.731: INFO: Pod downwardapi-volume-24370d98-bd97-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:54:09.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5724" for this suite.
Aug 13 06:54:15.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:54:15.835: INFO: namespace downward-api-5724 deletion completed in 6.100426238s

â€¢ [SLOW TEST:10.188 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:54:15.836: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 13 06:54:15.870: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:54:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5295" for this suite.
Aug 13 06:54:43.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:54:43.897: INFO: namespace init-container-5295 deletion completed in 22.096727543s

â€¢ [SLOW TEST:28.062 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:54:43.897: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 13 06:54:43.928: INFO: PodSpec: initContainers in spec.initContainers
Aug 13 06:55:28.370: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-3b03257e-bd97-11e9-8bf8-0a58ac140308", GenerateName:"", Namespace:"init-container-4111", SelfLink:"/api/v1/namespaces/init-container-4111/pods/pod-init-3b03257e-bd97-11e9-8bf8-0a58ac140308", UID:"3b0395ef-bd97-11e9-868a-506b8de0bf77", ResourceVersion:"17530", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63701276083, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"928415046"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-b9sbj", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002afbd00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b9sbj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b9sbj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-b9sbj", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0018906c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"karbon-multi-fourteen-aca02f-k8s-worker-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0028a05a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration(nil), HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(nil), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc001890730)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276083, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276083, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276083, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276083, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.40.139", PodIP:"172.20.4.161", StartTime:(*v1.Time)(0xc002f146c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002609e30)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002609ea0)}, Ready:false, RestartCount:3, Image:"docker.io/busybox:1.29", ImageID:"docker-pullable://docker.io/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://db9644bef3c9640f943021d550d7125f36fcd250d495de034926a33070ed5d42"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f14700), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002f146e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:55:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4111" for this suite.
Aug 13 06:55:50.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:55:50.472: INFO: namespace init-container-4111 deletion completed in 22.09650704s

â€¢ [SLOW TEST:66.575 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:55:50.472: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 06:55:50.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7352'
Aug 13 06:55:50.802: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 06:55:50.803: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Aug 13 06:55:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete jobs e2e-test-nginx-job --namespace=kubectl-7352'
Aug 13 06:55:50.918: INFO: stderr: ""
Aug 13 06:55:50.918: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:55:50.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7352" for this suite.
Aug 13 06:55:56.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:55:57.029: INFO: namespace kubectl-7352 deletion completed in 6.107527587s

â€¢ [SLOW TEST:6.557 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:55:57.030: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6822
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-6822
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6822
Aug 13 06:55:57.084: INFO: Found 0 stateful pods, waiting for 1
Aug 13 06:56:07.089: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 13 06:56:07.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 06:56:07.257: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 06:56:07.257: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 06:56:07.257: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 06:56:07.261: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 13 06:56:17.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 06:56:17.265: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 06:56:17.314: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:17.314: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:07 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:17.314: INFO: 
Aug 13 06:56:17.314: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 13 06:56:18.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994482217s
Aug 13 06:56:19.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989711054s
Aug 13 06:56:20.327: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986199999s
Aug 13 06:56:21.331: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981899041s
Aug 13 06:56:22.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977612402s
Aug 13 06:56:23.346: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.97380485s
Aug 13 06:56:24.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.963313446s
Aug 13 06:56:25.355: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.958386284s
Aug 13 06:56:26.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.161728ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6822
Aug 13 06:56:27.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 06:56:27.540: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 06:56:27.540: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 06:56:27.540: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 06:56:27.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 06:56:27.704: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 13 06:56:27.704: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 06:56:27.704: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 06:56:27.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 06:56:28.068: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 13 06:56:28.068: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 06:56:28.068: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 06:56:28.072: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:56:28.072: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 06:56:28.072: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 13 06:56:28.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 06:56:28.242: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 06:56:28.242: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 06:56:28.242: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 06:56:28.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 06:56:28.408: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 06:56:28.408: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 06:56:28.408: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 06:56:28.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-6822 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 06:56:28.574: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 06:56:28.574: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 06:56:28.574: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 06:56:28.574: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 06:56:28.577: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 13 06:56:38.584: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 06:56:38.584: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 06:56:38.584: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 06:56:38.596: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:38.596: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:38.596: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:38.596: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:38.596: INFO: 
Aug 13 06:56:38.596: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:39.601: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:39.601: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:39.601: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:39.601: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:39.601: INFO: 
Aug 13 06:56:39.601: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:40.606: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:40.606: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:40.606: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:40.606: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:40.606: INFO: 
Aug 13 06:56:40.606: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:41.610: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:41.610: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:41.610: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:41.610: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:41.610: INFO: 
Aug 13 06:56:41.610: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:42.614: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:42.614: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:42.614: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:42.614: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:42.614: INFO: 
Aug 13 06:56:42.614: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:43.619: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:43.619: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:43.619: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:43.619: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:43.619: INFO: 
Aug 13 06:56:43.619: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:44.623: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:44.623: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:44.623: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:44.623: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:44.623: INFO: 
Aug 13 06:56:44.623: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:45.627: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:45.627: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:45.627: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:45.627: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:45.627: INFO: 
Aug 13 06:56:45.627: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:46.631: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:46.631: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:46.631: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:46.631: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:46.631: INFO: 
Aug 13 06:56:46.631: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 13 06:56:47.635: INFO: POD   NODE                                       PHASE    GRACE  CONDITIONS
Aug 13 06:56:47.635: INFO: ss-0  karbon-multi-fourteen-aca02f-k8s-worker-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:55:57 +0000 UTC  }]
Aug 13 06:56:47.635: INFO: ss-1  karbon-multi-fourteen-aca02f-k8s-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:47.635: INFO: ss-2  karbon-multi-fourteen-aca02f-k8s-worker-1  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:29 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 06:56:17 +0000 UTC  }]
Aug 13 06:56:47.635: INFO: 
Aug 13 06:56:47.635: INFO: StatefulSet ss has not reached scale 0, at 3
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6822
Aug 13 06:56:48.638: INFO: Scaling statefulset ss to 0
Aug 13 06:56:48.647: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 13 06:56:48.649: INFO: Deleting all statefulset in ns statefulset-6822
Aug 13 06:56:48.651: INFO: Scaling statefulset ss to 0
Aug 13 06:56:48.658: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 06:56:48.660: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:56:48.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6822" for this suite.
Aug 13 06:56:54.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:56:54.792: INFO: namespace statefulset-6822 deletion completed in 6.094405056s

â€¢ [SLOW TEST:57.762 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:56:54.792: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 06:56:54.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308" in namespace "projected-5998" to be "success or failure"
Aug 13 06:56:54.830: INFO: Pod "downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.124689ms
Aug 13 06:56:56.834: INFO: Pod "downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008824345s
Aug 13 06:56:58.838: INFO: Pod "downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012547916s
STEP: Saw pod success
Aug 13 06:56:58.838: INFO: Pod "downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:56:58.841: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 06:56:58.863: INFO: Waiting for pod downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:56:58.868: INFO: Pod downwardapi-volume-890792e0-bd97-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:56:58.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5998" for this suite.
Aug 13 06:57:04.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:57:04.978: INFO: namespace projected-5998 deletion completed in 6.104880684s

â€¢ [SLOW TEST:10.186 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:57:04.978: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Aug 13 06:57:05.018: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1392" to be "success or failure"
Aug 13 06:57:05.022: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.318701ms
Aug 13 06:57:07.026: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007715589s
Aug 13 06:57:09.032: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013841366s
STEP: Saw pod success
Aug 13 06:57:09.032: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 13 06:57:09.035: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 13 06:57:09.056: INFO: Waiting for pod pod-host-path-test to disappear
Aug 13 06:57:09.058: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:57:09.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1392" for this suite.
Aug 13 06:57:15.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:57:15.150: INFO: namespace hostpath-1392 deletion completed in 6.088796758s

â€¢ [SLOW TEST:10.173 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:57:15.151: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-952a6c0b-bd97-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 06:57:15.192: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308" in namespace "projected-1951" to be "success or failure"
Aug 13 06:57:15.194: INFO: Pod "pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.6877ms
Aug 13 06:57:17.198: INFO: Pod "pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006023853s
Aug 13 06:57:19.202: INFO: Pod "pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010491515s
STEP: Saw pod success
Aug 13 06:57:19.202: INFO: Pod "pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:57:19.205: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 13 06:57:19.225: INFO: Waiting for pod pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:57:19.228: INFO: Pod pod-projected-secrets-952b314c-bd97-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:57:19.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1951" for this suite.
Aug 13 06:57:25.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:57:25.321: INFO: namespace projected-1951 deletion completed in 6.089363393s

â€¢ [SLOW TEST:10.170 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:57:25.321: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 13 06:57:25.359: INFO: Waiting up to 5m0s for pod "pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308" in namespace "emptydir-7017" to be "success or failure"
Aug 13 06:57:25.362: INFO: Pod "pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673327ms
Aug 13 06:57:27.366: INFO: Pod "pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006777945s
Aug 13 06:57:29.370: INFO: Pod "pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010784271s
STEP: Saw pod success
Aug 13 06:57:29.370: INFO: Pod "pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:57:29.373: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 06:57:29.392: INFO: Waiting for pod pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:57:29.395: INFO: Pod pod-9b3a7ebc-bd97-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:57:29.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7017" for this suite.
Aug 13 06:57:35.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:57:35.490: INFO: namespace emptydir-7017 deletion completed in 6.091412022s

â€¢ [SLOW TEST:10.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:57:35.491: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-a14b0d6a-bd97-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 06:57:35.540: INFO: Waiting up to 5m0s for pod "pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308" in namespace "configmap-5993" to be "success or failure"
Aug 13 06:57:35.543: INFO: Pod "pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856328ms
Aug 13 06:57:37.547: INFO: Pod "pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00630112s
Aug 13 06:57:39.551: INFO: Pod "pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010352506s
STEP: Saw pod success
Aug 13 06:57:39.551: INFO: Pod "pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 06:57:39.553: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 06:57:39.577: INFO: Waiting for pod pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308 to disappear
Aug 13 06:57:39.580: INFO: Pod pod-configmaps-a14bd7b9-bd97-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 06:57:39.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5993" for this suite.
Aug 13 06:57:45.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 06:57:45.679: INFO: namespace configmap-5993 deletion completed in 6.094821017s

â€¢ [SLOW TEST:10.188 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 06:57:45.679: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6681
Aug 13 06:57:49.722: INFO: Started pod liveness-exec in namespace container-probe-6681
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 06:57:49.728: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:01:50.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6681" for this suite.
Aug 13 07:01:56.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:01:56.376: INFO: namespace container-probe-6681 deletion completed in 6.091634774s

â€¢ [SLOW TEST:250.697 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:01:56.376: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-738
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Aug 13 07:01:56.456: INFO: Found 0 stateful pods, waiting for 3
Aug 13 07:02:06.460: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:02:06.460: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:02:06.460: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 13 07:02:16.460: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:02:16.460: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:02:16.460: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:02:16.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-738 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:02:16.637: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:02:16.637: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:02:16.637: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 13 07:02:26.672: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 13 07:02:36.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-738 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:02:36.849: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:02:36.849: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:02:36.849: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:02:46.868: INFO: Waiting for StatefulSet statefulset-738/ss2 to complete update
Aug 13 07:02:46.868: INFO: Waiting for Pod statefulset-738/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Aug 13 07:02:46.868: INFO: Waiting for Pod statefulset-738/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Aug 13 07:02:46.868: INFO: Waiting for Pod statefulset-738/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
Aug 13 07:02:56.876: INFO: Waiting for StatefulSet statefulset-738/ss2 to complete update
Aug 13 07:02:56.876: INFO: Waiting for Pod statefulset-738/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Aug 13 07:02:56.876: INFO: Waiting for Pod statefulset-738/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Aug 13 07:03:06.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-738 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:03:07.056: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:03:07.056: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:03:07.056: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 07:03:17.090: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 13 07:03:27.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-738 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:03:27.299: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:03:27.299: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:03:27.299: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:03:37.319: INFO: Waiting for StatefulSet statefulset-738/ss2 to complete update
Aug 13 07:03:37.319: INFO: Waiting for Pod statefulset-738/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Aug 13 07:03:37.319: INFO: Waiting for Pod statefulset-738/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Aug 13 07:03:37.319: INFO: Waiting for Pod statefulset-738/ss2-2 to have revision ss2-787997d666 update revision ss2-c79899b9
Aug 13 07:03:47.325: INFO: Waiting for StatefulSet statefulset-738/ss2 to complete update
Aug 13 07:03:47.325: INFO: Waiting for Pod statefulset-738/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
Aug 13 07:03:47.325: INFO: Waiting for Pod statefulset-738/ss2-1 to have revision ss2-787997d666 update revision ss2-c79899b9
Aug 13 07:03:57.325: INFO: Waiting for StatefulSet statefulset-738/ss2 to complete update
Aug 13 07:03:57.325: INFO: Waiting for Pod statefulset-738/ss2-0 to have revision ss2-787997d666 update revision ss2-c79899b9
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 13 07:04:07.325: INFO: Deleting all statefulset in ns statefulset-738
Aug 13 07:04:07.328: INFO: Scaling statefulset ss2 to 0
Aug 13 07:04:47.346: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 07:04:47.349: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:04:47.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-738" for this suite.
Aug 13 07:04:53.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:04:53.491: INFO: namespace statefulset-738 deletion completed in 6.126725032s

â€¢ [SLOW TEST:177.115 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:04:53.491: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:04:53.540: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:04:54.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5221" for this suite.
Aug 13 07:05:00.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:05:00.754: INFO: namespace custom-resource-definition-5221 deletion completed in 6.105491741s

â€¢ [SLOW TEST:7.263 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:05:00.754: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:05:00.803: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308" in namespace "projected-1212" to be "success or failure"
Aug 13 07:05:00.810: INFO: Pod "downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493456ms
Aug 13 07:05:02.814: INFO: Pod "downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010516121s
Aug 13 07:05:04.818: INFO: Pod "downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01432645s
STEP: Saw pod success
Aug 13 07:05:04.818: INFO: Pod "downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:05:04.820: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:05:04.845: INFO: Waiting for pod downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:05:04.849: INFO: Pod downwardapi-volume-aab11bdb-bd98-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:05:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1212" for this suite.
Aug 13 07:05:10.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:05:10.948: INFO: namespace projected-1212 deletion completed in 6.094546125s

â€¢ [SLOW TEST:10.194 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:05:10.948: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:05:15.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4833" for this suite.
Aug 13 07:06:01.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:06:01.129: INFO: namespace kubelet-test-4833 deletion completed in 46.105522057s

â€¢ [SLOW TEST:50.181 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:06:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-vgbq
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 07:06:01.177: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vgbq" in namespace "subpath-3250" to be "success or failure"
Aug 13 07:06:01.186: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221581ms
Aug 13 07:06:03.189: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011695613s
Aug 13 07:06:05.194: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 4.016218387s
Aug 13 07:06:07.198: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 6.020517893s
Aug 13 07:06:09.202: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 8.024655838s
Aug 13 07:06:11.205: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 10.027851969s
Aug 13 07:06:13.209: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 12.031456239s
Aug 13 07:06:15.213: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 14.035891966s
Aug 13 07:06:17.217: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 16.039487766s
Aug 13 07:06:19.221: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 18.043469938s
Aug 13 07:06:21.224: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 20.047016877s
Aug 13 07:06:23.228: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Running", Reason="", readiness=true. Elapsed: 22.050985238s
Aug 13 07:06:25.233: INFO: Pod "pod-subpath-test-secret-vgbq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.055213646s
STEP: Saw pod success
Aug 13 07:06:25.233: INFO: Pod "pod-subpath-test-secret-vgbq" satisfied condition "success or failure"
Aug 13 07:06:25.236: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-subpath-test-secret-vgbq container test-container-subpath-secret-vgbq: <nil>
STEP: delete the pod
Aug 13 07:06:25.256: INFO: Waiting for pod pod-subpath-test-secret-vgbq to disappear
Aug 13 07:06:25.259: INFO: Pod pod-subpath-test-secret-vgbq no longer exists
STEP: Deleting pod pod-subpath-test-secret-vgbq
Aug 13 07:06:25.259: INFO: Deleting pod "pod-subpath-test-secret-vgbq" in namespace "subpath-3250"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:06:25.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3250" for this suite.
Aug 13 07:06:31.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:06:31.358: INFO: namespace subpath-3250 deletion completed in 6.093602287s

â€¢ [SLOW TEST:30.229 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:06:31.358: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-9822
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 07:06:31.395: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 07:06:57.493: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.177:8080/dial?request=hostName&protocol=http&host=172.20.4.176&port=8080&tries=1'] Namespace:pod-network-test-9822 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:06:57.493: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:06:57.576: INFO: Waiting for endpoints: map[]
Aug 13 07:06:57.581: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.177:8080/dial?request=hostName&protocol=http&host=172.20.2.53&port=8080&tries=1'] Namespace:pod-network-test-9822 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:06:57.581: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:06:57.662: INFO: Waiting for endpoints: map[]
Aug 13 07:06:57.665: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.4.177:8080/dial?request=hostName&protocol=http&host=172.20.3.66&port=8080&tries=1'] Namespace:pod-network-test-9822 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:06:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:06:57.748: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:06:57.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9822" for this suite.
Aug 13 07:07:19.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:07:19.851: INFO: namespace pod-network-test-9822 deletion completed in 22.099102548s

â€¢ [SLOW TEST:48.493 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:07:19.851: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:07:19.891: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308" in namespace "projected-3997" to be "success or failure"
Aug 13 07:07:19.896: INFO: Pod "downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.326226ms
Aug 13 07:07:21.900: INFO: Pod "downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009353336s
Aug 13 07:07:23.905: INFO: Pod "downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014382986s
STEP: Saw pod success
Aug 13 07:07:23.906: INFO: Pod "downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:07:23.909: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:07:23.930: INFO: Waiting for pod downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:07:23.933: INFO: Pod downwardapi-volume-fd988697-bd98-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:07:23.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3997" for this suite.
Aug 13 07:07:29.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:07:30.038: INFO: namespace projected-3997 deletion completed in 6.102071548s

â€¢ [SLOW TEST:10.187 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:07:30.038: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 13 07:07:34.622: INFO: Successfully updated pod "pod-update-03addf01-bd99-11e9-8bf8-0a58ac140308"
STEP: verifying the updated pod is in kubernetes
Aug 13 07:07:34.629: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:07:34.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7194" for this suite.
Aug 13 07:07:56.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:07:56.732: INFO: namespace pods-7194 deletion completed in 22.099668691s

â€¢ [SLOW TEST:26.694 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:07:56.732: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-13941435-bd99-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 07:07:56.773: INFO: Waiting up to 5m0s for pod "pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308" in namespace "configmap-7747" to be "success or failure"
Aug 13 07:07:56.778: INFO: Pod "pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.349192ms
Aug 13 07:07:58.782: INFO: Pod "pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009210805s
Aug 13 07:08:00.786: INFO: Pod "pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012990936s
STEP: Saw pod success
Aug 13 07:08:00.786: INFO: Pod "pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:08:00.788: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 07:08:00.808: INFO: Waiting for pod pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:08:00.810: INFO: Pod pod-configmaps-1394a39a-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:08:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7747" for this suite.
Aug 13 07:08:06.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:08:06.909: INFO: namespace configmap-7747 deletion completed in 6.095002661s

â€¢ [SLOW TEST:10.177 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:08:06.909: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 13 07:08:06.947: INFO: Waiting up to 5m0s for pod "pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308" in namespace "emptydir-9112" to be "success or failure"
Aug 13 07:08:06.951: INFO: Pod "pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.667349ms
Aug 13 07:08:08.955: INFO: Pod "pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007656119s
Aug 13 07:08:10.959: INFO: Pod "pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011885992s
STEP: Saw pod success
Aug 13 07:08:10.959: INFO: Pod "pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:08:10.963: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:08:10.984: INFO: Waiting for pod pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:08:10.988: INFO: Pod pod-19a4e2cd-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:08:10.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9112" for this suite.
Aug 13 07:08:17.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:08:17.093: INFO: namespace emptydir-9112 deletion completed in 6.100922584s

â€¢ [SLOW TEST:10.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:08:17.094: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Aug 13 07:08:17.648: INFO: created pod pod-service-account-defaultsa
Aug 13 07:08:17.648: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 13 07:08:17.657: INFO: created pod pod-service-account-mountsa
Aug 13 07:08:17.657: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 13 07:08:17.667: INFO: created pod pod-service-account-nomountsa
Aug 13 07:08:17.667: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 13 07:08:17.672: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 13 07:08:17.674: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 13 07:08:17.686: INFO: created pod pod-service-account-mountsa-mountspec
Aug 13 07:08:17.686: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 13 07:08:17.697: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 13 07:08:17.697: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 13 07:08:17.715: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 13 07:08:17.715: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 13 07:08:17.723: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 13 07:08:17.723: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 13 07:08:17.735: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 13 07:08:17.735: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:08:17.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9106" for this suite.
Aug 13 07:08:39.779: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:08:39.877: INFO: namespace svcaccounts-9106 deletion completed in 22.129839499s

â€¢ [SLOW TEST:22.783 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:08:39.877: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:08:39.949: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308" in namespace "projected-8901" to be "success or failure"
Aug 13 07:08:39.962: INFO: Pod "downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 12.754505ms
Aug 13 07:08:41.967: INFO: Pod "downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01715701s
Aug 13 07:08:43.971: INFO: Pod "downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021298449s
STEP: Saw pod success
Aug 13 07:08:43.971: INFO: Pod "downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:08:43.974: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:08:44.013: INFO: Waiting for pod downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:08:44.021: INFO: Pod downwardapi-volume-2d4b857f-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:08:44.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8901" for this suite.
Aug 13 07:08:50.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:08:50.134: INFO: namespace projected-8901 deletion completed in 6.108307251s

â€¢ [SLOW TEST:10.257 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:08:50.134: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 13 07:08:50.181: INFO: Waiting up to 5m0s for pod "pod-336a0768-bd99-11e9-8bf8-0a58ac140308" in namespace "emptydir-3146" to be "success or failure"
Aug 13 07:08:50.187: INFO: Pod "pod-336a0768-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.932877ms
Aug 13 07:08:52.191: INFO: Pod "pod-336a0768-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009892222s
Aug 13 07:08:54.195: INFO: Pod "pod-336a0768-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014102858s
STEP: Saw pod success
Aug 13 07:08:54.195: INFO: Pod "pod-336a0768-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:08:54.198: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-336a0768-bd99-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:08:54.220: INFO: Waiting for pod pod-336a0768-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:08:54.225: INFO: Pod pod-336a0768-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:08:54.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3146" for this suite.
Aug 13 07:09:00.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:09:00.323: INFO: namespace emptydir-3146 deletion completed in 6.093770228s

â€¢ [SLOW TEST:10.189 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:09:00.323: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-8s9s
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 07:09:00.371: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-8s9s" in namespace "subpath-6396" to be "success or failure"
Aug 13 07:09:00.377: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Pending", Reason="", readiness=false. Elapsed: 5.806043ms
Aug 13 07:09:02.382: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01009084s
Aug 13 07:09:04.387: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 4.015335601s
Aug 13 07:09:06.390: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 6.018698234s
Aug 13 07:09:08.394: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 8.022641833s
Aug 13 07:09:10.399: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 10.027213589s
Aug 13 07:09:12.403: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 12.031481037s
Aug 13 07:09:14.407: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 14.035085362s
Aug 13 07:09:16.410: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 16.038759353s
Aug 13 07:09:18.414: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 18.042806407s
Aug 13 07:09:20.418: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 20.046804157s
Aug 13 07:09:22.422: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Running", Reason="", readiness=true. Elapsed: 22.050594871s
Aug 13 07:09:24.426: INFO: Pod "pod-subpath-test-downwardapi-8s9s": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054563528s
STEP: Saw pod success
Aug 13 07:09:24.426: INFO: Pod "pod-subpath-test-downwardapi-8s9s" satisfied condition "success or failure"
Aug 13 07:09:24.429: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-subpath-test-downwardapi-8s9s container test-container-subpath-downwardapi-8s9s: <nil>
STEP: delete the pod
Aug 13 07:09:24.446: INFO: Waiting for pod pod-subpath-test-downwardapi-8s9s to disappear
Aug 13 07:09:24.449: INFO: Pod pod-subpath-test-downwardapi-8s9s no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-8s9s
Aug 13 07:09:24.449: INFO: Deleting pod "pod-subpath-test-downwardapi-8s9s" in namespace "subpath-6396"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:09:24.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6396" for this suite.
Aug 13 07:09:30.467: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:09:30.551: INFO: namespace subpath-6396 deletion completed in 6.095053492s

â€¢ [SLOW TEST:30.228 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:09:30.551: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 13 07:09:30.602: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20248,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 13 07:09:30.602: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20249,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 13 07:09:30.602: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20250,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 13 07:09:40.634: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20272,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 13 07:09:40.634: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20273,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 13 07:09:40.635: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9712,SelfLink:/api/v1/namespaces/watch-9712/configmaps/e2e-watch-test-label-changed,UID:4b7fc29b-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20274,Generation:0,CreationTimestamp:2019-08-13 07:09:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:09:40.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9712" for this suite.
Aug 13 07:09:46.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:09:46.728: INFO: namespace watch-9712 deletion completed in 6.088859756s

â€¢ [SLOW TEST:16.176 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:09:46.728: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:09:50.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8474" for this suite.
Aug 13 07:09:56.791: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:09:56.888: INFO: namespace kubelet-test-8474 deletion completed in 6.110421695s

â€¢ [SLOW TEST:10.161 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:09:56.889: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:09:56.919: INFO: Creating deployment "test-recreate-deployment"
Aug 13 07:09:56.925: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 13 07:09:56.933: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 13 07:09:58.940: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 13 07:09:58.944: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276996, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276996, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276996, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701276996, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d57d5ff7c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 07:10:00.947: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 13 07:10:00.954: INFO: Updating deployment test-recreate-deployment
Aug 13 07:10:00.954: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 13 07:10:01.049: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-6116,SelfLink:/apis/apps/v1/namespaces/deployment-6116/deployments/test-recreate-deployment,UID:5b3289e8-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20383,Generation:2,CreationTimestamp:2019-08-13 07:09:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-13 07:10:01 +0000 UTC 2019-08-13 07:10:01 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-13 07:10:01 +0000 UTC 2019-08-13 07:09:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 13 07:10:01.052: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-6116,SelfLink:/apis/apps/v1/namespaces/deployment-6116/replicasets/test-recreate-deployment-c9cbd8684,UID:5da05eb6-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20380,Generation:1,CreationTimestamp:2019-08-13 07:10:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5b3289e8-bd99-11e9-868a-506b8de0bf77 0xc002fcd570 0xc002fcd571}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 07:10:01.053: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 13 07:10:01.053: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-6116,SelfLink:/apis/apps/v1/namespaces/deployment-6116/replicasets/test-recreate-deployment-7d57d5ff7c,UID:5b336349-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20371,Generation:2,CreationTimestamp:2019-08-13 07:09:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 5b3289e8-bd99-11e9-868a-506b8de0bf77 0xc002fcd497 0xc002fcd498}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 07:10:01.055: INFO: Pod "test-recreate-deployment-c9cbd8684-vhh4h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-vhh4h,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-6116,SelfLink:/api/v1/namespaces/deployment-6116/pods/test-recreate-deployment-c9cbd8684-vhh4h,UID:5da11715-bd99-11e9-868a-506b8de0bf77,ResourceVersion:20382,Generation:0,CreationTimestamp:2019-08-13 07:10:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 5da05eb6-bd99-11e9-868a-506b8de0bf77 0xc002fcddf0 0xc002fcddf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-dgfw2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-dgfw2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-dgfw2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:10:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:10:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:10:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:10:01 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:,StartTime:2019-08-13 07:10:01 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:10:01.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6116" for this suite.
Aug 13 07:10:07.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:10:07.161: INFO: namespace deployment-6116 deletion completed in 6.102557258s

â€¢ [SLOW TEST:10.273 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:10:07.162: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 13 07:10:13.234: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.234: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.315: INFO: Exec stderr: ""
Aug 13 07:10:13.316: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.316: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.398: INFO: Exec stderr: ""
Aug 13 07:10:13.398: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.398: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.486: INFO: Exec stderr: ""
Aug 13 07:10:13.486: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.486: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.565: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 13 07:10:13.565: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.565: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.648: INFO: Exec stderr: ""
Aug 13 07:10:13.648: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.648: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.726: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 13 07:10:13.726: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.726: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.806: INFO: Exec stderr: ""
Aug 13 07:10:13.806: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.806: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.884: INFO: Exec stderr: ""
Aug 13 07:10:13.884: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.884: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:13.964: INFO: Exec stderr: ""
Aug 13 07:10:13.964: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8803 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:10:13.964: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:10:14.044: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:10:14.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8803" for this suite.
Aug 13 07:10:52.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:10:52.151: INFO: namespace e2e-kubelet-etc-hosts-8803 deletion completed in 38.10191867s

â€¢ [SLOW TEST:44.989 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:10:52.151: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-7c2421b8-bd99-11e9-8bf8-0a58ac140308
STEP: Creating configMap with name cm-test-opt-upd-7c24220b-bd99-11e9-8bf8-0a58ac140308
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7c2421b8-bd99-11e9-8bf8-0a58ac140308
STEP: Updating configmap cm-test-opt-upd-7c24220b-bd99-11e9-8bf8-0a58ac140308
STEP: Creating configMap with name cm-test-opt-create-7c24222e-bd99-11e9-8bf8-0a58ac140308
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:10:58.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8021" for this suite.
Aug 13 07:11:20.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:11:20.416: INFO: namespace projected-8021 deletion completed in 22.09246081s

â€¢ [SLOW TEST:28.265 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:11:20.416: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:11:20.445: INFO: Creating ReplicaSet my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308
Aug 13 07:11:20.456: INFO: Pod name my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308: Found 0 pods out of 1
Aug 13 07:11:25.460: INFO: Pod name my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308: Found 1 pods out of 1
Aug 13 07:11:25.460: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308" is running
Aug 13 07:11:25.463: INFO: Pod "my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308-js4qt" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 07:11:20 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 07:11:23 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 07:11:23 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-13 07:11:20 +0000 UTC Reason: Message:}])
Aug 13 07:11:25.463: INFO: Trying to dial the pod
Aug 13 07:11:30.475: INFO: Controller my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308: Got expected result from replica 1 [my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308-js4qt]: "my-hostname-basic-8cfb9839-bd99-11e9-8bf8-0a58ac140308-js4qt", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:11:30.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-9304" for this suite.
Aug 13 07:11:36.499: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:11:36.595: INFO: namespace replicaset-9304 deletion completed in 6.112887469s

â€¢ [SLOW TEST:16.179 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:11:36.596: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-8187
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 07:11:36.628: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 07:12:00.713: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.3.68 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8187 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:12:00.713: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:12:01.804: INFO: Found all expected endpoints: [netserver-0]
Aug 13 07:12:01.811: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.2.57 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8187 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:12:01.811: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:12:02.888: INFO: Found all expected endpoints: [netserver-1]
Aug 13 07:12:02.893: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.4.196 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8187 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:12:02.893: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:12:03.981: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:12:03.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8187" for this suite.
Aug 13 07:12:26.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:12:26.099: INFO: namespace pod-network-test-8187 deletion completed in 22.112647402s

â€¢ [SLOW TEST:49.504 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:12:26.100: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 13 07:12:26.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-8900'
Aug 13 07:12:26.542: INFO: stderr: ""
Aug 13 07:12:26.542: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 07:12:27.546: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:12:27.546: INFO: Found 0 / 1
Aug 13 07:12:28.546: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:12:28.546: INFO: Found 0 / 1
Aug 13 07:12:29.545: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:12:29.545: INFO: Found 1 / 1
Aug 13 07:12:29.545: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 13 07:12:29.549: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:12:29.549: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 07:12:29.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 patch pod redis-master-svzg7 --namespace=kubectl-8900 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 13 07:12:29.645: INFO: stderr: ""
Aug 13 07:12:29.645: INFO: stdout: "pod/redis-master-svzg7 patched\n"
STEP: checking annotations
Aug 13 07:12:29.648: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:12:29.648: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:12:29.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8900" for this suite.
Aug 13 07:12:51.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:12:51.751: INFO: namespace kubectl-8900 deletion completed in 22.098945664s

â€¢ [SLOW TEST:25.651 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:12:51.751: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-c36dabf4-bd99-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:12:51.844: INFO: Waiting up to 5m0s for pod "pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308" in namespace "secrets-6418" to be "success or failure"
Aug 13 07:12:51.849: INFO: Pod "pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.272696ms
Aug 13 07:12:53.852: INFO: Pod "pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007790911s
Aug 13 07:12:55.856: INFO: Pod "pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011621877s
STEP: Saw pod success
Aug 13 07:12:55.856: INFO: Pod "pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:12:55.859: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:12:55.879: INFO: Waiting for pod pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:12:55.881: INFO: Pod pod-secrets-c374e175-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:12:55.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6418" for this suite.
Aug 13 07:13:01.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:02.008: INFO: namespace secrets-6418 deletion completed in 6.122050667s
STEP: Destroying namespace "secret-namespace-6790" for this suite.
Aug 13 07:13:08.026: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:08.105: INFO: namespace secret-namespace-6790 deletion completed in 6.096633124s

â€¢ [SLOW TEST:16.354 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:13:08.105: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:13:08.144: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308" in namespace "downward-api-6315" to be "success or failure"
Aug 13 07:13:08.147: INFO: Pod "downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.925499ms
Aug 13 07:13:10.151: INFO: Pod "downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006733067s
Aug 13 07:13:12.155: INFO: Pod "downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010468756s
STEP: Saw pod success
Aug 13 07:13:12.155: INFO: Pod "downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:13:12.157: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:13:12.177: INFO: Waiting for pod downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:13:12.179: INFO: Pod downwardapi-volume-cd2c2713-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:13:12.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6315" for this suite.
Aug 13 07:13:18.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:18.286: INFO: namespace downward-api-6315 deletion completed in 6.10273547s

â€¢ [SLOW TEST:10.181 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:13:18.286: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:13:18.327: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308" in namespace "downward-api-4134" to be "success or failure"
Aug 13 07:13:18.331: INFO: Pod "downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.689533ms
Aug 13 07:13:20.335: INFO: Pod "downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007767594s
Aug 13 07:13:22.339: INFO: Pod "downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011593248s
STEP: Saw pod success
Aug 13 07:13:22.339: INFO: Pod "downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:13:22.342: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:13:22.361: INFO: Waiting for pod downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:13:22.364: INFO: Pod downwardapi-volume-d33dc260-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:13:22.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4134" for this suite.
Aug 13 07:13:28.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:28.463: INFO: namespace downward-api-4134 deletion completed in 6.096053959s

â€¢ [SLOW TEST:10.177 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:13:28.463: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 13 07:13:28.553: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 13 07:13:33.557: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:13:34.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3171" for this suite.
Aug 13 07:13:40.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:40.681: INFO: namespace replication-controller-3171 deletion completed in 6.100381395s

â€¢ [SLOW TEST:12.217 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:13:40.681: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 13 07:13:40.723: INFO: Waiting up to 5m0s for pod "pod-e0969585-bd99-11e9-8bf8-0a58ac140308" in namespace "emptydir-5548" to be "success or failure"
Aug 13 07:13:40.728: INFO: Pod "pod-e0969585-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.153286ms
Aug 13 07:13:42.733: INFO: Pod "pod-e0969585-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00962856s
Aug 13 07:13:44.737: INFO: Pod "pod-e0969585-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014433099s
STEP: Saw pod success
Aug 13 07:13:44.738: INFO: Pod "pod-e0969585-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:13:44.741: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-e0969585-bd99-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:13:44.765: INFO: Waiting for pod pod-e0969585-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:13:44.768: INFO: Pod pod-e0969585-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:13:44.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5548" for this suite.
Aug 13 07:13:50.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:13:50.882: INFO: namespace emptydir-5548 deletion completed in 6.109776392s

â€¢ [SLOW TEST:10.201 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:13:50.882: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 13 07:13:50.931: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 07:13:50.940: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 07:13:50.942: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-0 before test
Aug 13 07:13:50.950: INFO: prometheus-operator-694bbc8678-f5zfp from ntnx-system started at 2019-08-13 05:43:07 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 13 07:13:50.950: INFO: kubernetes-events-printer-6494d69c4b-p55sc from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 13 07:13:50.950: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 07:13:50.950: INFO: fluent-bit-78krq from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:13:50.950: INFO: kube-proxy-ds-8swrw from kube-system started at 2019-08-13 05:39:38 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:13:50.950: INFO: csi-node-ntnx-plugin-6f274 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 07:13:50.950: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-13 05:44:30 +0000 UTC (3 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 07:13:50.950: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 07:13:50.950: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-13 06:04:31 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 13 07:13:50.950: INFO: sonobuoy-e2e-job-5d1df1b5d76149a9 from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container e2e ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 07:13:50.950: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-hmngc from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:13:50.950: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:13:50.950: INFO: kube-flannel-ds-gv2mw from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:13:50.950: INFO: node-exporter-g2xn7 from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.950: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 07:13:50.950: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-1 before test
Aug 13 07:13:50.957: INFO: kube-proxy-ds-2tbd6 from kube-system started at 2019-08-13 05:39:37 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:13:50.957: INFO: csi-node-ntnx-plugin-jqc6g from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 07:13:50.957: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-13 05:43:25 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 07:13:50.957: INFO: fluent-bit-zzqvx from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:13:50.957: INFO: kibana-logging-68b6fc7cb6-hc5bk from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 13 07:13:50.957: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-13 05:43:45 +0000 UTC (3 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 07:13:50.957: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 07:13:50.957: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-x8sg2 from heptio-sonobuoy started at 2019-08-13 06:04:37 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:13:50.957: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:13:50.957: INFO: kube-flannel-ds-cfprh from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:13:50.957: INFO: node-exporter-rpqwn from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 07:13:50.957: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-13 05:41:30 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.957: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 13 07:13:50.957: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-2 before test
Aug 13 07:13:50.965: INFO: kube-flannel-ds-dt86l from kube-system started at 2019-08-13 05:39:43 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:13:50.965: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-13 05:43:17 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 07:13:50.965: INFO: kube-state-metrics-6bf48db96d-k6wlz from ntnx-system started at 2019-08-13 05:43:19 +0000 UTC (4 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 13 07:13:50.965: INFO: kube-proxy-ds-thn6s from kube-system started at 2019-08-13 05:39:39 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:13:50.965: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-h884r from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:13:50.965: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:13:50.965: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 07:13:50.965: INFO: fluent-bit-4bnfm from ntnx-system started at 2019-08-13 05:40:30 +0000 UTC (1 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:13:50.965: INFO: node-exporter-46562 from ntnx-system started at 2019-08-13 05:43:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 07:13:50.965: INFO: csi-node-ntnx-plugin-ncz7w from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 07:13:50.965: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:13:50.965: INFO: 	Container driver-registrar ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node karbon-multi-fourteen-aca02f-k8s-worker-0
STEP: verifying the node has the label node karbon-multi-fourteen-aca02f-k8s-worker-1
STEP: verifying the node has the label node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod sonobuoy requesting resource cpu=0m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod sonobuoy-e2e-job-5d1df1b5d76149a9 requesting resource cpu=0m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-h884r requesting resource cpu=0m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-hmngc requesting resource cpu=0m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-x8sg2 requesting resource cpu=0m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod kube-flannel-ds-cfprh requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod kube-flannel-ds-dt86l requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod kube-flannel-ds-gv2mw requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod kube-proxy-ds-2tbd6 requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod kube-proxy-ds-8swrw requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod kube-proxy-ds-thn6s requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod alertmanager-main-0 requesting resource cpu=105m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod alertmanager-main-1 requesting resource cpu=105m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod csi-attacher-ntnx-plugin-0 requesting resource cpu=200m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod csi-node-ntnx-plugin-6f274 requesting resource cpu=200m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod csi-node-ntnx-plugin-jqc6g requesting resource cpu=200m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod csi-node-ntnx-plugin-ncz7w requesting resource cpu=200m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod csi-provisioner-ntnx-plugin-0 requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod elasticsearch-logging-0 requesting resource cpu=500m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod fluent-bit-4bnfm requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod fluent-bit-78krq requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod fluent-bit-zzqvx requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod kibana-logging-68b6fc7cb6-hc5bk requesting resource cpu=200m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod kube-state-metrics-6bf48db96d-k6wlz requesting resource cpu=160m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod kubernetes-events-printer-6494d69c4b-p55sc requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod node-exporter-46562 requesting resource cpu=112m on Node karbon-multi-fourteen-aca02f-k8s-worker-2
Aug 13 07:13:51.036: INFO: Pod node-exporter-g2xn7 requesting resource cpu=112m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod node-exporter-rpqwn requesting resource cpu=112m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod prometheus-k8s-0 requesting resource cpu=215m on Node karbon-multi-fourteen-aca02f-k8s-worker-1
Aug 13 07:13:51.036: INFO: Pod prometheus-k8s-1 requesting resource cpu=215m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
Aug 13 07:13:51.036: INFO: Pod prometheus-operator-694bbc8678-f5zfp requesting resource cpu=100m on Node karbon-multi-fourteen-aca02f-k8s-worker-0
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308.15ba6a0e849c8db6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7907/filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308 to karbon-multi-fourteen-aca02f-k8s-worker-0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308.15ba6a0f1a2b9746], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308.15ba6a0f1c6d43e3], Reason = [Created], Message = [Created container filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308.15ba6a0f2492947e], Reason = [Started], Message = [Started container filler-pod-e6bdf282-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308.15ba6a0e8547b5d7], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7907/filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308 to karbon-multi-fourteen-aca02f-k8s-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308.15ba6a0f0b3451c5], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308.15ba6a0f0d7ff0f0], Reason = [Created], Message = [Created container filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308.15ba6a0f1532895b], Reason = [Started], Message = [Started container filler-pod-e6bf8dc4-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308.15ba6a0e86081b8b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7907/filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308 to karbon-multi-fourteen-aca02f-k8s-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308.15ba6a0f0d948634], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308.15ba6a0f100d4c5b], Reason = [Created], Message = [Created container filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308.15ba6a0f17e0844e], Reason = [Started], Message = [Started container filler-pod-e6c1085f-bd99-11e9-8bf8-0a58ac140308]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ba6a0f75cb39f5], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu.]
STEP: removing the label node off the node karbon-multi-fourteen-aca02f-k8s-worker-0
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-multi-fourteen-aca02f-k8s-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node karbon-multi-fourteen-aca02f-k8s-worker-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:13:56.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7907" for this suite.
Aug 13 07:14:02.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:14:02.253: INFO: namespace sched-pred-7907 deletion completed in 6.104352047s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:11.371 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:14:02.253: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 13 07:14:02.304: INFO: Waiting up to 5m0s for pod "pod-ed741d61-bd99-11e9-8bf8-0a58ac140308" in namespace "emptydir-762" to be "success or failure"
Aug 13 07:14:02.307: INFO: Pod "pod-ed741d61-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.385847ms
Aug 13 07:14:04.311: INFO: Pod "pod-ed741d61-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00754345s
Aug 13 07:14:06.316: INFO: Pod "pod-ed741d61-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01224541s
STEP: Saw pod success
Aug 13 07:14:06.316: INFO: Pod "pod-ed741d61-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:14:06.318: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-ed741d61-bd99-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:14:06.351: INFO: Waiting for pod pod-ed741d61-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:14:06.355: INFO: Pod pod-ed741d61-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:14:06.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-762" for this suite.
Aug 13 07:14:12.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:14:12.459: INFO: namespace emptydir-762 deletion completed in 6.098216786s

â€¢ [SLOW TEST:10.205 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:14:12.459: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:14:12.504: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308" in namespace "downward-api-1556" to be "success or failure"
Aug 13 07:14:12.516: INFO: Pod "downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 11.392511ms
Aug 13 07:14:14.519: INFO: Pod "downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015216776s
Aug 13 07:14:16.524: INFO: Pod "downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019645167s
STEP: Saw pod success
Aug 13 07:14:16.524: INFO: Pod "downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:14:16.527: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:14:16.548: INFO: Waiting for pod downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:14:16.551: INFO: Pod downwardapi-volume-f3881c19-bd99-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:14:16.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1556" for this suite.
Aug 13 07:14:22.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:14:22.655: INFO: namespace downward-api-1556 deletion completed in 6.100123336s

â€¢ [SLOW TEST:10.196 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:14:22.656: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Aug 13 07:14:22.684: INFO: namespace kubectl-5898
Aug 13 07:14:22.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-5898'
Aug 13 07:14:22.949: INFO: stderr: ""
Aug 13 07:14:22.949: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 07:14:23.953: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:14:23.953: INFO: Found 1 / 1
Aug 13 07:14:23.953: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 07:14:23.955: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:14:23.955: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 07:14:23.955: INFO: wait on redis-master startup in kubectl-5898 
Aug 13 07:14:23.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 logs redis-master-7gsxd redis-master --namespace=kubectl-5898'
Aug 13 07:14:24.060: INFO: stderr: ""
Aug 13 07:14:24.060: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Aug 07:14:23.698 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Aug 07:14:23.698 # Server started, Redis version 3.2.12\n1:M 13 Aug 07:14:23.698 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Aug 07:14:23.698 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 13 07:14:24.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5898'
Aug 13 07:14:24.183: INFO: stderr: ""
Aug 13 07:14:24.183: INFO: stdout: "service/rm2 exposed\n"
Aug 13 07:14:24.187: INFO: Service rm2 in namespace kubectl-5898 found.
STEP: exposing service
Aug 13 07:14:26.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5898'
Aug 13 07:14:26.297: INFO: stderr: ""
Aug 13 07:14:26.298: INFO: stdout: "service/rm3 exposed\n"
Aug 13 07:14:26.306: INFO: Service rm3 in namespace kubectl-5898 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:14:28.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5898" for this suite.
Aug 13 07:14:50.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:14:50.415: INFO: namespace kubectl-5898 deletion completed in 22.099048982s

â€¢ [SLOW TEST:27.760 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:14:50.416: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 118.227.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.227.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.227.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.227.118_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3763.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3763.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3763.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3763.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3763.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 118.227.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.227.118_udp@PTR;check="$$(dig +tcp +noall +answer +search 118.227.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.227.118_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 07:15:04.579: INFO: DNS probes using dns-3763/dns-test-0a2d4a9b-bd9a-11e9-8bf8-0a58ac140308 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:15:04.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3763" for this suite.
Aug 13 07:15:10.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:15:10.749: INFO: namespace dns-3763 deletion completed in 6.096755356s

â€¢ [SLOW TEST:20.333 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:15:10.749: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-1645722d-bd9a-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:15:10.788: INFO: Waiting up to 5m0s for pod "pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308" in namespace "secrets-645" to be "success or failure"
Aug 13 07:15:10.792: INFO: Pod "pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.261502ms
Aug 13 07:15:12.796: INFO: Pod "pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007651202s
Aug 13 07:15:14.800: INFO: Pod "pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011889575s
STEP: Saw pod success
Aug 13 07:15:14.800: INFO: Pod "pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:15:14.803: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:15:14.825: INFO: Waiting for pod pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:15:14.828: INFO: Pod pod-secrets-1645fec0-bd9a-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:15:14.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-645" for this suite.
Aug 13 07:15:20.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:15:20.930: INFO: namespace secrets-645 deletion completed in 6.09910021s

â€¢ [SLOW TEST:10.182 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:15:20.931: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Aug 13 07:15:20.970: INFO: Waiting up to 5m0s for pod "downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308" in namespace "downward-api-2481" to be "success or failure"
Aug 13 07:15:20.973: INFO: Pod "downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.078143ms
Aug 13 07:15:22.977: INFO: Pod "downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007023112s
Aug 13 07:15:24.981: INFO: Pod "downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011321209s
STEP: Saw pod success
Aug 13 07:15:24.981: INFO: Pod "downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:15:24.984: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308 container dapi-container: <nil>
STEP: delete the pod
Aug 13 07:15:25.014: INFO: Waiting for pod downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:15:25.017: INFO: Pod downward-api-1c57e081-bd9a-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:15:25.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2481" for this suite.
Aug 13 07:15:31.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:15:31.136: INFO: namespace downward-api-2481 deletion completed in 6.11414191s

â€¢ [SLOW TEST:10.206 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:15:31.137: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 07:15:31.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-3487'
Aug 13 07:15:31.285: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 07:15:31.285: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Aug 13 07:15:33.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3487'
Aug 13 07:15:33.396: INFO: stderr: ""
Aug 13 07:15:33.396: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:15:33.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3487" for this suite.
Aug 13 07:17:35.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:17:35.500: INFO: namespace kubectl-3487 deletion completed in 2m2.09970056s

â€¢ [SLOW TEST:124.363 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:17:35.500: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Aug 13 07:17:35.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 cluster-info'
Aug 13 07:17:35.622: INFO: stderr: ""
Aug 13 07:17:35.622: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:17:35.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9683" for this suite.
Aug 13 07:17:41.643: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:17:41.721: INFO: namespace kubectl-9683 deletion completed in 6.093040874s

â€¢ [SLOW TEST:6.221 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:17:41.721: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:17:45.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3956" for this suite.
Aug 13 07:18:39.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:18:39.885: INFO: namespace kubelet-test-3956 deletion completed in 54.092756265s

â€¢ [SLOW TEST:58.164 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:18:39.885: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Aug 13 07:18:39.913: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 13 07:18:39.923: INFO: Waiting for terminating namespaces to be deleted...
Aug 13 07:18:39.926: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-0 before test
Aug 13 07:18:39.936: INFO: kube-proxy-ds-8swrw from kube-system started at 2019-08-13 05:39:38 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:18:39.938: INFO: csi-node-ntnx-plugin-6f274 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 07:18:39.938: INFO: kube-flannel-ds-gv2mw from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:18:39.938: INFO: node-exporter-g2xn7 from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 07:18:39.938: INFO: prometheus-k8s-1 from ntnx-system started at 2019-08-13 05:44:30 +0000 UTC (3 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 07:18:39.938: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 07:18:39.938: INFO: sonobuoy from heptio-sonobuoy started at 2019-08-13 06:04:31 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 13 07:18:39.938: INFO: sonobuoy-e2e-job-5d1df1b5d76149a9 from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container e2e ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 13 07:18:39.938: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-hmngc from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:18:39.938: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:18:39.938: INFO: prometheus-operator-694bbc8678-f5zfp from ntnx-system started at 2019-08-13 05:43:07 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 13 07:18:39.938: INFO: csi-attacher-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container csi-attacher ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 07:18:39.938: INFO: fluent-bit-78krq from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:18:39.938: INFO: kubernetes-events-printer-6494d69c4b-p55sc from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.938: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 13 07:18:39.938: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-1 before test
Aug 13 07:18:39.945: INFO: elasticsearch-logging-0 from ntnx-system started at 2019-08-13 05:41:30 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container elasticsearch-logging ready: true, restart count 0
Aug 13 07:18:39.945: INFO: kube-proxy-ds-2tbd6 from kube-system started at 2019-08-13 05:39:37 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:18:39.945: INFO: csi-node-ntnx-plugin-jqc6g from ntnx-system started at 2019-08-13 05:40:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:18:39.945: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 07:18:39.945: INFO: fluent-bit-zzqvx from ntnx-system started at 2019-08-13 05:40:28 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:18:39.945: INFO: kibana-logging-68b6fc7cb6-hc5bk from ntnx-system started at 2019-08-13 05:40:29 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container kibana-logging ready: true, restart count 0
Aug 13 07:18:39.945: INFO: 	Container nginxhttp ready: true, restart count 0
Aug 13 07:18:39.945: INFO: alertmanager-main-1 from ntnx-system started at 2019-08-13 05:43:25 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 07:18:39.945: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 07:18:39.945: INFO: kube-flannel-ds-cfprh from kube-system started at 2019-08-13 05:39:42 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:18:39.945: INFO: node-exporter-rpqwn from ntnx-system started at 2019-08-13 05:43:06 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:18:39.945: INFO: 	Container node-exporter ready: true, restart count 0
Aug 13 07:18:39.945: INFO: prometheus-k8s-0 from ntnx-system started at 2019-08-13 05:43:45 +0000 UTC (3 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container prometheus ready: true, restart count 1
Aug 13 07:18:39.945: INFO: 	Container prometheus-config-reloader ready: true, restart count 0
Aug 13 07:18:39.945: INFO: 	Container rules-configmap-reloader ready: true, restart count 0
Aug 13 07:18:39.945: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-x8sg2 from heptio-sonobuoy started at 2019-08-13 06:04:37 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.945: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:18:39.945: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:18:39.945: INFO: 
Logging pods the kubelet thinks is on node karbon-multi-fourteen-aca02f-k8s-worker-2 before test
Aug 13 07:18:39.953: INFO: csi-node-ntnx-plugin-ncz7w from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container csi-node-ntnx-plugin ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 13 07:18:39.953: INFO: kube-flannel-ds-dt86l from kube-system started at 2019-08-13 05:39:43 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container kube-flannel ready: true, restart count 0
Aug 13 07:18:39.953: INFO: alertmanager-main-0 from ntnx-system started at 2019-08-13 05:43:17 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container alertmanager ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container config-reloader ready: true, restart count 0
Aug 13 07:18:39.953: INFO: kube-state-metrics-6bf48db96d-k6wlz from ntnx-system started at 2019-08-13 05:43:19 +0000 UTC (4 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container addon-resizer ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 13 07:18:39.953: INFO: kube-proxy-ds-thn6s from kube-system started at 2019-08-13 05:39:39 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 13 07:18:39.953: INFO: sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-h884r from heptio-sonobuoy started at 2019-08-13 06:04:39 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Aug 13 07:18:39.953: INFO: 	Container systemd-logs ready: true, restart count 1
Aug 13 07:18:39.953: INFO: csi-provisioner-ntnx-plugin-0 from ntnx-system started at 2019-08-13 05:40:09 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container ntnx-csi-plugin ready: true, restart count 0
Aug 13 07:18:39.953: INFO: fluent-bit-4bnfm from ntnx-system started at 2019-08-13 05:40:30 +0000 UTC (1 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 13 07:18:39.953: INFO: node-exporter-46562 from ntnx-system started at 2019-08-13 05:43:08 +0000 UTC (2 container statuses recorded)
Aug 13 07:18:39.953: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 13 07:18:39.953: INFO: 	Container node-exporter ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9559ecfd-bd9a-11e9-8bf8-0a58ac140308 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9559ecfd-bd9a-11e9-8bf8-0a58ac140308 off the node karbon-multi-fourteen-aca02f-k8s-worker-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9559ecfd-bd9a-11e9-8bf8-0a58ac140308
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:18:48.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1708" for this suite.
Aug 13 07:19:00.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:19:00.138: INFO: namespace sched-pred-1708 deletion completed in 12.097039628s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:20.253 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:19:00.138: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-t9ff6 in namespace proxy-8495
I0813 07:19:00.195988      18 runners.go:184] Created replication controller with name: proxy-service-t9ff6, namespace: proxy-8495, replica count: 1
I0813 07:19:01.246558      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 07:19:02.246797      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 07:19:03.246987      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 07:19:04.247186      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0813 07:19:05.247374      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 07:19:06.247599      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 07:19:07.247769      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 07:19:08.247985      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 07:19:09.248204      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0813 07:19:10.248461      18 runners.go:184] proxy-service-t9ff6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 13 07:19:10.252: INFO: setup took 10.083105737s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 13 07:19:10.260: INFO: (0) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 7.299697ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 13.333252ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 13.569107ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 14.009515ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 12.951553ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 13.718611ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 13.171952ms)
Aug 13 07:19:10.266: INFO: (0) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 13.95684ms)
Aug 13 07:19:10.269: INFO: (0) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 16.951189ms)
Aug 13 07:19:10.270: INFO: (0) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 17.52312ms)
Aug 13 07:19:10.270: INFO: (0) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 18.024352ms)
Aug 13 07:19:10.271: INFO: (0) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 17.760777ms)
Aug 13 07:19:10.271: INFO: (0) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 19.424855ms)
Aug 13 07:19:10.271: INFO: (0) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 17.877877ms)
Aug 13 07:19:10.271: INFO: (0) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 18.581939ms)
Aug 13 07:19:10.274: INFO: (0) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 21.333549ms)
Aug 13 07:19:10.280: INFO: (1) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 6.348851ms)
Aug 13 07:19:10.283: INFO: (1) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 8.842876ms)
Aug 13 07:19:10.284: INFO: (1) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 9.1738ms)
Aug 13 07:19:10.284: INFO: (1) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 9.156449ms)
Aug 13 07:19:10.284: INFO: (1) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 9.310461ms)
Aug 13 07:19:10.287: INFO: (1) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 12.300736ms)
Aug 13 07:19:10.287: INFO: (1) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 12.395727ms)
Aug 13 07:19:10.287: INFO: (1) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 12.770354ms)
Aug 13 07:19:10.287: INFO: (1) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 12.598573ms)
Aug 13 07:19:10.287: INFO: (1) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 12.767489ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 13.3093ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 13.372678ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 13.354348ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 13.341185ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 13.391134ms)
Aug 13 07:19:10.288: INFO: (1) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 13.389765ms)
Aug 13 07:19:10.292: INFO: (2) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 3.716994ms)
Aug 13 07:19:10.293: INFO: (2) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.737681ms)
Aug 13 07:19:10.294: INFO: (2) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.412827ms)
Aug 13 07:19:10.294: INFO: (2) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 5.55734ms)
Aug 13 07:19:10.295: INFO: (2) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.092411ms)
Aug 13 07:19:10.295: INFO: (2) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.382799ms)
Aug 13 07:19:10.295: INFO: (2) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.809712ms)
Aug 13 07:19:10.297: INFO: (2) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 7.881719ms)
Aug 13 07:19:10.297: INFO: (2) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 8.855421ms)
Aug 13 07:19:10.298: INFO: (2) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 8.47692ms)
Aug 13 07:19:10.298: INFO: (2) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 8.883143ms)
Aug 13 07:19:10.298: INFO: (2) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 9.402434ms)
Aug 13 07:19:10.298: INFO: (2) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 9.18028ms)
Aug 13 07:19:10.298: INFO: (2) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 8.8157ms)
Aug 13 07:19:10.299: INFO: (2) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 9.153042ms)
Aug 13 07:19:10.299: INFO: (2) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 10.083143ms)
Aug 13 07:19:10.303: INFO: (3) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 4.076864ms)
Aug 13 07:19:10.303: INFO: (3) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 4.422104ms)
Aug 13 07:19:10.304: INFO: (3) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.97499ms)
Aug 13 07:19:10.305: INFO: (3) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.722255ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.775226ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.241098ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 6.632789ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.521114ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 6.744705ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 6.229181ms)
Aug 13 07:19:10.306: INFO: (3) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 6.426031ms)
Aug 13 07:19:10.307: INFO: (3) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 7.389976ms)
Aug 13 07:19:10.307: INFO: (3) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 7.543006ms)
Aug 13 07:19:10.307: INFO: (3) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.618523ms)
Aug 13 07:19:10.307: INFO: (3) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 7.612391ms)
Aug 13 07:19:10.307: INFO: (3) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.407142ms)
Aug 13 07:19:10.313: INFO: (4) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 6.011528ms)
Aug 13 07:19:10.314: INFO: (4) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.651813ms)
Aug 13 07:19:10.314: INFO: (4) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 7.050066ms)
Aug 13 07:19:10.315: INFO: (4) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 7.587643ms)
Aug 13 07:19:10.315: INFO: (4) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 7.667658ms)
Aug 13 07:19:10.315: INFO: (4) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 7.585652ms)
Aug 13 07:19:10.316: INFO: (4) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 8.252858ms)
Aug 13 07:19:10.316: INFO: (4) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 8.139251ms)
Aug 13 07:19:10.316: INFO: (4) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 8.193095ms)
Aug 13 07:19:10.316: INFO: (4) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 8.472132ms)
Aug 13 07:19:10.316: INFO: (4) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 8.690311ms)
Aug 13 07:19:10.317: INFO: (4) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 9.302022ms)
Aug 13 07:19:10.317: INFO: (4) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 9.485926ms)
Aug 13 07:19:10.317: INFO: (4) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 9.617249ms)
Aug 13 07:19:10.317: INFO: (4) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 9.918898ms)
Aug 13 07:19:10.317: INFO: (4) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 9.939009ms)
Aug 13 07:19:10.322: INFO: (5) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 4.185155ms)
Aug 13 07:19:10.322: INFO: (5) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 4.238888ms)
Aug 13 07:19:10.322: INFO: (5) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 4.248843ms)
Aug 13 07:19:10.322: INFO: (5) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 4.839612ms)
Aug 13 07:19:10.323: INFO: (5) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.088208ms)
Aug 13 07:19:10.323: INFO: (5) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.813276ms)
Aug 13 07:19:10.324: INFO: (5) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 5.878986ms)
Aug 13 07:19:10.324: INFO: (5) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.989347ms)
Aug 13 07:19:10.324: INFO: (5) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.46686ms)
Aug 13 07:19:10.324: INFO: (5) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 6.322286ms)
Aug 13 07:19:10.324: INFO: (5) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.940646ms)
Aug 13 07:19:10.325: INFO: (5) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 7.373343ms)
Aug 13 07:19:10.326: INFO: (5) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 7.598694ms)
Aug 13 07:19:10.326: INFO: (5) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.749259ms)
Aug 13 07:19:10.326: INFO: (5) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 8.101861ms)
Aug 13 07:19:10.326: INFO: (5) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 7.961801ms)
Aug 13 07:19:10.329: INFO: (6) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 3.003415ms)
Aug 13 07:19:10.330: INFO: (6) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.671785ms)
Aug 13 07:19:10.330: INFO: (6) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.075741ms)
Aug 13 07:19:10.331: INFO: (6) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 4.259721ms)
Aug 13 07:19:10.331: INFO: (6) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.164608ms)
Aug 13 07:19:10.332: INFO: (6) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.858546ms)
Aug 13 07:19:10.332: INFO: (6) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 5.449764ms)
Aug 13 07:19:10.333: INFO: (6) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 6.684882ms)
Aug 13 07:19:10.333: INFO: (6) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 6.014658ms)
Aug 13 07:19:10.333: INFO: (6) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 7.125632ms)
Aug 13 07:19:10.333: INFO: (6) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.923589ms)
Aug 13 07:19:10.334: INFO: (6) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.971922ms)
Aug 13 07:19:10.334: INFO: (6) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 7.421154ms)
Aug 13 07:19:10.334: INFO: (6) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.706246ms)
Aug 13 07:19:10.334: INFO: (6) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.839185ms)
Aug 13 07:19:10.334: INFO: (6) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.446893ms)
Aug 13 07:19:10.338: INFO: (7) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.657498ms)
Aug 13 07:19:10.339: INFO: (7) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 4.63825ms)
Aug 13 07:19:10.339: INFO: (7) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 4.571893ms)
Aug 13 07:19:10.339: INFO: (7) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 4.872263ms)
Aug 13 07:19:10.339: INFO: (7) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 4.743829ms)
Aug 13 07:19:10.339: INFO: (7) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 4.926257ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.912156ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.330857ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.197058ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.902979ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 6.552213ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.925113ms)
Aug 13 07:19:10.341: INFO: (7) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.680617ms)
Aug 13 07:19:10.342: INFO: (7) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.183488ms)
Aug 13 07:19:10.342: INFO: (7) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 7.653922ms)
Aug 13 07:19:10.342: INFO: (7) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.689593ms)
Aug 13 07:19:10.346: INFO: (8) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.386216ms)
Aug 13 07:19:10.347: INFO: (8) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 3.606493ms)
Aug 13 07:19:10.347: INFO: (8) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 3.993071ms)
Aug 13 07:19:10.347: INFO: (8) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.945824ms)
Aug 13 07:19:10.347: INFO: (8) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 4.334864ms)
Aug 13 07:19:10.348: INFO: (8) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.894795ms)
Aug 13 07:19:10.348: INFO: (8) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 4.985061ms)
Aug 13 07:19:10.348: INFO: (8) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.13202ms)
Aug 13 07:19:10.349: INFO: (8) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.713777ms)
Aug 13 07:19:10.349: INFO: (8) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.183273ms)
Aug 13 07:19:10.349: INFO: (8) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.05172ms)
Aug 13 07:19:10.349: INFO: (8) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.734371ms)
Aug 13 07:19:10.350: INFO: (8) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.634969ms)
Aug 13 07:19:10.350: INFO: (8) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 7.043412ms)
Aug 13 07:19:10.350: INFO: (8) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.073086ms)
Aug 13 07:19:10.350: INFO: (8) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.236141ms)
Aug 13 07:19:10.353: INFO: (9) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 2.943925ms)
Aug 13 07:19:10.353: INFO: (9) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 2.963672ms)
Aug 13 07:19:10.354: INFO: (9) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 3.697372ms)
Aug 13 07:19:10.355: INFO: (9) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 4.551528ms)
Aug 13 07:19:10.355: INFO: (9) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.013605ms)
Aug 13 07:19:10.355: INFO: (9) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 4.722024ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.169682ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 5.182572ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.264183ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 5.343397ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 5.64464ms)
Aug 13 07:19:10.356: INFO: (9) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 5.943082ms)
Aug 13 07:19:10.357: INFO: (9) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.314707ms)
Aug 13 07:19:10.357: INFO: (9) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.277289ms)
Aug 13 07:19:10.357: INFO: (9) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.429758ms)
Aug 13 07:19:10.357: INFO: (9) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.391595ms)
Aug 13 07:19:10.361: INFO: (10) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 3.855433ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.927785ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.073195ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 5.073464ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.14253ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 5.007626ms)
Aug 13 07:19:10.362: INFO: (10) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 5.489834ms)
Aug 13 07:19:10.365: INFO: (10) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 7.312918ms)
Aug 13 07:19:10.365: INFO: (10) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.493493ms)
Aug 13 07:19:10.365: INFO: (10) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 7.398037ms)
Aug 13 07:19:10.365: INFO: (10) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 7.532382ms)
Aug 13 07:19:10.365: INFO: (10) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 7.59878ms)
Aug 13 07:19:10.366: INFO: (10) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 8.672186ms)
Aug 13 07:19:10.366: INFO: (10) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 9.220234ms)
Aug 13 07:19:10.367: INFO: (10) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 9.186329ms)
Aug 13 07:19:10.367: INFO: (10) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 9.405002ms)
Aug 13 07:19:10.371: INFO: (11) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 3.642855ms)
Aug 13 07:19:10.371: INFO: (11) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 3.882625ms)
Aug 13 07:19:10.371: INFO: (11) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 3.847147ms)
Aug 13 07:19:10.371: INFO: (11) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 3.931635ms)
Aug 13 07:19:10.373: INFO: (11) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 5.913646ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 6.568628ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.295107ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.973619ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.407563ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 5.960843ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 6.356172ms)
Aug 13 07:19:10.374: INFO: (11) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 6.632866ms)
Aug 13 07:19:10.375: INFO: (11) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.947921ms)
Aug 13 07:19:10.375: INFO: (11) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 7.139827ms)
Aug 13 07:19:10.376: INFO: (11) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 7.959356ms)
Aug 13 07:19:10.376: INFO: (11) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 8.679523ms)
Aug 13 07:19:10.380: INFO: (12) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 4.275796ms)
Aug 13 07:19:10.380: INFO: (12) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 4.283363ms)
Aug 13 07:19:10.380: INFO: (12) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 4.111335ms)
Aug 13 07:19:10.381: INFO: (12) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 4.658314ms)
Aug 13 07:19:10.382: INFO: (12) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.041627ms)
Aug 13 07:19:10.382: INFO: (12) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.021954ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 6.942586ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 7.144754ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 7.123885ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 7.06591ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 7.054117ms)
Aug 13 07:19:10.383: INFO: (12) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 7.140852ms)
Aug 13 07:19:10.384: INFO: (12) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 7.313783ms)
Aug 13 07:19:10.384: INFO: (12) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.397486ms)
Aug 13 07:19:10.384: INFO: (12) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 7.885783ms)
Aug 13 07:19:10.384: INFO: (12) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.887954ms)
Aug 13 07:19:10.387: INFO: (13) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 2.668728ms)
Aug 13 07:19:10.388: INFO: (13) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 3.618644ms)
Aug 13 07:19:10.388: INFO: (13) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 4.048795ms)
Aug 13 07:19:10.388: INFO: (13) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 4.368421ms)
Aug 13 07:19:10.389: INFO: (13) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.919745ms)
Aug 13 07:19:10.389: INFO: (13) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 5.21437ms)
Aug 13 07:19:10.389: INFO: (13) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 5.38699ms)
Aug 13 07:19:10.390: INFO: (13) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 5.799107ms)
Aug 13 07:19:10.390: INFO: (13) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.559044ms)
Aug 13 07:19:10.390: INFO: (13) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.951884ms)
Aug 13 07:19:10.390: INFO: (13) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.836461ms)
Aug 13 07:19:10.390: INFO: (13) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 5.895536ms)
Aug 13 07:19:10.391: INFO: (13) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.994646ms)
Aug 13 07:19:10.391: INFO: (13) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 7.127623ms)
Aug 13 07:19:10.391: INFO: (13) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.002431ms)
Aug 13 07:19:10.391: INFO: (13) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 7.237355ms)
Aug 13 07:19:10.394: INFO: (14) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 2.822173ms)
Aug 13 07:19:10.395: INFO: (14) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 3.630422ms)
Aug 13 07:19:10.395: INFO: (14) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.719851ms)
Aug 13 07:19:10.396: INFO: (14) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 3.93478ms)
Aug 13 07:19:10.396: INFO: (14) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 3.958096ms)
Aug 13 07:19:10.396: INFO: (14) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 4.202823ms)
Aug 13 07:19:10.397: INFO: (14) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.446878ms)
Aug 13 07:19:10.397: INFO: (14) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.778839ms)
Aug 13 07:19:10.397: INFO: (14) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 5.89734ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.052898ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 5.895531ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.134056ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.389549ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 6.382702ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.41834ms)
Aug 13 07:19:10.398: INFO: (14) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.350521ms)
Aug 13 07:19:10.401: INFO: (15) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 2.804538ms)
Aug 13 07:19:10.402: INFO: (15) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 3.77938ms)
Aug 13 07:19:10.402: INFO: (15) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 4.244961ms)
Aug 13 07:19:10.403: INFO: (15) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.392095ms)
Aug 13 07:19:10.403: INFO: (15) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 4.693358ms)
Aug 13 07:19:10.403: INFO: (15) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 4.882789ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.341446ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.466819ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 5.346703ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.784225ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 5.688244ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.098568ms)
Aug 13 07:19:10.404: INFO: (15) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.291571ms)
Aug 13 07:19:10.405: INFO: (15) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.344202ms)
Aug 13 07:19:10.405: INFO: (15) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.290369ms)
Aug 13 07:19:10.405: INFO: (15) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.561605ms)
Aug 13 07:19:10.408: INFO: (16) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 3.172744ms)
Aug 13 07:19:10.408: INFO: (16) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 3.493774ms)
Aug 13 07:19:10.409: INFO: (16) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 3.880542ms)
Aug 13 07:19:10.410: INFO: (16) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.901743ms)
Aug 13 07:19:10.410: INFO: (16) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.163603ms)
Aug 13 07:19:10.410: INFO: (16) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.403116ms)
Aug 13 07:19:10.410: INFO: (16) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 5.415529ms)
Aug 13 07:19:10.411: INFO: (16) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 5.863196ms)
Aug 13 07:19:10.411: INFO: (16) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 5.883636ms)
Aug 13 07:19:10.411: INFO: (16) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.879144ms)
Aug 13 07:19:10.411: INFO: (16) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 6.520351ms)
Aug 13 07:19:10.411: INFO: (16) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 6.563996ms)
Aug 13 07:19:10.412: INFO: (16) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.688847ms)
Aug 13 07:19:10.412: INFO: (16) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 6.77573ms)
Aug 13 07:19:10.412: INFO: (16) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 6.781434ms)
Aug 13 07:19:10.412: INFO: (16) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.95355ms)
Aug 13 07:19:10.416: INFO: (17) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 3.463798ms)
Aug 13 07:19:10.417: INFO: (17) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 4.317901ms)
Aug 13 07:19:10.424: INFO: (17) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 11.466108ms)
Aug 13 07:19:10.424: INFO: (17) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 11.099338ms)
Aug 13 07:19:10.424: INFO: (17) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 11.515059ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 11.493811ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 11.679101ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 11.62079ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 12.519088ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 11.982802ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 12.220736ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 12.632081ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 13.005013ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 13.12275ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 13.455237ms)
Aug 13 07:19:10.425: INFO: (17) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 13.022346ms)
Aug 13 07:19:10.431: INFO: (18) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 5.404975ms)
Aug 13 07:19:10.431: INFO: (18) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 5.53129ms)
Aug 13 07:19:10.431: INFO: (18) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 5.934475ms)
Aug 13 07:19:10.432: INFO: (18) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 6.228038ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 8.07432ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 8.010307ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 8.283111ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 8.054759ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 8.291693ms)
Aug 13 07:19:10.434: INFO: (18) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 8.406139ms)
Aug 13 07:19:10.435: INFO: (18) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 8.977884ms)
Aug 13 07:19:10.435: INFO: (18) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 9.054815ms)
Aug 13 07:19:10.435: INFO: (18) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 9.119059ms)
Aug 13 07:19:10.436: INFO: (18) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 9.95377ms)
Aug 13 07:19:10.436: INFO: (18) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 9.905062ms)
Aug 13 07:19:10.437: INFO: (18) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 10.920779ms)
Aug 13 07:19:10.441: INFO: (19) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 3.843104ms)
Aug 13 07:19:10.441: INFO: (19) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:460/proxy/: tls baz (200; 3.924851ms)
Aug 13 07:19:10.441: INFO: (19) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 4.056171ms)
Aug 13 07:19:10.443: INFO: (19) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj/proxy/rewriteme">test</a> (200; 5.325117ms)
Aug 13 07:19:10.443: INFO: (19) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname1/proxy/: tls baz (200; 6.077173ms)
Aug 13 07:19:10.443: INFO: (19) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:443/proxy/tlsrewritem... (200; 6.068909ms)
Aug 13 07:19:10.443: INFO: (19) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname1/proxy/: foo (200; 6.301469ms)
Aug 13 07:19:10.444: INFO: (19) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">... (200; 6.581863ms)
Aug 13 07:19:10.444: INFO: (19) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/: <a href="/api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:1080/proxy/rewriteme">test<... (200; 6.950907ms)
Aug 13 07:19:10.445: INFO: (19) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname2/proxy/: bar (200; 7.556276ms)
Aug 13 07:19:10.445: INFO: (19) /api/v1/namespaces/proxy-8495/pods/https:proxy-service-t9ff6-cgzqj:462/proxy/: tls qux (200; 7.868551ms)
Aug 13 07:19:10.445: INFO: (19) /api/v1/namespaces/proxy-8495/services/https:proxy-service-t9ff6:tlsportname2/proxy/: tls qux (200; 7.916051ms)
Aug 13 07:19:10.446: INFO: (19) /api/v1/namespaces/proxy-8495/services/proxy-service-t9ff6:portname2/proxy/: bar (200; 8.26933ms)
Aug 13 07:19:10.446: INFO: (19) /api/v1/namespaces/proxy-8495/pods/proxy-service-t9ff6-cgzqj:162/proxy/: bar (200; 8.424202ms)
Aug 13 07:19:10.446: INFO: (19) /api/v1/namespaces/proxy-8495/pods/http:proxy-service-t9ff6-cgzqj:160/proxy/: foo (200; 8.614798ms)
Aug 13 07:19:10.446: INFO: (19) /api/v1/namespaces/proxy-8495/services/http:proxy-service-t9ff6:portname1/proxy/: foo (200; 8.764748ms)
STEP: deleting ReplicationController proxy-service-t9ff6 in namespace proxy-8495, will wait for the garbage collector to delete the pods
Aug 13 07:19:10.506: INFO: Deleting ReplicationController proxy-service-t9ff6 took: 7.146497ms
Aug 13 07:19:10.906: INFO: Terminating ReplicationController proxy-service-t9ff6 pods took: 400.313969ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:19:13.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8495" for this suite.
Aug 13 07:19:19.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:19:19.118: INFO: namespace proxy-8495 deletion completed in 6.105142292s

â€¢ [SLOW TEST:18.979 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:19:19.118: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 13 07:19:19.157: INFO: Waiting up to 5m0s for pod "pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308" in namespace "emptydir-3175" to be "success or failure"
Aug 13 07:19:19.162: INFO: Pod "pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454093ms
Aug 13 07:19:21.166: INFO: Pod "pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008349163s
Aug 13 07:19:23.170: INFO: Pod "pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012335401s
STEP: Saw pod success
Aug 13 07:19:23.170: INFO: Pod "pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:19:23.174: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:19:23.197: INFO: Waiting for pod pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:19:23.201: INFO: Pod pod-aa504e1a-bd9a-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:19:23.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3175" for this suite.
Aug 13 07:19:29.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:19:29.295: INFO: namespace emptydir-3175 deletion completed in 6.091079475s

â€¢ [SLOW TEST:10.177 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:19:29.296: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Aug 13 07:19:29.328: INFO: Waiting up to 5m0s for pod "client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308" in namespace "containers-6774" to be "success or failure"
Aug 13 07:19:29.333: INFO: Pod "client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228512ms
Aug 13 07:19:31.337: INFO: Pod "client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008989374s
Aug 13 07:19:33.341: INFO: Pod "client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013060444s
STEP: Saw pod success
Aug 13 07:19:33.341: INFO: Pod "client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:19:33.344: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:19:33.368: INFO: Waiting for pod client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:19:33.371: INFO: Pod client-containers-b0602f58-bd9a-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:19:33.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6774" for this suite.
Aug 13 07:19:39.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:19:39.466: INFO: namespace containers-6774 deletion completed in 6.091696537s

â€¢ [SLOW TEST:10.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:19:39.466: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 13 07:19:39.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4086'
Aug 13 07:19:39.591: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 13 07:19:39.591: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 13 07:19:39.602: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-jtwqg]
Aug 13 07:19:39.602: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-jtwqg" in namespace "kubectl-4086" to be "running and ready"
Aug 13 07:19:39.606: INFO: Pod "e2e-test-nginx-rc-jtwqg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.130635ms
Aug 13 07:19:41.610: INFO: Pod "e2e-test-nginx-rc-jtwqg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008436957s
Aug 13 07:19:43.614: INFO: Pod "e2e-test-nginx-rc-jtwqg": Phase="Running", Reason="", readiness=true. Elapsed: 4.012501392s
Aug 13 07:19:43.614: INFO: Pod "e2e-test-nginx-rc-jtwqg" satisfied condition "running and ready"
Aug 13 07:19:43.614: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-jtwqg]
Aug 13 07:19:43.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 logs rc/e2e-test-nginx-rc --namespace=kubectl-4086'
Aug 13 07:19:43.717: INFO: stderr: ""
Aug 13 07:19:43.717: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Aug 13 07:19:43.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 delete rc e2e-test-nginx-rc --namespace=kubectl-4086'
Aug 13 07:19:43.804: INFO: stderr: ""
Aug 13 07:19:43.804: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:19:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4086" for this suite.
Aug 13 07:19:49.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:19:49.918: INFO: namespace kubectl-4086 deletion completed in 6.10960819s

â€¢ [SLOW TEST:10.452 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:19:49.918: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-66ks
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 07:19:49.963: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-66ks" in namespace "subpath-6940" to be "success or failure"
Aug 13 07:19:49.970: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Pending", Reason="", readiness=false. Elapsed: 7.04182ms
Aug 13 07:19:51.974: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010636945s
Aug 13 07:19:53.978: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 4.014487656s
Aug 13 07:19:55.981: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 6.017852191s
Aug 13 07:19:57.984: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 8.021146791s
Aug 13 07:19:59.989: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 10.025323662s
Aug 13 07:20:01.993: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 12.029880941s
Aug 13 07:20:03.998: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 14.03495391s
Aug 13 07:20:06.005: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 16.041351777s
Aug 13 07:20:08.010: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 18.046335827s
Aug 13 07:20:10.014: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 20.050593452s
Aug 13 07:20:12.019: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Running", Reason="", readiness=true. Elapsed: 22.055278699s
Aug 13 07:20:14.023: INFO: Pod "pod-subpath-test-configmap-66ks": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.059599717s
STEP: Saw pod success
Aug 13 07:20:14.023: INFO: Pod "pod-subpath-test-configmap-66ks" satisfied condition "success or failure"
Aug 13 07:20:14.026: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-subpath-test-configmap-66ks container test-container-subpath-configmap-66ks: <nil>
STEP: delete the pod
Aug 13 07:20:14.048: INFO: Waiting for pod pod-subpath-test-configmap-66ks to disappear
Aug 13 07:20:14.051: INFO: Pod pod-subpath-test-configmap-66ks no longer exists
STEP: Deleting pod pod-subpath-test-configmap-66ks
Aug 13 07:20:14.051: INFO: Deleting pod "pod-subpath-test-configmap-66ks" in namespace "subpath-6940"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:20:14.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6940" for this suite.
Aug 13 07:20:20.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:20:20.151: INFO: namespace subpath-6940 deletion completed in 6.094437676s

â€¢ [SLOW TEST:30.233 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:20:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Aug 13 07:20:20.180: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:20:23.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-257" for this suite.
Aug 13 07:20:29.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:20:29.953: INFO: namespace init-container-257 deletion completed in 6.105586381s

â€¢ [SLOW TEST:9.802 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:20:29.953: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d4896550-bd9a-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:20:30.002: INFO: Waiting up to 5m0s for pod "pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308" in namespace "secrets-3477" to be "success or failure"
Aug 13 07:20:30.009: INFO: Pod "pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 7.386685ms
Aug 13 07:20:32.014: INFO: Pod "pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012354715s
Aug 13 07:20:34.018: INFO: Pod "pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016569696s
STEP: Saw pod success
Aug 13 07:20:34.018: INFO: Pod "pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:20:34.022: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:20:34.044: INFO: Waiting for pod pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:20:34.046: INFO: Pod pod-secrets-d48a27b9-bd9a-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:20:34.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3477" for this suite.
Aug 13 07:20:40.062: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:20:40.150: INFO: namespace secrets-3477 deletion completed in 6.100613379s

â€¢ [SLOW TEST:10.197 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:20:40.151: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5813
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5813
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5813
Aug 13 07:20:40.210: INFO: Found 0 stateful pods, waiting for 1
Aug 13 07:20:50.214: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 13 07:20:50.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:20:50.387: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:20:50.387: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:20:50.387: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 07:20:50.393: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 13 07:21:00.397: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 07:21:00.397: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 07:21:00.411: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999513s
Aug 13 07:21:01.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996187451s
Aug 13 07:21:02.420: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991617259s
Aug 13 07:21:03.425: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987349679s
Aug 13 07:21:04.430: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982366827s
Aug 13 07:21:05.436: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.976548278s
Aug 13 07:21:06.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.971006049s
Aug 13 07:21:07.444: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967270419s
Aug 13 07:21:08.447: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963309462s
Aug 13 07:21:09.452: INFO: Verifying statefulset ss doesn't scale past 1 for another 959.58126ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5813
Aug 13 07:21:10.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:21:10.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:21:10.622: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:21:10.622: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:21:10.626: INFO: Found 1 stateful pods, waiting for 3
Aug 13 07:21:20.630: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:21:20.631: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 13 07:21:20.631: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 13 07:21:20.637: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:21:20.816: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:21:20.816: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:21:20.816: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 07:21:20.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:21:20.982: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:21:20.982: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:21:20.982: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 07:21:20.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 13 07:21:21.151: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 13 07:21:21.151: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 13 07:21:21.151: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 13 07:21:21.151: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 07:21:21.154: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 13 07:21:31.160: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 07:21:31.160: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 07:21:31.160: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 13 07:21:31.170: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999734s
Aug 13 07:21:32.174: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997100377s
Aug 13 07:21:33.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993140556s
Aug 13 07:21:34.183: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988965001s
Aug 13 07:21:35.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.983434477s
Aug 13 07:21:36.193: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977665482s
Aug 13 07:21:37.198: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973898315s
Aug 13 07:21:38.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969268598s
Aug 13 07:21:39.206: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.965187251s
Aug 13 07:21:40.210: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.287307ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5813
Aug 13 07:21:41.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:21:41.370: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:21:41.370: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:21:41.370: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:21:41.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:21:41.538: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:21:41.539: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:21:41.539: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:21:41.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 exec --namespace=statefulset-5813 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 13 07:21:41.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 13 07:21:41.713: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 13 07:21:41.713: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 13 07:21:41.713: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Aug 13 07:22:01.731: INFO: Deleting all statefulset in ns statefulset-5813
Aug 13 07:22:01.734: INFO: Scaling statefulset ss to 0
Aug 13 07:22:01.742: INFO: Waiting for statefulset status.replicas updated to 0
Aug 13 07:22:01.750: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:22:01.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5813" for this suite.
Aug 13 07:22:07.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:22:07.899: INFO: namespace statefulset-5813 deletion completed in 6.107687844s

â€¢ [SLOW TEST:87.748 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:22:07.899: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-5513/configmap-test-0eeebe93-bd9b-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 07:22:07.981: INFO: Waiting up to 5m0s for pod "pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308" in namespace "configmap-5513" to be "success or failure"
Aug 13 07:22:07.985: INFO: Pod "pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.921903ms
Aug 13 07:22:09.991: INFO: Pod "pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010232485s
Aug 13 07:22:11.995: INFO: Pod "pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014031499s
STEP: Saw pod success
Aug 13 07:22:11.995: INFO: Pod "pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:22:12.007: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308 container env-test: <nil>
STEP: delete the pod
Aug 13 07:22:12.029: INFO: Waiting for pod pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:22:12.034: INFO: Pod pod-configmaps-0eef883e-bd9b-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:22:12.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5513" for this suite.
Aug 13 07:22:18.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:22:18.147: INFO: namespace configmap-5513 deletion completed in 6.109765762s

â€¢ [SLOW TEST:10.248 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:22:18.147: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:22:18.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 version'
Aug 13 07:22:18.256: INFO: stderr: ""
Aug 13 07:22:18.256: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:45:25Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:22:18.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4162" for this suite.
Aug 13 07:22:24.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:22:24.352: INFO: namespace kubectl-4162 deletion completed in 6.091971358s

â€¢ [SLOW TEST:6.204 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:22:24.352: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6155.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6155.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6155.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6155.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6155.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 07:22:28.425: INFO: DNS probes using dns-6155/dns-test-18b843de-bd9b-11e9-8bf8-0a58ac140308 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:22:28.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6155" for this suite.
Aug 13 07:22:34.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:22:34.537: INFO: namespace dns-6155 deletion completed in 6.0967417s

â€¢ [SLOW TEST:10.185 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:22:34.537: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-6119
Aug 13 07:22:38.582: INFO: Started pod liveness-exec in namespace container-probe-6119
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 07:22:38.584: INFO: Initial restart count of pod liveness-exec is 0
Aug 13 07:23:28.685: INFO: Restart count of pod container-probe-6119/liveness-exec is now 1 (50.100458605s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:23:28.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6119" for this suite.
Aug 13 07:23:34.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:23:34.802: INFO: namespace container-probe-6119 deletion completed in 6.100272996s

â€¢ [SLOW TEST:60.265 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:23:34.802: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:23:34.838: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308" in namespace "downward-api-4200" to be "success or failure"
Aug 13 07:23:34.846: INFO: Pod "downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.589055ms
Aug 13 07:23:36.850: INFO: Pod "downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012473084s
Aug 13 07:23:38.854: INFO: Pod "downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016243113s
STEP: Saw pod success
Aug 13 07:23:38.854: INFO: Pod "downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:23:38.857: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:23:38.878: INFO: Waiting for pod downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:23:38.883: INFO: Pod downwardapi-volume-42b5e89f-bd9b-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:23:38.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4200" for this suite.
Aug 13 07:23:44.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:23:45.016: INFO: namespace downward-api-4200 deletion completed in 6.126315778s

â€¢ [SLOW TEST:10.214 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:23:45.016: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4047
Aug 13 07:23:49.072: INFO: Started pod liveness-http in namespace container-probe-4047
STEP: checking the pod's current state and verifying that restartCount is present
Aug 13 07:23:49.074: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:27:49.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4047" for this suite.
Aug 13 07:27:55.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:27:55.703: INFO: namespace container-probe-4047 deletion completed in 6.105516805s

â€¢ [SLOW TEST:250.687 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:27:55.703: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Aug 13 07:27:55.744: INFO: Waiting up to 5m0s for pod "client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308" in namespace "containers-8179" to be "success or failure"
Aug 13 07:27:55.747: INFO: Pod "client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.7101ms
Aug 13 07:27:57.752: INFO: Pod "client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008333654s
Aug 13 07:27:59.756: INFO: Pod "client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011889014s
STEP: Saw pod success
Aug 13 07:27:59.756: INFO: Pod "client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:27:59.759: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:27:59.782: INFO: Waiting for pod client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:27:59.785: INFO: Pod client-containers-de38c63f-bd9b-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:27:59.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8179" for this suite.
Aug 13 07:28:05.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:28:05.892: INFO: namespace containers-8179 deletion completed in 6.10198978s

â€¢ [SLOW TEST:10.188 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:28:05.892: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:28:05.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1169" for this suite.
Aug 13 07:28:11.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:28:12.059: INFO: namespace kubelet-test-1169 deletion completed in 6.09890963s

â€¢ [SLOW TEST:6.167 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:28:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-e7f89263-bd9b-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 07:28:12.103: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308" in namespace "projected-4361" to be "success or failure"
Aug 13 07:28:12.109: INFO: Pod "pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 5.984474ms
Aug 13 07:28:14.113: INFO: Pod "pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009589447s
Aug 13 07:28:16.118: INFO: Pod "pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014135808s
STEP: Saw pod success
Aug 13 07:28:16.118: INFO: Pod "pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:28:16.120: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 07:28:16.145: INFO: Waiting for pod pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:28:16.149: INFO: Pod pod-projected-configmaps-e7f944bb-bd9b-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:28:16.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4361" for this suite.
Aug 13 07:28:22.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:28:22.269: INFO: namespace projected-4361 deletion completed in 6.111898421s

â€¢ [SLOW TEST:10.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:28:22.270: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 13 07:28:30.391: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:30.394: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:32.394: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:32.400: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:34.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:34.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:36.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:36.402: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:38.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:38.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:40.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:40.399: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:42.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:42.399: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:44.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:44.399: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:46.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:46.399: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:48.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:48.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:50.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:50.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:52.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:52.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:54.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:54.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:56.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:56.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:28:58.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:28:58.398: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 13 07:29:00.395: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 13 07:29:00.398: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:29:00.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-799" for this suite.
Aug 13 07:29:22.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:29:22.503: INFO: namespace container-lifecycle-hook-799 deletion completed in 22.100975632s

â€¢ [SLOW TEST:60.234 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:29:22.503: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-11f513c9-bd9c-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:29:22.543: INFO: Waiting up to 5m0s for pod "pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308" in namespace "secrets-8750" to be "success or failure"
Aug 13 07:29:22.551: INFO: Pod "pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 7.435216ms
Aug 13 07:29:24.554: INFO: Pod "pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010627864s
Aug 13 07:29:26.557: INFO: Pod "pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013954525s
STEP: Saw pod success
Aug 13 07:29:26.557: INFO: Pod "pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:29:26.560: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:29:26.581: INFO: Waiting for pod pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:29:26.584: INFO: Pod pod-secrets-11f5af52-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:29:26.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8750" for this suite.
Aug 13 07:29:32.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:29:32.676: INFO: namespace secrets-8750 deletion completed in 6.088967999s

â€¢ [SLOW TEST:10.173 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:29:32.677: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-180987a7-bd9c-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume configMaps
Aug 13 07:29:32.752: INFO: Waiting up to 5m0s for pod "pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308" in namespace "configmap-8564" to be "success or failure"
Aug 13 07:29:32.757: INFO: Pod "pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868001ms
Aug 13 07:29:34.761: INFO: Pod "pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009264256s
Aug 13 07:29:36.765: INFO: Pod "pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013208804s
STEP: Saw pod success
Aug 13 07:29:36.765: INFO: Pod "pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:29:36.768: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 13 07:29:36.789: INFO: Waiting for pod pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:29:36.794: INFO: Pod pod-configmaps-180abc6b-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:29:36.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8564" for this suite.
Aug 13 07:29:42.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:29:42.898: INFO: namespace configmap-8564 deletion completed in 6.100010826s

â€¢ [SLOW TEST:10.221 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:29:42.898: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:29:42.944: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308" in namespace "downward-api-8650" to be "success or failure"
Aug 13 07:29:42.949: INFO: Pod "downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.46419ms
Aug 13 07:29:44.952: INFO: Pod "downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007974126s
Aug 13 07:29:46.956: INFO: Pod "downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011654703s
STEP: Saw pod success
Aug 13 07:29:46.956: INFO: Pod "downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:29:46.958: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:29:46.986: INFO: Waiting for pod downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:29:46.991: INFO: Pod downwardapi-volume-1e1e3b21-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:29:46.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8650" for this suite.
Aug 13 07:29:53.007: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:29:53.099: INFO: namespace downward-api-8650 deletion completed in 6.104746163s

â€¢ [SLOW TEST:10.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:29:53.099: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-2431465e-bd9c-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:29:53.136: INFO: Waiting up to 5m0s for pod "pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308" in namespace "secrets-3196" to be "success or failure"
Aug 13 07:29:53.140: INFO: Pod "pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.404413ms
Aug 13 07:29:55.145: INFO: Pod "pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00918978s
Aug 13 07:29:57.149: INFO: Pod "pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013031473s
STEP: Saw pod success
Aug 13 07:29:57.149: INFO: Pod "pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:29:57.152: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:29:57.173: INFO: Waiting for pod pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:29:57.176: INFO: Pod pod-secrets-2431c81c-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:29:57.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3196" for this suite.
Aug 13 07:30:03.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:30:03.297: INFO: namespace secrets-3196 deletion completed in 6.11689742s

â€¢ [SLOW TEST:10.197 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:30:03.297: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Aug 13 07:30:07.877: INFO: Successfully updated pod "annotationupdate2a469cb4-bd9c-11e9-8bf8-0a58ac140308"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:30:09.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5780" for this suite.
Aug 13 07:30:31.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:30:32.008: INFO: namespace projected-5780 deletion completed in 22.107533111s

â€¢ [SLOW TEST:28.711 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:30:32.009: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0813 07:31:12.082985      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 13 07:31:12.083: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:31:12.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-391" for this suite.
Aug 13 07:31:18.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:31:18.192: INFO: namespace gc-391 deletion completed in 6.105334262s

â€¢ [SLOW TEST:46.183 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:31:18.192: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Aug 13 07:31:18.231: INFO: Waiting up to 5m0s for pod "downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308" in namespace "projected-8734" to be "success or failure"
Aug 13 07:31:18.235: INFO: Pod "downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 4.186077ms
Aug 13 07:31:20.239: INFO: Pod "downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008039884s
Aug 13 07:31:22.242: INFO: Pod "downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011517489s
STEP: Saw pod success
Aug 13 07:31:22.242: INFO: Pod "downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:31:22.246: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308 container client-container: <nil>
STEP: delete the pod
Aug 13 07:31:22.265: INFO: Waiting for pod downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:31:22.267: INFO: Pod downwardapi-volume-56ea10f0-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:31:22.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8734" for this suite.
Aug 13 07:31:28.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:31:28.387: INFO: namespace projected-8734 deletion completed in 6.11493647s

â€¢ [SLOW TEST:10.195 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:31:28.387: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-423.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-423.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 13 07:31:32.470: INFO: DNS probes using dns-423/dns-test-5cfefa6b-bd9c-11e9-8bf8-0a58ac140308 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:31:32.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-423" for this suite.
Aug 13 07:31:38.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:31:38.592: INFO: namespace dns-423 deletion completed in 6.102882453s

â€¢ [SLOW TEST:10.205 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:31:38.592: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 13 07:31:46.672: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:46.675: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:48.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:48.679: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:50.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:50.680: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:52.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:52.679: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:54.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:54.692: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:56.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:56.679: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:31:58.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:31:58.680: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:32:00.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:32:00.679: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:32:02.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:32:02.681: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:32:04.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:32:04.679: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:32:06.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:32:06.680: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 13 07:32:08.675: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 13 07:32:08.680: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:32:08.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9043" for this suite.
Aug 13 07:32:30.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:32:30.801: INFO: namespace container-lifecycle-hook-9043 deletion completed in 22.10664372s

â€¢ [SLOW TEST:52.209 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:32:30.802: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 13 07:32:30.847: INFO: Waiting up to 5m0s for pod "pod-8232223f-bd9c-11e9-8bf8-0a58ac140308" in namespace "emptydir-5074" to be "success or failure"
Aug 13 07:32:30.850: INFO: Pod "pod-8232223f-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.736524ms
Aug 13 07:32:32.855: INFO: Pod "pod-8232223f-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007273737s
Aug 13 07:32:34.858: INFO: Pod "pod-8232223f-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010865674s
STEP: Saw pod success
Aug 13 07:32:34.858: INFO: Pod "pod-8232223f-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:32:34.861: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-8232223f-bd9c-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:32:34.883: INFO: Waiting for pod pod-8232223f-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:32:34.886: INFO: Pod pod-8232223f-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:32:34.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5074" for this suite.
Aug 13 07:32:40.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:32:40.985: INFO: namespace emptydir-5074 deletion completed in 6.094336031s

â€¢ [SLOW TEST:10.183 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:32:40.985: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4357
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 13 07:32:41.021: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 13 07:33:05.116: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.2.64:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4357 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:33:05.116: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:33:05.197: INFO: Found all expected endpoints: [netserver-0]
Aug 13 07:33:05.200: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.3.74:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4357 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:33:05.201: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:33:05.286: INFO: Found all expected endpoints: [netserver-1]
Aug 13 07:33:05.290: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.4.247:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4357 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 13 07:33:05.290: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
Aug 13 07:33:05.369: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:33:05.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4357" for this suite.
Aug 13 07:33:27.388: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:33:27.473: INFO: namespace pod-network-test-4357 deletion completed in 22.097068617s

â€¢ [SLOW TEST:46.488 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:33:27.473: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 13 07:33:27.509: INFO: Waiting up to 5m0s for pod "pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308" in namespace "emptydir-248" to be "success or failure"
Aug 13 07:33:27.513: INFO: Pod "pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 3.957824ms
Aug 13 07:33:29.517: INFO: Pod "pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007704449s
Aug 13 07:33:31.521: INFO: Pod "pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011778459s
STEP: Saw pod success
Aug 13 07:33:31.521: INFO: Pod "pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:33:31.524: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308 container test-container: <nil>
STEP: delete the pod
Aug 13 07:33:31.545: INFO: Waiting for pod pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:33:31.549: INFO: Pod pod-a3f86061-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:33:31.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-248" for this suite.
Aug 13 07:33:37.572: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:33:37.657: INFO: namespace emptydir-248 deletion completed in 6.097708224s

â€¢ [SLOW TEST:10.184 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:33:37.657: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-aa0a82ee-bd9c-11e9-8bf8-0a58ac140308
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:33:37.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7532" for this suite.
Aug 13 07:33:43.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:33:43.804: INFO: namespace configmap-7532 deletion completed in 6.11077076s

â€¢ [SLOW TEST:6.146 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:33:43.804: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:33:43.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 version --client'
Aug 13 07:33:43.902: INFO: stderr: ""
Aug 13 07:33:43.902: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Aug 13 07:33:43.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-9498'
Aug 13 07:33:44.332: INFO: stderr: ""
Aug 13 07:33:44.332: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 13 07:33:44.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 create -f - --namespace=kubectl-9498'
Aug 13 07:33:44.532: INFO: stderr: ""
Aug 13 07:33:44.532: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 13 07:33:45.536: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:33:45.536: INFO: Found 0 / 1
Aug 13 07:33:46.537: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:33:46.537: INFO: Found 0 / 1
Aug 13 07:33:47.536: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:33:47.536: INFO: Found 1 / 1
Aug 13 07:33:47.536: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 13 07:33:47.539: INFO: Selector matched 1 pods for map[app:redis]
Aug 13 07:33:47.539: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 13 07:33:47.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 describe pod redis-master-ssbfb --namespace=kubectl-9498'
Aug 13 07:33:47.633: INFO: stderr: ""
Aug 13 07:33:47.633: INFO: stdout: "Name:           redis-master-ssbfb\nNamespace:      kubectl-9498\nNode:           karbon-multi-fourteen-aca02f-k8s-worker-2/10.45.40.139\nStart Time:     Tue, 13 Aug 2019 07:33:44 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             172.20.4.250\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://64b154fd7ca9e3112440b763638d84662e5e8a542f1a87246f9696d4841eb914\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 13 Aug 2019 07:33:46 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rcf8l (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rcf8l:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rcf8l\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     <none>\nEvents:\n  Type    Reason     Age   From                                                Message\n  ----    ------     ----  ----                                                -------\n  Normal  Scheduled  3s    default-scheduler                                   Successfully assigned kubectl-9498/redis-master-ssbfb to karbon-multi-fourteen-aca02f-k8s-worker-2\n  Normal  Pulled     1s    kubelet, karbon-multi-fourteen-aca02f-k8s-worker-2  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, karbon-multi-fourteen-aca02f-k8s-worker-2  Created container redis-master\n  Normal  Started    1s    kubelet, karbon-multi-fourteen-aca02f-k8s-worker-2  Started container redis-master\n"
Aug 13 07:33:47.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 describe rc redis-master --namespace=kubectl-9498'
Aug 13 07:33:47.740: INFO: stderr: ""
Aug 13 07:33:47.740: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9498\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-ssbfb\n"
Aug 13 07:33:47.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 describe service redis-master --namespace=kubectl-9498'
Aug 13 07:33:47.830: INFO: stderr: ""
Aug 13 07:33:47.830: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9498\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                172.19.150.239\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.20.4.250:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 13 07:33:47.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 describe node karbon-multi-fourteen-aca02f-k8s-master-0'
Aug 13 07:33:47.938: INFO: stderr: ""
Aug 13 07:33:47.938: INFO: stdout: "Name:               karbon-multi-fourteen-aca02f-k8s-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=karbon-multi-fourteen-aca02f-k8s-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"be:0f:5c:a3:f8:a0\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.45.40.133\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 13 Aug 2019 05:37:39 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Tue, 13 Aug 2019 07:33:15 +0000   Tue, 13 Aug 2019 05:37:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Tue, 13 Aug 2019 07:33:15 +0000   Tue, 13 Aug 2019 05:37:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Tue, 13 Aug 2019 07:33:15 +0000   Tue, 13 Aug 2019 05:37:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Tue, 13 Aug 2019 07:33:15 +0000   Tue, 13 Aug 2019 05:37:34 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.40.133\n  Hostname:    karbon-multi-fourteen-aca02f-k8s-master-0\nCapacity:\n cpu:                4\n ephemeral-storage:  40883180Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3844204Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  40883180Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             3434604Ki\n pods:               110\nSystem Info:\n Machine ID:                 96984f3f31704ccca7da998c313997ae\n System UUID:                3835B5CE-7DB4-4870-97AC-DA4662C0938C\n Boot ID:                    0f9f0983-ee1e-4431-8d55-78b159b54993\n Kernel Version:             3.10.0-957.10.1.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://1.13.1\n Kubelet Version:            v1.14.0\n Kube-Proxy Version:         v1.14.0\nPodCIDR:                     172.20.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                        ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-de192f2239c448fa-gplvv     0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                kube-apiserver-karbon-multi-fourteen-aca02f-k8s-master-0    300m (7%)     0 (0%)      0 (0%)           0 (0%)         115m\n  kube-system                kube-dns-7f5499679c-r55pc                                   260m (6%)     200m (5%)   110Mi (3%)       170Mi (5%)     113m\n  kube-system                kube-flannel-ds-6pqj4                                       100m (2%)     500m (12%)  50Mi (1%)        50Mi (1%)      114m\n  kube-system                kube-proxy-ds-rwjqq                                         100m (2%)     100m (2%)   70Mi (2%)        70Mi (2%)      114m\n  ntnx-system                fluent-bit-lh2nx                                            100m (2%)     100m (2%)   50Mi (1%)        50Mi (1%)      113m\n  ntnx-system                node-exporter-qqdlg                                         112m (2%)     600m (15%)  200Mi (5%)       220Mi (6%)     110m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                972m (24%)   1500m (37%)\n  memory             480Mi (14%)  560Mi (16%)\n  ephemeral-storage  0 (0%)       0 (0%)\nEvents:              <none>\n"
Aug 13 07:33:47.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-872026410 describe namespace kubectl-9498'
Aug 13 07:33:48.032: INFO: stderr: ""
Aug 13 07:33:48.032: INFO: stdout: "Name:         kubectl-9498\nLabels:       e2e-framework=kubectl\n              e2e-run=4cc3a5e8-bd90-11e9-8bf8-0a58ac140308\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:33:48.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9498" for this suite.
Aug 13 07:34:10.051: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:34:10.154: INFO: namespace kubectl-9498 deletion completed in 22.116915881s

â€¢ [SLOW TEST:26.350 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:34:10.155: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-gd5l
STEP: Creating a pod to test atomic-volume-subpath
Aug 13 07:34:10.207: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-gd5l" in namespace "subpath-4107" to be "success or failure"
Aug 13 07:34:10.211: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Pending", Reason="", readiness=false. Elapsed: 3.913325ms
Aug 13 07:34:12.214: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 2.006827397s
Aug 13 07:34:14.218: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.010449885s
Aug 13 07:34:16.222: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 6.014890415s
Aug 13 07:34:18.227: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 8.019211021s
Aug 13 07:34:20.231: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 10.023469206s
Aug 13 07:34:22.235: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 12.027595001s
Aug 13 07:34:24.239: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 14.031191035s
Aug 13 07:34:26.242: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 16.034607446s
Aug 13 07:34:28.245: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 18.038048463s
Aug 13 07:34:30.249: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 20.042060477s
Aug 13 07:34:32.253: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Running", Reason="", readiness=true. Elapsed: 22.04531813s
Aug 13 07:34:34.256: INFO: Pod "pod-subpath-test-configmap-gd5l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.049065455s
STEP: Saw pod success
Aug 13 07:34:34.256: INFO: Pod "pod-subpath-test-configmap-gd5l" satisfied condition "success or failure"
Aug 13 07:34:34.259: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-subpath-test-configmap-gd5l container test-container-subpath-configmap-gd5l: <nil>
STEP: delete the pod
Aug 13 07:34:34.280: INFO: Waiting for pod pod-subpath-test-configmap-gd5l to disappear
Aug 13 07:34:34.283: INFO: Pod pod-subpath-test-configmap-gd5l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-gd5l
Aug 13 07:34:34.283: INFO: Deleting pod "pod-subpath-test-configmap-gd5l" in namespace "subpath-4107"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:34:34.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4107" for this suite.
Aug 13 07:34:40.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:34:40.384: INFO: namespace subpath-4107 deletion completed in 6.094957943s

â€¢ [SLOW TEST:30.230 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:34:40.384: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-cf6ee946-bd9c-11e9-8bf8-0a58ac140308
STEP: Creating a pod to test consume secrets
Aug 13 07:34:40.434: INFO: Waiting up to 5m0s for pod "pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308" in namespace "secrets-9340" to be "success or failure"
Aug 13 07:34:40.442: INFO: Pod "pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 8.41169ms
Aug 13 07:34:42.446: INFO: Pod "pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012187611s
Aug 13 07:34:44.450: INFO: Pod "pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016268801s
STEP: Saw pod success
Aug 13 07:34:44.450: INFO: Pod "pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308" satisfied condition "success or failure"
Aug 13 07:34:44.453: INFO: Trying to get logs from node karbon-multi-fourteen-aca02f-k8s-worker-2 pod pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308 container secret-volume-test: <nil>
STEP: delete the pod
Aug 13 07:34:44.474: INFO: Waiting for pod pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308 to disappear
Aug 13 07:34:44.478: INFO: Pod pod-secrets-cf6f98de-bd9c-11e9-8bf8-0a58ac140308 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:34:44.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9340" for this suite.
Aug 13 07:34:50.495: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:34:50.574: INFO: namespace secrets-9340 deletion completed in 6.091337324s

â€¢ [SLOW TEST:10.190 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Aug 13 07:34:50.575: INFO: >>> kubeConfig: /tmp/kubeconfig-872026410
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Aug 13 07:34:50.617: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 13 07:34:50.625: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 13 07:34:55.629: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 13 07:34:55.629: INFO: Creating deployment "test-rolling-update-deployment"
Aug 13 07:34:55.636: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 13 07:34:55.643: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 13 07:34:57.651: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 13 07:34:57.654: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701278495, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701278495, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701278495, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701278495, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-67599b4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 13 07:34:59.657: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Aug 13 07:34:59.666: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6532,SelfLink:/apis/apps/v1/namespaces/deployment-6532/deployments/test-rolling-update-deployment,UID:d87f1ea1-bd9c-11e9-868a-506b8de0bf77,ResourceVersion:25247,Generation:1,CreationTimestamp:2019-08-13 07:34:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-13 07:34:55 +0000 UTC 2019-08-13 07:34:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-13 07:34:58 +0000 UTC 2019-08-13 07:34:55 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 13 07:34:59.670: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-6532,SelfLink:/apis/apps/v1/namespaces/deployment-6532/replicasets/test-rolling-update-deployment-67599b4d9,UID:d8814945-bd9c-11e9-868a-506b8de0bf77,ResourceVersion:25237,Generation:1,CreationTimestamp:2019-08-13 07:34:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d87f1ea1-bd9c-11e9-868a-506b8de0bf77 0xc002bdde50 0xc002bdde51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 13 07:34:59.670: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 13 07:34:59.670: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6532,SelfLink:/apis/apps/v1/namespaces/deployment-6532/replicasets/test-rolling-update-controller,UID:d5826bc5-bd9c-11e9-868a-506b8de0bf77,ResourceVersion:25245,Generation:2,CreationTimestamp:2019-08-13 07:34:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment d87f1ea1-bd9c-11e9-868a-506b8de0bf77 0xc002bddce7 0xc002bddce8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 13 07:34:59.672: INFO: Pod "test-rolling-update-deployment-67599b4d9-bgs2g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-bgs2g,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-6532,SelfLink:/api/v1/namespaces/deployment-6532/pods/test-rolling-update-deployment-67599b4d9-bgs2g,UID:d881d96a-bd9c-11e9-868a-506b8de0bf77,ResourceVersion:25236,Generation:0,CreationTimestamp:2019-08-13 07:34:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 d8814945-bd9c-11e9-868a-506b8de0bf77 0xc00262b440 0xc00262b441}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xrfpv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xrfpv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-xrfpv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:karbon-multi-fourteen-aca02f-k8s-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:34:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:34:58 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:34:58 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-13 07:34:55 +0000 UTC  }],Message:,Reason:,HostIP:10.45.40.139,PodIP:172.20.4.254,StartTime:2019-08-13 07:34:55 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-13 07:34:57 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ceecfe1370b3584def70301221afe97261d2073df3e19a30e719270d999b5152}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Aug 13 07:34:59.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6532" for this suite.
Aug 13 07:35:05.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 13 07:35:05.788: INFO: namespace deployment-6532 deletion completed in 6.112739135s

â€¢ [SLOW TEST:15.213 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSAug 13 07:35:05.789: INFO: Running AfterSuite actions on all nodes
Aug 13 07:35:05.789: INFO: Running AfterSuite actions on node 1
Aug 13 07:35:05.789: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5397.260 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h29m58.635925738s
Test Suite Passed
