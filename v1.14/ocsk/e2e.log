I0913 16:49:22.163253      16 test_context.go:405] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-009922021
I0913 16:49:22.163377      16 e2e.go:240] Starting e2e run "6f07c906-d646-11e9-80a0-429a7732c741" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1568393360 - Will randomize all specs
Will run 204 of 3584 specs

Sep 13 16:49:22.344: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 16:49:22.346: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Sep 13 16:49:22.359: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Sep 13 16:49:22.384: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Sep 13 16:49:22.384: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Sep 13 16:49:22.384: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Sep 13 16:49:22.392: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds' (0 seconds elapsed)
Sep 13 16:49:22.392: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Sep 13 16:49:22.392: INFO: e2e test version: v1.14.0
Sep 13 16:49:22.393: INFO: kube-apiserver version: v1.14.3+0.0.1.el7
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:49:22.393: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
Sep 13 16:49:22.414: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep 13 16:49:22.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-8915'
Sep 13 16:49:22.751: INFO: stderr: ""
Sep 13 16:49:22.751: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 16:49:22.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8915'
Sep 13 16:49:22.833: INFO: stderr: ""
Sep 13 16:49:22.833: INFO: stdout: "update-demo-nautilus-ddvxn update-demo-nautilus-pbvtw "
Sep 13 16:49:22.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-ddvxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8915'
Sep 13 16:49:22.907: INFO: stderr: ""
Sep 13 16:49:22.907: INFO: stdout: ""
Sep 13 16:49:22.907: INFO: update-demo-nautilus-ddvxn is created but not running
Sep 13 16:49:27.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8915'
Sep 13 16:49:27.985: INFO: stderr: ""
Sep 13 16:49:27.985: INFO: stdout: "update-demo-nautilus-ddvxn update-demo-nautilus-pbvtw "
Sep 13 16:49:27.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-ddvxn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8915'
Sep 13 16:49:28.056: INFO: stderr: ""
Sep 13 16:49:28.056: INFO: stdout: "true"
Sep 13 16:49:28.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-ddvxn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8915'
Sep 13 16:49:28.129: INFO: stderr: ""
Sep 13 16:49:28.129: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 16:49:28.129: INFO: validating pod update-demo-nautilus-ddvxn
Sep 13 16:49:28.132: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 16:49:28.132: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 16:49:28.132: INFO: update-demo-nautilus-ddvxn is verified up and running
Sep 13 16:49:28.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-pbvtw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8915'
Sep 13 16:49:28.205: INFO: stderr: ""
Sep 13 16:49:28.205: INFO: stdout: "true"
Sep 13 16:49:28.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-pbvtw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8915'
Sep 13 16:49:28.277: INFO: stderr: ""
Sep 13 16:49:28.277: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 16:49:28.277: INFO: validating pod update-demo-nautilus-pbvtw
Sep 13 16:49:28.281: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 16:49:28.281: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 16:49:28.281: INFO: update-demo-nautilus-pbvtw is verified up and running
STEP: using delete to clean up resources
Sep 13 16:49:28.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-8915'
Sep 13 16:49:28.354: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:49:28.354: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 13 16:49:28.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8915'
Sep 13 16:49:28.431: INFO: stderr: "No resources found.\n"
Sep 13 16:49:28.431: INFO: stdout: ""
Sep 13 16:49:28.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=update-demo --namespace=kubectl-8915 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 16:49:28.504: INFO: stderr: ""
Sep 13 16:49:28.504: INFO: stdout: "update-demo-nautilus-ddvxn\nupdate-demo-nautilus-pbvtw\n"
Sep 13 16:49:29.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8915'
Sep 13 16:49:29.087: INFO: stderr: "No resources found.\n"
Sep 13 16:49:29.087: INFO: stdout: ""
Sep 13 16:49:29.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=update-demo --namespace=kubectl-8915 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 16:49:29.161: INFO: stderr: ""
Sep 13 16:49:29.161: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:49:29.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8915" for this suite.
Sep 13 16:49:35.171: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:49:35.229: INFO: namespace kubectl-8915 deletion completed in 6.065529223s

â€¢ [SLOW TEST:12.836 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:49:35.229: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Sep 13 16:49:44.275: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:49:45.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7096" for this suite.
Sep 13 16:50:07.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:50:07.357: INFO: namespace replicaset-7096 deletion completed in 22.067896899s

â€¢ [SLOW TEST:32.128 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:50:07.357: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-projected-all-test-volume-8aa7462d-d646-11e9-80a0-429a7732c741
STEP: Creating secret with name secret-projected-all-test-volume-8aa74615-d646-11e9-80a0-429a7732c741
STEP: Creating a pod to test Check all projections for projected volume plugin
Sep 13 16:50:07.387: INFO: Waiting up to 5m0s for pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741" in namespace "projected-2961" to be "success or failure"
Sep 13 16:50:07.390: INFO: Pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.413665ms
Sep 13 16:50:09.393: INFO: Pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005391249s
Sep 13 16:50:11.396: INFO: Pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008226114s
Sep 13 16:50:13.399: INFO: Pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011124218s
STEP: Saw pod success
Sep 13 16:50:13.399: INFO: Pod "projected-volume-8aa745df-d646-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 16:50:13.401: INFO: Trying to get logs from node k8s-test-002 pod projected-volume-8aa745df-d646-11e9-80a0-429a7732c741 container projected-all-volume-test: <nil>
STEP: delete the pod
Sep 13 16:50:13.423: INFO: Waiting for pod projected-volume-8aa745df-d646-11e9-80a0-429a7732c741 to disappear
Sep 13 16:50:13.425: INFO: Pod projected-volume-8aa745df-d646-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:50:13.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2961" for this suite.
Sep 13 16:50:19.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:50:19.497: INFO: namespace projected-2961 deletion completed in 6.069645237s

â€¢ [SLOW TEST:12.140 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:50:19.497: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-4438
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep 13 16:50:19.564: INFO: Found 0 stateful pods, waiting for 3
Sep 13 16:50:29.567: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 16:50:29.567: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 16:50:29.567: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Pending - Ready=false
Sep 13 16:50:39.567: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 16:50:39.567: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 16:50:39.567: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 16:50:39.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-4438 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 16:50:39.758: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 16:50:39.758: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 16:50:39.758: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 13 16:50:49.783: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Sep 13 16:50:59.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-4438 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 16:50:59.967: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 16:50:59.967: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 16:50:59.967: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 16:51:09.983: INFO: Waiting for StatefulSet statefulset-4438/ss2 to complete update
Sep 13 16:51:09.983: INFO: Waiting for Pod statefulset-4438/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep 13 16:51:09.983: INFO: Waiting for Pod statefulset-4438/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep 13 16:51:19.989: INFO: Waiting for StatefulSet statefulset-4438/ss2 to complete update
Sep 13 16:51:19.989: INFO: Waiting for Pod statefulset-4438/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Rolling back to a previous revision
Sep 13 16:51:29.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-4438 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 16:51:30.166: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 16:51:30.166: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 16:51:30.166: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 16:51:40.192: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Sep 13 16:51:50.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-4438 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 16:51:50.381: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 16:51:50.381: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 16:51:50.381: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 13 16:52:10.396: INFO: Deleting all statefulset in ns statefulset-4438
Sep 13 16:52:10.399: INFO: Scaling statefulset ss2 to 0
Sep 13 16:52:30.429: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 16:52:30.432: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:52:30.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4438" for this suite.
Sep 13 16:52:36.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:52:36.510: INFO: namespace statefulset-4438 deletion completed in 6.067034845s

â€¢ [SLOW TEST:137.013 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:52:36.510: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 16:52:36.551: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"e39013bc-d646-11e9-ad0a-02001700bd9c", Controller:(*bool)(0xc001b664e6), BlockOwnerDeletion:(*bool)(0xc001b664e7)}}
Sep 13 16:52:36.556: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"e38ebc21-d646-11e9-ad0a-02001700bd9c", Controller:(*bool)(0xc001b666c6), BlockOwnerDeletion:(*bool)(0xc001b666c7)}}
Sep 13 16:52:36.561: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"e38f28ac-d646-11e9-ad0a-02001700bd9c", Controller:(*bool)(0xc001d0ce56), BlockOwnerDeletion:(*bool)(0xc001d0ce57)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:52:41.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8281" for this suite.
Sep 13 16:52:47.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:52:47.637: INFO: namespace gc-8281 deletion completed in 6.06639455s

â€¢ [SLOW TEST:11.127 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:52:47.637: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 16:52:47.659: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:52:49.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5228" for this suite.
Sep 13 16:53:39.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:53:39.750: INFO: namespace pods-5228 deletion completed in 50.065169826s

â€¢ [SLOW TEST:52.114 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:53:39.751: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating all guestbook components
Sep 13 16:53:39.774: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Sep 13 16:53:39.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:39.936: INFO: stderr: ""
Sep 13 16:53:39.936: INFO: stdout: "service/redis-slave created\n"
Sep 13 16:53:39.936: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Sep 13 16:53:39.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:40.120: INFO: stderr: ""
Sep 13 16:53:40.120: INFO: stdout: "service/redis-master created\n"
Sep 13 16:53:40.120: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Sep 13 16:53:40.121: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:40.298: INFO: stderr: ""
Sep 13 16:53:40.298: INFO: stdout: "service/frontend created\n"
Sep 13 16:53:40.299: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Sep 13 16:53:40.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:40.474: INFO: stderr: ""
Sep 13 16:53:40.474: INFO: stdout: "deployment.apps/frontend created\n"
Sep 13 16:53:40.474: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Sep 13 16:53:40.474: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:40.635: INFO: stderr: ""
Sep 13 16:53:40.635: INFO: stdout: "deployment.apps/redis-master created\n"
Sep 13 16:53:40.636: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Sep 13 16:53:40.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-1261'
Sep 13 16:53:40.800: INFO: stderr: ""
Sep 13 16:53:40.800: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Sep 13 16:53:40.800: INFO: Waiting for all frontend pods to be Running.
Sep 13 16:54:10.852: INFO: Waiting for frontend to serve content.
Sep 13 16:54:10.865: INFO: Trying to add a new entry to the guestbook.
Sep 13 16:54:10.878: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Sep 13 16:54:10.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:10.985: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:10.985: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 16:54:10.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:11.134: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:11.134: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 16:54:11.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:11.265: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:11.265: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 16:54:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:11.370: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:11.370: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 16:54:11.371: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:11.455: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:11.455: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Sep 13 16:54:11.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-1261'
Sep 13 16:54:11.548: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 16:54:11.548: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:54:11.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1261" for this suite.
Sep 13 16:54:49.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:54:49.618: INFO: namespace kubectl-1261 deletion completed in 38.067132171s

â€¢ [SLOW TEST:69.867 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:54:49.618: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: executing a command with run --rm and attach with stdin
Sep 13 16:54:49.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 --namespace=kubectl-4756 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Sep 13 16:54:51.014: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Sep 13 16:54:51.014: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:54:53.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4756" for this suite.
Sep 13 16:54:59.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:54:59.089: INFO: namespace kubectl-4756 deletion completed in 6.069118997s

â€¢ [SLOW TEST:9.471 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:54:59.090: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 16:54:59.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741" in namespace "projected-1142" to be "success or failure"
Sep 13 16:54:59.121: INFO: Pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.801291ms
Sep 13 16:55:01.124: INFO: Pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00791746s
Sep 13 16:55:03.127: INFO: Pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010916119s
Sep 13 16:55:05.130: INFO: Pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.013647566s
STEP: Saw pod success
Sep 13 16:55:05.130: INFO: Pod "downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 16:55:05.132: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 16:55:05.147: INFO: Waiting for pod downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741 to disappear
Sep 13 16:55:05.149: INFO: Pod downwardapi-volume-388a1b76-d647-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:55:05.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1142" for this suite.
Sep 13 16:55:11.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:55:11.219: INFO: namespace projected-1142 deletion completed in 6.06746587s

â€¢ [SLOW TEST:12.129 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:55:11.219: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating pod
Sep 13 16:55:17.254: INFO: Pod pod-hostip-3fc50caf-d647-11e9-80a0-429a7732c741 has hostIP: 100.100.230.24
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:55:17.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9008" for this suite.
Sep 13 16:55:39.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:55:39.322: INFO: namespace pods-9008 deletion completed in 22.065502005s

â€¢ [SLOW TEST:28.103 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:55:39.322: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the initial replication controller
Sep 13 16:55:39.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-7898'
Sep 13 16:55:39.501: INFO: stderr: ""
Sep 13 16:55:39.501: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 16:55:39.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7898'
Sep 13 16:55:39.578: INFO: stderr: ""
Sep 13 16:55:39.578: INFO: stdout: "update-demo-nautilus-c956l update-demo-nautilus-gg9h4 "
Sep 13 16:55:39.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-c956l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:55:39.652: INFO: stderr: ""
Sep 13 16:55:39.652: INFO: stdout: ""
Sep 13 16:55:39.652: INFO: update-demo-nautilus-c956l is created but not running
Sep 13 16:55:44.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7898'
Sep 13 16:55:44.727: INFO: stderr: ""
Sep 13 16:55:44.727: INFO: stdout: "update-demo-nautilus-c956l update-demo-nautilus-gg9h4 "
Sep 13 16:55:44.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-c956l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:55:44.799: INFO: stderr: ""
Sep 13 16:55:44.799: INFO: stdout: "true"
Sep 13 16:55:44.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-c956l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:55:44.871: INFO: stderr: ""
Sep 13 16:55:44.871: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 16:55:44.871: INFO: validating pod update-demo-nautilus-c956l
Sep 13 16:55:44.875: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 16:55:44.875: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 16:55:44.875: INFO: update-demo-nautilus-c956l is verified up and running
Sep 13 16:55:44.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-gg9h4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:55:44.947: INFO: stderr: ""
Sep 13 16:55:44.947: INFO: stdout: "true"
Sep 13 16:55:44.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-gg9h4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:55:45.019: INFO: stderr: ""
Sep 13 16:55:45.019: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 16:55:45.019: INFO: validating pod update-demo-nautilus-gg9h4
Sep 13 16:55:45.022: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 16:55:45.022: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 16:55:45.022: INFO: update-demo-nautilus-gg9h4 is verified up and running
STEP: rolling-update to new replication controller
Sep 13 16:55:45.024: INFO: scanned /root for discovery docs: <nil>
Sep 13 16:55:45.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-7898'
Sep 13 16:56:09.387: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 13 16:56:09.387: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 16:56:09.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7898'
Sep 13 16:56:09.462: INFO: stderr: ""
Sep 13 16:56:09.462: INFO: stdout: "update-demo-kitten-bc5lx update-demo-kitten-kc6sc "
Sep 13 16:56:09.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-kitten-bc5lx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:56:09.535: INFO: stderr: ""
Sep 13 16:56:09.535: INFO: stdout: "true"
Sep 13 16:56:09.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-kitten-bc5lx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:56:09.609: INFO: stderr: ""
Sep 13 16:56:09.609: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 13 16:56:09.609: INFO: validating pod update-demo-kitten-bc5lx
Sep 13 16:56:09.613: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 13 16:56:09.613: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 13 16:56:09.613: INFO: update-demo-kitten-bc5lx is verified up and running
Sep 13 16:56:09.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-kitten-kc6sc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:56:09.684: INFO: stderr: ""
Sep 13 16:56:09.684: INFO: stdout: "true"
Sep 13 16:56:09.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-kitten-kc6sc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7898'
Sep 13 16:56:09.755: INFO: stderr: ""
Sep 13 16:56:09.755: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Sep 13 16:56:09.755: INFO: validating pod update-demo-kitten-kc6sc
Sep 13 16:56:09.758: INFO: got data: {
  "image": "kitten.jpg"
}

Sep 13 16:56:09.758: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Sep 13 16:56:09.758: INFO: update-demo-kitten-kc6sc is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:56:09.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7898" for this suite.
Sep 13 16:56:31.768: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:56:31.829: INFO: namespace kubectl-7898 deletion completed in 22.068471616s

â€¢ [SLOW TEST:52.507 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:56:31.830: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-228l
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 16:56:31.866: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-228l" in namespace "subpath-8530" to be "success or failure"
Sep 13 16:56:31.868: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Pending", Reason="", readiness=false. Elapsed: 2.455444ms
Sep 13 16:56:33.871: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 2.005277871s
Sep 13 16:56:35.874: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 4.008078168s
Sep 13 16:56:37.877: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 6.010930511s
Sep 13 16:56:39.879: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 8.013532816s
Sep 13 16:56:41.882: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 10.016309955s
Sep 13 16:56:43.885: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 12.019096649s
Sep 13 16:56:45.888: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 14.021945507s
Sep 13 16:56:47.891: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 16.024921683s
Sep 13 16:56:49.893: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 18.027565365s
Sep 13 16:56:51.896: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Running", Reason="", readiness=true. Elapsed: 20.030367223s
Sep 13 16:56:53.899: INFO: Pod "pod-subpath-test-configmap-228l": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.033309537s
STEP: Saw pod success
Sep 13 16:56:53.899: INFO: Pod "pod-subpath-test-configmap-228l" satisfied condition "success or failure"
Sep 13 16:56:53.901: INFO: Trying to get logs from node k8s-test-002 pod pod-subpath-test-configmap-228l container test-container-subpath-configmap-228l: <nil>
STEP: delete the pod
Sep 13 16:56:53.919: INFO: Waiting for pod pod-subpath-test-configmap-228l to disappear
Sep 13 16:56:53.923: INFO: Pod pod-subpath-test-configmap-228l no longer exists
STEP: Deleting pod pod-subpath-test-configmap-228l
Sep 13 16:56:53.923: INFO: Deleting pod "pod-subpath-test-configmap-228l" in namespace "subpath-8530"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:56:53.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8530" for this suite.
Sep 13 16:56:59.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:56:59.994: INFO: namespace subpath-8530 deletion completed in 6.066595243s

â€¢ [SLOW TEST:28.164 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:56:59.994: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Sep 13 16:57:00.029: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5431,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 13 16:57:00.029: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5432,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 13 16:57:00.029: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5433,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Sep 13 16:57:10.046: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5449,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 13 16:57:10.046: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5450,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Sep 13 16:57:10.046: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-3296,SelfLink:/api/v1/namespaces/watch-3296/configmaps/e2e-watch-test-label-changed,UID:809b299f-d647-11e9-ad0a-02001700bd9c,ResourceVersion:5451,Generation:0,CreationTimestamp:2019-09-13 16:57:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:57:10.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3296" for this suite.
Sep 13 16:57:16.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:57:16.116: INFO: namespace watch-3296 deletion completed in 6.067216522s

â€¢ [SLOW TEST:16.122 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:57:16.116: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 13 16:57:16.143: INFO: Waiting up to 5m0s for pod "pod-8a36ed32-d647-11e9-80a0-429a7732c741" in namespace "emptydir-404" to be "success or failure"
Sep 13 16:57:16.145: INFO: Pod "pod-8a36ed32-d647-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.655099ms
Sep 13 16:57:18.148: INFO: Pod "pod-8a36ed32-d647-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00552121s
STEP: Saw pod success
Sep 13 16:57:18.148: INFO: Pod "pod-8a36ed32-d647-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 16:57:18.150: INFO: Trying to get logs from node k8s-test-002 pod pod-8a36ed32-d647-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 16:57:18.164: INFO: Waiting for pod pod-8a36ed32-d647-11e9-80a0-429a7732c741 to disappear
Sep 13 16:57:18.167: INFO: Pod pod-8a36ed32-d647-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:57:18.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-404" for this suite.
Sep 13 16:57:24.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:57:24.234: INFO: namespace emptydir-404 deletion completed in 6.064502661s

â€¢ [SLOW TEST:8.118 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:57:24.234: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 13 16:57:34.288: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:34.291: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:36.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:36.294: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:38.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:38.294: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:40.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:40.293: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:42.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:42.294: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:44.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:44.294: INFO: Pod pod-with-poststart-http-hook still exists
Sep 13 16:57:46.291: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Sep 13 16:57:46.294: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 16:57:46.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1220" for this suite.
Sep 13 16:58:08.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 16:58:08.361: INFO: namespace container-lifecycle-hook-1220 deletion completed in 22.065339844s

â€¢ [SLOW TEST:44.128 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 16:58:08.362: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Sep 13 16:58:08.634: INFO: Pod name wrapped-volume-race-a96c384f-d647-11e9-80a0-429a7732c741: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a96c384f-d647-11e9-80a0-429a7732c741 in namespace emptydir-wrapper-5740, will wait for the garbage collector to delete the pods
Sep 13 16:58:36.744: INFO: Deleting ReplicationController wrapped-volume-race-a96c384f-d647-11e9-80a0-429a7732c741 took: 5.068324ms
Sep 13 16:58:37.044: INFO: Terminating ReplicationController wrapped-volume-race-a96c384f-d647-11e9-80a0-429a7732c741 pods took: 300.279631ms
STEP: Creating RC which spawns configmap-volume pods
Sep 13 16:59:16.556: INFO: Pod name wrapped-volume-race-d1fb79e8-d647-11e9-80a0-429a7732c741: Found 0 pods out of 5
Sep 13 16:59:21.561: INFO: Pod name wrapped-volume-race-d1fb79e8-d647-11e9-80a0-429a7732c741: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-d1fb79e8-d647-11e9-80a0-429a7732c741 in namespace emptydir-wrapper-5740, will wait for the garbage collector to delete the pods
Sep 13 16:59:31.644: INFO: Deleting ReplicationController wrapped-volume-race-d1fb79e8-d647-11e9-80a0-429a7732c741 took: 4.63607ms
Sep 13 16:59:31.944: INFO: Terminating ReplicationController wrapped-volume-race-d1fb79e8-d647-11e9-80a0-429a7732c741 pods took: 300.199402ms
STEP: Creating RC which spawns configmap-volume pods
Sep 13 17:00:15.956: INFO: Pod name wrapped-volume-race-f563315c-d647-11e9-80a0-429a7732c741: Found 0 pods out of 5
Sep 13 17:00:20.962: INFO: Pod name wrapped-volume-race-f563315c-d647-11e9-80a0-429a7732c741: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f563315c-d647-11e9-80a0-429a7732c741 in namespace emptydir-wrapper-5740, will wait for the garbage collector to delete the pods
Sep 13 17:00:31.043: INFO: Deleting ReplicationController wrapped-volume-race-f563315c-d647-11e9-80a0-429a7732c741 took: 6.827357ms
Sep 13 17:00:31.343: INFO: Terminating ReplicationController wrapped-volume-race-f563315c-d647-11e9-80a0-429a7732c741 pods took: 300.233575ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:01:06.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-5740" for this suite.
Sep 13 17:01:12.527: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:01:12.591: INFO: namespace emptydir-wrapper-5740 deletion completed in 6.070995028s

â€¢ [SLOW TEST:184.230 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:01:12.592: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-4432
Sep 13 17:01:18.625: INFO: Started pod liveness-http in namespace container-probe-4432
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 17:01:18.627: INFO: Initial restart count of pod liveness-http is 0
Sep 13 17:01:40.663: INFO: Restart count of pod container-probe-4432/liveness-http is now 1 (22.036145915s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:01:40.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4432" for this suite.
Sep 13 17:01:46.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:01:46.740: INFO: namespace container-probe-4432 deletion completed in 6.067756444s

â€¢ [SLOW TEST:34.149 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:01:46.741: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 13 17:01:49.289: INFO: Successfully updated pod "labelsupdate2b850020-d648-11e9-80a0-429a7732c741"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:01:53.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1572" for this suite.
Sep 13 17:02:15.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:02:15.377: INFO: namespace projected-1572 deletion completed in 22.065163024s

â€¢ [SLOW TEST:28.636 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:02:15.377: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:02:15.400: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Sep 13 17:02:15.405: INFO: Pod name sample-pod: Found 0 pods out of 1
Sep 13 17:02:20.408: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 17:02:20.408: INFO: Creating deployment "test-rolling-update-deployment"
Sep 13 17:02:20.411: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Sep 13 17:02:20.416: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Sep 13 17:02:22.421: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Sep 13 17:02:22.423: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 13 17:02:22.430: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2126,SelfLink:/apis/apps/v1/namespaces/deployment-2126/deployments/test-rolling-update-deployment,UID:3f92ed3b-d648-11e9-ad0a-02001700bd9c,ResourceVersion:6897,Generation:1,CreationTimestamp:2019-09-13 17:02:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-13 17:02:20 +0000 UTC 2019-09-13 17:02:20 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-13 17:02:22 +0000 UTC 2019-09-13 17:02:20 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-67599b4d9" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 17:02:22.433: INFO: New ReplicaSet "test-rolling-update-deployment-67599b4d9" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9,GenerateName:,Namespace:deployment-2126,SelfLink:/apis/apps/v1/namespaces/deployment-2126/replicasets/test-rolling-update-deployment-67599b4d9,UID:3f949beb-d648-11e9-ad0a-02001700bd9c,ResourceVersion:6886,Generation:1,CreationTimestamp:2019-09-13 17:02:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3f92ed3b-d648-11e9-ad0a-02001700bd9c 0xc001736000 0xc001736001}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 13 17:02:22.433: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Sep 13 17:02:22.433: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2126,SelfLink:/apis/apps/v1/namespaces/deployment-2126/replicasets/test-rolling-update-controller,UID:3c96b2fc-d648-11e9-ad0a-02001700bd9c,ResourceVersion:6896,Generation:2,CreationTimestamp:2019-09-13 17:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 3f92ed3b-d648-11e9-ad0a-02001700bd9c 0xc001989f2f 0xc001989f40}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 17:02:22.435: INFO: Pod "test-rolling-update-deployment-67599b4d9-hfz6s" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-67599b4d9-hfz6s,GenerateName:test-rolling-update-deployment-67599b4d9-,Namespace:deployment-2126,SelfLink:/api/v1/namespaces/deployment-2126/pods/test-rolling-update-deployment-67599b4d9-hfz6s,UID:3f95006e-d648-11e9-ad0a-02001700bd9c,ResourceVersion:6885,Generation:0,CreationTimestamp:2019-09-13 17:02:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 67599b4d9,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-67599b4d9 3f949beb-d648-11e9-ad0a-02001700bd9c 0xc001736db0 0xc001736db1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-84jxm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-84jxm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-84jxm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001736eb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001736f00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:02:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:02:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:02:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:02:20 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.40,StartTime:2019-09-13 17:02:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-13 17:02:21 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://504510e09cd717d6487a8a4297abcba70b187731aa4b1142a6573ec08567fb0a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:02:22.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2126" for this suite.
Sep 13 17:02:28.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:02:28.513: INFO: namespace deployment-2126 deletion completed in 6.074662241s

â€¢ [SLOW TEST:13.136 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:02:28.513: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7851.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7851.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 69.133.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.133.69_udp@PTR;check="$$(dig +tcp +noall +answer +search 69.133.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.133.69_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7851.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7851.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7851.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7851.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7851.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 69.133.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.133.69_udp@PTR;check="$$(dig +tcp +noall +answer +search 69.133.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.133.69_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 17:02:52.570: INFO: Unable to read wheezy_udp@dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.572: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.575: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.578: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.602: INFO: Unable to read jessie_tcp@dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.605: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local from pod dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741: the server could not find the requested resource (get pods dns-test-446cac5d-d648-11e9-80a0-429a7732c741)
Sep 13 17:02:52.627: INFO: Lookups using dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741 failed for: [wheezy_udp@dns-test-service.dns-7851.svc.cluster.local wheezy_tcp@dns-test-service.dns-7851.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local jessie_tcp@dns-test-service.dns-7851.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7851.svc.cluster.local]

Sep 13 17:02:57.684: INFO: DNS probes using dns-7851/dns-test-446cac5d-d648-11e9-80a0-429a7732c741 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:02:57.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7851" for this suite.
Sep 13 17:03:03.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:03:03.805: INFO: namespace dns-7851 deletion completed in 6.067572781s

â€¢ [SLOW TEST:35.292 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:03:03.805: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on tmpfs
Sep 13 17:03:03.831: INFO: Waiting up to 5m0s for pod "pod-5973d883-d648-11e9-80a0-429a7732c741" in namespace "emptydir-7689" to be "success or failure"
Sep 13 17:03:03.833: INFO: Pod "pod-5973d883-d648-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193352ms
Sep 13 17:03:05.836: INFO: Pod "pod-5973d883-d648-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005095703s
Sep 13 17:03:07.839: INFO: Pod "pod-5973d883-d648-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00802158s
Sep 13 17:03:09.842: INFO: Pod "pod-5973d883-d648-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010948917s
STEP: Saw pod success
Sep 13 17:03:09.842: INFO: Pod "pod-5973d883-d648-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:03:09.844: INFO: Trying to get logs from node k8s-test-002 pod pod-5973d883-d648-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:03:09.858: INFO: Waiting for pod pod-5973d883-d648-11e9-80a0-429a7732c741 to disappear
Sep 13 17:03:09.860: INFO: Pod pod-5973d883-d648-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:03:09.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7689" for this suite.
Sep 13 17:03:15.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:03:15.928: INFO: namespace emptydir-7689 deletion completed in 6.066026056s

â€¢ [SLOW TEST:12.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:03:15.928: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:03:15.963: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Sep 13 17:03:15.968: INFO: Number of nodes with available pods: 0
Sep 13 17:03:15.968: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Sep 13 17:03:15.981: INFO: Number of nodes with available pods: 0
Sep 13 17:03:15.982: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:16.985: INFO: Number of nodes with available pods: 0
Sep 13 17:03:16.985: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:17.984: INFO: Number of nodes with available pods: 1
Sep 13 17:03:17.984: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Sep 13 17:03:18.000: INFO: Number of nodes with available pods: 0
Sep 13 17:03:18.000: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Sep 13 17:03:18.009: INFO: Number of nodes with available pods: 0
Sep 13 17:03:18.009: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:19.012: INFO: Number of nodes with available pods: 0
Sep 13 17:03:19.012: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:20.012: INFO: Number of nodes with available pods: 0
Sep 13 17:03:20.012: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:21.012: INFO: Number of nodes with available pods: 0
Sep 13 17:03:21.012: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:22.012: INFO: Number of nodes with available pods: 0
Sep 13 17:03:22.012: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:23.012: INFO: Number of nodes with available pods: 0
Sep 13 17:03:23.012: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:03:24.011: INFO: Number of nodes with available pods: 1
Sep 13 17:03:24.012: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6174, will wait for the garbage collector to delete the pods
Sep 13 17:03:24.072: INFO: Deleting DaemonSet.extensions daemon-set took: 4.030563ms
Sep 13 17:03:24.372: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.205738ms
Sep 13 17:03:28.074: INFO: Number of nodes with available pods: 0
Sep 13 17:03:28.074: INFO: Number of running nodes: 0, number of available pods: 0
Sep 13 17:03:28.079: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6174/daemonsets","resourceVersion":"7149"},"items":null}

Sep 13 17:03:28.082: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6174/pods","resourceVersion":"7149"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:03:28.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6174" for this suite.
Sep 13 17:03:34.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:03:34.165: INFO: namespace daemonsets-6174 deletion completed in 6.068797082s

â€¢ [SLOW TEST:18.236 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:03:34.165: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating replication controller my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741
Sep 13 17:03:34.192: INFO: Pod name my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741: Found 0 pods out of 1
Sep 13 17:03:39.195: INFO: Pod name my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741: Found 1 pods out of 1
Sep 13 17:03:39.195: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741" are running
Sep 13 17:03:41.200: INFO: Pod "my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741-brtm5" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:03:34 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:03:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:03:34 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:03:34 +0000 UTC Reason: Message:}])
Sep 13 17:03:41.200: INFO: Trying to dial the pod
Sep 13 17:03:46.208: INFO: Controller my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741: Got expected result from replica 1 [my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741-brtm5]: "my-hostname-basic-6b8c6e0c-d648-11e9-80a0-429a7732c741-brtm5", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:03:46.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6113" for this suite.
Sep 13 17:03:52.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:03:52.278: INFO: namespace replication-controller-6113 deletion completed in 6.06707216s

â€¢ [SLOW TEST:18.113 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:03:52.278: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3154
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 17:03:52.300: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 13 17:04:16.351: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.88.1.45:8080/dial?request=hostName&protocol=udp&host=10.88.0.34&port=8081&tries=1'] Namespace:pod-network-test-3154 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:04:16.351: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:04:16.462: INFO: Waiting for endpoints: map[]
Sep 13 17:04:16.465: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.88.1.45:8080/dial?request=hostName&protocol=udp&host=10.88.1.44&port=8081&tries=1'] Namespace:pod-network-test-3154 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:04:16.465: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:04:16.577: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:04:16.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3154" for this suite.
Sep 13 17:04:38.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:04:38.646: INFO: namespace pod-network-test-3154 deletion completed in 22.06612858s

â€¢ [SLOW TEST:46.368 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:04:38.646: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:04:38.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3483" for this suite.
Sep 13 17:04:44.681: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:04:44.739: INFO: namespace services-3483 deletion completed in 6.065104294s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:6.092 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide secure master service  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:04:44.739: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:04:44.766: INFO: Pod name rollover-pod: Found 0 pods out of 1
Sep 13 17:04:49.769: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 17:04:49.769: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Sep 13 17:04:51.772: INFO: Creating deployment "test-rollover-deployment"
Sep 13 17:04:51.777: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Sep 13 17:04:53.782: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Sep 13 17:04:53.786: INFO: Ensure that both replica sets have 1 created replica
Sep 13 17:04:53.790: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Sep 13 17:04:53.795: INFO: Updating deployment test-rollover-deployment
Sep 13 17:04:53.795: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Sep 13 17:04:55.800: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Sep 13 17:04:55.804: INFO: Make sure deployment "test-rollover-deployment" is complete
Sep 13 17:04:55.809: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:04:55.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991093, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:04:57.814: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:04:57.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991095, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:04:59.814: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:04:59.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991095, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:05:01.814: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:05:01.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991095, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:05:03.814: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:05:03.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991095, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:05:05.814: INFO: all replica sets need to contain the pod-template-hash label
Sep 13 17:05:05.814: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991095, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703991091, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-766b4d6c9d\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:05:07.814: INFO: 
Sep 13 17:05:07.814: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 13 17:05:07.821: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-22,SelfLink:/apis/apps/v1/namespaces/deployment-22/deployments/test-rollover-deployment,UID:99cb2a75-d648-11e9-ad0a-02001700bd9c,ResourceVersion:7491,Generation:2,CreationTimestamp:2019-09-13 17:04:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-13 17:04:51 +0000 UTC 2019-09-13 17:04:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-13 17:05:05 +0000 UTC 2019-09-13 17:04:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-766b4d6c9d" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 17:05:07.823: INFO: New ReplicaSet "test-rollover-deployment-766b4d6c9d" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d,GenerateName:,Namespace:deployment-22,SelfLink:/apis/apps/v1/namespaces/deployment-22/replicasets/test-rollover-deployment-766b4d6c9d,UID:9afff215-d648-11e9-ad0a-02001700bd9c,ResourceVersion:7481,Generation:2,CreationTimestamp:2019-09-13 17:04:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99cb2a75-d648-11e9-ad0a-02001700bd9c 0xc002ccdbe7 0xc002ccdbe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 13 17:05:07.823: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Sep 13 17:05:07.823: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-22,SelfLink:/apis/apps/v1/namespaces/deployment-22/replicasets/test-rollover-controller,UID:959d49c8-d648-11e9-ad0a-02001700bd9c,ResourceVersion:7490,Generation:2,CreationTimestamp:2019-09-13 17:04:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99cb2a75-d648-11e9-ad0a-02001700bd9c 0xc002ccda3f 0xc002ccda50}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 17:05:07.823: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-6455657675,GenerateName:,Namespace:deployment-22,SelfLink:/apis/apps/v1/namespaces/deployment-22/replicasets/test-rollover-deployment-6455657675,UID:99cc7e36-d648-11e9-ad0a-02001700bd9c,ResourceVersion:7454,Generation:2,CreationTimestamp:2019-09-13 17:04:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 99cb2a75-d648-11e9-ad0a-02001700bd9c 0xc002ccdb17 0xc002ccdb18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 6455657675,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 17:05:07.826: INFO: Pod "test-rollover-deployment-766b4d6c9d-74vrc" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-766b4d6c9d-74vrc,GenerateName:test-rollover-deployment-766b4d6c9d-,Namespace:deployment-22,SelfLink:/api/v1/namespaces/deployment-22/pods/test-rollover-deployment-766b4d6c9d-74vrc,UID:9b02b9c5-d648-11e9-ad0a-02001700bd9c,ResourceVersion:7465,Generation:0,CreationTimestamp:2019-09-13 17:04:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 766b4d6c9d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-766b4d6c9d 9afff215-d648-11e9-ad0a-02001700bd9c 0xc00236c727 0xc00236c728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-txfj9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-txfj9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-txfj9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00236c7b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00236c7d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:04:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:04:55 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:04:55 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:04:53 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.48,StartTime:2019-09-13 17:04:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-13 17:04:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://7fd97c71430d877e785d354eb1d25752cef5f761fd9d2a6bd0ec7ede7d5dde0a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:05:07.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-22" for this suite.
Sep 13 17:05:13.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:05:13.895: INFO: namespace deployment-22 deletion completed in 6.06647481s

â€¢ [SLOW TEST:29.156 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support rollover [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:05:13.895: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-a6fe0a1e-d648-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:05:13.924: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741" in namespace "projected-3504" to be "success or failure"
Sep 13 17:05:13.926: INFO: Pod "pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.17633ms
Sep 13 17:05:15.929: INFO: Pod "pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00508918s
STEP: Saw pod success
Sep 13 17:05:15.929: INFO: Pod "pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:05:15.931: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:05:15.944: INFO: Waiting for pod pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741 to disappear
Sep 13 17:05:15.947: INFO: Pod pod-projected-secrets-a6fe6e73-d648-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:05:15.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3504" for this suite.
Sep 13 17:05:21.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:05:22.017: INFO: namespace projected-3504 deletion completed in 6.067664533s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:05:22.017: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-1179
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating stateful set ss in namespace statefulset-1179
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1179
Sep 13 17:05:22.050: INFO: Found 0 stateful pods, waiting for 1
Sep 13 17:05:32.057: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Sep 13 17:05:32.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:05:32.248: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:05:32.248: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:05:32.248: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:05:32.251: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 13 17:05:42.254: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:05:42.254: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:05:42.264: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:05:42.264: INFO: ss-0  k8s-test-002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:05:42.264: INFO: 
Sep 13 17:05:42.264: INFO: StatefulSet ss has not reached scale 3, at 1
Sep 13 17:05:43.267: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997426143s
Sep 13 17:05:44.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993866758s
Sep 13 17:05:45.274: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990424451s
Sep 13 17:05:46.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987469268s
Sep 13 17:05:47.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.984447792s
Sep 13 17:05:48.283: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981191104s
Sep 13 17:05:49.286: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978097411s
Sep 13 17:05:50.289: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975192737s
Sep 13 17:05:51.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 972.111717ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1179
Sep 13 17:05:52.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:05:52.468: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 17:05:52.468: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:05:52.468: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:05:52.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:05:52.655: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 13 17:05:52.655: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:05:52.655: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:05:52.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:05:52.838: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Sep 13 17:05:52.838: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:05:52.838: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:05:52.842: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Sep 13 17:06:02.845: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:06:02.845: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:06:02.845: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Sep 13 17:06:02.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:06:03.026: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:06:03.026: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:06:03.026: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:06:03.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:06:03.217: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:06:03.217: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:06:03.217: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:06:03.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:06:03.400: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:06:03.400: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:06:03.400: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:06:03.400: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:06:03.402: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Sep 13 17:06:13.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:06:13.407: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:06:13.408: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:06:13.416: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:13.416: INFO: ss-0  k8s-test-002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:13.416: INFO: ss-1  k8s-test-001  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:13.416: INFO: ss-2  k8s-test-002  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:13.416: INFO: 
Sep 13 17:06:13.416: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 13 17:06:14.419: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:14.419: INFO: ss-0  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:14.419: INFO: ss-1  k8s-test-001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:14.419: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:14.419: INFO: 
Sep 13 17:06:14.419: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 13 17:06:15.422: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:15.422: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:15.423: INFO: ss-1  k8s-test-001  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:15.423: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:15.423: INFO: 
Sep 13 17:06:15.423: INFO: StatefulSet ss has not reached scale 0, at 3
Sep 13 17:06:16.426: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:16.426: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:16.426: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:16.426: INFO: 
Sep 13 17:06:16.426: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:17.429: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:17.429: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:17.429: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:17.429: INFO: 
Sep 13 17:06:17.429: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:18.432: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:18.432: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:18.432: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:18.432: INFO: 
Sep 13 17:06:18.432: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:19.435: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:19.435: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:19.435: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:19.435: INFO: 
Sep 13 17:06:19.435: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:20.438: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:20.438: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:20.438: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:20.438: INFO: 
Sep 13 17:06:20.438: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:21.441: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:21.441: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:21.442: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:21.442: INFO: 
Sep 13 17:06:21.442: INFO: StatefulSet ss has not reached scale 0, at 2
Sep 13 17:06:22.445: INFO: POD   NODE          PHASE    GRACE  CONDITIONS
Sep 13 17:06:22.445: INFO: ss-0  k8s-test-002  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:02 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:22 +0000 UTC  }]
Sep 13 17:06:22.445: INFO: ss-2  k8s-test-002  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:06:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:05:42 +0000 UTC  }]
Sep 13 17:06:22.445: INFO: 
Sep 13 17:06:22.445: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1179
Sep 13 17:06:23.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:06:23.545: INFO: rc: 1
Sep 13 17:06:23.545: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001ef1650 exit status 1 <nil> <nil> true [0xc0023da2c8 0xc0023da2e0 0xc0023da2f8] [0xc0023da2c8 0xc0023da2e0 0xc0023da2f8] [0xc0023da2d8 0xc0023da2f0] [0x9bf9f0 0x9bf9f0] 0xc002623ec0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1

Sep 13 17:06:33.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:06:33.613: INFO: rc: 1
Sep 13 17:06:33.613: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0018e1d10 exit status 1 <nil> <nil> true [0xc002ff6250 0xc002ff6268 0xc002ff6280] [0xc002ff6250 0xc002ff6268 0xc002ff6280] [0xc002ff6260 0xc002ff6278] [0x9bf9f0 0x9bf9f0] 0xc0014b4180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:06:43.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:06:43.679: INFO: rc: 1
Sep 13 17:06:43.680: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ef19b0 exit status 1 <nil> <nil> true [0xc0023da300 0xc0023da318 0xc0023da330] [0xc0023da300 0xc0023da318 0xc0023da330] [0xc0023da310 0xc0023da328] [0x9bf9f0 0x9bf9f0] 0xc002498240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:06:53.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:06:53.751: INFO: rc: 1
Sep 13 17:06:53.751: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000ff26f0 exit status 1 <nil> <nil> true [0xc000f4bb70 0xc000f4bd18 0xc000f4bf80] [0xc000f4bb70 0xc000f4bd18 0xc000f4bf80] [0xc000f4bc30 0xc000f4bed0] [0x9bf9f0 0x9bf9f0] 0xc0024c3500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:03.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:03.818: INFO: rc: 1
Sep 13 17:07:03.818: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0010140c0 exit status 1 <nil> <nil> true [0xc002ff6288 0xc002ff62a0 0xc002ff62b8] [0xc002ff6288 0xc002ff62a0 0xc002ff62b8] [0xc002ff6298 0xc002ff62b0] [0x9bf9f0 0x9bf9f0] 0xc0014b44e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:13.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:13.890: INFO: rc: 1
Sep 13 17:07:13.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000fbc060 exit status 1 <nil> <nil> true [0xc0017f4110 0xc0017f4128 0xc0017f4140] [0xc0017f4110 0xc0017f4128 0xc0017f4140] [0xc0017f4120 0xc0017f4138] [0x9bf9f0 0x9bf9f0] 0xc0029784e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:23.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:23.956: INFO: rc: 1
Sep 13 17:07:23.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001014420 exit status 1 <nil> <nil> true [0xc002ff62c0 0xc002ff62d8 0xc002ff62f0] [0xc002ff62c0 0xc002ff62d8 0xc002ff62f0] [0xc002ff62d0 0xc002ff62e8] [0x9bf9f0 0x9bf9f0] 0xc0014b4840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:33.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:34.023: INFO: rc: 1
Sep 13 17:07:34.023: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265a300 exit status 1 <nil> <nil> true [0xc002ff6008 0xc002ff6020 0xc002ff6038] [0xc002ff6008 0xc002ff6020 0xc002ff6038] [0xc002ff6018 0xc002ff6030] [0x9bf9f0 0x9bf9f0] 0xc0030682a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:44.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:44.090: INFO: rc: 1
Sep 13 17:07:44.090: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265a630 exit status 1 <nil> <nil> true [0xc002ff6040 0xc002ff6058 0xc002ff6070] [0xc002ff6040 0xc002ff6058 0xc002ff6070] [0xc002ff6050 0xc002ff6068] [0x9bf9f0 0x9bf9f0] 0xc003068600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:07:54.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:07:54.159: INFO: rc: 1
Sep 13 17:07:54.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022c6330 exit status 1 <nil> <nil> true [0xc0017f4000 0xc0017f4018 0xc0017f4030] [0xc0017f4000 0xc0017f4018 0xc0017f4030] [0xc0017f4010 0xc0017f4028] [0x9bf9f0 0x9bf9f0] 0xc0026225a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:04.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:04.226: INFO: rc: 1
Sep 13 17:08:04.226: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265a990 exit status 1 <nil> <nil> true [0xc002ff6078 0xc002ff6090 0xc002ff60a8] [0xc002ff6078 0xc002ff6090 0xc002ff60a8] [0xc002ff6088 0xc002ff60a0] [0x9bf9f0 0x9bf9f0] 0xc003068c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:14.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:14.292: INFO: rc: 1
Sep 13 17:08:14.293: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8330 exit status 1 <nil> <nil> true [0xc000f4a050 0xc000f4a2c0 0xc000f4a560] [0xc000f4a050 0xc000f4a2c0 0xc000f4a560] [0xc000f4a1e0 0xc000f4a420] [0x9bf9f0 0x9bf9f0] 0xc0029345a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:24.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:24.360: INFO: rc: 1
Sep 13 17:08:24.360: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265acf0 exit status 1 <nil> <nil> true [0xc002ff60b0 0xc002ff60c8 0xc002ff60e0] [0xc002ff60b0 0xc002ff60c8 0xc002ff60e0] [0xc002ff60c0 0xc002ff60d8] [0x9bf9f0 0x9bf9f0] 0xc0030692c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:34.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:34.425: INFO: rc: 1
Sep 13 17:08:34.425: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8660 exit status 1 <nil> <nil> true [0xc000f4a6c8 0xc000f4a850 0xc000f4a940] [0xc000f4a6c8 0xc000f4a850 0xc000f4a940] [0xc000f4a808 0xc000f4a938] [0x9bf9f0 0x9bf9f0] 0xc002934b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:44.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:44.492: INFO: rc: 1
Sep 13 17:08:44.492: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265b050 exit status 1 <nil> <nil> true [0xc002ff60e8 0xc002ff6100 0xc002ff6118] [0xc002ff60e8 0xc002ff6100 0xc002ff6118] [0xc002ff60f8 0xc002ff6110] [0x9bf9f0 0x9bf9f0] 0xc003069980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:08:54.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:08:54.557: INFO: rc: 1
Sep 13 17:08:54.557: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265b6b0 exit status 1 <nil> <nil> true [0xc002ff6120 0xc002ff6138 0xc002ff6150] [0xc002ff6120 0xc002ff6138 0xc002ff6150] [0xc002ff6130 0xc002ff6148] [0x9bf9f0 0x9bf9f0] 0xc0028fc060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:04.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:04.624: INFO: rc: 1
Sep 13 17:09:04.624: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022c67e0 exit status 1 <nil> <nil> true [0xc0017f4038 0xc0017f4050 0xc0017f4070] [0xc0017f4038 0xc0017f4050 0xc0017f4070] [0xc0017f4048 0xc0017f4068] [0x9bf9f0 0x9bf9f0] 0xc002622c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:14.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:14.691: INFO: rc: 1
Sep 13 17:09:14.691: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8b40 exit status 1 <nil> <nil> true [0xc000f4aa50 0xc000f4ae48 0xc000f4b2d8] [0xc000f4aa50 0xc000f4ae48 0xc000f4b2d8] [0xc000f4ad98 0xc000f4b008] [0x9bf9f0 0x9bf9f0] 0xc002934ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:24.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:24.758: INFO: rc: 1
Sep 13 17:09:24.758: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8ea0 exit status 1 <nil> <nil> true [0xc000f4b358 0xc000f4b5b0 0xc000f4b878] [0xc000f4b358 0xc000f4b5b0 0xc000f4b878] [0xc000f4b538 0xc000f4b830] [0x9bf9f0 0x9bf9f0] 0xc0029352c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:34.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:34.831: INFO: rc: 1
Sep 13 17:09:34.831: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265ba70 exit status 1 <nil> <nil> true [0xc002ff6160 0xc002ff6178 0xc002ff6190] [0xc002ff6160 0xc002ff6178 0xc002ff6190] [0xc002ff6170 0xc002ff6188] [0x9bf9f0 0x9bf9f0] 0xc0028fc660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:44.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:44.898: INFO: rc: 1
Sep 13 17:09:44.898: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8360 exit status 1 <nil> <nil> true [0xc000f4a050 0xc000f4a2c0 0xc000f4a560] [0xc000f4a050 0xc000f4a2c0 0xc000f4a560] [0xc000f4a1e0 0xc000f4a420] [0x9bf9f0 0x9bf9f0] 0xc0030682a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:09:54.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:09:54.964: INFO: rc: 1
Sep 13 17:09:54.964: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265a330 exit status 1 <nil> <nil> true [0xc0017f4000 0xc0017f4018 0xc0017f4030] [0xc0017f4000 0xc0017f4018 0xc0017f4030] [0xc0017f4010 0xc0017f4028] [0x9bf9f0 0x9bf9f0] 0xc0029345a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:04.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:05.031: INFO: rc: 1
Sep 13 17:10:05.031: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001678300 exit status 1 <nil> <nil> true [0xc002ff6000 0xc002ff6018 0xc002ff6030] [0xc002ff6000 0xc002ff6018 0xc002ff6030] [0xc002ff6010 0xc002ff6028] [0x9bf9f0 0x9bf9f0] 0xc0026225a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:15.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:15.104: INFO: rc: 1
Sep 13 17:10:15.104: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265a6c0 exit status 1 <nil> <nil> true [0xc0017f4038 0xc0017f4050 0xc0017f4070] [0xc0017f4038 0xc0017f4050 0xc0017f4070] [0xc0017f4048 0xc0017f4068] [0x9bf9f0 0x9bf9f0] 0xc002934b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:25.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:25.170: INFO: rc: 1
Sep 13 17:10:25.170: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022c64b0 exit status 1 <nil> <nil> true [0xc0023da000 0xc0023da018 0xc0023da030] [0xc0023da000 0xc0023da018 0xc0023da030] [0xc0023da010 0xc0023da028] [0x9bf9f0 0x9bf9f0] 0xc0028fcba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:35.171: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:35.237: INFO: rc: 1
Sep 13 17:10:35.237: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022c6840 exit status 1 <nil> <nil> true [0xc0023da038 0xc0023da050 0xc0023da068] [0xc0023da038 0xc0023da050 0xc0023da068] [0xc0023da048 0xc0023da060] [0x9bf9f0 0x9bf9f0] 0xc0028fd200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:45.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:45.306: INFO: rc: 1
Sep 13 17:10:45.306: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8720 exit status 1 <nil> <nil> true [0xc000f4a6c8 0xc000f4a850 0xc000f4a940] [0xc000f4a6c8 0xc000f4a850 0xc000f4a940] [0xc000f4a808 0xc000f4a938] [0x9bf9f0 0x9bf9f0] 0xc003068600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:10:55.306: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:10:55.375: INFO: rc: 1
Sep 13 17:10:55.376: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001ff8bd0 exit status 1 <nil> <nil> true [0xc000f4aa50 0xc000f4ae48 0xc000f4b2d8] [0xc000f4aa50 0xc000f4ae48 0xc000f4b2d8] [0xc000f4ad98 0xc000f4b008] [0x9bf9f0 0x9bf9f0] 0xc003068c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:11:05.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:11:05.447: INFO: rc: 1
Sep 13 17:11:05.447: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265aa20 exit status 1 <nil> <nil> true [0xc0017f4078 0xc0017f4090 0xc0017f40b0] [0xc0017f4078 0xc0017f4090 0xc0017f40b0] [0xc0017f4088 0xc0017f40a0] [0x9bf9f0 0x9bf9f0] 0xc002934ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:11:15.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:11:15.516: INFO: rc: 1
Sep 13 17:11:15.516: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00265adb0 exit status 1 <nil> <nil> true [0xc0017f40b8 0xc0017f40d0 0xc0017f40e8] [0xc0017f40b8 0xc0017f40d0 0xc0017f40e8] [0xc0017f40c8 0xc0017f40e0] [0x9bf9f0 0x9bf9f0] 0xc0029352c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1

Sep 13 17:11:25.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-1179 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:11:25.584: INFO: rc: 1
Sep 13 17:11:25.585: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Sep 13 17:11:25.585: INFO: Scaling statefulset ss to 0
Sep 13 17:11:25.592: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 13 17:11:25.594: INFO: Deleting all statefulset in ns statefulset-1179
Sep 13 17:11:25.596: INFO: Scaling statefulset ss to 0
Sep 13 17:11:25.602: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:11:25.604: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:11:25.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1179" for this suite.
Sep 13 17:11:31.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:11:31.688: INFO: namespace statefulset-1179 deletion completed in 6.072904524s

â€¢ [SLOW TEST:369.671 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:11:31.689: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating server pod server in namespace prestop-5151
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5151
STEP: Deleting pre-stop pod
Sep 13 17:11:46.738: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:11:46.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5151" for this suite.
Sep 13 17:12:26.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:12:26.813: INFO: namespace prestop-5151 deletion completed in 40.068286752s

â€¢ [SLOW TEST:55.125 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:12:26.814: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-gm2f7 in namespace proxy-9272
I0913 17:12:26.849076      16 runners.go:184] Created replication controller with name: proxy-service-gm2f7, namespace: proxy-9272, replica count: 1
I0913 17:12:27.899469      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 17:12:28.899621      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 17:12:29.899818      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 17:12:30.899981      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 17:12:31.900188      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:32.900377      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:33.900543      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:34.900748      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:35.900921      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:36.901099      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:37.901260      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:38.901426      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0913 17:12:39.901617      16 runners.go:184] proxy-service-gm2f7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 17:12:39.904: INFO: setup took 13.067757875s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Sep 13 17:12:39.909: INFO: (0) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.709417ms)
Sep 13 17:12:39.910: INFO: (0) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.684401ms)
Sep 13 17:12:39.910: INFO: (0) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.846366ms)
Sep 13 17:12:39.910: INFO: (0) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.530001ms)
Sep 13 17:12:39.912: INFO: (0) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.969754ms)
Sep 13 17:12:39.912: INFO: (0) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.077487ms)
Sep 13 17:12:39.919: INFO: (0) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 14.557073ms)
Sep 13 17:12:39.919: INFO: (0) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 14.781173ms)
Sep 13 17:12:39.921: INFO: (0) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 17.466679ms)
Sep 13 17:12:39.925: INFO: (0) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 20.689616ms)
Sep 13 17:12:39.925: INFO: (0) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 20.832033ms)
Sep 13 17:12:39.928: INFO: (0) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 23.860703ms)
Sep 13 17:12:39.928: INFO: (0) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 24.21015ms)
Sep 13 17:12:39.928: INFO: (0) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 24.101256ms)
Sep 13 17:12:39.928: INFO: (0) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 23.98099ms)
Sep 13 17:12:39.931: INFO: (0) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 27.122142ms)
Sep 13 17:12:39.936: INFO: (1) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.066459ms)
Sep 13 17:12:39.938: INFO: (1) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 6.937786ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 8.047912ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 8.012575ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 8.165583ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.24411ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 8.054083ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 8.258427ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.256413ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 8.197944ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 8.494111ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 8.25953ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 8.409862ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.927265ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 8.888031ms)
Sep 13 17:12:39.940: INFO: (1) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.987718ms)
Sep 13 17:12:39.945: INFO: (2) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 4.33717ms)
Sep 13 17:12:39.945: INFO: (2) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 4.682899ms)
Sep 13 17:12:39.945: INFO: (2) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 4.858129ms)
Sep 13 17:12:39.945: INFO: (2) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 4.999063ms)
Sep 13 17:12:39.945: INFO: (2) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 4.849712ms)
Sep 13 17:12:39.946: INFO: (2) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 5.161248ms)
Sep 13 17:12:39.946: INFO: (2) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 5.214067ms)
Sep 13 17:12:39.946: INFO: (2) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.45492ms)
Sep 13 17:12:39.946: INFO: (2) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.533747ms)
Sep 13 17:12:39.948: INFO: (2) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 7.522084ms)
Sep 13 17:12:39.948: INFO: (2) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.298644ms)
Sep 13 17:12:39.948: INFO: (2) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.722811ms)
Sep 13 17:12:39.948: INFO: (2) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 7.995753ms)
Sep 13 17:12:39.949: INFO: (2) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.973592ms)
Sep 13 17:12:39.949: INFO: (2) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.996255ms)
Sep 13 17:12:39.949: INFO: (2) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.088508ms)
Sep 13 17:12:39.955: INFO: (3) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.527006ms)
Sep 13 17:12:39.955: INFO: (3) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.313032ms)
Sep 13 17:12:39.955: INFO: (3) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.035541ms)
Sep 13 17:12:39.955: INFO: (3) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 6.517716ms)
Sep 13 17:12:39.956: INFO: (3) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.773658ms)
Sep 13 17:12:39.956: INFO: (3) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 7.457702ms)
Sep 13 17:12:39.956: INFO: (3) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.593919ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.828229ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.23852ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 8.011403ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 7.838378ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.650784ms)
Sep 13 17:12:39.957: INFO: (3) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 8.384223ms)
Sep 13 17:12:39.958: INFO: (3) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 8.945127ms)
Sep 13 17:12:39.958: INFO: (3) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.929107ms)
Sep 13 17:12:39.958: INFO: (3) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.607863ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.598139ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.426947ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 4.871984ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 5.725909ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 5.386821ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 5.834993ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 5.699298ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.384868ms)
Sep 13 17:12:39.964: INFO: (4) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 6.352586ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 6.342106ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.152218ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.266134ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 6.847036ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.135306ms)
Sep 13 17:12:39.966: INFO: (4) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.082166ms)
Sep 13 17:12:39.967: INFO: (4) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.922866ms)
Sep 13 17:12:39.975: INFO: (5) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.367965ms)
Sep 13 17:12:39.975: INFO: (5) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.593759ms)
Sep 13 17:12:39.975: INFO: (5) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 7.87691ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 8.178247ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 8.148991ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 8.31866ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 8.278945ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 9.126067ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.869075ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.378883ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 8.303693ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.198233ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 8.780829ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 8.894682ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.694005ms)
Sep 13 17:12:39.976: INFO: (5) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 9.03117ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 5.544247ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 5.257989ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.930744ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 5.815488ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.950531ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.057782ms)
Sep 13 17:12:39.982: INFO: (6) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 6.130008ms)
Sep 13 17:12:39.983: INFO: (6) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.112465ms)
Sep 13 17:12:39.983: INFO: (6) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.105352ms)
Sep 13 17:12:39.983: INFO: (6) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 6.156938ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 8.785738ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.828398ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.092094ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 8.984021ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.955688ms)
Sep 13 17:12:39.985: INFO: (6) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 9.078879ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 7.037554ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.483902ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.283185ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 7.224715ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.852545ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 6.79082ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 7.580725ms)
Sep 13 17:12:39.993: INFO: (7) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.209065ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.311027ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 7.71664ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 8.03626ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.637179ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 7.775981ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.809002ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 7.743309ms)
Sep 13 17:12:39.994: INFO: (7) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 8.524637ms)
Sep 13 17:12:39.999: INFO: (8) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 4.362537ms)
Sep 13 17:12:39.999: INFO: (8) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 4.691164ms)
Sep 13 17:12:39.999: INFO: (8) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 4.788769ms)
Sep 13 17:12:39.999: INFO: (8) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 5.06663ms)
Sep 13 17:12:40.001: INFO: (8) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 6.520251ms)
Sep 13 17:12:40.001: INFO: (8) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.496046ms)
Sep 13 17:12:40.003: INFO: (8) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 8.683116ms)
Sep 13 17:12:40.003: INFO: (8) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.767504ms)
Sep 13 17:12:40.003: INFO: (8) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 8.833056ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 9.156385ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 8.938075ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 8.884834ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.229192ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 9.322977ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 9.246334ms)
Sep 13 17:12:40.004: INFO: (8) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 9.541338ms)
Sep 13 17:12:40.009: INFO: (9) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.022948ms)
Sep 13 17:12:40.010: INFO: (9) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 5.224757ms)
Sep 13 17:12:40.011: INFO: (9) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.559936ms)
Sep 13 17:12:40.011: INFO: (9) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 6.560557ms)
Sep 13 17:12:40.011: INFO: (9) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 7.028837ms)
Sep 13 17:12:40.011: INFO: (9) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 6.933427ms)
Sep 13 17:12:40.011: INFO: (9) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 7.22762ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 7.437785ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 7.434278ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.552421ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.683958ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.619006ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.702193ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.625577ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.977209ms)
Sep 13 17:12:40.012: INFO: (9) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.176583ms)
Sep 13 17:12:40.018: INFO: (10) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.505144ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 5.351554ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 5.510696ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 6.432987ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.775451ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.657067ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 7.098748ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.447745ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.295388ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.784879ms)
Sep 13 17:12:40.020: INFO: (10) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 6.779228ms)
Sep 13 17:12:40.022: INFO: (10) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 9.021161ms)
Sep 13 17:12:40.022: INFO: (10) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.96209ms)
Sep 13 17:12:40.022: INFO: (10) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.523505ms)
Sep 13 17:12:40.022: INFO: (10) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.113675ms)
Sep 13 17:12:40.022: INFO: (10) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.318709ms)
Sep 13 17:12:40.028: INFO: (11) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 4.923721ms)
Sep 13 17:12:40.028: INFO: (11) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.598629ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 8.382971ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.774168ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.175873ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.943356ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.57338ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.941781ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.865149ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 8.06829ms)
Sep 13 17:12:40.031: INFO: (11) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.767766ms)
Sep 13 17:12:40.032: INFO: (11) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 9.056617ms)
Sep 13 17:12:40.033: INFO: (11) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.975023ms)
Sep 13 17:12:40.033: INFO: (11) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 9.709234ms)
Sep 13 17:12:40.033: INFO: (11) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 10.113503ms)
Sep 13 17:12:40.033: INFO: (11) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 9.186711ms)
Sep 13 17:12:40.038: INFO: (12) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 4.785964ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 6.691764ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.442294ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 6.31257ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 6.164363ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.213204ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.57262ms)
Sep 13 17:12:40.039: INFO: (12) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 6.293595ms)
Sep 13 17:12:40.040: INFO: (12) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.064115ms)
Sep 13 17:12:40.040: INFO: (12) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 6.655876ms)
Sep 13 17:12:40.041: INFO: (12) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.343569ms)
Sep 13 17:12:40.044: INFO: (12) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 11.178886ms)
Sep 13 17:12:40.045: INFO: (12) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 12.604964ms)
Sep 13 17:12:40.046: INFO: (12) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 13.201175ms)
Sep 13 17:12:40.047: INFO: (12) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 13.497441ms)
Sep 13 17:12:40.050: INFO: (12) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 16.4823ms)
Sep 13 17:12:40.075: INFO: (13) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 24.491468ms)
Sep 13 17:12:40.095: INFO: (13) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 44.78516ms)
Sep 13 17:12:40.095: INFO: (13) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 44.603459ms)
Sep 13 17:12:40.095: INFO: (13) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 44.522736ms)
Sep 13 17:12:40.095: INFO: (13) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 45.047173ms)
Sep 13 17:12:40.095: INFO: (13) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 45.124117ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 45.384486ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 45.357554ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 45.54109ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 45.609218ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 45.684069ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 45.604168ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 45.960478ms)
Sep 13 17:12:40.096: INFO: (13) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 46.10481ms)
Sep 13 17:12:40.097: INFO: (13) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 46.271202ms)
Sep 13 17:12:40.097: INFO: (13) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 46.2011ms)
Sep 13 17:12:40.101: INFO: (14) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 4.370442ms)
Sep 13 17:12:40.101: INFO: (14) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 3.979949ms)
Sep 13 17:12:40.101: INFO: (14) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 4.113569ms)
Sep 13 17:12:40.104: INFO: (14) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 7.016564ms)
Sep 13 17:12:40.104: INFO: (14) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.297242ms)
Sep 13 17:12:40.104: INFO: (14) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 6.341074ms)
Sep 13 17:12:40.104: INFO: (14) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.76982ms)
Sep 13 17:12:40.104: INFO: (14) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 7.402919ms)
Sep 13 17:12:40.105: INFO: (14) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 6.987299ms)
Sep 13 17:12:40.105: INFO: (14) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.904122ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 10.127609ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 9.139003ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 9.752575ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 9.070223ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 9.077726ms)
Sep 13 17:12:40.107: INFO: (14) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 9.797659ms)
Sep 13 17:12:40.112: INFO: (15) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 4.134017ms)
Sep 13 17:12:40.114: INFO: (15) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.215929ms)
Sep 13 17:12:40.114: INFO: (15) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.781753ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 7.006085ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 7.154613ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.400455ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.4661ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.843819ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.875238ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.375238ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.648771ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.923789ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 8.037924ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.111471ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 8.061127ms)
Sep 13 17:12:40.115: INFO: (15) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 7.958113ms)
Sep 13 17:12:40.119: INFO: (16) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 3.3108ms)
Sep 13 17:12:40.120: INFO: (16) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 4.015005ms)
Sep 13 17:12:40.120: INFO: (16) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 3.895129ms)
Sep 13 17:12:40.122: INFO: (16) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.444238ms)
Sep 13 17:12:40.122: INFO: (16) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 5.947455ms)
Sep 13 17:12:40.123: INFO: (16) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 6.177998ms)
Sep 13 17:12:40.123: INFO: (16) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 5.96066ms)
Sep 13 17:12:40.123: INFO: (16) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 6.202554ms)
Sep 13 17:12:40.123: INFO: (16) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 7.549404ms)
Sep 13 17:12:40.123: INFO: (16) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.630466ms)
Sep 13 17:12:40.124: INFO: (16) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 7.139244ms)
Sep 13 17:12:40.124: INFO: (16) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.062099ms)
Sep 13 17:12:40.124: INFO: (16) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.098447ms)
Sep 13 17:12:40.124: INFO: (16) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.774138ms)
Sep 13 17:12:40.124: INFO: (16) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 7.570585ms)
Sep 13 17:12:40.126: INFO: (16) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 10.142187ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 5.554437ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 5.340274ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.432307ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 5.533377ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 5.541021ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 5.66245ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 6.128836ms)
Sep 13 17:12:40.132: INFO: (17) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 5.927868ms)
Sep 13 17:12:40.133: INFO: (17) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 6.873927ms)
Sep 13 17:12:40.133: INFO: (17) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.659733ms)
Sep 13 17:12:40.134: INFO: (17) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 6.989593ms)
Sep 13 17:12:40.134: INFO: (17) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 6.953135ms)
Sep 13 17:12:40.134: INFO: (17) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.172517ms)
Sep 13 17:12:40.135: INFO: (17) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.933546ms)
Sep 13 17:12:40.135: INFO: (17) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.767594ms)
Sep 13 17:12:40.135: INFO: (17) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 8.61079ms)
Sep 13 17:12:40.139: INFO: (18) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 3.604032ms)
Sep 13 17:12:40.139: INFO: (18) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 3.920697ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 7.272665ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 7.378985ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.278726ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.253649ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 7.76487ms)
Sep 13 17:12:40.143: INFO: (18) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 8.028315ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 8.1052ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 8.052791ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 8.002607ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 8.247047ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 8.53609ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 8.873222ms)
Sep 13 17:12:40.144: INFO: (18) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 8.589178ms)
Sep 13 17:12:40.145: INFO: (18) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 9.376278ms)
Sep 13 17:12:40.149: INFO: (19) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g/proxy/rewriteme">test</a> (200; 4.08684ms)
Sep 13 17:12:40.149: INFO: (19) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">test<... (200; 4.139859ms)
Sep 13 17:12:40.149: INFO: (19) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 3.870402ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 6.289098ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname1/proxy/: foo (200; 6.795709ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/services/http:proxy-service-gm2f7:portname2/proxy/: bar (200; 6.576987ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:462/proxy/: tls qux (200; 6.74802ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/pods/proxy-service-gm2f7-2jt5g:162/proxy/: bar (200; 6.760422ms)
Sep 13 17:12:40.152: INFO: (19) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname2/proxy/: tls qux (200; 7.305376ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:443/proxy/tlsrewritem... (200; 7.612623ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/services/https:proxy-service-gm2f7:tlsportname1/proxy/: tls baz (200; 7.23304ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname1/proxy/: foo (200; 7.212672ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:160/proxy/: foo (200; 7.037181ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/: <a href="/api/v1/namespaces/proxy-9272/pods/http:proxy-service-gm2f7-2jt5g:1080/proxy/rewriteme">... (200; 7.612044ms)
Sep 13 17:12:40.153: INFO: (19) /api/v1/namespaces/proxy-9272/pods/https:proxy-service-gm2f7-2jt5g:460/proxy/: tls baz (200; 7.634295ms)
Sep 13 17:12:40.154: INFO: (19) /api/v1/namespaces/proxy-9272/services/proxy-service-gm2f7:portname2/proxy/: bar (200; 8.369556ms)
STEP: deleting ReplicationController proxy-service-gm2f7 in namespace proxy-9272, will wait for the garbage collector to delete the pods
Sep 13 17:12:40.210: INFO: Deleting ReplicationController proxy-service-gm2f7 took: 4.165677ms
Sep 13 17:12:40.510: INFO: Terminating ReplicationController proxy-service-gm2f7 pods took: 300.203932ms
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:12:41.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9272" for this suite.
Sep 13 17:12:47.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:12:47.979: INFO: namespace proxy-9272 deletion completed in 6.06600946s

â€¢ [SLOW TEST:21.165 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:12:47.979: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:12:52.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5500" for this suite.
Sep 13 17:13:36.030: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:13:36.088: INFO: namespace kubelet-test-5500 deletion completed in 44.066004581s

â€¢ [SLOW TEST:48.109 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:13:36.089: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d252cbe6-d649-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:13:36.117: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741" in namespace "projected-9981" to be "success or failure"
Sep 13 17:13:36.123: INFO: Pod "pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 5.383626ms
Sep 13 17:13:38.126: INFO: Pod "pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008287986s
STEP: Saw pod success
Sep 13 17:13:38.126: INFO: Pod "pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:13:38.128: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:13:38.140: INFO: Waiting for pod pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741 to disappear
Sep 13 17:13:38.142: INFO: Pod pod-projected-configmaps-d2532f1a-d649-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:13:38.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9981" for this suite.
Sep 13 17:13:44.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:13:44.213: INFO: namespace projected-9981 deletion completed in 6.068491782s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:13:44.213: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-d72af5df-d649-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:13:44.247: INFO: Waiting up to 5m0s for pod "pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741" in namespace "secrets-4139" to be "success or failure"
Sep 13 17:13:44.250: INFO: Pod "pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.895281ms
Sep 13 17:13:46.254: INFO: Pod "pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007532558s
STEP: Saw pod success
Sep 13 17:13:46.254: INFO: Pod "pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:13:46.257: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741 container secret-env-test: <nil>
STEP: delete the pod
Sep 13 17:13:46.269: INFO: Waiting for pod pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741 to disappear
Sep 13 17:13:46.272: INFO: Pod pod-secrets-d72b84a3-d649-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:13:46.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4139" for this suite.
Sep 13 17:13:52.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:13:52.349: INFO: namespace secrets-4139 deletion completed in 6.074805236s

â€¢ [SLOW TEST:8.136 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:13:52.349: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-dc03e84a-d649-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:13:52.378: INFO: Waiting up to 5m0s for pod "pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741" in namespace "secrets-7807" to be "success or failure"
Sep 13 17:13:52.380: INFO: Pod "pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.465632ms
Sep 13 17:13:54.383: INFO: Pod "pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005564629s
STEP: Saw pod success
Sep 13 17:13:54.384: INFO: Pod "pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:13:54.386: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:13:54.400: INFO: Waiting for pod pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741 to disappear
Sep 13 17:13:54.402: INFO: Pod pod-secrets-dc044c41-d649-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:13:54.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7807" for this suite.
Sep 13 17:14:00.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:14:00.471: INFO: namespace secrets-7807 deletion completed in 6.06619093s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:14:00.471: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1354
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 17:14:00.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-358'
Sep 13 17:14:00.677: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 13 17:14:00.678: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Sep 13 17:14:00.686: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-9c87f]
Sep 13 17:14:00.686: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-9c87f" in namespace "kubectl-358" to be "running and ready"
Sep 13 17:14:00.688: INFO: Pod "e2e-test-nginx-rc-9c87f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48042ms
Sep 13 17:14:02.691: INFO: Pod "e2e-test-nginx-rc-9c87f": Phase="Running", Reason="", readiness=true. Elapsed: 2.005697518s
Sep 13 17:14:02.692: INFO: Pod "e2e-test-nginx-rc-9c87f" satisfied condition "running and ready"
Sep 13 17:14:02.692: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-9c87f]
Sep 13 17:14:02.692: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 logs rc/e2e-test-nginx-rc --namespace=kubectl-358'
Sep 13 17:14:02.800: INFO: stderr: ""
Sep 13 17:14:02.800: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1359
Sep 13 17:14:02.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete rc e2e-test-nginx-rc --namespace=kubectl-358'
Sep 13 17:14:02.891: INFO: stderr: ""
Sep 13 17:14:02.891: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:14:02.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-358" for this suite.
Sep 13 17:14:24.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:14:24.972: INFO: namespace kubectl-358 deletion completed in 22.078814515s

â€¢ [SLOW TEST:24.501 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:14:24.972: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:14:49.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1578" for this suite.
Sep 13 17:14:55.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:14:55.121: INFO: namespace namespaces-1578 deletion completed in 6.071017368s
STEP: Destroying namespace "nsdeletetest-9936" for this suite.
Sep 13 17:14:55.123: INFO: Namespace nsdeletetest-9936 was already deleted
STEP: Destroying namespace "nsdeletetest-8182" for this suite.
Sep 13 17:15:01.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:15:01.200: INFO: namespace nsdeletetest-8182 deletion completed in 6.076666402s

â€¢ [SLOW TEST:36.228 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:15:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-050e3016-d64a-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:15:01.232: INFO: Waiting up to 5m0s for pod "pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741" in namespace "configmap-977" to be "success or failure"
Sep 13 17:15:01.239: INFO: Pod "pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 6.482431ms
Sep 13 17:15:03.242: INFO: Pod "pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009524028s
Sep 13 17:15:05.245: INFO: Pod "pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012535811s
STEP: Saw pod success
Sep 13 17:15:05.245: INFO: Pod "pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:15:05.247: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:15:05.259: INFO: Waiting for pod pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:15:05.261: INFO: Pod pod-configmaps-050e9828-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:15:05.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-977" for this suite.
Sep 13 17:15:11.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:15:11.330: INFO: namespace configmap-977 deletion completed in 6.066356211s

â€¢ [SLOW TEST:10.129 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:15:11.330: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:15:13.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4374" for this suite.
Sep 13 17:15:19.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:15:19.452: INFO: namespace emptydir-wrapper-4374 deletion completed in 6.06594593s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:15:19.452: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-3003
Sep 13 17:15:23.487: INFO: Started pod liveness-exec in namespace container-probe-3003
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 17:15:23.490: INFO: Initial restart count of pod liveness-exec is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:19:23.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3003" for this suite.
Sep 13 17:19:29.851: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:19:29.915: INFO: namespace container-probe-3003 deletion completed in 6.071856161s

â€¢ [SLOW TEST:250.463 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:19:29.915: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 13 17:19:34.459: INFO: Successfully updated pod "pod-update-activedeadlineseconds-a53875b9-d64a-11e9-80a0-429a7732c741"
Sep 13 17:19:34.459: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-a53875b9-d64a-11e9-80a0-429a7732c741" in namespace "pods-1840" to be "terminated due to deadline exceeded"
Sep 13 17:19:34.462: INFO: Pod "pod-update-activedeadlineseconds-a53875b9-d64a-11e9-80a0-429a7732c741": Phase="Running", Reason="", readiness=true. Elapsed: 2.59704ms
Sep 13 17:19:36.465: INFO: Pod "pod-update-activedeadlineseconds-a53875b9-d64a-11e9-80a0-429a7732c741": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.005397394s
Sep 13 17:19:36.465: INFO: Pod "pod-update-activedeadlineseconds-a53875b9-d64a-11e9-80a0-429a7732c741" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:19:36.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1840" for this suite.
Sep 13 17:19:42.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:19:42.536: INFO: namespace pods-1840 deletion completed in 6.069121673s

â€¢ [SLOW TEST:12.621 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:19:42.537: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 13 17:19:42.564: INFO: Waiting up to 5m0s for pod "pod-acbe5d99-d64a-11e9-80a0-429a7732c741" in namespace "emptydir-3731" to be "success or failure"
Sep 13 17:19:42.567: INFO: Pod "pod-acbe5d99-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.944483ms
Sep 13 17:19:44.570: INFO: Pod "pod-acbe5d99-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005754656s
STEP: Saw pod success
Sep 13 17:19:44.570: INFO: Pod "pod-acbe5d99-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:19:44.572: INFO: Trying to get logs from node k8s-test-002 pod pod-acbe5d99-d64a-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:19:44.586: INFO: Waiting for pod pod-acbe5d99-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:19:44.588: INFO: Pod pod-acbe5d99-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:19:44.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3731" for this suite.
Sep 13 17:19:50.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:19:50.660: INFO: namespace emptydir-3731 deletion completed in 6.069444201s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:19:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:19:50.686: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741" in namespace "downward-api-627" to be "success or failure"
Sep 13 17:19:50.689: INFO: Pod "downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.48554ms
Sep 13 17:19:52.692: INFO: Pod "downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00575145s
STEP: Saw pod success
Sep 13 17:19:52.692: INFO: Pod "downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:19:52.695: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:19:52.709: INFO: Waiting for pod downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:19:52.711: INFO: Pod downwardapi-volume-b195cfd6-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:19:52.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-627" for this suite.
Sep 13 17:19:58.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:19:58.781: INFO: namespace downward-api-627 deletion completed in 6.067473727s

â€¢ [SLOW TEST:8.121 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:19:58.781: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name projected-secret-test-b66cee4e-d64a-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:19:58.810: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741" in namespace "projected-1064" to be "success or failure"
Sep 13 17:19:58.814: INFO: Pod "pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.892094ms
Sep 13 17:20:00.817: INFO: Pod "pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007237314s
Sep 13 17:20:02.821: INFO: Pod "pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010545664s
STEP: Saw pod success
Sep 13 17:20:02.821: INFO: Pod "pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:20:02.824: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:20:02.838: INFO: Waiting for pod pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:20:02.840: INFO: Pod pod-projected-secrets-b66d51c1-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:20:02.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1064" for this suite.
Sep 13 17:20:08.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:20:08.911: INFO: namespace projected-1064 deletion completed in 6.06837527s

â€¢ [SLOW TEST:10.130 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:20:08.911: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 13 17:20:08.942: INFO: Waiting up to 5m0s for pod "downward-api-bc775c29-d64a-11e9-80a0-429a7732c741" in namespace "downward-api-7569" to be "success or failure"
Sep 13 17:20:08.945: INFO: Pod "downward-api-bc775c29-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773041ms
Sep 13 17:20:10.948: INFO: Pod "downward-api-bc775c29-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005722295s
Sep 13 17:20:12.951: INFO: Pod "downward-api-bc775c29-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008907985s
STEP: Saw pod success
Sep 13 17:20:12.951: INFO: Pod "downward-api-bc775c29-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:20:12.953: INFO: Trying to get logs from node k8s-test-002 pod downward-api-bc775c29-d64a-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:20:12.967: INFO: Waiting for pod downward-api-bc775c29-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:20:12.969: INFO: Pod downward-api-bc775c29-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:20:12.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7569" for this suite.
Sep 13 17:20:18.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:20:19.040: INFO: namespace downward-api-7569 deletion completed in 6.06732623s

â€¢ [SLOW TEST:10.128 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:20:19.040: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7693.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7693.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 17:20:23.099: INFO: DNS probes using dns-7693/dns-test-c2806f63-d64a-11e9-80a0-429a7732c741 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:20:23.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7693" for this suite.
Sep 13 17:20:29.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:20:29.177: INFO: namespace dns-7693 deletion completed in 6.068517727s

â€¢ [SLOW TEST:10.137 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:20:29.178: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating cluster-info
Sep 13 17:20:29.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 cluster-info'
Sep 13 17:20:29.277: INFO: stderr: ""
Sep 13 17:20:29.277: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:20:29.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2149" for this suite.
Sep 13 17:20:35.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:20:35.354: INFO: namespace kubectl-2149 deletion completed in 6.07332371s

â€¢ [SLOW TEST:6.176 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:20:35.354: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:20:35.380: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741" in namespace "downward-api-8675" to be "success or failure"
Sep 13 17:20:35.384: INFO: Pod "downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.516027ms
Sep 13 17:20:37.387: INFO: Pod "downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006642806s
Sep 13 17:20:39.390: INFO: Pod "downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009478007s
STEP: Saw pod success
Sep 13 17:20:39.390: INFO: Pod "downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:20:39.392: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:20:39.404: INFO: Waiting for pod downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741 to disappear
Sep 13 17:20:39.406: INFO: Pod downwardapi-volume-cc39a1cf-d64a-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:20:39.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8675" for this suite.
Sep 13 17:20:45.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:20:45.487: INFO: namespace downward-api-8675 deletion completed in 6.078311322s

â€¢ [SLOW TEST:10.133 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:20:45.487: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-7411
Sep 13 17:20:49.565: INFO: Started pod liveness-http in namespace container-probe-7411
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 17:20:49.568: INFO: Initial restart count of pod liveness-http is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:24:49.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7411" for this suite.
Sep 13 17:24:55.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:24:55.987: INFO: namespace container-probe-7411 deletion completed in 6.069092385s

â€¢ [SLOW TEST:250.500 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:24:55.987: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0913 17:25:06.026577      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 17:25:06.026: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:25:06.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6573" for this suite.
Sep 13 17:25:12.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:25:12.098: INFO: namespace gc-6573 deletion completed in 6.06919063s

â€¢ [SLOW TEST:16.111 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:25:12.098: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on tmpfs
Sep 13 17:25:12.124: INFO: Waiting up to 5m0s for pod "pod-712d4f0e-d64b-11e9-80a0-429a7732c741" in namespace "emptydir-7034" to be "success or failure"
Sep 13 17:25:12.128: INFO: Pod "pod-712d4f0e-d64b-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.515454ms
Sep 13 17:25:14.131: INFO: Pod "pod-712d4f0e-d64b-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007274005s
STEP: Saw pod success
Sep 13 17:25:14.131: INFO: Pod "pod-712d4f0e-d64b-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:25:14.133: INFO: Trying to get logs from node k8s-test-002 pod pod-712d4f0e-d64b-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:25:14.148: INFO: Waiting for pod pod-712d4f0e-d64b-11e9-80a0-429a7732c741 to disappear
Sep 13 17:25:14.150: INFO: Pod pod-712d4f0e-d64b-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:25:14.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7034" for this suite.
Sep 13 17:25:20.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:25:20.219: INFO: namespace emptydir-7034 deletion completed in 6.066276334s

â€¢ [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:25:20.219: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 13 17:25:22.770: INFO: Successfully updated pod "annotationupdate760487a9-d64b-11e9-80a0-429a7732c741"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:25:26.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8169" for this suite.
Sep 13 17:25:48.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:25:48.870: INFO: namespace downward-api-8169 deletion completed in 22.077242659s

â€¢ [SLOW TEST:28.652 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:25:48.870: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-8718806e-d64b-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:25:48.930: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741" in namespace "projected-7697" to be "success or failure"
Sep 13 17:25:48.933: INFO: Pod "pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.841619ms
Sep 13 17:25:50.936: INFO: Pod "pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005576116s
STEP: Saw pod success
Sep 13 17:25:50.936: INFO: Pod "pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:25:50.938: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:25:50.951: INFO: Waiting for pod pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741 to disappear
Sep 13 17:25:50.953: INFO: Pod pod-projected-secrets-871d8820-d64b-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:25:50.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7697" for this suite.
Sep 13 17:25:56.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:25:57.036: INFO: namespace projected-7697 deletion completed in 6.080985602s

â€¢ [SLOW TEST:8.166 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:25:57.037: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-8bf662d9-d64b-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:25:57.065: INFO: Waiting up to 5m0s for pod "pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741" in namespace "configmap-2574" to be "success or failure"
Sep 13 17:25:57.067: INFO: Pod "pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.471424ms
Sep 13 17:25:59.071: INFO: Pod "pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005589489s
STEP: Saw pod success
Sep 13 17:25:59.071: INFO: Pod "pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:25:59.073: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:25:59.085: INFO: Waiting for pod pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741 to disappear
Sep 13 17:25:59.087: INFO: Pod pod-configmaps-8bf6c100-d64b-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:25:59.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2574" for this suite.
Sep 13 17:26:05.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:26:05.165: INFO: namespace configmap-2574 deletion completed in 6.075685034s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:26:05.166: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-6813/configmap-test-90cee87c-d64b-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:26:05.195: INFO: Waiting up to 5m0s for pod "pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741" in namespace "configmap-6813" to be "success or failure"
Sep 13 17:26:05.197: INFO: Pod "pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.142636ms
Sep 13 17:26:07.200: INFO: Pod "pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004916384s
STEP: Saw pod success
Sep 13 17:26:07.200: INFO: Pod "pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:26:07.202: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741 container env-test: <nil>
STEP: delete the pod
Sep 13 17:26:07.215: INFO: Waiting for pod pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741 to disappear
Sep 13 17:26:07.217: INFO: Pod pod-configmaps-90cf5a5a-d64b-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:26:07.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6813" for this suite.
Sep 13 17:26:13.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:26:13.286: INFO: namespace configmap-6813 deletion completed in 6.066509101s

â€¢ [SLOW TEST:8.121 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:26:13.287: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-exec in namespace container-probe-5805
Sep 13 17:26:15.319: INFO: Started pod liveness-exec in namespace container-probe-5805
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 17:26:15.321: INFO: Initial restart count of pod liveness-exec is 0
Sep 13 17:27:07.396: INFO: Restart count of pod container-probe-5805/liveness-exec is now 1 (52.07560879s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:27:07.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5805" for this suite.
Sep 13 17:27:13.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:27:13.473: INFO: namespace container-probe-5805 deletion completed in 6.06820599s

â€¢ [SLOW TEST:60.186 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:27:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1190
STEP: creating an rc
Sep 13 17:27:13.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-2889'
Sep 13 17:27:13.764: INFO: stderr: ""
Sep 13 17:27:13.764: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Waiting for Redis master to start.
Sep 13 17:27:14.767: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:27:14.767: INFO: Found 0 / 1
Sep 13 17:27:15.767: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:27:15.767: INFO: Found 1 / 1
Sep 13 17:27:15.767: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 13 17:27:15.769: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:27:15.769: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Sep 13 17:27:15.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 logs redis-master-l887q redis-master --namespace=kubectl-2889'
Sep 13 17:27:15.858: INFO: stderr: ""
Sep 13 17:27:15.858: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Sep 17:27:14.734 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Sep 17:27:14.734 # Server started, Redis version 3.2.12\n1:M 13 Sep 17:27:14.735 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Sep 17:27:14.735 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Sep 13 17:27:15.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 log redis-master-l887q redis-master --namespace=kubectl-2889 --tail=1'
Sep 13 17:27:15.941: INFO: stderr: ""
Sep 13 17:27:15.941: INFO: stdout: "1:M 13 Sep 17:27:14.735 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Sep 13 17:27:15.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 log redis-master-l887q redis-master --namespace=kubectl-2889 --limit-bytes=1'
Sep 13 17:27:16.025: INFO: stderr: ""
Sep 13 17:27:16.025: INFO: stdout: " "
STEP: exposing timestamps
Sep 13 17:27:16.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 log redis-master-l887q redis-master --namespace=kubectl-2889 --tail=1 --timestamps'
Sep 13 17:27:16.108: INFO: stderr: ""
Sep 13 17:27:16.108: INFO: stdout: "2019-09-13T17:27:14.735214665Z 1:M 13 Sep 17:27:14.735 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Sep 13 17:27:18.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 log redis-master-l887q redis-master --namespace=kubectl-2889 --since=1s'
Sep 13 17:27:18.693: INFO: stderr: ""
Sep 13 17:27:18.693: INFO: stdout: ""
Sep 13 17:27:18.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 log redis-master-l887q redis-master --namespace=kubectl-2889 --since=24h'
Sep 13 17:27:18.778: INFO: stderr: ""
Sep 13 17:27:18.778: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Sep 17:27:14.734 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Sep 17:27:14.734 # Server started, Redis version 3.2.12\n1:M 13 Sep 17:27:14.735 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Sep 17:27:14.735 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1196
STEP: using delete to clean up resources
Sep 13 17:27:18.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-2889'
Sep 13 17:27:18.857: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 17:27:18.857: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Sep 13 17:27:18.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=nginx --no-headers --namespace=kubectl-2889'
Sep 13 17:27:18.934: INFO: stderr: "No resources found.\n"
Sep 13 17:27:18.935: INFO: stdout: ""
Sep 13 17:27:18.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=nginx --namespace=kubectl-2889 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 17:27:19.007: INFO: stderr: ""
Sep 13 17:27:19.007: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:27:19.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2889" for this suite.
Sep 13 17:27:41.020: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:27:41.084: INFO: namespace kubectl-2889 deletion completed in 22.073538549s

â€¢ [SLOW TEST:27.611 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:27:41.084: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:27:45.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-887" for this suite.
Sep 13 17:28:23.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:28:23.196: INFO: namespace kubelet-test-887 deletion completed in 38.067909712s

â€¢ [SLOW TEST:42.112 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a read only busybox container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:28:23.197: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:28:27.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2108" for this suite.
Sep 13 17:28:33.238: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:28:33.298: INFO: namespace kubelet-test-2108 deletion completed in 6.067398527s

â€¢ [SLOW TEST:10.102 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:28:33.299: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 17:28:33.341: INFO: Number of nodes with available pods: 0
Sep 13 17:28:33.341: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:28:34.347: INFO: Number of nodes with available pods: 0
Sep 13 17:28:34.348: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:28:35.346: INFO: Number of nodes with available pods: 2
Sep 13 17:28:35.346: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Sep 13 17:28:35.361: INFO: Number of nodes with available pods: 1
Sep 13 17:28:35.361: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:28:36.367: INFO: Number of nodes with available pods: 1
Sep 13 17:28:36.367: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:28:37.366: INFO: Number of nodes with available pods: 2
Sep 13 17:28:37.366: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3257, will wait for the garbage collector to delete the pods
Sep 13 17:28:37.427: INFO: Deleting DaemonSet.extensions daemon-set took: 4.813349ms
Sep 13 17:28:37.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.206561ms
Sep 13 17:28:45.530: INFO: Number of nodes with available pods: 0
Sep 13 17:28:45.530: INFO: Number of running nodes: 0, number of available pods: 0
Sep 13 17:28:45.531: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3257/daemonsets","resourceVersion":"10597"},"items":null}

Sep 13 17:28:45.533: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3257/pods","resourceVersion":"10597"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:28:45.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3257" for this suite.
Sep 13 17:28:51.551: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:28:51.610: INFO: namespace daemonsets-3257 deletion completed in 6.066831731s

â€¢ [SLOW TEST:18.311 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:28:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:28:51.637: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741" in namespace "projected-9897" to be "success or failure"
Sep 13 17:28:51.642: INFO: Pod "downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.699394ms
Sep 13 17:28:53.645: INFO: Pod "downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007614322s
STEP: Saw pod success
Sep 13 17:28:53.645: INFO: Pod "downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:28:53.648: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:28:53.660: INFO: Waiting for pod downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741 to disappear
Sep 13 17:28:53.663: INFO: Pod downwardapi-volume-f4046cae-d64b-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:28:53.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9897" for this suite.
Sep 13 17:28:59.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:28:59.732: INFO: namespace projected-9897 deletion completed in 6.067008583s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:28:59.733: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:265
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a replication controller
Sep 13 17:28:59.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-4074'
Sep 13 17:28:59.910: INFO: stderr: ""
Sep 13 17:28:59.910: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 17:28:59.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:28:59.991: INFO: stderr: ""
Sep 13 17:28:59.991: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-m4mmq "
Sep 13 17:28:59.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:00.067: INFO: stderr: ""
Sep 13 17:29:00.067: INFO: stdout: ""
Sep 13 17:29:00.067: INFO: update-demo-nautilus-92fpm is created but not running
Sep 13 17:29:05.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:05.152: INFO: stderr: ""
Sep 13 17:29:05.152: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-m4mmq "
Sep 13 17:29:05.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:05.224: INFO: stderr: ""
Sep 13 17:29:05.224: INFO: stdout: "true"
Sep 13 17:29:05.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:05.297: INFO: stderr: ""
Sep 13 17:29:05.297: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:05.297: INFO: validating pod update-demo-nautilus-92fpm
Sep 13 17:29:05.301: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:05.301: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:05.301: INFO: update-demo-nautilus-92fpm is verified up and running
Sep 13 17:29:05.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-m4mmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:05.373: INFO: stderr: ""
Sep 13 17:29:05.373: INFO: stdout: "true"
Sep 13 17:29:05.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-m4mmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:05.445: INFO: stderr: ""
Sep 13 17:29:05.445: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:05.445: INFO: validating pod update-demo-nautilus-m4mmq
Sep 13 17:29:05.449: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:05.449: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:05.449: INFO: update-demo-nautilus-m4mmq is verified up and running
STEP: scaling down the replication controller
Sep 13 17:29:05.451: INFO: scanned /root for discovery docs: <nil>
Sep 13 17:29:05.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-4074'
Sep 13 17:29:06.548: INFO: stderr: ""
Sep 13 17:29:06.548: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 17:29:06.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:06.627: INFO: stderr: ""
Sep 13 17:29:06.627: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-m4mmq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 13 17:29:11.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:11.702: INFO: stderr: ""
Sep 13 17:29:11.702: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-m4mmq "
STEP: Replicas for name=update-demo: expected=1 actual=2
Sep 13 17:29:16.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:16.779: INFO: stderr: ""
Sep 13 17:29:16.779: INFO: stdout: "update-demo-nautilus-92fpm "
Sep 13 17:29:16.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:16.851: INFO: stderr: ""
Sep 13 17:29:16.851: INFO: stdout: "true"
Sep 13 17:29:16.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:16.925: INFO: stderr: ""
Sep 13 17:29:16.925: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:16.925: INFO: validating pod update-demo-nautilus-92fpm
Sep 13 17:29:16.928: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:16.928: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:16.928: INFO: update-demo-nautilus-92fpm is verified up and running
STEP: scaling up the replication controller
Sep 13 17:29:16.930: INFO: scanned /root for discovery docs: <nil>
Sep 13 17:29:16.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-4074'
Sep 13 17:29:18.025: INFO: stderr: ""
Sep 13 17:29:18.025: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Sep 13 17:29:18.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:18.100: INFO: stderr: ""
Sep 13 17:29:18.100: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-gv2gm "
Sep 13 17:29:18.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:18.175: INFO: stderr: ""
Sep 13 17:29:18.175: INFO: stdout: "true"
Sep 13 17:29:18.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:18.254: INFO: stderr: ""
Sep 13 17:29:18.254: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:18.254: INFO: validating pod update-demo-nautilus-92fpm
Sep 13 17:29:18.257: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:18.257: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:18.257: INFO: update-demo-nautilus-92fpm is verified up and running
Sep 13 17:29:18.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-gv2gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:18.329: INFO: stderr: ""
Sep 13 17:29:18.329: INFO: stdout: ""
Sep 13 17:29:18.329: INFO: update-demo-nautilus-gv2gm is created but not running
Sep 13 17:29:23.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4074'
Sep 13 17:29:23.405: INFO: stderr: ""
Sep 13 17:29:23.405: INFO: stdout: "update-demo-nautilus-92fpm update-demo-nautilus-gv2gm "
Sep 13 17:29:23.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:23.477: INFO: stderr: ""
Sep 13 17:29:23.477: INFO: stdout: "true"
Sep 13 17:29:23.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-92fpm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:23.549: INFO: stderr: ""
Sep 13 17:29:23.549: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:23.549: INFO: validating pod update-demo-nautilus-92fpm
Sep 13 17:29:23.552: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:23.552: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:23.552: INFO: update-demo-nautilus-92fpm is verified up and running
Sep 13 17:29:23.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-gv2gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:23.626: INFO: stderr: ""
Sep 13 17:29:23.626: INFO: stdout: "true"
Sep 13 17:29:23.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods update-demo-nautilus-gv2gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4074'
Sep 13 17:29:23.698: INFO: stderr: ""
Sep 13 17:29:23.698: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Sep 13 17:29:23.698: INFO: validating pod update-demo-nautilus-gv2gm
Sep 13 17:29:23.701: INFO: got data: {
  "image": "nautilus.jpg"
}

Sep 13 17:29:23.701: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Sep 13 17:29:23.701: INFO: update-demo-nautilus-gv2gm is verified up and running
STEP: using delete to clean up resources
Sep 13 17:29:23.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-4074'
Sep 13 17:29:23.778: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 17:29:23.778: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Sep 13 17:29:23.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4074'
Sep 13 17:29:23.858: INFO: stderr: "No resources found.\n"
Sep 13 17:29:23.858: INFO: stdout: ""
Sep 13 17:29:23.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=update-demo --namespace=kubectl-4074 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 17:29:23.932: INFO: stderr: ""
Sep 13 17:29:23.932: INFO: stdout: "update-demo-nautilus-92fpm\nupdate-demo-nautilus-gv2gm\n"
Sep 13 17:29:24.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4074'
Sep 13 17:29:24.509: INFO: stderr: "No resources found.\n"
Sep 13 17:29:24.509: INFO: stdout: ""
Sep 13 17:29:24.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=update-demo --namespace=kubectl-4074 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 17:29:24.585: INFO: stderr: ""
Sep 13 17:29:24.585: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:29:24.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4074" for this suite.
Sep 13 17:29:30.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:29:30.654: INFO: namespace kubectl-4074 deletion completed in 6.066874301s

â€¢ [SLOW TEST:30.922 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:29:30.654: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Starting the proxy
Sep 13 17:29:30.679: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-009922021 proxy --unix-socket=/tmp/kubectl-proxy-unix316656128/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:29:30.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-957" for this suite.
Sep 13 17:29:36.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:29:36.811: INFO: namespace kubectl-957 deletion completed in 6.071909716s

â€¢ [SLOW TEST:6.156 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:29:36.811: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-downwardapi-h2v9
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 17:29:36.841: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-h2v9" in namespace "subpath-3841" to be "success or failure"
Sep 13 17:29:36.845: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.326143ms
Sep 13 17:29:38.848: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007127567s
Sep 13 17:29:40.851: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009922289s
Sep 13 17:29:42.854: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 6.01295555s
Sep 13 17:29:44.856: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 8.015624576s
Sep 13 17:29:46.859: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 10.018388841s
Sep 13 17:29:48.862: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 12.021412223s
Sep 13 17:29:50.865: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 14.024055581s
Sep 13 17:29:52.868: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 16.026712984s
Sep 13 17:29:54.870: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 18.029614277s
Sep 13 17:29:56.873: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 20.03248932s
Sep 13 17:29:58.876: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Running", Reason="", readiness=true. Elapsed: 22.0354s
Sep 13 17:30:00.879: INFO: Pod "pod-subpath-test-downwardapi-h2v9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038255277s
STEP: Saw pod success
Sep 13 17:30:00.879: INFO: Pod "pod-subpath-test-downwardapi-h2v9" satisfied condition "success or failure"
Sep 13 17:30:00.881: INFO: Trying to get logs from node k8s-test-002 pod pod-subpath-test-downwardapi-h2v9 container test-container-subpath-downwardapi-h2v9: <nil>
STEP: delete the pod
Sep 13 17:30:00.895: INFO: Waiting for pod pod-subpath-test-downwardapi-h2v9 to disappear
Sep 13 17:30:00.897: INFO: Pod pod-subpath-test-downwardapi-h2v9 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-h2v9
Sep 13 17:30:00.897: INFO: Deleting pod "pod-subpath-test-downwardapi-h2v9" in namespace "subpath-3841"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:30:00.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3841" for this suite.
Sep 13 17:30:06.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:30:06.972: INFO: namespace subpath-3841 deletion completed in 6.069596128s

â€¢ [SLOW TEST:30.161 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:30:06.972: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-map-20ef9096-d64c-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:30:07.000: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741" in namespace "projected-7494" to be "success or failure"
Sep 13 17:30:07.006: INFO: Pod "pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 5.364525ms
Sep 13 17:30:09.009: INFO: Pod "pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008632927s
STEP: Saw pod success
Sep 13 17:30:09.009: INFO: Pod "pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:30:09.012: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:30:09.026: INFO: Waiting for pod pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:30:09.028: INFO: Pod pod-projected-secrets-20efebd4-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:30:09.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7494" for this suite.
Sep 13 17:30:15.039: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:30:15.110: INFO: namespace projected-7494 deletion completed in 6.079292295s

â€¢ [SLOW TEST:8.138 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:30:15.110: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:30:33.146: INFO: Container started at 2019-09-13 17:30:16 +0000 UTC, pod became ready at 2019-09-13 17:30:31 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:30:33.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1315" for this suite.
Sep 13 17:30:55.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:30:55.221: INFO: namespace container-probe-1315 deletion completed in 22.072015493s

â€¢ [SLOW TEST:40.110 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:30:55.222: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 13 17:30:55.243: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 17:30:55.248: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 17:30:55.251: INFO: 
Logging pods the kubelet thinks is on node k8s-test-001 before test
Sep 13 17:30:55.265: INFO: coredns-5bc65d7f4b-dfrkk from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container coredns ready: true, restart count 2
Sep 13 17:30:55.265: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-xk4h6 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:30:55.265: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 13 17:30:55.265: INFO: etcd-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:30:55.265: INFO: kube-controller-manager-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:30:55.265: INFO: kube-proxy-zcrzb from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 17:30:55.265: INFO: coredns-5bc65d7f4b-gwg5v from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container coredns ready: true, restart count 2
Sep 13 17:30:55.265: INFO: testdns-rbqhm from default started at 2019-09-13 16:37:29 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container webserver ready: true, restart count 3
Sep 13 17:30:55.265: INFO: kube-apiserver-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:30:55.265: INFO: kube-scheduler-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:30:55.265: INFO: kube-flannel-ds-b9v5p from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container kube-flannel ready: true, restart count 2
Sep 13 17:30:55.265: INFO: kubernetes-dashboard-7646bf6898-rqsn9 from kube-system started at 2019-09-13 16:13:29 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.265: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 13 17:30:55.265: INFO: 
Logging pods the kubelet thinks is on node k8s-test-002 before test
Sep 13 17:30:55.272: INFO: kube-proxy-vmlts from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 17:30:55.272: INFO: testdns-4r5d2 from default started at 2019-09-13 16:34:53 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container webserver ready: true, restart count 3
Sep 13 17:30:55.272: INFO: sonobuoy-e2e-job-04d4c12741174325 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container e2e ready: true, restart count 0
Sep 13 17:30:55.272: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:30:55.272: INFO: kube-flannel-ds-t4cvn from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container kube-flannel ready: true, restart count 1
Sep 13 17:30:55.272: INFO: nginx-65f88748fd-bqq75 from default started at 2019-09-13 16:34:33 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container nginx ready: true, restart count 0
Sep 13 17:30:55.272: INFO: sonobuoy from sonobuoy started at 2019-09-13 16:48:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 13 17:30:55.272: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-6vgpm from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:30:55.272: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:30:55.272: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15c40fb87d6f02af], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:30:56.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1097" for this suite.
Sep 13 17:31:02.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:31:02.358: INFO: namespace sched-pred-1097 deletion completed in 6.068220121s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:7.136 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:31:02.358: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:31:02.380: INFO: Creating deployment "nginx-deployment"
Sep 13 17:31:02.385: INFO: Waiting for observed generation 1
Sep 13 17:31:04.391: INFO: Waiting for all required pods to come up
Sep 13 17:31:04.394: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Sep 13 17:31:06.400: INFO: Waiting for deployment "nginx-deployment" to complete
Sep 13 17:31:06.405: INFO: Updating deployment "nginx-deployment" with a non-existent image
Sep 13 17:31:06.410: INFO: Updating deployment nginx-deployment
Sep 13 17:31:06.410: INFO: Waiting for observed generation 2
Sep 13 17:31:08.415: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Sep 13 17:31:08.418: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Sep 13 17:31:08.420: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 13 17:31:08.425: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Sep 13 17:31:08.425: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Sep 13 17:31:08.427: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Sep 13 17:31:08.431: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Sep 13 17:31:08.431: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Sep 13 17:31:08.437: INFO: Updating deployment nginx-deployment
Sep 13 17:31:08.437: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Sep 13 17:31:08.444: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Sep 13 17:31:08.449: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 13 17:31:08.470: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/deployments/nginx-deployment,UID:41f30f0b-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11227,Generation:3,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-09-13 17:31:06 +0000 UTC 2019-09-13 17:31:02 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-5f9595f595" is progressing.} {Available False 2019-09-13 17:31:08 +0000 UTC 2019-09-13 17:31:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Sep 13 17:31:08.512: INFO: New ReplicaSet "nginx-deployment-5f9595f595" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/replicasets/nginx-deployment-5f9595f595,UID:4459d81b-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11208,Generation:3,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 41f30f0b-d64c-11e9-ad0a-02001700bd9c 0xc003011d47 0xc003011d48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 17:31:08.512: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Sep 13 17:31:08.513: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8,GenerateName:,Namespace:deployment-9669,SelfLink:/apis/apps/v1/namespaces/deployment-9669/replicasets/nginx-deployment-6f478d8d8,UID:41f3de33-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11206,Generation:3,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 41f30f0b-d64c-11e9-ad0a-02001700bd9c 0xc003011e37 0xc003011e38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Sep 13 17:31:08.545: INFO: Pod "nginx-deployment-5f9595f595-6qbtd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-6qbtd,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-6qbtd,UID:445b249b-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11171,Generation:0,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc0031438b7 0xc0031438b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003143990} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003143a20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:,StartTime:2019-09-13 17:31:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.545: INFO: Pod "nginx-deployment-5f9595f595-7fpjh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-7fpjh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-7fpjh,UID:44663d13-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11192,Generation:0,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc003143cd0 0xc003143cd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003143e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003143e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:,StartTime:2019-09-13 17:31:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.546: INFO: Pod "nginx-deployment-5f9595f595-84rrs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-84rrs,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-84rrs,UID:45954723-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11248,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66070 0xc001b66071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b660f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.546: INFO: Pod "nginx-deployment-5f9595f595-9z28f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-9z28f,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-9z28f,UID:4591beb9-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11233,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66280 0xc001b66281}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66310} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66330}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.546: INFO: Pod "nginx-deployment-5f9595f595-dhmtk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-dhmtk,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-dhmtk,UID:446557ee-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11194,Generation:0,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b663c0 0xc001b663c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:,StartTime:2019-09-13 17:31:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.546: INFO: Pod "nginx-deployment-5f9595f595-htgnn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-htgnn,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-htgnn,UID:445a8d1a-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11165,Generation:0,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66530 0xc001b66531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b665b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b665d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:,StartTime:2019-09-13 17:31:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.546: INFO: Pod "nginx-deployment-5f9595f595-j4x5t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-j4x5t,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-j4x5t,UID:4598d943-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11251,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b666c0 0xc001b666c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-kxnnh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-kxnnh,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-kxnnh,UID:445b3c9f-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11174,Generation:0,CreationTimestamp:2019-09-13 17:31:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b667c7 0xc001b667c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66840} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66860}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:06 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:,StartTime:2019-09-13 17:31:06 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-lfc7p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-lfc7p,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-lfc7p,UID:45958aa6-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11250,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66930 0xc001b66931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b669b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b669d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-p2dk8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-p2dk8,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-p2dk8,UID:4591dc20-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11237,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66a90 0xc001b66a91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66b40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-pjx86" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-pjx86,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-pjx86,UID:45908421-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11255,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b66d70 0xc001b66d71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b66e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b66ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:,StartTime:2019-09-13 17:31:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-rlrl4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-rlrl4,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-rlrl4,UID:4595694a-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11257,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b67010 0xc001b67011}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67090} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b670b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-5f9595f595-tngvm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-5f9595f595-tngvm,GenerateName:nginx-deployment-5f9595f595-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-5f9595f595-tngvm,UID:4594c916-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11260,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 5f9595f595,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-5f9595f595 4459d81b-d64c-11e9-ad0a-02001700bd9c 0xc001b671a0 0xc001b671a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.547: INFO: Pod "nginx-deployment-6f478d8d8-2md7h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-2md7h,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-2md7h,UID:41faa570-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11143,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b672d0 0xc001b672d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b673d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:10.88.0.43,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:04 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://ca292868ee9fa035e8e5afe6d469a5265268e258745d9c2c787627dd10555d55}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-5f2dn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5f2dn,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-5f2dn,UID:45909aa5-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11226,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b67560 0xc001b67561}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-5jg9f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-5jg9f,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-5jg9f,UID:458f92f6-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11235,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b677c0 0xc001b677c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:,StartTime:2019-09-13 17:31:08 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-7jmg2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7jmg2,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-7jmg2,UID:459574e2-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11258,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b67a07 0xc001b67a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-7prnz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-7prnz,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-7prnz,UID:45951739-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11259,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b67c10 0xc001b67c11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67cb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-bbn9r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bbn9r,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-bbn9r,UID:45928716-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11231,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b67dd0 0xc001b67dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001b67ea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001b67ec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.548: INFO: Pod "nginx-deployment-6f478d8d8-bnvld" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-bnvld,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-bnvld,UID:459258d1-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11239,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc001b67fd0 0xc001b67fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-dvm6j" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-dvm6j,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-dvm6j,UID:41f506d4-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11111,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b20f0 0xc0025b20f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2160} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2180}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.90,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1724c0c414be9681662087f758bf0215a0e51d5fc0e87020aeaaddaf20ebb826}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-hnl5r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-hnl5r,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-hnl5r,UID:45927a28-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11245,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2250 0xc0025b2251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b22c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b22e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-llt6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-llt6s,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-llt6s,UID:45927647-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11242,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2380 0xc0025b2381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2400} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-ngkx7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-ngkx7,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-ngkx7,UID:41f57bf5-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11124,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b24b0 0xc0025b24b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2550}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:10.88.0.40,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fa7903c9e2c6ec7e298bb457dfc936edbd29a1605f8025e4e9202863e732ece8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-rmr2x" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-rmr2x,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-rmr2x,UID:41f6be07-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11121,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2620 0xc0025b2621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b26a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b26d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:10.88.0.42,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://59b9b33a9553132b265d3f420b55bb412b2c7f7f483fdc7ff482c366b7c9bd1c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-s8ksx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-s8ksx,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-s8ksx,UID:41f58a08-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11134,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b27e0 0xc0025b27e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:05 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:05 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.91,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0d012c3c4f14d1af2adc4c0c3e03a896cb397210c51d965ec9d426615e10b2fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.549: INFO: Pod "nginx-deployment-6f478d8d8-v5gdr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-v5gdr,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-v5gdr,UID:4595362b-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11249,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2950 0xc0025b2951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b29c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b29e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-vvfsw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-vvfsw,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-vvfsw,UID:41f70ca8-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11131,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2a60 0xc0025b2a61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2ad0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2af0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.93,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://39b4871292792b84b9ce9ad7ab8f26c28f09fb3cfe8e43f016401c2224ee7926}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-w5v7l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-w5v7l,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-w5v7l,UID:45955414-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11252,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2c00 0xc0025b2c01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2c70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2c90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-wtrz9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-wtrz9,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-wtrz9,UID:41f92c25-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11102,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2d20 0xc0025b2d21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2d90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2db0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:03 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:03 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.92,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://0facbea02fb07389d6586dd681e4681d16102bde324d496a531025ca53e8b877}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-xg7h8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xg7h8,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-xg7h8,UID:41f6d94e-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11118,Generation:0,CreationTimestamp:2019-09-13 17:31:02 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b2e90 0xc0025b2e91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b2f30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b2f50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:04 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:02 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.23,PodIP:10.88.0.41,StartTime:2019-09-13 17:31:02 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-09-13 17:31:03 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://3d59336f18da055421ae6b31cb1df13e86ebff1e657261411d783ca15dd02dd5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-xmrch" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xmrch,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-xmrch,UID:45950025-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11253,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b3040 0xc0025b3041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-001,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b30b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b30e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Sep 13 17:31:08.550: INFO: Pod "nginx-deployment-6f478d8d8-xv92t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-6f478d8d8-xv92t,GenerateName:nginx-deployment-6f478d8d8-,Namespace:deployment-9669,SelfLink:/api/v1/namespaces/deployment-9669/pods/nginx-deployment-6f478d8d8-xv92t,UID:459048de-d64c-11e9-ad0a-02001700bd9c,ResourceVersion:11216,Generation:0,CreationTimestamp:2019-09-13 17:31:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 6f478d8d8,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-6f478d8d8 41f3de33-d64c-11e9-ad0a-02001700bd9c 0xc0025b3160 0xc0025b3161}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fw5gv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fw5gv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-fw5gv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b31d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b31f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:31:08 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:31:08.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9669" for this suite.
Sep 13 17:31:16.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:31:16.678: INFO: namespace deployment-9669 deletion completed in 8.100377402s

â€¢ [SLOW TEST:14.320 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:31:16.679: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Sep 13 17:31:22.738: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:22.741: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:24.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:24.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:26.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:26.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:28.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:28.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:30.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:30.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:32.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:32.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:34.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:34.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:36.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:36.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:38.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:38.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:40.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:40.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:42.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:42.750: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:44.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:44.744: INFO: Pod pod-with-poststart-exec-hook still exists
Sep 13 17:31:46.741: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Sep 13 17:31:46.744: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:31:46.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3605" for this suite.
Sep 13 17:32:08.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:32:08.817: INFO: namespace container-lifecycle-hook-3605 deletion completed in 22.07007645s

â€¢ [SLOW TEST:52.138 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:32:08.818: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 13 17:32:08.851: INFO: Waiting up to 5m0s for pod "pod-6990e91c-d64c-11e9-80a0-429a7732c741" in namespace "emptydir-6763" to be "success or failure"
Sep 13 17:32:08.854: INFO: Pod "pod-6990e91c-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.593503ms
Sep 13 17:32:10.857: INFO: Pod "pod-6990e91c-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005791141s
STEP: Saw pod success
Sep 13 17:32:10.857: INFO: Pod "pod-6990e91c-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:32:10.860: INFO: Trying to get logs from node k8s-test-002 pod pod-6990e91c-d64c-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:32:10.873: INFO: Waiting for pod pod-6990e91c-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:32:10.875: INFO: Pod pod-6990e91c-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:32:10.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6763" for this suite.
Sep 13 17:32:16.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:32:16.947: INFO: namespace emptydir-6763 deletion completed in 6.069763686s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:32:16.948: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Sep 13 17:32:16.974: INFO: Pod name pod-release: Found 0 pods out of 1
Sep 13 17:32:21.977: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:32:22.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2498" for this suite.
Sep 13 17:32:28.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:32:29.059: INFO: namespace replication-controller-2498 deletion completed in 6.068815525s

â€¢ [SLOW TEST:12.112 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:32:29.060: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:32:29.088: INFO: Waiting up to 5m0s for pod "downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741" in namespace "downward-api-5760" to be "success or failure"
Sep 13 17:32:29.090: INFO: Pod "downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.74651ms
Sep 13 17:32:31.094: INFO: Pod "downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006213836s
STEP: Saw pod success
Sep 13 17:32:31.094: INFO: Pod "downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:32:31.097: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:32:31.110: INFO: Waiting for pod downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:32:31.112: INFO: Pod downwardapi-volume-75a0b02c-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:32:31.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5760" for this suite.
Sep 13 17:32:37.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:32:37.182: INFO: namespace downward-api-5760 deletion completed in 6.06748739s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:32:37.182: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-7a7850f3-d64c-11e9-80a0-429a7732c741
STEP: Creating configMap with name cm-test-opt-upd-7a785132-d64c-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7a7850f3-d64c-11e9-80a0-429a7732c741
STEP: Updating configmap cm-test-opt-upd-7a785132-d64c-11e9-80a0-429a7732c741
STEP: Creating configMap with name cm-test-opt-create-7a78514e-d64c-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:32:41.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3201" for this suite.
Sep 13 17:33:03.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:33:03.350: INFO: namespace configmap-3201 deletion completed in 22.06904492s

â€¢ [SLOW TEST:26.167 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:33:03.350: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-8a10da6f-d64c-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:33:03.380: INFO: Waiting up to 5m0s for pod "pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741" in namespace "configmap-8929" to be "success or failure"
Sep 13 17:33:03.382: INFO: Pod "pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313367ms
Sep 13 17:33:05.385: INFO: Pod "pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005132645s
Sep 13 17:33:07.388: INFO: Pod "pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007949489s
STEP: Saw pod success
Sep 13 17:33:07.388: INFO: Pod "pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:33:07.390: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:33:07.402: INFO: Waiting for pod pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:33:07.404: INFO: Pod pod-configmaps-8a114271-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:33:07.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8929" for this suite.
Sep 13 17:33:13.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:33:13.474: INFO: namespace configmap-8929 deletion completed in 6.067467322s

â€¢ [SLOW TEST:10.125 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:33:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's command
Sep 13 17:33:13.506: INFO: Waiting up to 5m0s for pod "var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741" in namespace "var-expansion-8158" to be "success or failure"
Sep 13 17:33:13.508: INFO: Pod "var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.126115ms
Sep 13 17:33:15.511: INFO: Pod "var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005017027s
STEP: Saw pod success
Sep 13 17:33:15.511: INFO: Pod "var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:33:15.513: INFO: Trying to get logs from node k8s-test-002 pod var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:33:15.527: INFO: Waiting for pod var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:33:15.529: INFO: Pod var-expansion-901a3dee-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:33:15.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8158" for this suite.
Sep 13 17:33:21.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:33:21.600: INFO: namespace var-expansion-8158 deletion completed in 6.068220278s

â€¢ [SLOW TEST:8.126 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:33:21.600: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1455
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 17:33:21.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9596'
Sep 13 17:33:21.709: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 13 17:33:21.709: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1460
Sep 13 17:33:23.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9596'
Sep 13 17:33:23.811: INFO: stderr: ""
Sep 13 17:33:23.811: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:33:23.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9596" for this suite.
Sep 13 17:33:29.822: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:33:29.881: INFO: namespace kubectl-9596 deletion completed in 6.066927158s

â€¢ [SLOW TEST:8.281 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:33:29.881: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-99e10900-d64c-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:33:29.910: INFO: Waiting up to 5m0s for pod "pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741" in namespace "configmap-3620" to be "success or failure"
Sep 13 17:33:29.912: INFO: Pod "pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525406ms
Sep 13 17:33:31.915: INFO: Pod "pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005349762s
Sep 13 17:33:33.918: INFO: Pod "pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008153561s
STEP: Saw pod success
Sep 13 17:33:33.918: INFO: Pod "pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:33:33.920: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:33:33.932: INFO: Waiting for pod pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:33:33.934: INFO: Pod pod-configmaps-99e16ad8-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:33:33.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3620" for this suite.
Sep 13 17:33:39.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:33:40.004: INFO: namespace configmap-3620 deletion completed in 6.067129736s

â€¢ [SLOW TEST:10.123 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:33:40.004: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1326
I0913 17:33:40.029420      16 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1326, replica count: 1
I0913 17:33:41.079716      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0913 17:33:42.079898      16 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Sep 13 17:33:42.187: INFO: Created: latency-svc-49wzw
Sep 13 17:33:42.191: INFO: Got endpoints: latency-svc-49wzw [11.673184ms]
Sep 13 17:33:42.204: INFO: Created: latency-svc-4nn85
Sep 13 17:33:42.213: INFO: Got endpoints: latency-svc-4nn85 [21.136265ms]
Sep 13 17:33:42.217: INFO: Created: latency-svc-82pk2
Sep 13 17:33:42.224: INFO: Got endpoints: latency-svc-82pk2 [32.659336ms]
Sep 13 17:33:42.224: INFO: Created: latency-svc-7xxzz
Sep 13 17:33:42.228: INFO: Got endpoints: latency-svc-7xxzz [35.504312ms]
Sep 13 17:33:42.235: INFO: Created: latency-svc-bpthf
Sep 13 17:33:42.240: INFO: Got endpoints: latency-svc-bpthf [47.39828ms]
Sep 13 17:33:42.247: INFO: Created: latency-svc-jkbvp
Sep 13 17:33:42.249: INFO: Got endpoints: latency-svc-jkbvp [57.47195ms]
Sep 13 17:33:42.256: INFO: Created: latency-svc-mvfs4
Sep 13 17:33:42.260: INFO: Got endpoints: latency-svc-mvfs4 [67.523867ms]
Sep 13 17:33:42.264: INFO: Created: latency-svc-4w4hv
Sep 13 17:33:42.267: INFO: Got endpoints: latency-svc-4w4hv [74.855853ms]
Sep 13 17:33:42.274: INFO: Created: latency-svc-wcltt
Sep 13 17:33:42.277: INFO: Got endpoints: latency-svc-wcltt [17.537233ms]
Sep 13 17:33:42.285: INFO: Created: latency-svc-z9tph
Sep 13 17:33:42.287: INFO: Got endpoints: latency-svc-z9tph [94.753721ms]
Sep 13 17:33:42.293: INFO: Created: latency-svc-2rtmv
Sep 13 17:33:42.296: INFO: Got endpoints: latency-svc-2rtmv [104.086827ms]
Sep 13 17:33:42.305: INFO: Created: latency-svc-tpxwf
Sep 13 17:33:42.307: INFO: Got endpoints: latency-svc-tpxwf [114.38577ms]
Sep 13 17:33:42.312: INFO: Created: latency-svc-dtw49
Sep 13 17:33:42.326: INFO: Got endpoints: latency-svc-dtw49 [134.421536ms]
Sep 13 17:33:42.332: INFO: Created: latency-svc-wjxgd
Sep 13 17:33:42.336: INFO: Got endpoints: latency-svc-wjxgd [143.672218ms]
Sep 13 17:33:42.343: INFO: Created: latency-svc-9klzs
Sep 13 17:33:42.350: INFO: Got endpoints: latency-svc-9klzs [157.911042ms]
Sep 13 17:33:42.357: INFO: Created: latency-svc-s6626
Sep 13 17:33:42.362: INFO: Got endpoints: latency-svc-s6626 [169.751912ms]
Sep 13 17:33:42.363: INFO: Created: latency-svc-pf9rh
Sep 13 17:33:42.370: INFO: Created: latency-svc-wh8rn
Sep 13 17:33:42.370: INFO: Got endpoints: latency-svc-pf9rh [177.941549ms]
Sep 13 17:33:42.376: INFO: Created: latency-svc-v66gg
Sep 13 17:33:42.377: INFO: Got endpoints: latency-svc-wh8rn [163.81746ms]
Sep 13 17:33:42.381: INFO: Got endpoints: latency-svc-v66gg [157.136999ms]
Sep 13 17:33:42.389: INFO: Created: latency-svc-rqz5q
Sep 13 17:33:42.393: INFO: Got endpoints: latency-svc-rqz5q [164.82829ms]
Sep 13 17:33:42.396: INFO: Created: latency-svc-b42vx
Sep 13 17:33:42.398: INFO: Got endpoints: latency-svc-b42vx [157.654029ms]
Sep 13 17:33:42.405: INFO: Created: latency-svc-2cklx
Sep 13 17:33:42.412: INFO: Created: latency-svc-wcj69
Sep 13 17:33:42.412: INFO: Got endpoints: latency-svc-2cklx [163.048086ms]
Sep 13 17:33:42.417: INFO: Got endpoints: latency-svc-wcj69 [149.924556ms]
Sep 13 17:33:42.421: INFO: Created: latency-svc-ph5hd
Sep 13 17:33:42.436: INFO: Got endpoints: latency-svc-ph5hd [158.434807ms]
Sep 13 17:33:42.438: INFO: Created: latency-svc-kkdq5
Sep 13 17:33:42.442: INFO: Got endpoints: latency-svc-kkdq5 [155.204768ms]
Sep 13 17:33:42.451: INFO: Created: latency-svc-cc2x9
Sep 13 17:33:42.456: INFO: Got endpoints: latency-svc-cc2x9 [159.481964ms]
Sep 13 17:33:42.467: INFO: Created: latency-svc-lfg6v
Sep 13 17:33:42.477: INFO: Got endpoints: latency-svc-lfg6v [170.296434ms]
Sep 13 17:33:42.546: INFO: Created: latency-svc-48426
Sep 13 17:33:42.549: INFO: Got endpoints: latency-svc-48426 [222.56689ms]
Sep 13 17:33:42.556: INFO: Created: latency-svc-ctlxf
Sep 13 17:33:42.559: INFO: Got endpoints: latency-svc-ctlxf [223.631571ms]
Sep 13 17:33:42.565: INFO: Created: latency-svc-bpndz
Sep 13 17:33:42.572: INFO: Got endpoints: latency-svc-bpndz [222.334554ms]
Sep 13 17:33:42.573: INFO: Created: latency-svc-lww7z
Sep 13 17:33:42.578: INFO: Got endpoints: latency-svc-lww7z [215.845712ms]
Sep 13 17:33:42.584: INFO: Created: latency-svc-6kst7
Sep 13 17:33:42.588: INFO: Got endpoints: latency-svc-6kst7 [218.160271ms]
Sep 13 17:33:42.594: INFO: Created: latency-svc-g8pgr
Sep 13 17:33:42.598: INFO: Got endpoints: latency-svc-g8pgr [221.204711ms]
Sep 13 17:33:42.605: INFO: Created: latency-svc-fw2np
Sep 13 17:33:42.609: INFO: Got endpoints: latency-svc-fw2np [227.851689ms]
Sep 13 17:33:42.616: INFO: Created: latency-svc-jwgh2
Sep 13 17:33:42.619: INFO: Got endpoints: latency-svc-jwgh2 [226.533103ms]
Sep 13 17:33:42.626: INFO: Created: latency-svc-c5bnk
Sep 13 17:33:42.629: INFO: Got endpoints: latency-svc-c5bnk [231.584854ms]
Sep 13 17:33:42.636: INFO: Created: latency-svc-8kkqv
Sep 13 17:33:42.636: INFO: Got endpoints: latency-svc-8kkqv [223.776002ms]
Sep 13 17:33:42.643: INFO: Created: latency-svc-z28b4
Sep 13 17:33:42.655: INFO: Got endpoints: latency-svc-z28b4 [238.676179ms]
Sep 13 17:33:42.658: INFO: Created: latency-svc-8stsh
Sep 13 17:33:42.663: INFO: Created: latency-svc-scc5r
Sep 13 17:33:42.663: INFO: Got endpoints: latency-svc-8stsh [227.747694ms]
Sep 13 17:33:42.670: INFO: Created: latency-svc-q85ml
Sep 13 17:33:42.677: INFO: Created: latency-svc-zdsw2
Sep 13 17:33:42.687: INFO: Created: latency-svc-ntwbw
Sep 13 17:33:42.694: INFO: Got endpoints: latency-svc-scc5r [252.112175ms]
Sep 13 17:33:42.695: INFO: Created: latency-svc-vntfx
Sep 13 17:33:42.701: INFO: Created: latency-svc-znzfm
Sep 13 17:33:42.708: INFO: Created: latency-svc-sstw6
Sep 13 17:33:42.714: INFO: Created: latency-svc-9bkml
Sep 13 17:33:42.724: INFO: Created: latency-svc-kjv8q
Sep 13 17:33:42.734: INFO: Created: latency-svc-mxmgf
Sep 13 17:33:42.742: INFO: Created: latency-svc-wh8r2
Sep 13 17:33:42.743: INFO: Got endpoints: latency-svc-q85ml [287.273703ms]
Sep 13 17:33:42.763: INFO: Created: latency-svc-kcfhb
Sep 13 17:33:42.771: INFO: Created: latency-svc-zx7nw
Sep 13 17:33:42.782: INFO: Created: latency-svc-d4qg7
Sep 13 17:33:42.791: INFO: Created: latency-svc-pcg9f
Sep 13 17:33:42.793: INFO: Got endpoints: latency-svc-zdsw2 [316.084449ms]
Sep 13 17:33:42.800: INFO: Created: latency-svc-bnpns
Sep 13 17:33:42.808: INFO: Created: latency-svc-sq9sp
Sep 13 17:33:42.818: INFO: Created: latency-svc-8cvzw
Sep 13 17:33:42.841: INFO: Got endpoints: latency-svc-ntwbw [291.733443ms]
Sep 13 17:33:42.888: INFO: Created: latency-svc-f8g5n
Sep 13 17:33:42.892: INFO: Got endpoints: latency-svc-vntfx [332.294446ms]
Sep 13 17:33:42.902: INFO: Created: latency-svc-ktbdp
Sep 13 17:33:42.941: INFO: Got endpoints: latency-svc-znzfm [368.948909ms]
Sep 13 17:33:42.951: INFO: Created: latency-svc-mgpq8
Sep 13 17:33:42.991: INFO: Got endpoints: latency-svc-sstw6 [413.38803ms]
Sep 13 17:33:43.005: INFO: Created: latency-svc-cmj6g
Sep 13 17:33:43.041: INFO: Got endpoints: latency-svc-9bkml [452.423407ms]
Sep 13 17:33:43.052: INFO: Created: latency-svc-pt75h
Sep 13 17:33:43.095: INFO: Got endpoints: latency-svc-kjv8q [497.420295ms]
Sep 13 17:33:43.105: INFO: Created: latency-svc-pc6xl
Sep 13 17:33:43.141: INFO: Got endpoints: latency-svc-mxmgf [531.655002ms]
Sep 13 17:33:43.153: INFO: Created: latency-svc-8mcbg
Sep 13 17:33:43.191: INFO: Got endpoints: latency-svc-wh8r2 [572.294633ms]
Sep 13 17:33:43.204: INFO: Created: latency-svc-lrcx6
Sep 13 17:33:43.241: INFO: Got endpoints: latency-svc-kcfhb [611.485612ms]
Sep 13 17:33:43.251: INFO: Created: latency-svc-68h4f
Sep 13 17:33:43.290: INFO: Got endpoints: latency-svc-zx7nw [654.183281ms]
Sep 13 17:33:43.301: INFO: Created: latency-svc-thmfg
Sep 13 17:33:43.342: INFO: Got endpoints: latency-svc-d4qg7 [686.209367ms]
Sep 13 17:33:43.352: INFO: Created: latency-svc-cm76t
Sep 13 17:33:43.391: INFO: Got endpoints: latency-svc-pcg9f [727.850851ms]
Sep 13 17:33:43.401: INFO: Created: latency-svc-mlf76
Sep 13 17:33:43.441: INFO: Got endpoints: latency-svc-bnpns [746.547872ms]
Sep 13 17:33:43.452: INFO: Created: latency-svc-pl7w8
Sep 13 17:33:43.491: INFO: Got endpoints: latency-svc-sq9sp [748.441251ms]
Sep 13 17:33:43.505: INFO: Created: latency-svc-4sjft
Sep 13 17:33:43.541: INFO: Got endpoints: latency-svc-8cvzw [748.15918ms]
Sep 13 17:33:43.552: INFO: Created: latency-svc-hrhxp
Sep 13 17:33:43.591: INFO: Got endpoints: latency-svc-f8g5n [749.840409ms]
Sep 13 17:33:43.602: INFO: Created: latency-svc-x9xv6
Sep 13 17:33:43.643: INFO: Got endpoints: latency-svc-ktbdp [751.430197ms]
Sep 13 17:33:43.658: INFO: Created: latency-svc-b56vr
Sep 13 17:33:43.691: INFO: Got endpoints: latency-svc-mgpq8 [749.342053ms]
Sep 13 17:33:43.701: INFO: Created: latency-svc-vhj4q
Sep 13 17:33:43.741: INFO: Got endpoints: latency-svc-cmj6g [749.477097ms]
Sep 13 17:33:43.753: INFO: Created: latency-svc-7v9nx
Sep 13 17:33:43.790: INFO: Got endpoints: latency-svc-pt75h [749.28205ms]
Sep 13 17:33:43.800: INFO: Created: latency-svc-9t22h
Sep 13 17:33:43.841: INFO: Got endpoints: latency-svc-pc6xl [745.43972ms]
Sep 13 17:33:43.850: INFO: Created: latency-svc-5gqnl
Sep 13 17:33:43.891: INFO: Got endpoints: latency-svc-8mcbg [749.744879ms]
Sep 13 17:33:43.900: INFO: Created: latency-svc-k65fg
Sep 13 17:33:43.941: INFO: Got endpoints: latency-svc-lrcx6 [749.30889ms]
Sep 13 17:33:43.951: INFO: Created: latency-svc-mglmk
Sep 13 17:33:43.991: INFO: Got endpoints: latency-svc-68h4f [749.742205ms]
Sep 13 17:33:44.000: INFO: Created: latency-svc-rtnfv
Sep 13 17:33:44.042: INFO: Got endpoints: latency-svc-thmfg [751.915558ms]
Sep 13 17:33:44.052: INFO: Created: latency-svc-fzrsc
Sep 13 17:33:44.091: INFO: Got endpoints: latency-svc-cm76t [749.0368ms]
Sep 13 17:33:44.103: INFO: Created: latency-svc-cj96g
Sep 13 17:33:44.141: INFO: Got endpoints: latency-svc-mlf76 [749.96907ms]
Sep 13 17:33:44.155: INFO: Created: latency-svc-4r7nx
Sep 13 17:33:44.191: INFO: Got endpoints: latency-svc-pl7w8 [749.975904ms]
Sep 13 17:33:44.202: INFO: Created: latency-svc-g5q5l
Sep 13 17:33:44.241: INFO: Got endpoints: latency-svc-4sjft [749.422384ms]
Sep 13 17:33:44.254: INFO: Created: latency-svc-4s22s
Sep 13 17:33:44.291: INFO: Got endpoints: latency-svc-hrhxp [749.550023ms]
Sep 13 17:33:44.366: INFO: Got endpoints: latency-svc-x9xv6 [774.856253ms]
Sep 13 17:33:44.372: INFO: Created: latency-svc-9q4vf
Sep 13 17:33:44.386: INFO: Created: latency-svc-7c9mt
Sep 13 17:33:44.391: INFO: Got endpoints: latency-svc-b56vr [747.297161ms]
Sep 13 17:33:44.401: INFO: Created: latency-svc-p5r4k
Sep 13 17:33:44.441: INFO: Got endpoints: latency-svc-vhj4q [749.876747ms]
Sep 13 17:33:44.451: INFO: Created: latency-svc-xnwtx
Sep 13 17:33:44.491: INFO: Got endpoints: latency-svc-7v9nx [750.036077ms]
Sep 13 17:33:44.510: INFO: Created: latency-svc-sj9cf
Sep 13 17:33:44.541: INFO: Got endpoints: latency-svc-9t22h [750.517261ms]
Sep 13 17:33:44.551: INFO: Created: latency-svc-rqbk7
Sep 13 17:33:44.591: INFO: Got endpoints: latency-svc-5gqnl [750.077264ms]
Sep 13 17:33:44.600: INFO: Created: latency-svc-p89vd
Sep 13 17:33:44.641: INFO: Got endpoints: latency-svc-k65fg [749.911843ms]
Sep 13 17:33:44.652: INFO: Created: latency-svc-ndrn4
Sep 13 17:33:44.691: INFO: Got endpoints: latency-svc-mglmk [749.656044ms]
Sep 13 17:33:44.701: INFO: Created: latency-svc-9tmdp
Sep 13 17:33:44.741: INFO: Got endpoints: latency-svc-rtnfv [750.12285ms]
Sep 13 17:33:44.750: INFO: Created: latency-svc-ptqb9
Sep 13 17:33:44.791: INFO: Got endpoints: latency-svc-fzrsc [748.390104ms]
Sep 13 17:33:44.801: INFO: Created: latency-svc-pxhpk
Sep 13 17:33:44.841: INFO: Got endpoints: latency-svc-cj96g [749.913447ms]
Sep 13 17:33:44.851: INFO: Created: latency-svc-7wld4
Sep 13 17:33:44.891: INFO: Got endpoints: latency-svc-4r7nx [749.021339ms]
Sep 13 17:33:44.900: INFO: Created: latency-svc-w5pv6
Sep 13 17:33:44.941: INFO: Got endpoints: latency-svc-g5q5l [749.931991ms]
Sep 13 17:33:44.951: INFO: Created: latency-svc-84h7t
Sep 13 17:33:44.991: INFO: Got endpoints: latency-svc-4s22s [749.533392ms]
Sep 13 17:33:45.000: INFO: Created: latency-svc-2q8b2
Sep 13 17:33:45.041: INFO: Got endpoints: latency-svc-9q4vf [750.454122ms]
Sep 13 17:33:45.051: INFO: Created: latency-svc-2nb5d
Sep 13 17:33:45.091: INFO: Got endpoints: latency-svc-7c9mt [724.879537ms]
Sep 13 17:33:45.104: INFO: Created: latency-svc-8lb76
Sep 13 17:33:45.142: INFO: Got endpoints: latency-svc-p5r4k [750.770837ms]
Sep 13 17:33:45.152: INFO: Created: latency-svc-hhfjv
Sep 13 17:33:45.191: INFO: Got endpoints: latency-svc-xnwtx [750.095448ms]
Sep 13 17:33:45.206: INFO: Created: latency-svc-p4l8b
Sep 13 17:33:45.241: INFO: Got endpoints: latency-svc-sj9cf [750.377839ms]
Sep 13 17:33:45.252: INFO: Created: latency-svc-q82xx
Sep 13 17:33:45.291: INFO: Got endpoints: latency-svc-rqbk7 [749.540606ms]
Sep 13 17:33:45.301: INFO: Created: latency-svc-xp4n4
Sep 13 17:33:45.341: INFO: Got endpoints: latency-svc-p89vd [750.02598ms]
Sep 13 17:33:45.350: INFO: Created: latency-svc-fbljc
Sep 13 17:33:45.403: INFO: Got endpoints: latency-svc-ndrn4 [762.49119ms]
Sep 13 17:33:45.425: INFO: Created: latency-svc-jr82n
Sep 13 17:33:45.441: INFO: Got endpoints: latency-svc-9tmdp [750.100768ms]
Sep 13 17:33:45.451: INFO: Created: latency-svc-pk64d
Sep 13 17:33:45.492: INFO: Got endpoints: latency-svc-ptqb9 [751.03837ms]
Sep 13 17:33:45.503: INFO: Created: latency-svc-8sgbs
Sep 13 17:33:45.541: INFO: Got endpoints: latency-svc-pxhpk [749.886456ms]
Sep 13 17:33:45.551: INFO: Created: latency-svc-sjtmt
Sep 13 17:33:45.591: INFO: Got endpoints: latency-svc-7wld4 [749.900182ms]
Sep 13 17:33:45.600: INFO: Created: latency-svc-5k5rj
Sep 13 17:33:45.640: INFO: Got endpoints: latency-svc-w5pv6 [749.590961ms]
Sep 13 17:33:45.653: INFO: Created: latency-svc-b6s74
Sep 13 17:33:45.691: INFO: Got endpoints: latency-svc-84h7t [750.049894ms]
Sep 13 17:33:45.701: INFO: Created: latency-svc-jcjl6
Sep 13 17:33:45.741: INFO: Got endpoints: latency-svc-2q8b2 [749.944364ms]
Sep 13 17:33:45.752: INFO: Created: latency-svc-t6s27
Sep 13 17:33:45.791: INFO: Got endpoints: latency-svc-2nb5d [749.187342ms]
Sep 13 17:33:45.799: INFO: Created: latency-svc-h9v28
Sep 13 17:33:45.841: INFO: Got endpoints: latency-svc-8lb76 [750.273243ms]
Sep 13 17:33:45.850: INFO: Created: latency-svc-4klr5
Sep 13 17:33:45.891: INFO: Got endpoints: latency-svc-hhfjv [749.017433ms]
Sep 13 17:33:45.901: INFO: Created: latency-svc-lrmgj
Sep 13 17:33:45.941: INFO: Got endpoints: latency-svc-p4l8b [749.751443ms]
Sep 13 17:33:45.952: INFO: Created: latency-svc-4z9c2
Sep 13 17:33:45.991: INFO: Got endpoints: latency-svc-q82xx [749.425921ms]
Sep 13 17:33:45.999: INFO: Created: latency-svc-9bpdv
Sep 13 17:33:46.042: INFO: Got endpoints: latency-svc-xp4n4 [751.198672ms]
Sep 13 17:33:46.052: INFO: Created: latency-svc-tlx2k
Sep 13 17:33:46.091: INFO: Got endpoints: latency-svc-fbljc [749.688502ms]
Sep 13 17:33:46.099: INFO: Created: latency-svc-bf2ch
Sep 13 17:33:46.141: INFO: Got endpoints: latency-svc-jr82n [737.438395ms]
Sep 13 17:33:46.152: INFO: Created: latency-svc-6tm84
Sep 13 17:33:46.191: INFO: Got endpoints: latency-svc-pk64d [749.929526ms]
Sep 13 17:33:46.200: INFO: Created: latency-svc-xwspk
Sep 13 17:33:46.240: INFO: Got endpoints: latency-svc-8sgbs [748.527242ms]
Sep 13 17:33:46.253: INFO: Created: latency-svc-n6rdq
Sep 13 17:33:46.291: INFO: Got endpoints: latency-svc-sjtmt [750.019605ms]
Sep 13 17:33:46.300: INFO: Created: latency-svc-c6hc5
Sep 13 17:33:46.341: INFO: Got endpoints: latency-svc-5k5rj [749.964962ms]
Sep 13 17:33:46.352: INFO: Created: latency-svc-c5kgs
Sep 13 17:33:46.391: INFO: Got endpoints: latency-svc-b6s74 [750.476623ms]
Sep 13 17:33:46.401: INFO: Created: latency-svc-n9rmz
Sep 13 17:33:46.440: INFO: Got endpoints: latency-svc-jcjl6 [749.286797ms]
Sep 13 17:33:46.450: INFO: Created: latency-svc-2qzg9
Sep 13 17:33:46.491: INFO: Got endpoints: latency-svc-t6s27 [750.368512ms]
Sep 13 17:33:46.501: INFO: Created: latency-svc-5w87k
Sep 13 17:33:46.542: INFO: Got endpoints: latency-svc-h9v28 [751.033231ms]
Sep 13 17:33:46.551: INFO: Created: latency-svc-85bhl
Sep 13 17:33:46.591: INFO: Got endpoints: latency-svc-4klr5 [749.861948ms]
Sep 13 17:33:46.601: INFO: Created: latency-svc-2lpfv
Sep 13 17:33:46.641: INFO: Got endpoints: latency-svc-lrmgj [750.254397ms]
Sep 13 17:33:46.651: INFO: Created: latency-svc-8lb4d
Sep 13 17:33:46.691: INFO: Got endpoints: latency-svc-4z9c2 [749.673364ms]
Sep 13 17:33:46.701: INFO: Created: latency-svc-f7qlc
Sep 13 17:33:46.741: INFO: Got endpoints: latency-svc-9bpdv [750.281218ms]
Sep 13 17:33:46.751: INFO: Created: latency-svc-zjnp5
Sep 13 17:33:46.791: INFO: Got endpoints: latency-svc-tlx2k [748.942551ms]
Sep 13 17:33:46.801: INFO: Created: latency-svc-kvl94
Sep 13 17:33:46.841: INFO: Got endpoints: latency-svc-bf2ch [749.927913ms]
Sep 13 17:33:46.850: INFO: Created: latency-svc-xsgd8
Sep 13 17:33:46.891: INFO: Got endpoints: latency-svc-6tm84 [749.833997ms]
Sep 13 17:33:46.901: INFO: Created: latency-svc-gv6m2
Sep 13 17:33:46.941: INFO: Got endpoints: latency-svc-xwspk [749.912052ms]
Sep 13 17:33:46.950: INFO: Created: latency-svc-nwnkh
Sep 13 17:33:46.993: INFO: Got endpoints: latency-svc-n6rdq [752.300051ms]
Sep 13 17:33:47.004: INFO: Created: latency-svc-6kzb8
Sep 13 17:33:47.041: INFO: Got endpoints: latency-svc-c6hc5 [750.304351ms]
Sep 13 17:33:47.051: INFO: Created: latency-svc-r5grd
Sep 13 17:33:47.091: INFO: Got endpoints: latency-svc-c5kgs [750.014997ms]
Sep 13 17:33:47.104: INFO: Created: latency-svc-86txr
Sep 13 17:33:47.141: INFO: Got endpoints: latency-svc-n9rmz [749.561283ms]
Sep 13 17:33:47.151: INFO: Created: latency-svc-5pvsm
Sep 13 17:33:47.191: INFO: Got endpoints: latency-svc-2qzg9 [750.274836ms]
Sep 13 17:33:47.204: INFO: Created: latency-svc-t2ph7
Sep 13 17:33:47.241: INFO: Got endpoints: latency-svc-5w87k [749.315062ms]
Sep 13 17:33:47.300: INFO: Got endpoints: latency-svc-85bhl [758.488218ms]
Sep 13 17:33:47.311: INFO: Created: latency-svc-l8499
Sep 13 17:33:47.316: INFO: Created: latency-svc-99thz
Sep 13 17:33:47.341: INFO: Got endpoints: latency-svc-2lpfv [749.928564ms]
Sep 13 17:33:47.351: INFO: Created: latency-svc-v87lr
Sep 13 17:33:47.391: INFO: Got endpoints: latency-svc-8lb4d [749.999507ms]
Sep 13 17:33:47.407: INFO: Created: latency-svc-wdqhz
Sep 13 17:33:47.442: INFO: Got endpoints: latency-svc-f7qlc [751.132136ms]
Sep 13 17:33:47.452: INFO: Created: latency-svc-h628s
Sep 13 17:33:47.494: INFO: Got endpoints: latency-svc-zjnp5 [753.384017ms]
Sep 13 17:33:47.505: INFO: Created: latency-svc-bqzhv
Sep 13 17:33:47.541: INFO: Got endpoints: latency-svc-kvl94 [749.924767ms]
Sep 13 17:33:47.552: INFO: Created: latency-svc-jg8xn
Sep 13 17:33:47.591: INFO: Got endpoints: latency-svc-xsgd8 [749.879753ms]
Sep 13 17:33:47.601: INFO: Created: latency-svc-4rs6m
Sep 13 17:33:47.641: INFO: Got endpoints: latency-svc-gv6m2 [750.508535ms]
Sep 13 17:33:47.694: INFO: Got endpoints: latency-svc-nwnkh [752.794861ms]
Sep 13 17:33:47.703: INFO: Created: latency-svc-cv872
Sep 13 17:33:47.712: INFO: Created: latency-svc-hmkxw
Sep 13 17:33:47.741: INFO: Got endpoints: latency-svc-6kzb8 [747.716859ms]
Sep 13 17:33:47.750: INFO: Created: latency-svc-fbmpj
Sep 13 17:33:47.791: INFO: Got endpoints: latency-svc-r5grd [749.302117ms]
Sep 13 17:33:47.801: INFO: Created: latency-svc-pqtjm
Sep 13 17:33:47.840: INFO: Got endpoints: latency-svc-86txr [749.526639ms]
Sep 13 17:33:47.850: INFO: Created: latency-svc-vxztp
Sep 13 17:33:47.891: INFO: Got endpoints: latency-svc-5pvsm [750.025177ms]
Sep 13 17:33:47.901: INFO: Created: latency-svc-g7d5v
Sep 13 17:33:47.941: INFO: Got endpoints: latency-svc-t2ph7 [750.019124ms]
Sep 13 17:33:47.951: INFO: Created: latency-svc-qzlws
Sep 13 17:33:47.991: INFO: Got endpoints: latency-svc-l8499 [750.337203ms]
Sep 13 17:33:48.001: INFO: Created: latency-svc-dncdk
Sep 13 17:33:48.043: INFO: Got endpoints: latency-svc-99thz [742.888116ms]
Sep 13 17:33:48.053: INFO: Created: latency-svc-cbflm
Sep 13 17:33:48.091: INFO: Got endpoints: latency-svc-v87lr [749.978969ms]
Sep 13 17:33:48.101: INFO: Created: latency-svc-87bwx
Sep 13 17:33:48.141: INFO: Got endpoints: latency-svc-wdqhz [749.464983ms]
Sep 13 17:33:48.156: INFO: Created: latency-svc-5gcph
Sep 13 17:33:48.191: INFO: Got endpoints: latency-svc-h628s [748.847684ms]
Sep 13 17:33:48.201: INFO: Created: latency-svc-pxshj
Sep 13 17:33:48.241: INFO: Got endpoints: latency-svc-bqzhv [746.169772ms]
Sep 13 17:33:48.255: INFO: Created: latency-svc-86gzs
Sep 13 17:33:48.291: INFO: Got endpoints: latency-svc-jg8xn [749.845719ms]
Sep 13 17:33:48.301: INFO: Created: latency-svc-rq7z5
Sep 13 17:33:48.341: INFO: Got endpoints: latency-svc-4rs6m [750.166683ms]
Sep 13 17:33:48.350: INFO: Created: latency-svc-lvb6r
Sep 13 17:33:48.391: INFO: Got endpoints: latency-svc-cv872 [749.503575ms]
Sep 13 17:33:48.401: INFO: Created: latency-svc-fzql6
Sep 13 17:33:48.442: INFO: Got endpoints: latency-svc-hmkxw [748.000462ms]
Sep 13 17:33:48.460: INFO: Created: latency-svc-q7fbw
Sep 13 17:33:48.491: INFO: Got endpoints: latency-svc-fbmpj [750.374151ms]
Sep 13 17:33:48.501: INFO: Created: latency-svc-rmfw5
Sep 13 17:33:48.541: INFO: Got endpoints: latency-svc-pqtjm [749.858333ms]
Sep 13 17:33:48.551: INFO: Created: latency-svc-9jgp2
Sep 13 17:33:48.591: INFO: Got endpoints: latency-svc-vxztp [750.258315ms]
Sep 13 17:33:48.601: INFO: Created: latency-svc-bj87v
Sep 13 17:33:48.641: INFO: Got endpoints: latency-svc-g7d5v [749.951187ms]
Sep 13 17:33:48.650: INFO: Created: latency-svc-h9jkr
Sep 13 17:33:48.691: INFO: Got endpoints: latency-svc-qzlws [750.045796ms]
Sep 13 17:33:48.700: INFO: Created: latency-svc-bd8dk
Sep 13 17:33:48.741: INFO: Got endpoints: latency-svc-dncdk [750.120405ms]
Sep 13 17:33:48.752: INFO: Created: latency-svc-4bw29
Sep 13 17:33:48.791: INFO: Got endpoints: latency-svc-cbflm [747.805235ms]
Sep 13 17:33:48.801: INFO: Created: latency-svc-ntc2s
Sep 13 17:33:48.841: INFO: Got endpoints: latency-svc-87bwx [750.188814ms]
Sep 13 17:33:48.854: INFO: Created: latency-svc-sp866
Sep 13 17:33:48.891: INFO: Got endpoints: latency-svc-5gcph [750.426991ms]
Sep 13 17:33:48.902: INFO: Created: latency-svc-plzd7
Sep 13 17:33:48.941: INFO: Got endpoints: latency-svc-pxshj [750.050765ms]
Sep 13 17:33:48.951: INFO: Created: latency-svc-4dv7b
Sep 13 17:33:49.019: INFO: Got endpoints: latency-svc-86gzs [778.411183ms]
Sep 13 17:33:49.030: INFO: Created: latency-svc-7fcd6
Sep 13 17:33:49.042: INFO: Got endpoints: latency-svc-rq7z5 [751.032749ms]
Sep 13 17:33:49.052: INFO: Created: latency-svc-cfpvd
Sep 13 17:33:49.091: INFO: Got endpoints: latency-svc-lvb6r [750.235511ms]
Sep 13 17:33:49.100: INFO: Created: latency-svc-tvqmq
Sep 13 17:33:49.141: INFO: Got endpoints: latency-svc-fzql6 [749.863352ms]
Sep 13 17:33:49.151: INFO: Created: latency-svc-x5pqk
Sep 13 17:33:49.191: INFO: Got endpoints: latency-svc-q7fbw [749.25546ms]
Sep 13 17:33:49.200: INFO: Created: latency-svc-s5s9w
Sep 13 17:33:49.241: INFO: Got endpoints: latency-svc-rmfw5 [749.84617ms]
Sep 13 17:33:49.252: INFO: Created: latency-svc-szkxt
Sep 13 17:33:49.291: INFO: Got endpoints: latency-svc-9jgp2 [749.928184ms]
Sep 13 17:33:49.301: INFO: Created: latency-svc-s6tdf
Sep 13 17:33:49.341: INFO: Got endpoints: latency-svc-bj87v [749.959132ms]
Sep 13 17:33:49.350: INFO: Created: latency-svc-9mrxx
Sep 13 17:33:49.392: INFO: Got endpoints: latency-svc-h9jkr [750.788711ms]
Sep 13 17:33:49.402: INFO: Created: latency-svc-vlpbg
Sep 13 17:33:49.441: INFO: Got endpoints: latency-svc-bd8dk [750.403617ms]
Sep 13 17:33:49.452: INFO: Created: latency-svc-fccw7
Sep 13 17:33:49.495: INFO: Got endpoints: latency-svc-4bw29 [753.322131ms]
Sep 13 17:33:49.507: INFO: Created: latency-svc-mvddp
Sep 13 17:33:49.544: INFO: Got endpoints: latency-svc-ntc2s [753.3903ms]
Sep 13 17:33:49.555: INFO: Created: latency-svc-w78pc
Sep 13 17:33:49.591: INFO: Got endpoints: latency-svc-sp866 [749.778653ms]
Sep 13 17:33:49.602: INFO: Created: latency-svc-kpzm5
Sep 13 17:33:49.640: INFO: Got endpoints: latency-svc-plzd7 [749.163276ms]
Sep 13 17:33:49.656: INFO: Created: latency-svc-x8pj4
Sep 13 17:33:49.691: INFO: Got endpoints: latency-svc-4dv7b [750.665278ms]
Sep 13 17:33:49.701: INFO: Created: latency-svc-vsd4m
Sep 13 17:33:49.741: INFO: Got endpoints: latency-svc-7fcd6 [721.464171ms]
Sep 13 17:33:49.749: INFO: Created: latency-svc-tnldc
Sep 13 17:33:49.791: INFO: Got endpoints: latency-svc-cfpvd [749.184927ms]
Sep 13 17:33:49.802: INFO: Created: latency-svc-s6klm
Sep 13 17:33:49.841: INFO: Got endpoints: latency-svc-tvqmq [749.438734ms]
Sep 13 17:33:49.851: INFO: Created: latency-svc-k8mvp
Sep 13 17:33:49.891: INFO: Got endpoints: latency-svc-x5pqk [750.010939ms]
Sep 13 17:33:49.901: INFO: Created: latency-svc-nxmhh
Sep 13 17:33:49.940: INFO: Got endpoints: latency-svc-s5s9w [749.254128ms]
Sep 13 17:33:50.014: INFO: Got endpoints: latency-svc-szkxt [772.775294ms]
Sep 13 17:33:50.017: INFO: Created: latency-svc-kr6pl
Sep 13 17:33:50.028: INFO: Created: latency-svc-xpg22
Sep 13 17:33:50.041: INFO: Got endpoints: latency-svc-s6tdf [750.310563ms]
Sep 13 17:33:50.091: INFO: Got endpoints: latency-svc-9mrxx [749.871487ms]
Sep 13 17:33:50.141: INFO: Got endpoints: latency-svc-vlpbg [749.424317ms]
Sep 13 17:33:50.191: INFO: Got endpoints: latency-svc-fccw7 [749.524865ms]
Sep 13 17:33:50.240: INFO: Got endpoints: latency-svc-mvddp [745.878253ms]
Sep 13 17:33:50.291: INFO: Got endpoints: latency-svc-w78pc [746.153572ms]
Sep 13 17:33:50.341: INFO: Got endpoints: latency-svc-kpzm5 [749.365065ms]
Sep 13 17:33:50.391: INFO: Got endpoints: latency-svc-x8pj4 [750.510948ms]
Sep 13 17:33:50.441: INFO: Got endpoints: latency-svc-vsd4m [749.431872ms]
Sep 13 17:33:50.491: INFO: Got endpoints: latency-svc-tnldc [750.411823ms]
Sep 13 17:33:50.541: INFO: Got endpoints: latency-svc-s6klm [749.704163ms]
Sep 13 17:33:50.591: INFO: Got endpoints: latency-svc-k8mvp [750.584878ms]
Sep 13 17:33:50.640: INFO: Got endpoints: latency-svc-nxmhh [749.500641ms]
Sep 13 17:33:50.691: INFO: Got endpoints: latency-svc-kr6pl [750.2132ms]
Sep 13 17:33:50.741: INFO: Got endpoints: latency-svc-xpg22 [726.852635ms]
Sep 13 17:33:50.741: INFO: Latencies: [17.537233ms 21.136265ms 32.659336ms 35.504312ms 47.39828ms 57.47195ms 67.523867ms 74.855853ms 94.753721ms 104.086827ms 114.38577ms 134.421536ms 143.672218ms 149.924556ms 155.204768ms 157.136999ms 157.654029ms 157.911042ms 158.434807ms 159.481964ms 163.048086ms 163.81746ms 164.82829ms 169.751912ms 170.296434ms 177.941549ms 215.845712ms 218.160271ms 221.204711ms 222.334554ms 222.56689ms 223.631571ms 223.776002ms 226.533103ms 227.747694ms 227.851689ms 231.584854ms 238.676179ms 252.112175ms 287.273703ms 291.733443ms 316.084449ms 332.294446ms 368.948909ms 413.38803ms 452.423407ms 497.420295ms 531.655002ms 572.294633ms 611.485612ms 654.183281ms 686.209367ms 721.464171ms 724.879537ms 726.852635ms 727.850851ms 737.438395ms 742.888116ms 745.43972ms 745.878253ms 746.153572ms 746.169772ms 746.547872ms 747.297161ms 747.716859ms 747.805235ms 748.000462ms 748.15918ms 748.390104ms 748.441251ms 748.527242ms 748.847684ms 748.942551ms 749.017433ms 749.021339ms 749.0368ms 749.163276ms 749.184927ms 749.187342ms 749.254128ms 749.25546ms 749.28205ms 749.286797ms 749.302117ms 749.30889ms 749.315062ms 749.342053ms 749.365065ms 749.422384ms 749.424317ms 749.425921ms 749.431872ms 749.438734ms 749.464983ms 749.477097ms 749.500641ms 749.503575ms 749.524865ms 749.526639ms 749.533392ms 749.540606ms 749.550023ms 749.561283ms 749.590961ms 749.656044ms 749.673364ms 749.688502ms 749.704163ms 749.742205ms 749.744879ms 749.751443ms 749.778653ms 749.833997ms 749.840409ms 749.845719ms 749.84617ms 749.858333ms 749.861948ms 749.863352ms 749.871487ms 749.876747ms 749.879753ms 749.886456ms 749.900182ms 749.911843ms 749.912052ms 749.913447ms 749.924767ms 749.927913ms 749.928184ms 749.928564ms 749.929526ms 749.931991ms 749.944364ms 749.951187ms 749.959132ms 749.964962ms 749.96907ms 749.975904ms 749.978969ms 749.999507ms 750.010939ms 750.014997ms 750.019124ms 750.019605ms 750.025177ms 750.02598ms 750.036077ms 750.045796ms 750.049894ms 750.050765ms 750.077264ms 750.095448ms 750.100768ms 750.120405ms 750.12285ms 750.166683ms 750.188814ms 750.2132ms 750.235511ms 750.254397ms 750.258315ms 750.273243ms 750.274836ms 750.281218ms 750.304351ms 750.310563ms 750.337203ms 750.368512ms 750.374151ms 750.377839ms 750.403617ms 750.411823ms 750.426991ms 750.454122ms 750.476623ms 750.508535ms 750.510948ms 750.517261ms 750.584878ms 750.665278ms 750.770837ms 750.788711ms 751.032749ms 751.033231ms 751.03837ms 751.132136ms 751.198672ms 751.430197ms 751.915558ms 752.300051ms 752.794861ms 753.322131ms 753.384017ms 753.3903ms 758.488218ms 762.49119ms 772.775294ms 774.856253ms 778.411183ms]
Sep 13 17:33:50.741: INFO: 50 %ile: 749.540606ms
Sep 13 17:33:50.741: INFO: 90 %ile: 750.665278ms
Sep 13 17:33:50.741: INFO: 99 %ile: 774.856253ms
Sep 13 17:33:50.741: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:33:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1326" for this suite.
Sep 13 17:34:00.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:00.819: INFO: namespace svc-latency-1326 deletion completed in 10.074391373s

â€¢ [SLOW TEST:20.815 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should not be very high  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:00.819: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1510
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 17:34:00.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-1938'
Sep 13 17:34:00.924: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 13 17:34:00.925: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1515
Sep 13 17:34:00.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete jobs e2e-test-nginx-job --namespace=kubectl-1938'
Sep 13 17:34:01.006: INFO: stderr: ""
Sep 13 17:34:01.006: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:34:01.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1938" for this suite.
Sep 13 17:34:07.019: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:07.082: INFO: namespace kubectl-1938 deletion completed in 6.072809209s

â€¢ [SLOW TEST:6.263 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override arguments
Sep 13 17:34:07.110: INFO: Waiting up to 5m0s for pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741" in namespace "containers-8147" to be "success or failure"
Sep 13 17:34:07.113: INFO: Pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.909667ms
Sep 13 17:34:09.116: INFO: Pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00591261s
Sep 13 17:34:11.119: INFO: Pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009070705s
Sep 13 17:34:13.122: INFO: Pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.012158236s
STEP: Saw pod success
Sep 13 17:34:13.122: INFO: Pod "client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:34:13.124: INFO: Trying to get logs from node k8s-test-002 pod client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:34:13.137: INFO: Waiting for pod client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:34:13.139: INFO: Pod client-containers-b00d9fde-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:34:13.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8147" for this suite.
Sep 13 17:34:19.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:19.211: INFO: namespace containers-8147 deletion completed in 6.068958503s

â€¢ [SLOW TEST:12.128 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:19.211: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-b7483c66-d64c-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:34:19.240: INFO: Waiting up to 5m0s for pod "pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741" in namespace "secrets-8205" to be "success or failure"
Sep 13 17:34:19.243: INFO: Pod "pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.606708ms
Sep 13 17:34:21.246: INFO: Pod "pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005827801s
STEP: Saw pod success
Sep 13 17:34:21.246: INFO: Pod "pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:34:21.248: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:34:21.261: INFO: Waiting for pod pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:34:21.263: INFO: Pod pod-secrets-b748a52d-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:34:21.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8205" for this suite.
Sep 13 17:34:27.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:27.334: INFO: namespace secrets-8205 deletion completed in 6.068281202s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:27.334: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 17:34:27.377: INFO: Number of nodes with available pods: 0
Sep 13 17:34:27.377: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:34:28.383: INFO: Number of nodes with available pods: 0
Sep 13 17:34:28.383: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:34:29.382: INFO: Number of nodes with available pods: 2
Sep 13 17:34:29.382: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Sep 13 17:34:29.395: INFO: Number of nodes with available pods: 1
Sep 13 17:34:29.395: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:30.400: INFO: Number of nodes with available pods: 1
Sep 13 17:34:30.400: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:31.401: INFO: Number of nodes with available pods: 1
Sep 13 17:34:31.401: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:32.400: INFO: Number of nodes with available pods: 1
Sep 13 17:34:32.400: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:33.401: INFO: Number of nodes with available pods: 1
Sep 13 17:34:33.401: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:34.400: INFO: Number of nodes with available pods: 1
Sep 13 17:34:34.400: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:35.401: INFO: Number of nodes with available pods: 1
Sep 13 17:34:35.401: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:34:36.400: INFO: Number of nodes with available pods: 2
Sep 13 17:34:36.400: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7129, will wait for the garbage collector to delete the pods
Sep 13 17:34:36.459: INFO: Deleting DaemonSet.extensions daemon-set took: 4.060559ms
Sep 13 17:34:36.759: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.28411ms
Sep 13 17:34:45.561: INFO: Number of nodes with available pods: 0
Sep 13 17:34:45.561: INFO: Number of running nodes: 0, number of available pods: 0
Sep 13 17:34:45.563: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7129/daemonsets","resourceVersion":"13498"},"items":null}

Sep 13 17:34:45.565: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7129/pods","resourceVersion":"13498"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:34:45.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7129" for this suite.
Sep 13 17:34:51.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:51.647: INFO: namespace daemonsets-7129 deletion completed in 6.072322025s

â€¢ [SLOW TEST:24.313 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:51.647: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test env composition
Sep 13 17:34:51.675: INFO: Waiting up to 5m0s for pod "var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741" in namespace "var-expansion-1521" to be "success or failure"
Sep 13 17:34:51.678: INFO: Pod "var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.604724ms
Sep 13 17:34:53.680: INFO: Pod "var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005525431s
STEP: Saw pod success
Sep 13 17:34:53.680: INFO: Pod "var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:34:53.683: INFO: Trying to get logs from node k8s-test-002 pod var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:34:53.697: INFO: Waiting for pod var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:34:53.699: INFO: Pod var-expansion-ca9dc9df-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:34:53.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1521" for this suite.
Sep 13 17:34:59.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:34:59.770: INFO: namespace var-expansion-1521 deletion completed in 6.067911406s

â€¢ [SLOW TEST:8.123 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:34:59.770: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:34:59.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741" in namespace "projected-5819" to be "success or failure"
Sep 13 17:34:59.801: INFO: Pod "downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.633448ms
Sep 13 17:35:01.804: INFO: Pod "downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006579403s
STEP: Saw pod success
Sep 13 17:35:01.804: INFO: Pod "downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:35:01.806: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:35:01.819: INFO: Waiting for pod downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:35:01.822: INFO: Pod downwardapi-volume-cf7508e1-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:35:01.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5819" for this suite.
Sep 13 17:35:07.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:35:07.899: INFO: namespace projected-5819 deletion completed in 6.074565038s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:35:07.899: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 13 17:35:07.926: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 17:35:07.931: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 17:35:07.933: INFO: 
Logging pods the kubelet thinks is on node k8s-test-001 before test
Sep 13 17:35:07.938: INFO: kube-controller-manager-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:35:07.938: INFO: kube-proxy-zcrzb from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 17:35:07.938: INFO: coredns-5bc65d7f4b-gwg5v from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container coredns ready: true, restart count 2
Sep 13 17:35:07.938: INFO: etcd-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:35:07.938: INFO: kube-scheduler-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:35:07.938: INFO: kube-flannel-ds-b9v5p from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container kube-flannel ready: true, restart count 2
Sep 13 17:35:07.938: INFO: kubernetes-dashboard-7646bf6898-rqsn9 from kube-system started at 2019-09-13 16:13:29 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 13 17:35:07.938: INFO: testdns-rbqhm from default started at 2019-09-13 16:37:29 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container webserver ready: true, restart count 3
Sep 13 17:35:07.938: INFO: kube-apiserver-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 17:35:07.938: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-xk4h6 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:35:07.938: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 13 17:35:07.938: INFO: coredns-5bc65d7f4b-dfrkk from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.938: INFO: 	Container coredns ready: true, restart count 2
Sep 13 17:35:07.938: INFO: 
Logging pods the kubelet thinks is on node k8s-test-002 before test
Sep 13 17:35:07.944: INFO: kube-proxy-vmlts from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 17:35:07.944: INFO: sonobuoy-e2e-job-04d4c12741174325 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container e2e ready: true, restart count 0
Sep 13 17:35:07.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:35:07.944: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-6vgpm from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 17:35:07.944: INFO: 	Container systemd-logs ready: true, restart count 0
Sep 13 17:35:07.944: INFO: kube-flannel-ds-t4cvn from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container kube-flannel ready: true, restart count 1
Sep 13 17:35:07.944: INFO: nginx-65f88748fd-bqq75 from default started at 2019-09-13 16:34:33 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container nginx ready: true, restart count 0
Sep 13 17:35:07.944: INFO: sonobuoy from sonobuoy started at 2019-09-13 16:48:31 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 13 17:35:07.944: INFO: testdns-4r5d2 from default started at 2019-09-13 16:34:53 +0000 UTC (1 container statuses recorded)
Sep 13 17:35:07.944: INFO: 	Container webserver ready: true, restart count 3
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d586a8bf-d64c-11e9-80a0-429a7732c741 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d586a8bf-d64c-11e9-80a0-429a7732c741 off the node k8s-test-002
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d586a8bf-d64c-11e9-80a0-429a7732c741
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:35:12.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5189" for this suite.
Sep 13 17:35:30.015: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:35:30.077: INFO: namespace sched-pred-5189 deletion completed in 18.068837169s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:22.178 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:35:30.077: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:69
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Registering the sample API server.
Sep 13 17:35:30.897: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Sep 13 17:35:32.930: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:34.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:36.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:38.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:40.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:42.933: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703992930, loc:(*time.Location)(0x89f10e0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-65db6755fc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Sep 13 17:35:45.763: INFO: Waited 819.678132ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:60
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:35:46.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2136" for this suite.
Sep 13 17:35:52.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:35:52.367: INFO: namespace aggregator-2136 deletion completed in 6.164360342s

â€¢ [SLOW TEST:22.290 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:35:52.367: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:35:58.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7009" for this suite.
Sep 13 17:36:04.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:04.522: INFO: namespace namespaces-7009 deletion completed in 6.071628641s
STEP: Destroying namespace "nsdeletetest-8843" for this suite.
Sep 13 17:36:04.524: INFO: Namespace nsdeletetest-8843 was already deleted
STEP: Destroying namespace "nsdeletetest-4281" for this suite.
Sep 13 17:36:10.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:10.591: INFO: namespace nsdeletetest-4281 deletion completed in 6.067648132s

â€¢ [SLOW TEST:18.224 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:36:10.591: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:36:10.622: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741" in namespace "projected-6833" to be "success or failure"
Sep 13 17:36:10.625: INFO: Pod "downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328644ms
Sep 13 17:36:12.628: INFO: Pod "downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006366733s
Sep 13 17:36:14.631: INFO: Pod "downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009282693s
STEP: Saw pod success
Sep 13 17:36:14.631: INFO: Pod "downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:36:14.633: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:36:14.648: INFO: Waiting for pod downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741 to disappear
Sep 13 17:36:14.650: INFO: Pod downwardapi-volume-f9ab955c-d64c-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:36:14.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6833" for this suite.
Sep 13 17:36:20.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:20.729: INFO: namespace projected-6833 deletion completed in 6.076446023s

â€¢ [SLOW TEST:10.138 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:36:20.729: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: starting the proxy server
Sep 13 17:36:20.752: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-009922021 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:36:20.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8743" for this suite.
Sep 13 17:36:26.829: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:26.894: INFO: namespace kubectl-8743 deletion completed in 6.072755869s

â€¢ [SLOW TEST:6.164 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:36:26.894: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:36:26.920: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Sep 13 17:36:31.923: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Sep 13 17:36:31.923: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 13 17:36:33.944: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3612,SelfLink:/apis/apps/v1/namespaces/deployment-3612/deployments/test-cleanup-deployment,UID:065fbfa8-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:13951,Generation:1,CreationTimestamp:2019-09-13 17:36:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-09-13 17:36:31 +0000 UTC 2019-09-13 17:36:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-09-13 17:36:33 +0000 UTC 2019-09-13 17:36:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55cbfbc8f5" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Sep 13 17:36:33.947: INFO: New ReplicaSet "test-cleanup-deployment-55cbfbc8f5" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5,GenerateName:,Namespace:deployment-3612,SelfLink:/apis/apps/v1/namespaces/deployment-3612/replicasets/test-cleanup-deployment-55cbfbc8f5,UID:06610d7a-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:13940,Generation:1,CreationTimestamp:2019-09-13 17:36:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 065fbfa8-d64d-11e9-ad0a-02001700bd9c 0xc001065c87 0xc001065c88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Sep 13 17:36:33.951: INFO: Pod "test-cleanup-deployment-55cbfbc8f5-swf4h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55cbfbc8f5-swf4h,GenerateName:test-cleanup-deployment-55cbfbc8f5-,Namespace:deployment-3612,SelfLink:/api/v1/namespaces/deployment-3612/pods/test-cleanup-deployment-55cbfbc8f5-swf4h,UID:06617a6a-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:13939,Generation:0,CreationTimestamp:2019-09-13 17:36:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55cbfbc8f5,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55cbfbc8f5 06610d7a-d64d-11e9-ad0a-02001700bd9c 0xc001989ff7 0xc001989ff8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sqgkq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sqgkq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-sqgkq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028700d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002870250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:36:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:36:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:36:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 17:36:31 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.133,StartTime:2019-09-13 17:36:31 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-09-13 17:36:32 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://76f592eefbf9855ca4640e4593d52f00b3db8ff7355cd5e7ab100923a4b79229}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:36:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3612" for this suite.
Sep 13 17:36:39.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:40.023: INFO: namespace deployment-3612 deletion completed in 6.069756213s

â€¢ [SLOW TEST:13.130 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:36:40.024: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0913 17:36:46.065565      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 17:36:46.065: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:36:46.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9351" for this suite.
Sep 13 17:36:52.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:36:52.139: INFO: namespace gc-9351 deletion completed in 6.071226106s

â€¢ [SLOW TEST:12.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:36:52.139: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name cm-test-opt-del-126f88bb-d64d-11e9-80a0-429a7732c741
STEP: Creating configMap with name cm-test-opt-upd-126f88fc-d64d-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-126f88bb-d64d-11e9-80a0-429a7732c741
STEP: Updating configmap cm-test-opt-upd-126f88fc-d64d-11e9-80a0-429a7732c741
STEP: Creating configMap with name cm-test-opt-create-126f8918-d64d-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:38:06.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5905" for this suite.
Sep 13 17:38:28.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:38:28.572: INFO: namespace projected-5905 deletion completed in 22.068473857s

â€¢ [SLOW TEST:96.434 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:38:28.573: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test substitution in container's args
Sep 13 17:38:28.600: INFO: Waiting up to 5m0s for pod "var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741" in namespace "var-expansion-5605" to be "success or failure"
Sep 13 17:38:28.603: INFO: Pod "var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.456356ms
Sep 13 17:38:30.605: INFO: Pod "var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005177852s
STEP: Saw pod success
Sep 13 17:38:30.605: INFO: Pod "var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:38:30.608: INFO: Trying to get logs from node k8s-test-002 pod var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:38:30.621: INFO: Waiting for pod var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:38:30.623: INFO: Pod var-expansion-4be9f25d-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:38:30.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5605" for this suite.
Sep 13 17:38:36.634: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:38:36.695: INFO: namespace var-expansion-5605 deletion completed in 6.069911324s

â€¢ [SLOW TEST:8.123 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:38:36.695: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:177
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:38:36.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6821" for this suite.
Sep 13 17:38:58.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:38:58.798: INFO: namespace pods-6821 deletion completed in 22.070397992s

â€¢ [SLOW TEST:22.102 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:38:58.798: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-4651
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 17:38:58.820: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 13 17:39:18.871: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.88.0.61 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4651 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:39:18.871: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:39:19.973: INFO: Found all expected endpoints: [netserver-0]
Sep 13 17:39:19.976: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.88.1.143 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4651 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:39:19.976: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:39:21.083: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:39:21.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4651" for this suite.
Sep 13 17:39:43.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:39:43.162: INFO: namespace pod-network-test-4651 deletion completed in 22.076285098s

â€¢ [SLOW TEST:44.364 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:39:43.163: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-785f7ab7-d64d-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:39:43.193: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741" in namespace "projected-1086" to be "success or failure"
Sep 13 17:39:43.196: INFO: Pod "pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.521539ms
Sep 13 17:39:45.198: INFO: Pod "pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005319419s
STEP: Saw pod success
Sep 13 17:39:45.199: INFO: Pod "pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:39:45.201: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:39:45.213: INFO: Waiting for pod pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:39:45.216: INFO: Pod pod-projected-secrets-785fd97e-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:39:45.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1086" for this suite.
Sep 13 17:39:51.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:39:51.287: INFO: namespace projected-1086 deletion completed in 6.068279147s

â€¢ [SLOW TEST:8.125 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:39:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:39:51.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 version'
Sep 13 17:39:51.376: INFO: stderr: ""
Sep 13 17:39:51.376: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.3+0.0.1.el7\", GitCommit:\"15ebebac645a8d5d222af348bbfe07588df5277a\", GitTreeState:\"clean\", BuildDate:\"2019-08-13T23:09:29Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:39:51.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1315" for this suite.
Sep 13 17:39:57.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:39:57.447: INFO: namespace kubectl-1315 deletion completed in 6.067997597s

â€¢ [SLOW TEST:6.160 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:39:57.447: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2231.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2231.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2231.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2231.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Sep 13 17:39:59.511: INFO: DNS probes using dns-2231/dns-test-80e301ae-d64d-11e9-80a0-429a7732c741 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:39:59.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2231" for this suite.
Sep 13 17:40:05.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:40:05.589: INFO: namespace dns-2231 deletion completed in 6.067996515s

â€¢ [SLOW TEST:8.142 seconds]
[sig-network] DNS
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:40:05.589: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-projected-xcmp
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 17:40:05.624: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xcmp" in namespace "subpath-8021" to be "success or failure"
Sep 13 17:40:05.627: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Pending", Reason="", readiness=false. Elapsed: 3.058167ms
Sep 13 17:40:07.630: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 2.005879371s
Sep 13 17:40:09.633: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 4.008999227s
Sep 13 17:40:11.636: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 6.011628691s
Sep 13 17:40:13.641: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 8.017301604s
Sep 13 17:40:15.644: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 10.020298067s
Sep 13 17:40:17.647: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 12.023201093s
Sep 13 17:40:19.650: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 14.026088152s
Sep 13 17:40:21.653: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 16.028999364s
Sep 13 17:40:23.656: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 18.031815208s
Sep 13 17:40:25.659: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 20.034643896s
Sep 13 17:40:27.662: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Running", Reason="", readiness=true. Elapsed: 22.037692117s
Sep 13 17:40:29.665: INFO: Pod "pod-subpath-test-projected-xcmp": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.040538517s
STEP: Saw pod success
Sep 13 17:40:29.665: INFO: Pod "pod-subpath-test-projected-xcmp" satisfied condition "success or failure"
Sep 13 17:40:29.667: INFO: Trying to get logs from node k8s-test-002 pod pod-subpath-test-projected-xcmp container test-container-subpath-projected-xcmp: <nil>
STEP: delete the pod
Sep 13 17:40:29.682: INFO: Waiting for pod pod-subpath-test-projected-xcmp to disappear
Sep 13 17:40:29.684: INFO: Pod pod-subpath-test-projected-xcmp no longer exists
STEP: Deleting pod pod-subpath-test-projected-xcmp
Sep 13 17:40:29.684: INFO: Deleting pod "pod-subpath-test-projected-xcmp" in namespace "subpath-8021"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:40:29.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8021" for this suite.
Sep 13 17:40:35.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:40:35.756: INFO: namespace subpath-8021 deletion completed in 6.067472261s

â€¢ [SLOW TEST:30.167 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:40:35.756: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:40:35.784: INFO: Waiting up to 5m0s for pod "downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741" in namespace "downward-api-7378" to be "success or failure"
Sep 13 17:40:35.787: INFO: Pod "downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.327544ms
Sep 13 17:40:37.789: INFO: Pod "downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005176841s
Sep 13 17:40:39.793: INFO: Pod "downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008306404s
STEP: Saw pod success
Sep 13 17:40:39.793: INFO: Pod "downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:40:39.795: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:40:39.808: INFO: Waiting for pod downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:40:39.810: INFO: Pod downwardapi-volume-97b87ab0-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:40:39.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7378" for this suite.
Sep 13 17:40:45.820: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:40:45.882: INFO: namespace downward-api-7378 deletion completed in 6.0700578s

â€¢ [SLOW TEST:10.126 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:40:45.882: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Sep 13 17:40:45.921: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7209,SelfLink:/api/v1/namespaces/watch-7209/configmaps/e2e-watch-test-resource-version,UID:9dc1afe7-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14821,Generation:0,CreationTimestamp:2019-09-13 17:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 13 17:40:45.921: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7209,SelfLink:/api/v1/namespaces/watch-7209/configmaps/e2e-watch-test-resource-version,UID:9dc1afe7-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14822,Generation:0,CreationTimestamp:2019-09-13 17:40:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:40:45.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7209" for this suite.
Sep 13 17:40:51.932: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:40:51.996: INFO: namespace watch-7209 deletion completed in 6.07239916s

â€¢ [SLOW TEST:6.114 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:40:51.996: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-a16659cf-d64d-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 17:40:52.024: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741" in namespace "projected-9007" to be "success or failure"
Sep 13 17:40:52.027: INFO: Pod "pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.501983ms
Sep 13 17:40:54.029: INFO: Pod "pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005147376s
Sep 13 17:40:56.032: INFO: Pod "pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008150131s
STEP: Saw pod success
Sep 13 17:40:56.032: INFO: Pod "pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:40:56.035: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 17:40:56.047: INFO: Waiting for pod pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:40:56.049: INFO: Pod pod-projected-configmaps-a166be08-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:40:56.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9007" for this suite.
Sep 13 17:41:02.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:02.119: INFO: namespace projected-9007 deletion completed in 6.067438607s

â€¢ [SLOW TEST:10.123 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:02.119: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:02.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1854" for this suite.
Sep 13 17:41:08.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:08.225: INFO: namespace kubelet-test-1854 deletion completed in 6.069268035s

â€¢ [SLOW TEST:6.106 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:08.225: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:41:08.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741" in namespace "projected-8121" to be "success or failure"
Sep 13 17:41:08.255: INFO: Pod "downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.239129ms
Sep 13 17:41:10.258: INFO: Pod "downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005226926s
Sep 13 17:41:12.261: INFO: Pod "downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007902385s
STEP: Saw pod success
Sep 13 17:41:12.261: INFO: Pod "downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:41:12.263: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:41:12.277: INFO: Waiting for pod downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:41:12.279: INFO: Pod downwardapi-volume-ab12ebd9-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:12.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8121" for this suite.
Sep 13 17:41:18.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:18.349: INFO: namespace projected-8121 deletion completed in 6.067577929s

â€¢ [SLOW TEST:10.124 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:18.350: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 13 17:41:18.372: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:21.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5483" for this suite.
Sep 13 17:41:27.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:27.672: INFO: namespace init-container-5483 deletion completed in 6.070808881s

â€¢ [SLOW TEST:9.322 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:27.672: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Sep 13 17:41:27.702: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:b6aa7fe4-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14985,Generation:0,CreationTimestamp:2019-09-13 17:41:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 13 17:41:27.702: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:b6aa7fe4-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14986,Generation:0,CreationTimestamp:2019-09-13 17:41:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Sep 13 17:41:27.711: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:b6aa7fe4-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14987,Generation:0,CreationTimestamp:2019-09-13 17:41:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 13 17:41:27.711: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4299,SelfLink:/api/v1/namespaces/watch-4299/configmaps/e2e-watch-test-watch-closed,UID:b6aa7fe4-d64d-11e9-ad0a-02001700bd9c,ResourceVersion:14988,Generation:0,CreationTimestamp:2019-09-13 17:41:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:27.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4299" for this suite.
Sep 13 17:41:33.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:33.783: INFO: namespace watch-4299 deletion completed in 6.069071816s

â€¢ [SLOW TEST:6.111 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:33.783: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 13 17:41:33.811: INFO: Waiting up to 5m0s for pod "pod-ba4eec75-d64d-11e9-80a0-429a7732c741" in namespace "emptydir-2925" to be "success or failure"
Sep 13 17:41:33.814: INFO: Pod "pod-ba4eec75-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.671321ms
Sep 13 17:41:35.817: INFO: Pod "pod-ba4eec75-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006121606s
Sep 13 17:41:37.820: INFO: Pod "pod-ba4eec75-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008982013s
STEP: Saw pod success
Sep 13 17:41:37.820: INFO: Pod "pod-ba4eec75-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:41:37.823: INFO: Trying to get logs from node k8s-test-002 pod pod-ba4eec75-d64d-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:41:37.837: INFO: Waiting for pod pod-ba4eec75-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:41:37.839: INFO: Pod pod-ba4eec75-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:37.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2925" for this suite.
Sep 13 17:41:43.850: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:43.911: INFO: namespace emptydir-2925 deletion completed in 6.069388953s

â€¢ [SLOW TEST:10.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:43.912: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 13 17:41:43.940: INFO: Waiting up to 5m0s for pod "downward-api-c0587a67-d64d-11e9-80a0-429a7732c741" in namespace "downward-api-2408" to be "success or failure"
Sep 13 17:41:43.943: INFO: Pod "downward-api-c0587a67-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.720804ms
Sep 13 17:41:45.946: INFO: Pod "downward-api-c0587a67-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005591381s
Sep 13 17:41:47.949: INFO: Pod "downward-api-c0587a67-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008541687s
STEP: Saw pod success
Sep 13 17:41:47.949: INFO: Pod "downward-api-c0587a67-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:41:47.951: INFO: Trying to get logs from node k8s-test-002 pod downward-api-c0587a67-d64d-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:41:47.964: INFO: Waiting for pod downward-api-c0587a67-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:41:47.966: INFO: Pod downward-api-c0587a67-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:47.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2408" for this suite.
Sep 13 17:41:53.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:41:54.038: INFO: namespace downward-api-2408 deletion completed in 6.069440461s

â€¢ [SLOW TEST:10.127 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:41:54.039: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 13 17:41:54.066: INFO: Waiting up to 5m0s for pod "pod-c6618152-d64d-11e9-80a0-429a7732c741" in namespace "emptydir-7904" to be "success or failure"
Sep 13 17:41:54.069: INFO: Pod "pod-c6618152-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718758ms
Sep 13 17:41:56.072: INFO: Pod "pod-c6618152-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005792464s
STEP: Saw pod success
Sep 13 17:41:56.072: INFO: Pod "pod-c6618152-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:41:56.074: INFO: Trying to get logs from node k8s-test-002 pod pod-c6618152-d64d-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:41:56.087: INFO: Waiting for pod pod-c6618152-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:41:56.089: INFO: Pod pod-c6618152-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:41:56.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7904" for this suite.
Sep 13 17:42:02.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:42:02.161: INFO: namespace emptydir-7904 deletion completed in 6.069153752s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:42:02.161: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-cb394f48-d64d-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:42:02.194: INFO: Waiting up to 5m0s for pod "pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741" in namespace "secrets-3330" to be "success or failure"
Sep 13 17:42:02.200: INFO: Pod "pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 6.309736ms
Sep 13 17:42:04.203: INFO: Pod "pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009154s
STEP: Saw pod success
Sep 13 17:42:04.203: INFO: Pod "pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:42:04.205: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:42:04.218: INFO: Waiting for pod pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:42:04.220: INFO: Pod pod-secrets-cb39b2b9-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:42:04.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3330" for this suite.
Sep 13 17:42:10.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:42:10.291: INFO: namespace secrets-3330 deletion completed in 6.068233194s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:42:10.291: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:42:10.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 version --client'
Sep 13 17:42:10.371: INFO: stderr: ""
Sep 13 17:42:10.371: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"14\", GitVersion:\"v1.14.0\", GitCommit:\"641856db18352033a0d96dbc99153fa3b27298e5\", GitTreeState:\"clean\", BuildDate:\"2019-03-25T15:53:57Z\", GoVersion:\"go1.12.1\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
Sep 13 17:42:10.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-8141'
Sep 13 17:42:10.781: INFO: stderr: ""
Sep 13 17:42:10.781: INFO: stdout: "replicationcontroller/redis-master created\n"
Sep 13 17:42:10.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-8141'
Sep 13 17:42:10.950: INFO: stderr: ""
Sep 13 17:42:10.950: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 13 17:42:11.953: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:42:11.953: INFO: Found 0 / 1
Sep 13 17:42:12.953: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:42:12.953: INFO: Found 1 / 1
Sep 13 17:42:12.953: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 13 17:42:12.955: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:42:12.955: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 17:42:12.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 describe pod redis-master-6hl5g --namespace=kubectl-8141'
Sep 13 17:42:13.043: INFO: stderr: ""
Sep 13 17:42:13.043: INFO: stdout: "Name:               redis-master-6hl5g\nNamespace:          kubectl-8141\nPriority:           0\nPriorityClassName:  <none>\nNode:               k8s-test-002/100.100.230.24\nStart Time:         Fri, 13 Sep 2019 17:42:10 +0000\nLabels:             app=redis\n                    role=master\nAnnotations:        <none>\nStatus:             Running\nIP:                 10.88.1.156\nControlled By:      ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://39aa2df83b847f209a27f39b6af829af091dfc86ab58607d3e3f861f8632cd1f\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 13 Sep 2019 17:42:11 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lxkj9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-lxkj9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lxkj9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                   Message\n  ----    ------     ----  ----                   -------\n  Normal  Scheduled  3s    default-scheduler      Successfully assigned kubectl-8141/redis-master-6hl5g to k8s-test-002\n  Normal  Pulled     2s    kubelet, k8s-test-002  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, k8s-test-002  Created container redis-master\n  Normal  Started    2s    kubelet, k8s-test-002  Started container redis-master\n"
Sep 13 17:42:13.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 describe rc redis-master --namespace=kubectl-8141'
Sep 13 17:42:13.139: INFO: stderr: ""
Sep 13 17:42:13.139: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-8141\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-6hl5g\n"
Sep 13 17:42:13.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 describe service redis-master --namespace=kubectl-8141'
Sep 13 17:42:13.221: INFO: stderr: ""
Sep 13 17:42:13.221: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-8141\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.108.242.135\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.88.1.156:6379\nSession Affinity:  None\nEvents:            <none>\n"
Sep 13 17:42:13.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 describe node k8s-test-001'
Sep 13 17:42:13.323: INFO: stderr: ""
Sep 13 17:42:13.323: INFO: stdout: "Name:               k8s-test-001\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-test-001\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"f2:7d:bb:54:40:a6\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 100.100.230.23\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 13 Sep 2019 16:12:39 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 13 Sep 2019 17:41:32 +0000   Fri, 13 Sep 2019 16:12:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 13 Sep 2019 17:41:32 +0000   Fri, 13 Sep 2019 16:12:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 13 Sep 2019 17:41:32 +0000   Fri, 13 Sep 2019 16:12:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 13 Sep 2019 17:41:32 +0000   Fri, 13 Sep 2019 16:13:29 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  100.100.230.23\n  Hostname:    k8s-test-001\nCapacity:\n cpu:                4\n ephemeral-storage:  40223552Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16147812Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  37070025462\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16045412Ki\n pods:               110\nSystem Info:\n Machine ID:                 ec380d2252484225af9dfb61a65c56e7\n System UUID:                C4CE72B1-1532-4304-96BA-367FFED0B175\n Boot ID:                    a4cdd8c4-99d7-4903-8cd9-661fbdf99038\n Kernel Version:             4.14.35-1902.3.2.el7uek.x86_64\n OS Image:                   Oracle Linux Server 7.6\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.1\n Kubelet Version:            v1.14.3+1.0.1.el7\n Kube-Proxy Version:         v1.14.3+1.0.1.el7\nPodCIDR:                     10.88.0.0/24\nNon-terminated Pods:         (11 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  default                    testdns-rbqhm                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         64m\n  kube-system                coredns-5bc65d7f4b-dfrkk                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     89m\n  kube-system                coredns-5bc65d7f4b-gwg5v                                   100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     89m\n  kube-system                etcd-k8s-test-001                                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                kube-apiserver-k8s-test-001                                250m (6%)     0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                kube-controller-manager-k8s-test-001                       200m (5%)     0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                kube-flannel-ds-b9v5p                                      100m (2%)     100m (2%)   100Mi (0%)       100Mi (0%)     89m\n  kube-system                kube-proxy-zcrzb                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  kube-system                kube-scheduler-k8s-test-001                                100m (2%)     0 (0%)      0 (0%)           0 (0%)         88m\n  kube-system                kubernetes-dashboard-7646bf6898-rqsn9                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         89m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-xk4h6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (21%)  100m (2%)\n  memory             240Mi (1%)  440Mi (2%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Sep 13 17:42:13.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 describe namespace kubectl-8141'
Sep 13 17:42:13.407: INFO: stderr: ""
Sep 13 17:42:13.407: INFO: stdout: "Name:         kubectl-8141\nLabels:       e2e-framework=kubectl\n              e2e-run=6f07c906-d646-11e9-80a0-429a7732c741\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:42:13.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8141" for this suite.
Sep 13 17:42:29.418: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:42:29.479: INFO: namespace kubectl-8141 deletion completed in 16.06925886s

â€¢ [SLOW TEST:19.188 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:42:29.479: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
Sep 13 17:42:30.028: INFO: created pod pod-service-account-defaultsa
Sep 13 17:42:30.028: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Sep 13 17:42:30.032: INFO: created pod pod-service-account-mountsa
Sep 13 17:42:30.032: INFO: pod pod-service-account-mountsa service account token volume mount: true
Sep 13 17:42:30.039: INFO: created pod pod-service-account-nomountsa
Sep 13 17:42:30.039: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Sep 13 17:42:30.045: INFO: created pod pod-service-account-defaultsa-mountspec
Sep 13 17:42:30.045: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Sep 13 17:42:30.049: INFO: created pod pod-service-account-mountsa-mountspec
Sep 13 17:42:30.049: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Sep 13 17:42:30.054: INFO: created pod pod-service-account-nomountsa-mountspec
Sep 13 17:42:30.054: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Sep 13 17:42:30.059: INFO: created pod pod-service-account-defaultsa-nomountspec
Sep 13 17:42:30.059: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Sep 13 17:42:30.066: INFO: created pod pod-service-account-mountsa-nomountspec
Sep 13 17:42:30.066: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Sep 13 17:42:30.078: INFO: created pod pod-service-account-nomountsa-nomountspec
Sep 13 17:42:30.078: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:42:30.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9364" for this suite.
Sep 13 17:42:52.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:42:52.240: INFO: namespace svcaccounts-9364 deletion completed in 22.121517967s

â€¢ [SLOW TEST:22.761 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:42:52.240: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-e912c422-d64d-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:42:54.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9460" for this suite.
Sep 13 17:43:16.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:43:16.362: INFO: namespace configmap-9460 deletion completed in 22.067341777s

â€¢ [SLOW TEST:24.122 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:43:16.362: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 13 17:43:16.389: INFO: Waiting up to 5m0s for pod "downward-api-f7731f49-d64d-11e9-80a0-429a7732c741" in namespace "downward-api-6499" to be "success or failure"
Sep 13 17:43:16.392: INFO: Pod "downward-api-f7731f49-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.345698ms
Sep 13 17:43:18.395: INFO: Pod "downward-api-f7731f49-d64d-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005440362s
Sep 13 17:43:20.398: INFO: Pod "downward-api-f7731f49-d64d-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008321586s
STEP: Saw pod success
Sep 13 17:43:20.398: INFO: Pod "downward-api-f7731f49-d64d-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:43:20.401: INFO: Trying to get logs from node k8s-test-002 pod downward-api-f7731f49-d64d-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:43:20.414: INFO: Waiting for pod downward-api-f7731f49-d64d-11e9-80a0-429a7732c741 to disappear
Sep 13 17:43:20.416: INFO: Pod downward-api-f7731f49-d64d-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:43:20.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6499" for this suite.
Sep 13 17:43:26.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:43:26.487: INFO: namespace downward-api-6499 deletion completed in 6.068178912s

â€¢ [SLOW TEST:10.125 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:43:26.487: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:43:26.512: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:43:28.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3545" for this suite.
Sep 13 17:44:06.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:44:06.686: INFO: namespace pods-3545 deletion completed in 38.067744093s

â€¢ [SLOW TEST:40.200 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:44:06.686: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:44:06.721: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Sep 13 17:44:06.728: INFO: Number of nodes with available pods: 0
Sep 13 17:44:06.728: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:44:07.736: INFO: Number of nodes with available pods: 0
Sep 13 17:44:07.736: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 17:44:08.733: INFO: Number of nodes with available pods: 2
Sep 13 17:44:08.733: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Sep 13 17:44:08.751: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:08.751: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:09.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:09.760: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:10.761: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:10.761: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:11.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:11.760: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:11.760: INFO: Pod daemon-set-czcxm is not available
Sep 13 17:44:12.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:12.761: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:12.761: INFO: Pod daemon-set-czcxm is not available
Sep 13 17:44:13.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:13.760: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:13.760: INFO: Pod daemon-set-czcxm is not available
Sep 13 17:44:14.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:14.760: INFO: Wrong image for pod: daemon-set-czcxm. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:14.760: INFO: Pod daemon-set-czcxm is not available
Sep 13 17:44:15.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:15.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:16.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:16.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:17.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:17.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:18.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:18.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:19.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:19.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:20.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:20.760: INFO: Pod daemon-set-wgn9r is not available
Sep 13 17:44:21.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:21.760: INFO: Pod daemon-set-68xcw is not available
Sep 13 17:44:22.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:22.761: INFO: Pod daemon-set-68xcw is not available
Sep 13 17:44:23.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:23.760: INFO: Pod daemon-set-68xcw is not available
Sep 13 17:44:24.760: INFO: Wrong image for pod: daemon-set-68xcw. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Sep 13 17:44:24.760: INFO: Pod daemon-set-68xcw is not available
Sep 13 17:44:25.761: INFO: Pod daemon-set-nmdmx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Sep 13 17:44:25.769: INFO: Number of nodes with available pods: 1
Sep 13 17:44:25.769: INFO: Node k8s-test-002 is running more than one daemon pod
Sep 13 17:44:26.774: INFO: Number of nodes with available pods: 2
Sep 13 17:44:26.774: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9557, will wait for the garbage collector to delete the pods
Sep 13 17:44:26.841: INFO: Deleting DaemonSet.extensions daemon-set took: 4.365694ms
Sep 13 17:44:27.141: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.23175ms
Sep 13 17:44:35.544: INFO: Number of nodes with available pods: 0
Sep 13 17:44:35.544: INFO: Number of running nodes: 0, number of available pods: 0
Sep 13 17:44:35.546: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9557/daemonsets","resourceVersion":"15657"},"items":null}

Sep 13 17:44:35.548: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9557/pods","resourceVersion":"15657"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:44:35.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9557" for this suite.
Sep 13 17:44:41.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:44:41.626: INFO: namespace daemonsets-9557 deletion completed in 6.068797766s

â€¢ [SLOW TEST:34.939 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:44:41.626: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1414
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 17:44:41.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-5070'
Sep 13 17:44:41.728: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 13 17:44:41.728: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Sep 13 17:44:41.735: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Sep 13 17:44:41.737: INFO: scanned /root for discovery docs: <nil>
Sep 13 17:44:41.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-5070'
Sep 13 17:44:57.494: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Sep 13 17:44:57.494: INFO: stdout: "Created e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4\nScaling up e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Sep 13 17:44:57.494: INFO: stdout: "Created e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4\nScaling up e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Sep 13 17:44:57.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-5070'
Sep 13 17:44:57.569: INFO: stderr: ""
Sep 13 17:44:57.569: INFO: stdout: "e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4-fvhmw "
Sep 13 17:44:57.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4-fvhmw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5070'
Sep 13 17:44:57.643: INFO: stderr: ""
Sep 13 17:44:57.643: INFO: stdout: "true"
Sep 13 17:44:57.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4-fvhmw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5070'
Sep 13 17:44:57.715: INFO: stderr: ""
Sep 13 17:44:57.715: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Sep 13 17:44:57.715: INFO: e2e-test-nginx-rc-ce572064b0a7f5aee4947b16beb1e6f4-fvhmw is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
Sep 13 17:44:57.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete rc e2e-test-nginx-rc --namespace=kubectl-5070'
Sep 13 17:44:57.796: INFO: stderr: ""
Sep 13 17:44:57.796: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:44:57.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5070" for this suite.
Sep 13 17:45:03.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:03.874: INFO: namespace kubectl-5070 deletion completed in 6.073899911s

â€¢ [SLOW TEST:22.247 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:03.874: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-37882434-d64e-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:45:03.924: INFO: Waiting up to 5m0s for pod "pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741" in namespace "secrets-1485" to be "success or failure"
Sep 13 17:45:03.928: INFO: Pod "pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03371ms
Sep 13 17:45:05.932: INFO: Pod "pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007154786s
Sep 13 17:45:07.934: INFO: Pod "pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010071455s
STEP: Saw pod success
Sep 13 17:45:07.935: INFO: Pod "pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:45:07.937: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:45:07.950: INFO: Waiting for pod pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:45:07.952: INFO: Pod pod-secrets-378ba85d-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:45:07.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1485" for this suite.
Sep 13 17:45:13.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:14.023: INFO: namespace secrets-1485 deletion completed in 6.067986302s
STEP: Destroying namespace "secret-namespace-1056" for this suite.
Sep 13 17:45:20.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:20.091: INFO: namespace secret-namespace-1056 deletion completed in 6.068047183s

â€¢ [SLOW TEST:16.218 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:45:20.119: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741" in namespace "downward-api-4404" to be "success or failure"
Sep 13 17:45:20.122: INFO: Pod "downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.313639ms
Sep 13 17:45:22.125: INFO: Pod "downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005427877s
Sep 13 17:45:24.128: INFO: Pod "downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008385573s
STEP: Saw pod success
Sep 13 17:45:24.128: INFO: Pod "downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:45:24.130: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:45:24.143: INFO: Waiting for pod downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:45:24.145: INFO: Pod downwardapi-volume-4132d2f2-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:45:24.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4404" for this suite.
Sep 13 17:45:30.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:30.224: INFO: namespace downward-api-4404 deletion completed in 6.075874893s

â€¢ [SLOW TEST:10.132 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:30.224: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on node default medium
Sep 13 17:45:30.252: INFO: Waiting up to 5m0s for pod "pod-473cfa12-d64e-11e9-80a0-429a7732c741" in namespace "emptydir-2151" to be "success or failure"
Sep 13 17:45:30.255: INFO: Pod "pod-473cfa12-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.678794ms
Sep 13 17:45:32.258: INFO: Pod "pod-473cfa12-d64e-11e9-80a0-429a7732c741": Phase="Running", Reason="", readiness=true. Elapsed: 2.005708895s
Sep 13 17:45:34.261: INFO: Pod "pod-473cfa12-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008701847s
STEP: Saw pod success
Sep 13 17:45:34.261: INFO: Pod "pod-473cfa12-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:45:34.263: INFO: Trying to get logs from node k8s-test-002 pod pod-473cfa12-d64e-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:45:34.275: INFO: Waiting for pod pod-473cfa12-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:45:34.277: INFO: Pod pod-473cfa12-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:45:34.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2151" for this suite.
Sep 13 17:45:40.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:40.350: INFO: namespace emptydir-2151 deletion completed in 6.069730035s

â€¢ [SLOW TEST:10.126 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:40.350: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 13 17:45:40.378: INFO: Waiting up to 5m0s for pod "downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741" in namespace "downward-api-7625" to be "success or failure"
Sep 13 17:45:40.381: INFO: Pod "downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.289573ms
Sep 13 17:45:42.384: INFO: Pod "downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005149234s
Sep 13 17:45:44.386: INFO: Pod "downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008005068s
STEP: Saw pod success
Sep 13 17:45:44.386: INFO: Pod "downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:45:44.389: INFO: Trying to get logs from node k8s-test-002 pod downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 17:45:44.402: INFO: Waiting for pod downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:45:44.404: INFO: Pod downward-api-4d4612f3-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:45:44.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7625" for this suite.
Sep 13 17:45:50.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:50.474: INFO: namespace downward-api-7625 deletion completed in 6.067183269s

â€¢ [SLOW TEST:10.124 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:50.474: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0666 on tmpfs
Sep 13 17:45:50.501: INFO: Waiting up to 5m0s for pod "pod-534ea60a-d64e-11e9-80a0-429a7732c741" in namespace "emptydir-6822" to be "success or failure"
Sep 13 17:45:50.506: INFO: Pod "pod-534ea60a-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 4.757981ms
Sep 13 17:45:52.509: INFO: Pod "pod-534ea60a-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007508526s
STEP: Saw pod success
Sep 13 17:45:52.509: INFO: Pod "pod-534ea60a-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:45:52.511: INFO: Trying to get logs from node k8s-test-002 pod pod-534ea60a-d64e-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:45:52.525: INFO: Waiting for pod pod-534ea60a-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:45:52.527: INFO: Pod pod-534ea60a-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:45:52.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6822" for this suite.
Sep 13 17:45:58.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:45:58.596: INFO: namespace emptydir-6822 deletion completed in 6.067435793s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:45:58.597: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 13 17:46:02.653: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 17:46:02.655: INFO: Pod pod-with-prestop-http-hook still exists
Sep 13 17:46:04.656: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 17:46:04.658: INFO: Pod pod-with-prestop-http-hook still exists
Sep 13 17:46:06.656: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Sep 13 17:46:06.658: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:46:06.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2187" for this suite.
Sep 13 17:46:28.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:46:28.736: INFO: namespace container-lifecycle-hook-2187 deletion completed in 22.068057297s

â€¢ [SLOW TEST:30.139 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:46:28.736: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 13 17:46:28.758: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:46:32.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6729" for this suite.
Sep 13 17:46:38.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:46:38.123: INFO: namespace init-container-6729 deletion completed in 6.070612795s

â€¢ [SLOW TEST:9.387 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:46:38.123: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with configMap that has name projected-configmap-test-upd-6fb58e36-d64e-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-6fb58e36-d64e-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:47:54.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8880" for this suite.
Sep 13 17:48:16.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:48:16.524: INFO: namespace projected-8880 deletion completed in 22.069001059s

â€¢ [SLOW TEST:98.401 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:48:16.524: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1318
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 17:48:16.546: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4912'
Sep 13 17:48:16.628: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Sep 13 17:48:16.628: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1324
Sep 13 17:48:16.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4912'
Sep 13 17:48:16.713: INFO: stderr: ""
Sep 13 17:48:16.713: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:48:16.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4912" for this suite.
Sep 13 17:48:22.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:48:22.802: INFO: namespace kubectl-4912 deletion completed in 6.086496645s

â€¢ [SLOW TEST:6.278 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:48:22.803: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep 13 17:48:22.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-4977'
Sep 13 17:48:22.983: INFO: stderr: ""
Sep 13 17:48:22.983: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 13 17:48:23.986: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:48:23.986: INFO: Found 0 / 1
Sep 13 17:48:24.986: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:48:24.986: INFO: Found 1 / 1
Sep 13 17:48:24.986: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Sep 13 17:48:24.988: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:48:24.988: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 17:48:24.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 patch pod redis-master-qtmm6 --namespace=kubectl-4977 -p {"metadata":{"annotations":{"x":"y"}}}'
Sep 13 17:48:25.066: INFO: stderr: ""
Sep 13 17:48:25.066: INFO: stdout: "pod/redis-master-qtmm6 patched\n"
STEP: checking annotations
Sep 13 17:48:25.069: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 17:48:25.069: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:48:25.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4977" for this suite.
Sep 13 17:48:47.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:48:47.145: INFO: namespace kubectl-4977 deletion completed in 22.067115896s

â€¢ [SLOW TEST:24.343 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:48:47.146: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-3009
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a new StatefulSet
Sep 13 17:48:47.180: INFO: Found 0 stateful pods, waiting for 3
Sep 13 17:48:57.183: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:48:57.183: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:48:57.183: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Sep 13 17:48:57.205: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Sep 13 17:49:07.234: INFO: Updating stateful set ss2
Sep 13 17:49:07.239: INFO: Waiting for Pod statefulset-3009/ss2-2 to have revision ss2-c79899b9 update revision ss2-787997d666
STEP: Restoring Pods to the correct revision when they are deleted
Sep 13 17:49:17.292: INFO: Found 2 stateful pods, waiting for 3
Sep 13 17:49:27.298: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:49:27.298: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:49:27.298: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Sep 13 17:49:27.319: INFO: Updating stateful set ss2
Sep 13 17:49:27.335: INFO: Waiting for Pod statefulset-3009/ss2-1 to have revision ss2-c79899b9 update revision ss2-787997d666
Sep 13 17:49:37.358: INFO: Updating stateful set ss2
Sep 13 17:49:37.369: INFO: Waiting for StatefulSet statefulset-3009/ss2 to complete update
Sep 13 17:49:37.369: INFO: Waiting for Pod statefulset-3009/ss2-0 to have revision ss2-c79899b9 update revision ss2-787997d666
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 13 17:49:47.375: INFO: Deleting all statefulset in ns statefulset-3009
Sep 13 17:49:47.377: INFO: Scaling statefulset ss2 to 0
Sep 13 17:50:07.389: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:50:07.391: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:50:07.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3009" for this suite.
Sep 13 17:50:13.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:50:13.473: INFO: namespace statefulset-3009 deletion completed in 6.070097585s

â€¢ [SLOW TEST:86.328 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:50:13.474: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir volume type on node default medium
Sep 13 17:50:13.505: INFO: Waiting up to 5m0s for pod "pod-f011b992-d64e-11e9-80a0-429a7732c741" in namespace "emptydir-9985" to be "success or failure"
Sep 13 17:50:13.507: INFO: Pod "pod-f011b992-d64e-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.384571ms
Sep 13 17:50:15.510: INFO: Pod "pod-f011b992-d64e-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005416167s
STEP: Saw pod success
Sep 13 17:50:15.511: INFO: Pod "pod-f011b992-d64e-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:50:15.513: INFO: Trying to get logs from node k8s-test-002 pod pod-f011b992-d64e-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 17:50:15.525: INFO: Waiting for pod pod-f011b992-d64e-11e9-80a0-429a7732c741 to disappear
Sep 13 17:50:15.527: INFO: Pod pod-f011b992-d64e-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:50:15.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9985" for this suite.
Sep 13 17:50:21.539: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:50:21.600: INFO: namespace emptydir-9985 deletion completed in 6.070031502s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:50:21.601: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0913 17:50:31.668483      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 17:50:31.668: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:50:31.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5496" for this suite.
Sep 13 17:50:37.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:50:37.739: INFO: namespace gc-5496 deletion completed in 6.068452285s

â€¢ [SLOW TEST:16.138 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:50:37.739: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:50:37.762: INFO: Creating ReplicaSet my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741
Sep 13 17:50:37.768: INFO: Pod name my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741: Found 0 pods out of 1
Sep 13 17:50:42.771: INFO: Pod name my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741: Found 1 pods out of 1
Sep 13 17:50:42.771: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741" is running
Sep 13 17:50:42.774: INFO: Pod "my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741-gxbbq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:50:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:50:39 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:50:39 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-09-13 17:50:37 +0000 UTC Reason: Message:}])
Sep 13 17:50:42.774: INFO: Trying to dial the pod
Sep 13 17:50:47.782: INFO: Controller my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741: Got expected result from replica 1 [my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741-gxbbq]: "my-hostname-basic-fe87e3f6-d64e-11e9-80a0-429a7732c741-gxbbq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:50:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6329" for this suite.
Sep 13 17:50:53.792: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:50:53.863: INFO: namespace replicaset-6329 deletion completed in 6.07822983s

â€¢ [SLOW TEST:16.124 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:50:53.863: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-6243
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-6243
STEP: Creating statefulset with conflicting port in namespace statefulset-6243
STEP: Waiting until pod test-pod will start running in namespace statefulset-6243
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6243
Sep 13 17:50:57.922: INFO: Observed stateful pod in namespace: statefulset-6243, name: ss-0, uid: 0a4e2072-d64f-11e9-ad0a-02001700bd9c, status phase: Pending. Waiting for statefulset controller to delete.
Sep 13 17:50:58.104: INFO: Observed stateful pod in namespace: statefulset-6243, name: ss-0, uid: 0a4e2072-d64f-11e9-ad0a-02001700bd9c, status phase: Failed. Waiting for statefulset controller to delete.
Sep 13 17:50:58.110: INFO: Observed stateful pod in namespace: statefulset-6243, name: ss-0, uid: 0a4e2072-d64f-11e9-ad0a-02001700bd9c, status phase: Failed. Waiting for statefulset controller to delete.
Sep 13 17:50:58.114: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6243
STEP: Removing pod with conflicting port in namespace statefulset-6243
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6243 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 13 17:51:02.138: INFO: Deleting all statefulset in ns statefulset-6243
Sep 13 17:51:02.141: INFO: Scaling statefulset ss to 0
Sep 13 17:51:22.152: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:51:22.154: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:51:22.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6243" for this suite.
Sep 13 17:51:28.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:51:28.236: INFO: namespace statefulset-6243 deletion completed in 6.071031944s

â€¢ [SLOW TEST:34.373 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:51:28.236: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:51:33.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6125" for this suite.
Sep 13 17:51:55.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:51:55.349: INFO: namespace replication-controller-6125 deletion completed in 22.068094424s

â€¢ [SLOW TEST:27.114 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:51:55.350: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:51:55.377: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741" in namespace "projected-9824" to be "success or failure"
Sep 13 17:51:55.379: INFO: Pod "downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.257593ms
Sep 13 17:51:57.382: INFO: Pod "downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005158566s
STEP: Saw pod success
Sep 13 17:51:57.382: INFO: Pod "downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:51:57.384: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:51:57.399: INFO: Waiting for pod downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741 to disappear
Sep 13 17:51:57.401: INFO: Pod downwardapi-volume-2cca4e93-d64f-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:51:57.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9824" for this suite.
Sep 13 17:52:03.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:52:03.475: INFO: namespace projected-9824 deletion completed in 6.07185036s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:52:03.475: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0913 17:52:34.025721      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 17:52:34.025: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:52:34.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7583" for this suite.
Sep 13 17:52:40.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:52:40.096: INFO: namespace gc-7583 deletion completed in 6.068806301s

â€¢ [SLOW TEST:36.621 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:52:40.097: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:52:40.125: INFO: Waiting up to 5m0s for pod "downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741" in namespace "projected-7263" to be "success or failure"
Sep 13 17:52:40.128: INFO: Pod "downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.158194ms
Sep 13 17:52:42.132: INFO: Pod "downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006549078s
STEP: Saw pod success
Sep 13 17:52:42.132: INFO: Pod "downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:52:42.134: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:52:42.148: INFO: Waiting for pod downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741 to disappear
Sep 13 17:52:42.150: INFO: Pod downwardapi-volume-477652d0-d64f-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:52:42.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7263" for this suite.
Sep 13 17:52:48.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:52:48.221: INFO: namespace projected-7263 deletion completed in 6.068829214s

â€¢ [SLOW TEST:8.125 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:52:48.222: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Sep 13 17:52:48.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17582,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 13 17:52:48.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17582,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Sep 13 17:52:58.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17596,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Sep 13 17:52:58.254: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17596,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Sep 13 17:53:08.261: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17611,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 13 17:53:08.261: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17611,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Sep 13 17:53:18.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17625,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Sep 13 17:53:18.266: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-a,UID:4c4e357a-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17625,Generation:0,CreationTimestamp:2019-09-13 17:52:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Sep 13 17:53:28.272: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-b,UID:642904f4-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17639,Generation:0,CreationTimestamp:2019-09-13 17:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 13 17:53:28.272: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-b,UID:642904f4-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17639,Generation:0,CreationTimestamp:2019-09-13 17:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Sep 13 17:53:38.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-b,UID:642904f4-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17654,Generation:0,CreationTimestamp:2019-09-13 17:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Sep 13 17:53:38.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-3960,SelfLink:/api/v1/namespaces/watch-3960/configmaps/e2e-watch-test-configmap-b,UID:642904f4-d64f-11e9-ad0a-02001700bd9c,ResourceVersion:17654,Generation:0,CreationTimestamp:2019-09-13 17:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:53:48.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3960" for this suite.
Sep 13 17:53:54.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:53:54.347: INFO: namespace watch-3960 deletion completed in 6.067353423s

â€¢ [SLOW TEST:66.126 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:53:54.348: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:53:56.396: INFO: Waiting up to 5m0s for pod "client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741" in namespace "pods-956" to be "success or failure"
Sep 13 17:53:56.399: INFO: Pod "client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.005758ms
Sep 13 17:53:58.402: INFO: Pod "client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005938781s
STEP: Saw pod success
Sep 13 17:53:58.402: INFO: Pod "client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:53:58.404: INFO: Trying to get logs from node k8s-test-002 pod client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741 container env3cont: <nil>
STEP: delete the pod
Sep 13 17:53:58.417: INFO: Waiting for pod client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741 to disappear
Sep 13 17:53:58.419: INFO: Pod client-envvars-74ec00ca-d64f-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:53:58.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-956" for this suite.
Sep 13 17:54:36.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:54:36.494: INFO: namespace pods-956 deletion completed in 38.072042246s

â€¢ [SLOW TEST:42.146 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:54:36.494: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 13 17:54:39.045: INFO: Successfully updated pod "annotationupdate8cd6f283-d64f-11e9-80a0-429a7732c741"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:54:43.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8571" for this suite.
Sep 13 17:55:05.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:55:05.136: INFO: namespace projected-8571 deletion completed in 22.06876252s

â€¢ [SLOW TEST:28.642 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:55:05.136: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1108
STEP: creating the pod
Sep 13 17:55:05.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-3202'
Sep 13 17:55:05.421: INFO: stderr: ""
Sep 13 17:55:05.421: INFO: stdout: "pod/pause created\n"
Sep 13 17:55:05.421: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Sep 13 17:55:05.421: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3202" to be "running and ready"
Sep 13 17:55:05.423: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.3788ms
Sep 13 17:55:07.426: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.005302605s
Sep 13 17:55:07.426: INFO: Pod "pause" satisfied condition "running and ready"
Sep 13 17:55:07.426: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: adding the label testing-label with value testing-label-value to a pod
Sep 13 17:55:07.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 label pods pause testing-label=testing-label-value --namespace=kubectl-3202'
Sep 13 17:55:07.503: INFO: stderr: ""
Sep 13 17:55:07.503: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Sep 13 17:55:07.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pod pause -L testing-label --namespace=kubectl-3202'
Sep 13 17:55:07.576: INFO: stderr: ""
Sep 13 17:55:07.576: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Sep 13 17:55:07.576: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 label pods pause testing-label- --namespace=kubectl-3202'
Sep 13 17:55:07.652: INFO: stderr: ""
Sep 13 17:55:07.652: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Sep 13 17:55:07.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pod pause -L testing-label --namespace=kubectl-3202'
Sep 13 17:55:07.727: INFO: stderr: ""
Sep 13 17:55:07.727: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1115
STEP: using delete to clean up resources
Sep 13 17:55:07.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete --grace-period=0 --force -f - --namespace=kubectl-3202'
Sep 13 17:55:07.803: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Sep 13 17:55:07.803: INFO: stdout: "pod \"pause\" force deleted\n"
Sep 13 17:55:07.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get rc,svc -l name=pause --no-headers --namespace=kubectl-3202'
Sep 13 17:55:07.902: INFO: stderr: "No resources found.\n"
Sep 13 17:55:07.902: INFO: stdout: ""
Sep 13 17:55:07.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pods -l name=pause --namespace=kubectl-3202 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Sep 13 17:55:07.989: INFO: stderr: ""
Sep 13 17:55:07.989: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:55:07.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3202" for this suite.
Sep 13 17:55:14.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:55:14.061: INFO: namespace kubectl-3202 deletion completed in 6.068642891s

â€¢ [SLOW TEST:8.925 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:55:14.061: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 17:55:14.091: INFO: (0) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.967214ms)
Sep 13 17:55:14.094: INFO: (1) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.080268ms)
Sep 13 17:55:14.097: INFO: (2) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.800332ms)
Sep 13 17:55:14.100: INFO: (3) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.953919ms)
Sep 13 17:55:14.103: INFO: (4) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.951586ms)
Sep 13 17:55:14.105: INFO: (5) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.702878ms)
Sep 13 17:55:14.108: INFO: (6) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.843282ms)
Sep 13 17:55:14.111: INFO: (7) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.094284ms)
Sep 13 17:55:14.114: INFO: (8) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.761258ms)
Sep 13 17:55:14.117: INFO: (9) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.82035ms)
Sep 13 17:55:14.120: INFO: (10) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.970902ms)
Sep 13 17:55:14.123: INFO: (11) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.131825ms)
Sep 13 17:55:14.126: INFO: (12) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.769875ms)
Sep 13 17:55:14.129: INFO: (13) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.844404ms)
Sep 13 17:55:14.132: INFO: (14) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.815601ms)
Sep 13 17:55:14.135: INFO: (15) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.811101ms)
Sep 13 17:55:14.138: INFO: (16) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.808156ms)
Sep 13 17:55:14.142: INFO: (17) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.94333ms)
Sep 13 17:55:14.145: INFO: (18) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.935516ms)
Sep 13 17:55:14.147: INFO: (19) /api/v1/nodes/k8s-test-001:10250/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.803037ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:55:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3498" for this suite.
Sep 13 17:55:20.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:55:20.218: INFO: namespace proxy-3498 deletion completed in 6.06796625s

â€¢ [SLOW TEST:6.157 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:55:20.218: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:59
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:74
STEP: Creating service test in namespace statefulset-5648
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-5648
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5648
Sep 13 17:55:20.254: INFO: Found 0 stateful pods, waiting for 1
Sep 13 17:55:30.258: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Sep 13 17:55:30.261: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:55:30.449: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:55:30.449: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:55:30.449: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:55:30.453: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Sep 13 17:55:40.456: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:55:40.456: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:55:40.466: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999519s
Sep 13 17:55:41.469: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997108045s
Sep 13 17:55:42.472: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994134889s
Sep 13 17:55:43.475: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991085549s
Sep 13 17:55:44.478: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988043601s
Sep 13 17:55:45.481: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985043193s
Sep 13 17:55:46.484: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981947135s
Sep 13 17:55:47.487: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.979163444s
Sep 13 17:55:48.490: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.97600063s
Sep 13 17:55:49.493: INFO: Verifying statefulset ss doesn't scale past 1 for another 972.753197ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5648
Sep 13 17:55:50.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:55:50.666: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 17:55:50.666: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:55:50.666: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:55:50.669: INFO: Found 1 stateful pods, waiting for 3
Sep 13 17:56:00.672: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:56:00.673: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Sep 13 17:56:00.673: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Sep 13 17:56:00.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:56:00.853: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:56:00.853: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:56:00.853: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:56:00.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:56:01.036: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:56:01.037: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:56:01.037: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:56:01.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Sep 13 17:56:01.220: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Sep 13 17:56:01.220: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Sep 13 17:56:01.220: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Sep 13 17:56:01.220: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:56:01.223: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Sep 13 17:56:11.228: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:56:11.228: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:56:11.228: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Sep 13 17:56:11.237: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999579s
Sep 13 17:56:12.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996950359s
Sep 13 17:56:13.243: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993863648s
Sep 13 17:56:14.247: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990487955s
Sep 13 17:56:15.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.987268655s
Sep 13 17:56:16.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983967842s
Sep 13 17:56:17.257: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980498532s
Sep 13 17:56:18.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977200034s
Sep 13 17:56:19.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.973958774s
Sep 13 17:56:20.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.860752ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5648
Sep 13 17:56:21.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:56:21.439: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 17:56:21.439: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:56:21.439: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:56:21.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:56:21.616: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 17:56:21.616: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:56:21.616: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:56:21.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 exec --namespace=statefulset-5648 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Sep 13 17:56:21.790: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Sep 13 17:56:21.790: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Sep 13 17:56:21.790: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Sep 13 17:56:21.790: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:85
Sep 13 17:56:51.803: INFO: Deleting all statefulset in ns statefulset-5648
Sep 13 17:56:51.806: INFO: Scaling statefulset ss to 0
Sep 13 17:56:51.812: INFO: Waiting for statefulset status.replicas updated to 0
Sep 13 17:56:51.814: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:56:51.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5648" for this suite.
Sep 13 17:56:57.833: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:56:57.894: INFO: namespace statefulset-5648 deletion completed in 6.069454757s

â€¢ [SLOW TEST:97.676 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:56:57.895: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Sep 13 17:57:01.941: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:01.941: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.043: INFO: Exec stderr: ""
Sep 13 17:57:02.044: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.044: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.149: INFO: Exec stderr: ""
Sep 13 17:57:02.149: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.149: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.248: INFO: Exec stderr: ""
Sep 13 17:57:02.248: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.248: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.353: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Sep 13 17:57:02.353: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.353: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.452: INFO: Exec stderr: ""
Sep 13 17:57:02.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.453: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.552: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Sep 13 17:57:02.552: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.552: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.659: INFO: Exec stderr: ""
Sep 13 17:57:02.660: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.660: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.763: INFO: Exec stderr: ""
Sep 13 17:57:02.763: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.763: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.869: INFO: Exec stderr: ""
Sep 13 17:57:02.869: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-475 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 17:57:02.869: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 17:57:02.974: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:57:02.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-475" for this suite.
Sep 13 17:57:46.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:57:47.049: INFO: namespace e2e-kubelet-etc-hosts-475 deletion completed in 44.072040288s

â€¢ [SLOW TEST:49.155 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:57:47.050: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:58:47.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9089" for this suite.
Sep 13 17:59:09.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:59:09.152: INFO: namespace container-probe-9089 deletion completed in 22.068247912s

â€¢ [SLOW TEST:82.103 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:59:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 17:59:09.178: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741" in namespace "downward-api-5198" to be "success or failure"
Sep 13 17:59:09.182: INFO: Pod "downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.100636ms
Sep 13 17:59:11.185: INFO: Pod "downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006618868s
STEP: Saw pod success
Sep 13 17:59:11.185: INFO: Pod "downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:59:11.188: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 17:59:11.201: INFO: Waiting for pod downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 17:59:11.204: INFO: Pod downwardapi-volume-2f5b34b2-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:59:11.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5198" for this suite.
Sep 13 17:59:17.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:59:17.275: INFO: namespace downward-api-5198 deletion completed in 6.068656487s

â€¢ [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:59:17.275: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating projection with secret that has name projected-secret-test-3432b171-d650-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 17:59:17.304: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741" in namespace "projected-6759" to be "success or failure"
Sep 13 17:59:17.307: INFO: Pod "pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798518ms
Sep 13 17:59:19.310: INFO: Pod "pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005960942s
STEP: Saw pod success
Sep 13 17:59:19.310: INFO: Pod "pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 17:59:19.313: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741 container projected-secret-volume-test: <nil>
STEP: delete the pod
Sep 13 17:59:19.325: INFO: Waiting for pod pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 17:59:19.327: INFO: Pod pod-projected-secrets-34331dbb-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:59:19.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6759" for this suite.
Sep 13 17:59:25.337: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:59:25.399: INFO: namespace projected-6759 deletion completed in 6.069273938s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:59:25.399: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 13 17:59:25.421: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:59:29.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3276" for this suite.
Sep 13 17:59:51.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:59:51.605: INFO: namespace init-container-3276 deletion completed in 22.069831782s

â€¢ [SLOW TEST:26.206 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:59:51.605: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: validating api versions
Sep 13 17:59:51.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 api-versions'
Sep 13 17:59:51.701: INFO: stderr: ""
Sep 13 17:59:51.701: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 17:59:51.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2926" for this suite.
Sep 13 17:59:57.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 17:59:57.771: INFO: namespace kubectl-2926 deletion completed in 6.067803079s

â€¢ [SLOW TEST:6.166 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 17:59:57.772: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:00:22.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5567" for this suite.
Sep 13 18:00:28.955: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:00:29.015: INFO: namespace container-runtime-5567 deletion completed in 6.067509176s

â€¢ [SLOW TEST:31.243 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  blackbox test
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:37
    when starting a container that exits
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:00:29.015: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test hostPath mode
Sep 13 18:00:29.042: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7541" to be "success or failure"
Sep 13 18:00:29.045: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.282599ms
Sep 13 18:00:31.048: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006322849s
STEP: Saw pod success
Sep 13 18:00:31.048: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Sep 13 18:00:31.050: INFO: Trying to get logs from node k8s-test-002 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Sep 13 18:00:31.064: INFO: Waiting for pod pod-host-path-test to disappear
Sep 13 18:00:31.066: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:00:31.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7541" for this suite.
Sep 13 18:00:37.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:00:37.137: INFO: namespace hostpath-7541 deletion completed in 6.067850578s

â€¢ [SLOW TEST:8.122 seconds]
[sig-storage] HostPath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:00:37.138: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-3643
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 18:00:37.160: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 13 18:00:59.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.88.1.219:8080/dial?request=hostName&protocol=http&host=10.88.1.218&port=8080&tries=1'] Namespace:pod-network-test-3643 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 18:00:59.214: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 18:00:59.321: INFO: Waiting for endpoints: map[]
Sep 13 18:00:59.324: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.88.1.219:8080/dial?request=hostName&protocol=http&host=10.88.0.76&port=8080&tries=1'] Namespace:pod-network-test-3643 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 18:00:59.324: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 18:00:59.445: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:00:59.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3643" for this suite.
Sep 13 18:01:21.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:01:21.531: INFO: namespace pod-network-test-3643 deletion completed in 22.082683623s

â€¢ [SLOW TEST:44.393 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:01:21.531: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-7e42b721-d650-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:01:21.561: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741" in namespace "projected-5804" to be "success or failure"
Sep 13 18:01:21.564: INFO: Pod "pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830629ms
Sep 13 18:01:23.567: INFO: Pod "pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005910443s
STEP: Saw pod success
Sep 13 18:01:23.567: INFO: Pod "pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:01:23.569: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:01:23.584: INFO: Waiting for pod pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:01:23.586: INFO: Pod pod-projected-configmaps-7e431a39-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:01:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5804" for this suite.
Sep 13 18:01:29.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:01:29.657: INFO: namespace projected-5804 deletion completed in 6.06848487s

â€¢ [SLOW TEST:8.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:01:29.658: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-831ac806-d650-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:01:29.688: INFO: Waiting up to 5m0s for pod "pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741" in namespace "configmap-8350" to be "success or failure"
Sep 13 18:01:29.691: INFO: Pod "pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831712ms
Sep 13 18:01:31.695: INFO: Pod "pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006436271s
Sep 13 18:01:33.698: INFO: Pod "pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009271446s
STEP: Saw pod success
Sep 13 18:01:33.698: INFO: Pod "pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:01:33.700: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:01:33.714: INFO: Waiting for pod pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:01:33.716: INFO: Pod pod-configmaps-831b3e55-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:01:33.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8350" for this suite.
Sep 13 18:01:39.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:01:39.785: INFO: namespace configmap-8350 deletion completed in 6.066763155s

â€¢ [SLOW TEST:10.127 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:01:39.785: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating the pod
Sep 13 18:01:42.332: INFO: Successfully updated pod "labelsupdate89240374-d650-11e9-80a0-429a7732c741"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:01:46.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9251" for this suite.
Sep 13 18:02:08.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:02:08.422: INFO: namespace downward-api-9251 deletion completed in 22.067387449s

â€¢ [SLOW TEST:28.637 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:02:08.422: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating Redis RC
Sep 13 18:02:08.445: INFO: namespace kubectl-8094
Sep 13 18:02:08.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 create -f - --namespace=kubectl-8094'
Sep 13 18:02:08.598: INFO: stderr: ""
Sep 13 18:02:08.598: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Sep 13 18:02:09.601: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 18:02:09.601: INFO: Found 0 / 1
Sep 13 18:02:10.601: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 18:02:10.601: INFO: Found 1 / 1
Sep 13 18:02:10.601: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Sep 13 18:02:10.603: INFO: Selector matched 1 pods for map[app:redis]
Sep 13 18:02:10.603: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Sep 13 18:02:10.603: INFO: wait on redis-master startup in kubectl-8094 
Sep 13 18:02:10.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 logs redis-master-qmq5g redis-master --namespace=kubectl-8094'
Sep 13 18:02:10.689: INFO: stderr: ""
Sep 13 18:02:10.689: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 13 Sep 18:02:09.580 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 13 Sep 18:02:09.580 # Server started, Redis version 3.2.12\n1:M 13 Sep 18:02:09.581 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 13 Sep 18:02:09.581 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Sep 13 18:02:10.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8094'
Sep 13 18:02:10.789: INFO: stderr: ""
Sep 13 18:02:10.789: INFO: stdout: "service/rm2 exposed\n"
Sep 13 18:02:10.793: INFO: Service rm2 in namespace kubectl-8094 found.
STEP: exposing service
Sep 13 18:02:12.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8094'
Sep 13 18:02:12.891: INFO: stderr: ""
Sep 13 18:02:12.891: INFO: stdout: "service/rm3 exposed\n"
Sep 13 18:02:12.895: INFO: Service rm3 in namespace kubectl-8094 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:02:14.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8094" for this suite.
Sep 13 18:02:28.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:02:28.970: INFO: namespace kubectl-8094 deletion completed in 14.067850869s

â€¢ [SLOW TEST:20.548 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create services for rc  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:02:28.971: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap that has name configmap-test-emptyKey-a6752406-d650-11e9-80a0-429a7732c741
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:02:28.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2270" for this suite.
Sep 13 18:02:35.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:02:35.066: INFO: namespace configmap-2270 deletion completed in 6.0692859s

â€¢ [SLOW TEST:6.095 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:02:35.066: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:43
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
Sep 13 18:02:35.090: INFO: PodSpec: initContainers in spec.initContainers
Sep 13 18:03:20.775: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-aa177888-d650-11e9-80a0-429a7732c741", GenerateName:"", Namespace:"init-container-2723", SelfLink:"/api/v1/namespaces/init-container-2723/pods/pod-init-aa177888-d650-11e9-80a0-429a7732c741", UID:"aa17d9ea-d650-11e9-ad0a-02001700bd9c", ResourceVersion:"19285", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63703994555, loc:(*time.Location)(0x89f10e0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"90349827"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7fnfx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0026061c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7fnfx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7fnfx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7fnfx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0025b3ff8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-test-002", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001fc79e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002350080)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0023500a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0023500a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0023500ac)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703994555, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703994555, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703994555, loc:(*time.Location)(0x89f10e0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63703994555, loc:(*time.Location)(0x89f10e0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"100.100.230.24", PodIP:"10.88.1.224", StartTime:(*v1.Time)(0xc002309a60), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d1bea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc001d1bf10)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://c23861f76eadbc85e19e27e077fc50aeb87291cb56d1c5ad20d0a027490601fa"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002309b20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002309ae0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:03:20.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2723" for this suite.
Sep 13 18:03:42.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:03:42.862: INFO: namespace init-container-2723 deletion completed in 22.082240242s

â€¢ [SLOW TEST:67.796 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:03:42.862: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 13 18:03:42.895: INFO: Waiting up to 5m0s for pod "pod-d280f2ac-d650-11e9-80a0-429a7732c741" in namespace "emptydir-6489" to be "success or failure"
Sep 13 18:03:42.898: INFO: Pod "pod-d280f2ac-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260056ms
Sep 13 18:03:44.901: INFO: Pod "pod-d280f2ac-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006344409s
STEP: Saw pod success
Sep 13 18:03:44.901: INFO: Pod "pod-d280f2ac-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:03:44.904: INFO: Trying to get logs from node k8s-test-002 pod pod-d280f2ac-d650-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:03:44.917: INFO: Waiting for pod pod-d280f2ac-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:03:44.920: INFO: Pod pod-d280f2ac-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:03:44.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6489" for this suite.
Sep 13 18:03:50.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:03:50.991: INFO: namespace emptydir-6489 deletion completed in 6.068358558s

â€¢ [SLOW TEST:8.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:03:50.991: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1583
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 18:03:51.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6395'
Sep 13 18:03:51.099: INFO: stderr: ""
Sep 13 18:03:51.099: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1588
Sep 13 18:03:51.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete pods e2e-test-nginx-pod --namespace=kubectl-6395'
Sep 13 18:04:05.074: INFO: stderr: ""
Sep 13 18:04:05.074: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:05.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6395" for this suite.
Sep 13 18:04:11.085: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:11.146: INFO: namespace kubectl-6395 deletion completed in 6.069305538s

â€¢ [SLOW TEST:20.156 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:11.147: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-e35bf79d-d650-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 18:04:11.176: INFO: Waiting up to 5m0s for pod "pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741" in namespace "secrets-4152" to be "success or failure"
Sep 13 18:04:11.179: INFO: Pod "pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.396553ms
Sep 13 18:04:13.182: INFO: Pod "pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005433566s
STEP: Saw pod success
Sep 13 18:04:13.182: INFO: Pod "pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:04:13.184: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 18:04:13.198: INFO: Waiting for pod pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:04:13.200: INFO: Pod pod-secrets-e35c5d92-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4152" for this suite.
Sep 13 18:04:19.210: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:19.271: INFO: namespace secrets-4152 deletion completed in 6.069212753s

â€¢ [SLOW TEST:8.125 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:19.272: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-map-e833a995-d650-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:04:19.301: INFO: Waiting up to 5m0s for pod "pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741" in namespace "configmap-2449" to be "success or failure"
Sep 13 18:04:19.305: INFO: Pod "pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.726192ms
Sep 13 18:04:21.308: INFO: Pod "pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006635104s
STEP: Saw pod success
Sep 13 18:04:21.308: INFO: Pod "pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:04:21.310: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:04:21.323: INFO: Waiting for pod pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:04:21.325: INFO: Pod pod-configmaps-e8341436-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:21.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2449" for this suite.
Sep 13 18:04:27.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:27.396: INFO: namespace configmap-2449 deletion completed in 6.068050149s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:27.396: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override command
Sep 13 18:04:27.422: INFO: Waiting up to 5m0s for pod "client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741" in namespace "containers-5603" to be "success or failure"
Sep 13 18:04:27.425: INFO: Pod "client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.558207ms
Sep 13 18:04:29.428: INFO: Pod "client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005702762s
STEP: Saw pod success
Sep 13 18:04:29.428: INFO: Pod "client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:04:29.430: INFO: Trying to get logs from node k8s-test-002 pod client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:04:29.444: INFO: Waiting for pod client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:04:29.446: INFO: Pod client-containers-ed0b5fd5-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:29.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5603" for this suite.
Sep 13 18:04:35.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:35.522: INFO: namespace containers-5603 deletion completed in 6.073732396s

â€¢ [SLOW TEST:8.126 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:35.522: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: getting the auto-created API token
STEP: reading a file in the container
Sep 13 18:04:38.061: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6111 pod-service-account-f230aa55-d650-11e9-80a0-429a7732c741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Sep 13 18:04:38.240: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6111 pod-service-account-f230aa55-d650-11e9-80a0-429a7732c741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Sep 13 18:04:38.422: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6111 pod-service-account-f230aa55-d650-11e9-80a0-429a7732c741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:38.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6111" for this suite.
Sep 13 18:04:44.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:44.674: INFO: namespace svcaccounts-6111 deletion completed in 6.07131268s

â€¢ [SLOW TEST:9.152 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:22
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:44.674: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 18:04:44.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741" in namespace "downward-api-4990" to be "success or failure"
Sep 13 18:04:44.707: INFO: Pod "downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 6.244634ms
Sep 13 18:04:46.710: INFO: Pod "downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009116477s
STEP: Saw pod success
Sep 13 18:04:46.710: INFO: Pod "downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:04:46.712: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 18:04:46.725: INFO: Waiting for pod downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:04:46.727: INFO: Pod downwardapi-volume-f757c9c7-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:46.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4990" for this suite.
Sep 13 18:04:52.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:04:52.814: INFO: namespace downward-api-4990 deletion completed in 6.084369505s

â€¢ [SLOW TEST:8.140 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:04:52.814: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward api env vars
Sep 13 18:04:52.846: INFO: Waiting up to 5m0s for pod "downward-api-fc328563-d650-11e9-80a0-429a7732c741" in namespace "downward-api-6682" to be "success or failure"
Sep 13 18:04:52.849: INFO: Pod "downward-api-fc328563-d650-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083384ms
Sep 13 18:04:54.852: INFO: Pod "downward-api-fc328563-d650-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006014898s
STEP: Saw pod success
Sep 13 18:04:54.852: INFO: Pod "downward-api-fc328563-d650-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:04:54.854: INFO: Trying to get logs from node k8s-test-002 pod downward-api-fc328563-d650-11e9-80a0-429a7732c741 container dapi-container: <nil>
STEP: delete the pod
Sep 13 18:04:54.868: INFO: Waiting for pod downward-api-fc328563-d650-11e9-80a0-429a7732c741 to disappear
Sep 13 18:04:54.870: INFO: Pod downward-api-fc328563-d650-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:04:54.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6682" for this suite.
Sep 13 18:05:00.880: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:00.940: INFO: namespace downward-api-6682 deletion completed in 6.066525624s

â€¢ [SLOW TEST:8.126 seconds]
[sig-node] Downward API
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:38
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:00.940: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on node default medium
Sep 13 18:05:00.967: INFO: Waiting up to 5m0s for pod "pod-0109d716-d651-11e9-80a0-429a7732c741" in namespace "emptydir-9138" to be "success or failure"
Sep 13 18:05:00.970: INFO: Pod "pod-0109d716-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158346ms
Sep 13 18:05:02.972: INFO: Pod "pod-0109d716-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004907457s
STEP: Saw pod success
Sep 13 18:05:02.972: INFO: Pod "pod-0109d716-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:05:02.975: INFO: Trying to get logs from node k8s-test-002 pod pod-0109d716-d651-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:05:02.988: INFO: Waiting for pod pod-0109d716-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:05:02.990: INFO: Pod pod-0109d716-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:02.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9138" for this suite.
Sep 13 18:05:09.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:09.061: INFO: namespace emptydir-9138 deletion completed in 6.068325777s

â€¢ [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:09.061: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:65
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 18:05:09.084: INFO: Creating deployment "test-recreate-deployment"
Sep 13 18:05:09.087: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Sep 13 18:05:09.092: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Sep 13 18:05:11.097: INFO: Waiting deployment "test-recreate-deployment" to complete
Sep 13 18:05:11.099: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Sep 13 18:05:11.104: INFO: Updating deployment test-recreate-deployment
Sep 13 18:05:11.104: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:59
Sep 13 18:05:11.157: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-3657,SelfLink:/apis/apps/v1/namespaces/deployment-3657/deployments/test-recreate-deployment,UID:05e13964-d651-11e9-ad0a-02001700bd9c,ResourceVersion:19713,Generation:2,CreationTimestamp:2019-09-13 18:05:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-09-13 18:05:11 +0000 UTC 2019-09-13 18:05:11 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-09-13 18:05:11 +0000 UTC 2019-09-13 18:05:09 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-c9cbd8684" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Sep 13 18:05:11.160: INFO: New ReplicaSet "test-recreate-deployment-c9cbd8684" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684,GenerateName:,Namespace:deployment-3657,SelfLink:/apis/apps/v1/namespaces/deployment-3657/replicasets/test-recreate-deployment-c9cbd8684,UID:071955c9-d651-11e9-ad0a-02001700bd9c,ResourceVersion:19711,Generation:1,CreationTimestamp:2019-09-13 18:05:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 05e13964-d651-11e9-ad0a-02001700bd9c 0xc0033e54c0 0xc0033e54c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 18:05:11.160: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Sep 13 18:05:11.160: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-7d57d5ff7c,GenerateName:,Namespace:deployment-3657,SelfLink:/apis/apps/v1/namespaces/deployment-3657/replicasets/test-recreate-deployment-7d57d5ff7c,UID:05e1c09b-d651-11e9-ad0a-02001700bd9c,ResourceVersion:19701,Generation:2,CreationTimestamp:2019-09-13 18:05:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 05e13964-d651-11e9-ad0a-02001700bd9c 0xc0033e53f7 0xc0033e53f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 7d57d5ff7c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Sep 13 18:05:11.162: INFO: Pod "test-recreate-deployment-c9cbd8684-nnk65" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-c9cbd8684-nnk65,GenerateName:test-recreate-deployment-c9cbd8684-,Namespace:deployment-3657,SelfLink:/api/v1/namespaces/deployment-3657/pods/test-recreate-deployment-c9cbd8684-nnk65,UID:0719c719-d651-11e9-ad0a-02001700bd9c,ResourceVersion:19712,Generation:0,CreationTimestamp:2019-09-13 18:05:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: c9cbd8684,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-c9cbd8684 071955c9-d651-11e9-ad0a-02001700bd9c 0xc00307db90 0xc00307db91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-bcp7p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-bcp7p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-bcp7p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00307dc00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00307dc20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:05:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:05:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:05:11 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:05:11 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:,StartTime:2019-09-13 18:05:11 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:11.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3657" for this suite.
Sep 13 18:05:17.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:17.235: INFO: namespace deployment-3657 deletion completed in 6.069917388s

â€¢ [SLOW TEST:8.174 seconds]
[sig-apps] Deployment
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:17.235: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 18:05:17.264: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741" in namespace "downward-api-2172" to be "success or failure"
Sep 13 18:05:17.267: INFO: Pod "downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441718ms
Sep 13 18:05:19.270: INFO: Pod "downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005608555s
STEP: Saw pod success
Sep 13 18:05:19.270: INFO: Pod "downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:05:19.272: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 18:05:19.285: INFO: Waiting for pod downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:05:19.287: INFO: Pod downwardapi-volume-0ac068ad-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:19.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2172" for this suite.
Sep 13 18:05:25.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:25.356: INFO: namespace downward-api-2172 deletion completed in 6.066920957s

â€¢ [SLOW TEST:8.121 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:25.357: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-0f979a87-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:05:25.387: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741" in namespace "projected-4655" to be "success or failure"
Sep 13 18:05:25.389: INFO: Pod "pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.314539ms
Sep 13 18:05:27.392: INFO: Pod "pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005330803s
STEP: Saw pod success
Sep 13 18:05:27.392: INFO: Pod "pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:05:27.394: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:05:27.407: INFO: Waiting for pod pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:05:27.409: INFO: Pod pod-projected-configmaps-0f97f96c-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:27.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4655" for this suite.
Sep 13 18:05:33.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:33.480: INFO: namespace projected-4655 deletion completed in 6.069193988s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:33.481: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 18:05:33.509: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:34.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4224" for this suite.
Sep 13 18:05:40.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:05:40.629: INFO: namespace custom-resource-definition-4224 deletion completed in 6.0785828s

â€¢ [SLOW TEST:7.148 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  Simple CustomResourceDefinition
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:05:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 18:05:40.664: INFO: Create a RollingUpdate DaemonSet
Sep 13 18:05:40.667: INFO: Check that daemon pods launch on every node of the cluster
Sep 13 18:05:40.672: INFO: Number of nodes with available pods: 0
Sep 13 18:05:40.672: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 18:05:41.679: INFO: Number of nodes with available pods: 0
Sep 13 18:05:41.679: INFO: Node k8s-test-001 is running more than one daemon pod
Sep 13 18:05:42.677: INFO: Number of nodes with available pods: 2
Sep 13 18:05:42.678: INFO: Number of running nodes: 2, number of available pods: 2
Sep 13 18:05:42.678: INFO: Update the DaemonSet to trigger a rollout
Sep 13 18:05:42.683: INFO: Updating DaemonSet daemon-set
Sep 13 18:05:46.692: INFO: Roll back the DaemonSet before rollout is complete
Sep 13 18:05:46.699: INFO: Updating DaemonSet daemon-set
Sep 13 18:05:46.699: INFO: Make sure DaemonSet rollback is complete
Sep 13 18:05:46.702: INFO: Wrong image for pod: daemon-set-ms7pp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 13 18:05:46.702: INFO: Pod daemon-set-ms7pp is not available
Sep 13 18:05:47.708: INFO: Wrong image for pod: daemon-set-ms7pp. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Sep 13 18:05:47.708: INFO: Pod daemon-set-ms7pp is not available
Sep 13 18:05:48.708: INFO: Pod daemon-set-d7w5j is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2225, will wait for the garbage collector to delete the pods
Sep 13 18:05:48.772: INFO: Deleting DaemonSet.extensions daemon-set took: 4.288899ms
Sep 13 18:05:49.072: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.205357ms
Sep 13 18:05:55.575: INFO: Number of nodes with available pods: 0
Sep 13 18:05:55.575: INFO: Number of running nodes: 0, number of available pods: 0
Sep 13 18:05:55.577: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2225/daemonsets","resourceVersion":"19947"},"items":null}

Sep 13 18:05:55.579: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2225/pods","resourceVersion":"19947"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:05:55.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2225" for this suite.
Sep 13 18:06:01.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:06:01.656: INFO: namespace daemonsets-2225 deletion completed in 6.067627816s

â€¢ [SLOW TEST:21.027 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:06:01.656: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-secret-fp98
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 18:06:01.688: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-fp98" in namespace "subpath-5486" to be "success or failure"
Sep 13 18:06:01.690: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Pending", Reason="", readiness=false. Elapsed: 2.202768ms
Sep 13 18:06:03.693: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 2.00505841s
Sep 13 18:06:05.696: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 4.008109959s
Sep 13 18:06:07.699: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 6.011041563s
Sep 13 18:06:09.702: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 8.013977338s
Sep 13 18:06:11.705: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 10.016859138s
Sep 13 18:06:13.708: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 12.019681196s
Sep 13 18:06:15.711: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 14.022463532s
Sep 13 18:06:17.713: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 16.025211943s
Sep 13 18:06:19.716: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 18.028116907s
Sep 13 18:06:21.719: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Running", Reason="", readiness=true. Elapsed: 20.030833869s
Sep 13 18:06:23.731: INFO: Pod "pod-subpath-test-secret-fp98": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.043304138s
STEP: Saw pod success
Sep 13 18:06:23.731: INFO: Pod "pod-subpath-test-secret-fp98" satisfied condition "success or failure"
Sep 13 18:06:23.734: INFO: Trying to get logs from node k8s-test-002 pod pod-subpath-test-secret-fp98 container test-container-subpath-secret-fp98: <nil>
STEP: delete the pod
Sep 13 18:06:23.750: INFO: Waiting for pod pod-subpath-test-secret-fp98 to disappear
Sep 13 18:06:23.752: INFO: Pod pod-subpath-test-secret-fp98 no longer exists
STEP: Deleting pod pod-subpath-test-secret-fp98
Sep 13 18:06:23.752: INFO: Deleting pod "pod-subpath-test-secret-fp98" in namespace "subpath-5486"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:06:23.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5486" for this suite.
Sep 13 18:06:29.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:06:29.827: INFO: namespace subpath-5486 deletion completed in 6.070545257s

â€¢ [SLOW TEST:28.171 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:06:29.828: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-map-3605166c-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:06:29.858: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741" in namespace "projected-7838" to be "success or failure"
Sep 13 18:06:29.860: INFO: Pod "pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.193502ms
Sep 13 18:06:31.863: INFO: Pod "pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005056648s
Sep 13 18:06:33.866: INFO: Pod "pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00811428s
STEP: Saw pod success
Sep 13 18:06:33.866: INFO: Pod "pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:06:33.868: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:06:33.880: INFO: Waiting for pod pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:06:33.882: INFO: Pod pod-projected-configmaps-360577f3-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:06:33.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7838" for this suite.
Sep 13 18:06:39.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:06:40.011: INFO: namespace projected-7838 deletion completed in 6.126432794s

â€¢ [SLOW TEST:10.183 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:06:40.011: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Sep 13 18:06:44.567: INFO: Successfully updated pod "pod-update-3c17d518-d651-11e9-80a0-429a7732c741"
STEP: verifying the updated pod is in kubernetes
Sep 13 18:06:44.572: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:06:44.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6795" for this suite.
Sep 13 18:07:06.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:07:06.644: INFO: namespace pods-6795 deletion completed in 22.06963766s

â€¢ [SLOW TEST:26.633 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:07:06.644: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:38
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod pod-subpath-test-configmap-jdrc
STEP: Creating a pod to test atomic-volume-subpath
Sep 13 18:07:06.677: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-jdrc" in namespace "subpath-6624" to be "success or failure"
Sep 13 18:07:06.679: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.664738ms
Sep 13 18:07:08.682: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005545828s
Sep 13 18:07:10.685: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 4.008667419s
Sep 13 18:07:12.688: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 6.011460414s
Sep 13 18:07:14.691: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 8.014555926s
Sep 13 18:07:16.694: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 10.017565868s
Sep 13 18:07:18.697: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 12.020461685s
Sep 13 18:07:20.700: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 14.023644362s
Sep 13 18:07:22.703: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 16.026639557s
Sep 13 18:07:24.706: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 18.029509555s
Sep 13 18:07:26.709: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 20.03267512s
Sep 13 18:07:28.713: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Running", Reason="", readiness=true. Elapsed: 22.035875019s
Sep 13 18:07:30.716: INFO: Pod "pod-subpath-test-configmap-jdrc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.038829216s
STEP: Saw pod success
Sep 13 18:07:30.716: INFO: Pod "pod-subpath-test-configmap-jdrc" satisfied condition "success or failure"
Sep 13 18:07:30.718: INFO: Trying to get logs from node k8s-test-002 pod pod-subpath-test-configmap-jdrc container test-container-subpath-configmap-jdrc: <nil>
STEP: delete the pod
Sep 13 18:07:30.732: INFO: Waiting for pod pod-subpath-test-configmap-jdrc to disappear
Sep 13 18:07:30.734: INFO: Pod pod-subpath-test-configmap-jdrc no longer exists
STEP: Deleting pod pod-subpath-test-configmap-jdrc
Sep 13 18:07:30.734: INFO: Deleting pod "pod-subpath-test-configmap-jdrc" in namespace "subpath-6624"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:07:30.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6624" for this suite.
Sep 13 18:07:36.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:07:36.809: INFO: namespace subpath-6624 deletion completed in 6.070891367s

â€¢ [SLOW TEST:30.165 seconds]
[sig-storage] Subpath
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:34
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:07:36.809: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 18:07:36.839: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741" in namespace "projected-9743" to be "success or failure"
Sep 13 18:07:36.842: INFO: Pod "downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.672341ms
Sep 13 18:07:38.845: INFO: Pod "downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005523095s
Sep 13 18:07:40.848: INFO: Pod "downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00900365s
STEP: Saw pod success
Sep 13 18:07:40.848: INFO: Pod "downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:07:40.851: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 18:07:40.863: INFO: Waiting for pod downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:07:40.866: INFO: Pod downwardapi-volume-5df1fe28-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:07:40.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9743" for this suite.
Sep 13 18:07:46.876: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:07:46.938: INFO: namespace projected-9743 deletion completed in 6.070009371s

â€¢ [SLOW TEST:10.129 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:07:46.939: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test override all
Sep 13 18:07:46.966: INFO: Waiting up to 5m0s for pod "client-containers-63fb37ae-d651-11e9-80a0-429a7732c741" in namespace "containers-9010" to be "success or failure"
Sep 13 18:07:46.968: INFO: Pod "client-containers-63fb37ae-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.352921ms
Sep 13 18:07:48.971: INFO: Pod "client-containers-63fb37ae-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004902727s
STEP: Saw pod success
Sep 13 18:07:48.971: INFO: Pod "client-containers-63fb37ae-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:07:48.973: INFO: Trying to get logs from node k8s-test-002 pod client-containers-63fb37ae-d651-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:07:48.986: INFO: Waiting for pod client-containers-63fb37ae-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:07:48.988: INFO: Pod client-containers-63fb37ae-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:07:48.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9010" for this suite.
Sep 13 18:07:54.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:07:55.061: INFO: namespace containers-9010 deletion completed in 6.070462983s

â€¢ [SLOW TEST:8.122 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:07:55.061: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service endpoint-test2 in namespace services-7984
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7984 to expose endpoints map[]
Sep 13 18:07:55.095: INFO: Get endpoints failed (2.189344ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Sep 13 18:07:56.098: INFO: successfully validated that service endpoint-test2 in namespace services-7984 exposes endpoints map[] (1.004776779s elapsed)
STEP: Creating pod pod1 in namespace services-7984
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7984 to expose endpoints map[pod1:[80]]
Sep 13 18:07:58.117: INFO: successfully validated that service endpoint-test2 in namespace services-7984 exposes endpoints map[pod1:[80]] (2.014879533s elapsed)
STEP: Creating pod pod2 in namespace services-7984
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7984 to expose endpoints map[pod1:[80] pod2:[80]]
Sep 13 18:08:02.159: INFO: Unexpected endpoints: found map[696dc762-d651-11e9-ad0a-02001700bd9c:[80]], expected map[pod1:[80] pod2:[80]] (4.038120136s elapsed, will retry)
Sep 13 18:08:03.166: INFO: successfully validated that service endpoint-test2 in namespace services-7984 exposes endpoints map[pod1:[80] pod2:[80]] (5.044942298s elapsed)
STEP: Deleting pod pod1 in namespace services-7984
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7984 to expose endpoints map[pod2:[80]]
Sep 13 18:08:04.185: INFO: successfully validated that service endpoint-test2 in namespace services-7984 exposes endpoints map[pod2:[80]] (1.015350538s elapsed)
STEP: Deleting pod pod2 in namespace services-7984
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-7984 to expose endpoints map[]
Sep 13 18:08:05.195: INFO: successfully validated that service endpoint-test2 in namespace services-7984 exposes endpoints map[] (1.005774813s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:08:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7984" for this suite.
Sep 13 18:08:27.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:08:27.303: INFO: namespace services-7984 deletion completed in 22.077010535s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:32.242 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:08:27.303: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-map-7c0a59fa-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 18:08:27.334: INFO: Waiting up to 5m0s for pod "pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741" in namespace "secrets-8407" to be "success or failure"
Sep 13 18:08:27.339: INFO: Pod "pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 5.38056ms
Sep 13 18:08:29.342: INFO: Pod "pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008362279s
STEP: Saw pod success
Sep 13 18:08:29.343: INFO: Pod "pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:08:29.345: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 18:08:29.359: INFO: Waiting for pod pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:08:29.362: INFO: Pod pod-secrets-7c0ad217-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:08:29.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8407" for this suite.
Sep 13 18:08:35.373: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:08:35.438: INFO: namespace secrets-8407 deletion completed in 6.072491726s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:08:35.438: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating secret secrets-7191/secret-test-80e3e6f3-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 18:08:35.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741" in namespace "secrets-7191" to be "success or failure"
Sep 13 18:08:35.471: INFO: Pod "pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.31525ms
Sep 13 18:08:37.475: INFO: Pod "pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005438706s
STEP: Saw pod success
Sep 13 18:08:37.475: INFO: Pod "pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:08:37.477: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741 container env-test: <nil>
STEP: delete the pod
Sep 13 18:08:37.494: INFO: Waiting for pod pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:08:37.497: INFO: Pod pod-configmaps-80e45120-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:08:37.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7191" for this suite.
Sep 13 18:08:43.508: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:08:43.570: INFO: namespace secrets-7191 deletion completed in 6.069974293s

â€¢ [SLOW TEST:8.132 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:08:43.571: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-85bdc10a-d651-11e9-80a0-429a7732c741
STEP: Creating secret with name s-test-opt-upd-85bdc22d-d651-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-85bdc10a-d651-11e9-80a0-429a7732c741
STEP: Updating secret s-test-opt-upd-85bdc22d-d651-11e9-80a0-429a7732c741
STEP: Creating secret with name s-test-opt-create-85bdc24a-d651-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:10:09.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7286" for this suite.
Sep 13 18:10:32.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:10:32.067: INFO: namespace projected-7286 deletion completed in 22.07317624s

â€¢ [SLOW TEST:108.496 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:10:32.067: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0913 18:10:33.118904      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 18:10:33.118: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:10:33.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8898" for this suite.
Sep 13 18:10:39.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:10:39.192: INFO: namespace gc-8898 deletion completed in 6.070918559s

â€¢ [SLOW TEST:7.125 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:10:39.192: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 18:10:39.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741" in namespace "projected-3165" to be "success or failure"
Sep 13 18:10:39.223: INFO: Pod "downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880902ms
Sep 13 18:10:41.227: INFO: Pod "downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007142126s
STEP: Saw pod success
Sep 13 18:10:41.227: INFO: Pod "downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:10:41.229: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 18:10:41.243: INFO: Waiting for pod downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:10:41.245: INFO: Pod downwardapi-volume-caa70e91-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:10:41.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3165" for this suite.
Sep 13 18:10:47.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:10:47.320: INFO: namespace projected-3165 deletion completed in 6.07294659s

â€¢ [SLOW TEST:8.128 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:10:47.321: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-cf7f787c-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:10:47.351: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741" in namespace "projected-1252" to be "success or failure"
Sep 13 18:10:47.354: INFO: Pod "pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.572424ms
Sep 13 18:10:49.357: INFO: Pod "pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005412817s
STEP: Saw pod success
Sep 13 18:10:49.357: INFO: Pod "pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:10:49.359: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:10:49.372: INFO: Waiting for pod pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:10:49.374: INFO: Pod pod-projected-configmaps-cf7fe0b3-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:10:49.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1252" for this suite.
Sep 13 18:10:55.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:10:55.449: INFO: namespace projected-1252 deletion completed in 6.072075112s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:10:55.450: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-volume-d457e311-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:10:55.481: INFO: Waiting up to 5m0s for pod "pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741" in namespace "configmap-619" to be "success or failure"
Sep 13 18:10:55.483: INFO: Pod "pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.329146ms
Sep 13 18:10:57.486: INFO: Pod "pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005460256s
STEP: Saw pod success
Sep 13 18:10:57.486: INFO: Pod "pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:10:57.489: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741 container configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:10:57.505: INFO: Waiting for pod pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:10:57.507: INFO: Pod pod-configmaps-d4584665-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:10:57.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-619" for this suite.
Sep 13 18:11:03.517: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:03.583: INFO: namespace configmap-619 deletion completed in 6.073290081s

â€¢ [SLOW TEST:8.134 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:03.583: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name projected-configmap-test-volume-d930cc32-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:11:03.614: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741" in namespace "projected-8059" to be "success or failure"
Sep 13 18:11:03.616: INFO: Pod "pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.301294ms
Sep 13 18:11:05.619: INFO: Pod "pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005127156s
STEP: Saw pod success
Sep 13 18:11:05.619: INFO: Pod "pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:11:05.621: INFO: Trying to get logs from node k8s-test-002 pod pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Sep 13 18:11:05.635: INFO: Waiting for pod pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:11:05.637: INFO: Pod pod-projected-configmaps-d9313393-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:05.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8059" for this suite.
Sep 13 18:11:11.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:11.713: INFO: namespace projected-8059 deletion completed in 6.074230254s

â€¢ [SLOW TEST:8.130 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:11.714: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0777 on tmpfs
Sep 13 18:11:11.742: INFO: Waiting up to 5m0s for pod "pod-de099a29-d651-11e9-80a0-429a7732c741" in namespace "emptydir-1476" to be "success or failure"
Sep 13 18:11:11.745: INFO: Pod "pod-de099a29-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.76726ms
Sep 13 18:11:13.748: INFO: Pod "pod-de099a29-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005741501s
STEP: Saw pod success
Sep 13 18:11:13.748: INFO: Pod "pod-de099a29-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:11:13.750: INFO: Trying to get logs from node k8s-test-002 pod pod-de099a29-d651-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:11:13.768: INFO: Waiting for pod pod-de099a29-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:11:13.771: INFO: Pod pod-de099a29-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:13.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1476" for this suite.
Sep 13 18:11:19.782: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:19.844: INFO: namespace emptydir-1476 deletion completed in 6.0701784s

â€¢ [SLOW TEST:8.131 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:19.844: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
Sep 13 18:11:19.873: INFO: (0) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 4.434443ms)
Sep 13 18:11:19.877: INFO: (1) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.135752ms)
Sep 13 18:11:19.879: INFO: (2) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.808327ms)
Sep 13 18:11:19.882: INFO: (3) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.978858ms)
Sep 13 18:11:19.885: INFO: (4) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.827732ms)
Sep 13 18:11:19.888: INFO: (5) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.977164ms)
Sep 13 18:11:19.891: INFO: (6) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.911992ms)
Sep 13 18:11:19.894: INFO: (7) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.912923ms)
Sep 13 18:11:19.897: INFO: (8) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.086128ms)
Sep 13 18:11:19.900: INFO: (9) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.989938ms)
Sep 13 18:11:19.903: INFO: (10) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.820039ms)
Sep 13 18:11:19.907: INFO: (11) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 3.254735ms)
Sep 13 18:11:19.910: INFO: (12) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.957597ms)
Sep 13 18:11:19.912: INFO: (13) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.860285ms)
Sep 13 18:11:19.915: INFO: (14) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.918884ms)
Sep 13 18:11:19.918: INFO: (15) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.97506ms)
Sep 13 18:11:19.921: INFO: (16) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.781647ms)
Sep 13 18:11:19.924: INFO: (17) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.828444ms)
Sep 13 18:11:19.927: INFO: (18) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.780565ms)
Sep 13 18:11:19.930: INFO: (19) /api/v1/nodes/k8s-test-001/proxy/logs/: <pre>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</a>
<a href="btmp">btmp</a>
<a href... (200; 2.792857ms)
[AfterEach] version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:19.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8917" for this suite.
Sep 13 18:11:25.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:26.002: INFO: namespace proxy-8917 deletion completed in 6.069623908s

â€¢ [SLOW TEST:6.158 seconds]
[sig-network] Proxy
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  version v1
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:26.002: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79
Sep 13 18:11:26.027: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Sep 13 18:11:26.032: INFO: Waiting for terminating namespaces to be deleted...
Sep 13 18:11:26.035: INFO: 
Logging pods the kubelet thinks is on node k8s-test-001 before test
Sep 13 18:11:26.044: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-xk4h6 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 13 18:11:26.044: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 13 18:11:26.044: INFO: coredns-5bc65d7f4b-dfrkk from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container coredns ready: true, restart count 2
Sep 13 18:11:26.044: INFO: kube-controller-manager-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 18:11:26.044: INFO: kube-proxy-zcrzb from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 18:11:26.044: INFO: coredns-5bc65d7f4b-gwg5v from kube-system started at 2019-09-13 16:13:31 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container coredns ready: true, restart count 2
Sep 13 18:11:26.044: INFO: etcd-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 18:11:26.044: INFO: kube-scheduler-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 18:11:26.044: INFO: kube-flannel-ds-b9v5p from kube-system started at 2019-09-13 16:12:59 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container kube-flannel ready: true, restart count 2
Sep 13 18:11:26.044: INFO: kubernetes-dashboard-7646bf6898-rqsn9 from kube-system started at 2019-09-13 16:13:29 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container kubernetes-dashboard ready: true, restart count 1
Sep 13 18:11:26.044: INFO: testdns-rbqhm from default started at 2019-09-13 16:37:29 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.044: INFO: 	Container webserver ready: true, restart count 5
Sep 13 18:11:26.044: INFO: kube-apiserver-k8s-test-001 from kube-system started at <nil> (0 container statuses recorded)
Sep 13 18:11:26.044: INFO: 
Logging pods the kubelet thinks is on node k8s-test-002 before test
Sep 13 18:11:26.049: INFO: kube-proxy-vmlts from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.049: INFO: 	Container kube-proxy ready: true, restart count 1
Sep 13 18:11:26.049: INFO: sonobuoy-e2e-job-04d4c12741174325 from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 18:11:26.049: INFO: 	Container e2e ready: true, restart count 0
Sep 13 18:11:26.049: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Sep 13 18:11:26.050: INFO: kube-flannel-ds-t4cvn from kube-system started at 2019-09-13 16:13:55 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.050: INFO: 	Container kube-flannel ready: true, restart count 1
Sep 13 18:11:26.050: INFO: nginx-65f88748fd-bqq75 from default started at 2019-09-13 16:34:33 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.050: INFO: 	Container nginx ready: true, restart count 0
Sep 13 18:11:26.050: INFO: sonobuoy from sonobuoy started at 2019-09-13 16:48:31 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.050: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Sep 13 18:11:26.050: INFO: sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-6vgpm from sonobuoy started at 2019-09-13 16:48:40 +0000 UTC (2 container statuses recorded)
Sep 13 18:11:26.050: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Sep 13 18:11:26.050: INFO: 	Container systemd-logs ready: true, restart count 1
Sep 13 18:11:26.050: INFO: testdns-4r5d2 from default started at 2019-09-13 16:34:53 +0000 UTC (1 container statuses recorded)
Sep 13 18:11:26.050: INFO: 	Container webserver ready: true, restart count 5
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: verifying the node has the label node k8s-test-001
STEP: verifying the node has the label node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod nginx-65f88748fd-bqq75 requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod testdns-4r5d2 requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod testdns-rbqhm requesting resource cpu=0m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod coredns-5bc65d7f4b-dfrkk requesting resource cpu=100m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod coredns-5bc65d7f4b-gwg5v requesting resource cpu=100m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod etcd-k8s-test-001 requesting resource cpu=0m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kube-apiserver-k8s-test-001 requesting resource cpu=250m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kube-controller-manager-k8s-test-001 requesting resource cpu=200m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kube-flannel-ds-b9v5p requesting resource cpu=100m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kube-flannel-ds-t4cvn requesting resource cpu=100m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod kube-proxy-vmlts requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod kube-proxy-zcrzb requesting resource cpu=0m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kube-scheduler-k8s-test-001 requesting resource cpu=100m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod kubernetes-dashboard-7646bf6898-rqsn9 requesting resource cpu=0m on Node k8s-test-001
Sep 13 18:11:26.074: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod sonobuoy-e2e-job-04d4c12741174325 requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-6vgpm requesting resource cpu=0m on Node k8s-test-002
Sep 13 18:11:26.074: INFO: Pod sonobuoy-systemd-logs-daemon-set-27cc238ed1764cb4-xk4h6 requesting resource cpu=0m on Node k8s-test-001
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e69535fa-d651-11e9-80a0-429a7732c741.15c411ee74424362], Reason = [Scheduled], Message = [Successfully assigned sched-pred-862/filler-pod-e69535fa-d651-11e9-80a0-429a7732c741 to k8s-test-001]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e69535fa-d651-11e9-80a0-429a7732c741.15c411eea7b30774], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e69535fa-d651-11e9-80a0-429a7732c741.15c411eea9e71256], Reason = [Created], Message = [Created container filler-pod-e69535fa-d651-11e9-80a0-429a7732c741]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e69535fa-d651-11e9-80a0-429a7732c741.15c411eeb72dc55a], Reason = [Started], Message = [Started container filler-pod-e69535fa-d651-11e9-80a0-429a7732c741]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e695eb05-d651-11e9-80a0-429a7732c741.15c411ee74b7be24], Reason = [Scheduled], Message = [Successfully assigned sched-pred-862/filler-pod-e695eb05-d651-11e9-80a0-429a7732c741 to k8s-test-002]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e695eb05-d651-11e9-80a0-429a7732c741.15c411eea6811573], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e695eb05-d651-11e9-80a0-429a7732c741.15c411eea8e693ee], Reason = [Created], Message = [Created container filler-pod-e695eb05-d651-11e9-80a0-429a7732c741]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e695eb05-d651-11e9-80a0-429a7732c741.15c411eeb4d1235d], Reason = [Started], Message = [Started container filler-pod-e695eb05-d651-11e9-80a0-429a7732c741]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15c411eeecf5d975], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node k8s-test-001
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-test-002
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:29.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-862" for this suite.
Sep 13 18:11:35.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:35.217: INFO: namespace sched-pred-862 deletion completed in 6.077321319s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70

â€¢ [SLOW TEST:9.215 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:35.217: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:135
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Sep 13 18:11:35.243: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Sep 13 18:11:42.267: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:42.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2233" for this suite.
Sep 13 18:11:48.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:48.340: INFO: namespace pods-2233 deletion completed in 6.068871652s

â€¢ [SLOW TEST:13.123 seconds]
[k8s.io] Pods
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:48.341: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name secret-test-f3de1f01-d651-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume secrets
Sep 13 18:11:48.369: INFO: Waiting up to 5m0s for pod "pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741" in namespace "secrets-800" to be "success or failure"
Sep 13 18:11:48.372: INFO: Pod "pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.309299ms
Sep 13 18:11:50.375: INFO: Pod "pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005370273s
STEP: Saw pod success
Sep 13 18:11:50.375: INFO: Pod "pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:11:50.377: INFO: Trying to get logs from node k8s-test-002 pod pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741 container secret-volume-test: <nil>
STEP: delete the pod
Sep 13 18:11:50.390: INFO: Waiting for pod pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:11:50.392: INFO: Pod pod-secrets-f3de8159-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:50.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-800" for this suite.
Sep 13 18:11:56.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:11:56.464: INFO: namespace secrets-800 deletion completed in 6.069227141s

â€¢ [SLOW TEST:8.124 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:11:56.464: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test use defaults
Sep 13 18:11:56.490: INFO: Waiting up to 5m0s for pod "client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741" in namespace "containers-5646" to be "success or failure"
Sep 13 18:11:56.493: INFO: Pod "client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848322ms
Sep 13 18:11:58.496: INFO: Pod "client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005673382s
STEP: Saw pod success
Sep 13 18:11:58.496: INFO: Pod "client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:11:58.498: INFO: Trying to get logs from node k8s-test-002 pod client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:11:58.511: INFO: Waiting for pod client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:11:58.513: INFO: Pod client-containers-f8b5b5df-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:11:58.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5646" for this suite.
Sep 13 18:12:04.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:12:04.585: INFO: namespace containers-5646 deletion completed in 6.068876364s

â€¢ [SLOW TEST:8.121 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:12:04.585: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test emptydir 0644 on node default medium
Sep 13 18:12:04.612: INFO: Waiting up to 5m0s for pod "pod-fd8ce978-d651-11e9-80a0-429a7732c741" in namespace "emptydir-6834" to be "success or failure"
Sep 13 18:12:04.614: INFO: Pod "pod-fd8ce978-d651-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.566402ms
Sep 13 18:12:06.617: INFO: Pod "pod-fd8ce978-d651-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005521788s
STEP: Saw pod success
Sep 13 18:12:06.617: INFO: Pod "pod-fd8ce978-d651-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:12:06.620: INFO: Trying to get logs from node k8s-test-002 pod pod-fd8ce978-d651-11e9-80a0-429a7732c741 container test-container: <nil>
STEP: delete the pod
Sep 13 18:12:06.633: INFO: Waiting for pod pod-fd8ce978-d651-11e9-80a0-429a7732c741 to disappear
Sep 13 18:12:06.634: INFO: Pod pod-fd8ce978-d651-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:12:06.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6834" for this suite.
Sep 13 18:12:12.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:12:12.706: INFO: namespace emptydir-6834 deletion completed in 6.068648234s

â€¢ [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:12:12.706: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:213
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1619
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: running the image docker.io/library/nginx:1.14-alpine
Sep 13 18:12:12.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-876'
Sep 13 18:12:12.927: INFO: stderr: ""
Sep 13 18:12:12.927: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Sep 13 18:12:17.977: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 get pod e2e-test-nginx-pod --namespace=kubectl-876 -o json'
Sep 13 18:12:18.052: INFO: stderr: ""
Sep 13 18:12:18.052: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-09-13T18:12:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-876\",\n        \"resourceVersion\": \"21139\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-876/pods/e2e-test-nginx-pod\",\n        \"uid\": \"028032ad-d652-11e9-ad0a-02001700bd9c\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-zmmjt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-test-002\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-zmmjt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-zmmjt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-13T18:12:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-13T18:12:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-13T18:12:14Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-09-13T18:12:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://d8cfa12ba657fa0afbe33d6870b0fff347cdc153fe3a73e7a63352c950e6e3a6\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-09-13T18:12:13Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"100.100.230.24\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.88.1.12\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-09-13T18:12:12Z\"\n    }\n}\n"
STEP: replace the image in the pod
Sep 13 18:12:18.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 replace -f - --namespace=kubectl-876'
Sep 13 18:12:18.215: INFO: stderr: ""
Sep 13 18:12:18.215: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1624
Sep 13 18:12:18.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-009922021 delete pods e2e-test-nginx-pod --namespace=kubectl-876'
Sep 13 18:12:19.814: INFO: stderr: ""
Sep 13 18:12:19.814: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:12:19.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-876" for this suite.
Sep 13 18:12:25.824: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:12:25.886: INFO: namespace kubectl-876 deletion completed in 6.070032555s

â€¢ [SLOW TEST:13.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:12:25.887: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Sep 13 18:12:29.923: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-0a3f38a2-d652-11e9-80a0-429a7732c741,GenerateName:,Namespace:events-393,SelfLink:/api/v1/namespaces/events-393/pods/send-events-0a3f38a2-d652-11e9-80a0-429a7732c741,UID:0a3fa073-d652-11e9-ad0a-02001700bd9c,ResourceVersion:21190,Generation:0,CreationTimestamp:2019-09-13 18:12:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 908859702,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-sjppb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-sjppb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-sjppb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-test-002,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025b31f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025b3210}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:12:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:12:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:12:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-09-13 18:12:25 +0000 UTC  }],Message:,Reason:,HostIP:100.100.230.24,PodIP:10.88.1.13,StartTime:2019-09-13 18:12:25 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-09-13 18:12:26 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://d4327ed50ac66250d04fe7a178683aca9d17ce6ee6b29816aede6fc5af52dedd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Sep 13 18:12:31.926: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Sep 13 18:12:33.929: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:12:33.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-393" for this suite.
Sep 13 18:13:11.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:13:12.007: INFO: namespace events-393 deletion completed in 38.071595456s

â€¢ [SLOW TEST:46.121 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:13:12.008: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:13:14.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6579" for this suite.
Sep 13 18:13:52.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:13:52.121: INFO: namespace kubelet-test-6579 deletion completed in 38.067741043s

â€¢ [SLOW TEST:40.113 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:13:52.121: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0913 18:14:32.166331      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Sep 13 18:14:32.166: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:14:32.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2284" for this suite.
Sep 13 18:14:38.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:14:38.238: INFO: namespace gc-2284 deletion completed in 6.069841458s

â€¢ [SLOW TEST:46.117 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:14:38.238: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap configmap-9186/configmap-test-59227f81-d652-11e9-80a0-429a7732c741
STEP: Creating a pod to test consume configMaps
Sep 13 18:14:38.269: INFO: Waiting up to 5m0s for pod "pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741" in namespace "configmap-9186" to be "success or failure"
Sep 13 18:14:38.272: INFO: Pod "pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 3.420697ms
Sep 13 18:14:40.275: INFO: Pod "pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006386814s
STEP: Saw pod success
Sep 13 18:14:40.275: INFO: Pod "pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:14:40.278: INFO: Trying to get logs from node k8s-test-002 pod pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741 container env-test: <nil>
STEP: delete the pod
Sep 13 18:14:40.292: INFO: Waiting for pod pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741 to disappear
Sep 13 18:14:40.294: INFO: Pod pod-configmaps-5922f154-d652-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:14:40.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9186" for this suite.
Sep 13 18:14:46.304: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:14:46.364: INFO: namespace configmap-9186 deletion completed in 6.067088141s

â€¢ [SLOW TEST:8.126 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:14:46.364: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:61
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Sep 13 18:14:50.415: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:14:50.417: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:14:52.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:14:52.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:14:54.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:14:54.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:14:56.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:14:56.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:14:58.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:14:58.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:00.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:00.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:02.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:02.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:04.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:04.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:06.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:06.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:08.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:08.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:10.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:10.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:12.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:12.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:14.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:14.420: INFO: Pod pod-with-prestop-exec-hook still exists
Sep 13 18:15:16.417: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Sep 13 18:15:16.420: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:15:16.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9026" for this suite.
Sep 13 18:15:38.437: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:15:38.500: INFO: namespace container-lifecycle-hook-9026 deletion completed in 22.071038433s

â€¢ [SLOW TEST:52.136 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  when create a pod with lifecycle hook
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:40
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:15:38.500: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating configMap with name configmap-test-upd-7d0e5ad8-d652-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-7d0e5ad8-d652-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:16:46.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2941" for this suite.
Sep 13 18:17:08.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:17:08.870: INFO: namespace configmap-2941 deletion completed in 22.069134967s

â€¢ [SLOW TEST:90.370 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:17:08.871: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:86
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: creating service multi-endpoint-test in namespace services-1668
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1668 to expose endpoints map[]
Sep 13 18:17:08.902: INFO: Get endpoints failed (2.439685ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Sep 13 18:17:09.905: INFO: successfully validated that service multi-endpoint-test in namespace services-1668 exposes endpoints map[] (1.005075338s elapsed)
STEP: Creating pod pod1 in namespace services-1668
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1668 to expose endpoints map[pod1:[100]]
Sep 13 18:17:11.925: INFO: successfully validated that service multi-endpoint-test in namespace services-1668 exposes endpoints map[pod1:[100]] (2.015690013s elapsed)
STEP: Creating pod pod2 in namespace services-1668
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1668 to expose endpoints map[pod1:[100] pod2:[101]]
Sep 13 18:17:13.952: INFO: successfully validated that service multi-endpoint-test in namespace services-1668 exposes endpoints map[pod1:[100] pod2:[101]] (2.023601749s elapsed)
STEP: Deleting pod pod1 in namespace services-1668
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1668 to expose endpoints map[pod2:[101]]
Sep 13 18:17:13.965: INFO: successfully validated that service multi-endpoint-test in namespace services-1668 exposes endpoints map[pod2:[101]] (9.048242ms elapsed)
STEP: Deleting pod pod2 in namespace services-1668
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1668 to expose endpoints map[]
Sep 13 18:17:14.976: INFO: successfully validated that service multi-endpoint-test in namespace services-1668 exposes endpoints map[] (1.006534029s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:17:14.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1668" for this suite.
Sep 13 18:17:21.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:17:21.066: INFO: namespace services-1668 deletion completed in 6.068992634s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91

â€¢ [SLOW TEST:12.195 seconds]
[sig-network] Services
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:17:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating pod liveness-http in namespace container-probe-1235
Sep 13 18:17:23.100: INFO: Started pod liveness-http in namespace container-probe-1235
STEP: checking the pod's current state and verifying that restartCount is present
Sep 13 18:17:23.102: INFO: Initial restart count of pod liveness-http is 0
Sep 13 18:17:33.119: INFO: Restart count of pod container-probe-1235/liveness-http is now 1 (10.016791062s elapsed)
Sep 13 18:17:55.156: INFO: Restart count of pod container-probe-1235/liveness-http is now 2 (32.05417606s elapsed)
Sep 13 18:18:15.185: INFO: Restart count of pod container-probe-1235/liveness-http is now 3 (52.083044714s elapsed)
Sep 13 18:18:35.214: INFO: Restart count of pod container-probe-1235/liveness-http is now 4 (1m12.111739413s elapsed)
Sep 13 18:19:47.320: INFO: Restart count of pod container-probe-1235/liveness-http is now 5 (2m24.218161838s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:19:47.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1235" for this suite.
Sep 13 18:19:53.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:19:53.397: INFO: namespace container-probe-1235 deletion completed in 6.067624874s

â€¢ [SLOW TEST:152.331 seconds]
[k8s.io] Probing container
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:687
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:19:53.397: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating a pod to test downward API volume plugin
Sep 13 18:19:53.425: INFO: Waiting up to 5m0s for pod "downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741" in namespace "downward-api-1900" to be "success or failure"
Sep 13 18:19:53.427: INFO: Pod "downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741": Phase="Pending", Reason="", readiness=false. Elapsed: 2.441205ms
Sep 13 18:19:55.430: INFO: Pod "downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005297869s
STEP: Saw pod success
Sep 13 18:19:55.430: INFO: Pod "downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741" satisfied condition "success or failure"
Sep 13 18:19:55.432: INFO: Trying to get logs from node k8s-test-002 pod downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741 container client-container: <nil>
STEP: delete the pod
Sep 13 18:19:55.446: INFO: Waiting for pod downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741 to disappear
Sep 13 18:19:55.448: INFO: Pod downwardapi-volume-14fc0eeb-d653-11e9-80a0-429a7732c741 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:19:55.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1900" for this suite.
Sep 13 18:20:01.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:20:01.527: INFO: namespace downward-api-1900 deletion completed in 6.076426806s

â€¢ [SLOW TEST:8.129 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:20:01.527: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Performing setup for networking test in namespace pod-network-test-7705
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Sep 13 18:20:01.549: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Sep 13 18:20:19.599: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.88.1.28:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7705 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 18:20:19.599: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 18:20:19.708: INFO: Found all expected endpoints: [netserver-0]
Sep 13 18:20:19.711: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.88.0.88:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7705 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Sep 13 18:20:19.711: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
Sep 13 18:20:19.820: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:20:19.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7705" for this suite.
Sep 13 18:20:41.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:20:41.893: INFO: namespace pod-network-test-7705 deletion completed in 22.070170265s

â€¢ [SLOW TEST:40.366 seconds]
[sig-network] Networking
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:149
STEP: Creating a kubernetes client
Sep 13 18:20:41.893: INFO: >>> kubeConfig: /tmp/kubeconfig-009922021
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
STEP: Creating secret with name s-test-opt-del-31e420ad-d653-11e9-80a0-429a7732c741
STEP: Creating secret with name s-test-opt-upd-31e42100-d653-11e9-80a0-429a7732c741
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-31e420ad-d653-11e9-80a0-429a7732c741
STEP: Updating secret s-test-opt-upd-31e42100-d653-11e9-80a0-429a7732c741
STEP: Creating secret with name s-test-opt-create-31e42121-d653-11e9-80a0-429a7732c741
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
Sep 13 18:20:45.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9156" for this suite.
Sep 13 18:21:07.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Sep 13 18:21:08.062: INFO: namespace secrets-9156 deletion completed in 22.071590944s

â€¢ [SLOW TEST:26.169 seconds]
[sig-storage] Secrets
/workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.14.0-rc.1.5+641856db183520/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
------------------------------
SSSSSSSep 13 18:21:08.062: INFO: Running AfterSuite actions on all nodes
Sep 13 18:21:08.062: INFO: Running AfterSuite actions on node 1
Sep 13 18:21:08.062: INFO: Skipping dumping logs from cluster

Ran 204 of 3584 Specs in 5505.719 seconds
SUCCESS! -- 204 Passed | 0 Failed | 0 Pending | 3380 Skipped PASS

Ginkgo ran 1 suite in 1h31m47.133566984s
Test Suite Passed
